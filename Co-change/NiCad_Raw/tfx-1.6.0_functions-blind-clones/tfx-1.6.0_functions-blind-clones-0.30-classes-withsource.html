<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; tfx-1.6.0</td>
<td><b>Clone pairs:</b> &nbsp; 568</td>
<td><b>Clone classes:</b> &nbsp; 146</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 3-2</td>
<td style="background-color:white">Granularity: &nbsp; functions-blind</td>
<td style="background-color:white">Max diff threshold: &nbsp; 30%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions-blind: &nbsp; 3207</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 4 fragments, nominal size 15 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag69')" href="javascript:;">
tfx-1.6.0/tfx/components/infra_validator/executor_test.py: 214-228
</a>
<div class="mid" id="frag69" style="display:none"><pre>
  def testValidateOnce_LoadOnly_FailIfRunnerWaitRaises(self):
    infra_validator = executor.Executor(self._context)
    with mock.patch.object(self._serving_binary, 'MakeClient'):
      with mock.patch.object(
          executor, '_create_model_server_runner') as mock_runner_factory:
        mock_runner = mock_runner_factory.return_value
        mock_runner.WaitUntilRunning.side_effect = ValueError
        with self.assertRaises(ValueError):
          infra_validator._ValidateOnce(
              model_path=self._model_path,
              serving_binary=self._serving_binary,
              serving_spec=self._serving_spec,
              validation_spec=self._validation_spec,
              requests=[])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag70')" href="javascript:;">
tfx-1.6.0/tfx/components/infra_validator/executor_test.py: 229-245
</a>
<div class="mid" id="frag70" style="display:none"><pre>
  def testValidateOnce_LoadOnly_FailIfClientWaitRaises(self):
    infra_validator = executor.Executor(self._context)
    with mock.patch.object(self._serving_binary,
                           'MakeClient') as mock_client_factory:
      mock_client = mock_client_factory.return_value
      with mock.patch.object(
          executor, '_create_model_server_runner') as mock_runner_factory:
        mock_client.WaitUntilModelLoaded.side_effect = ValueError
        with self.assertRaises(ValueError):
          infra_validator._ValidateOnce(
              model_path=self._model_path,
              serving_binary=self._serving_binary,
              serving_spec=self._serving_spec,
              validation_spec=self._validation_spec,
              requests=[])
        mock_runner_factory.return_value.WaitUntilRunning.assert_called()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag72')" href="javascript:;">
tfx-1.6.0/tfx/components/infra_validator/executor_test.py: 263-280
</a>
<div class="mid" id="frag72" style="display:none"><pre>
  def testValidateOnce_LoadAndQuery_FailIfSendRequestsRaises(self):
    infra_validator = executor.Executor(self._context)
    with mock.patch.object(self._serving_binary,
                           'MakeClient') as mock_client_factory:
      mock_client = mock_client_factory.return_value
      with mock.patch.object(
          executor, '_create_model_server_runner') as mock_runner_factory:
        mock_client.SendRequests.side_effect = ValueError
        with self.assertRaises(ValueError):
          infra_validator._ValidateOnce(
              model_path=self._model_path,
              serving_binary=self._serving_binary,
              serving_spec=self._serving_spec,
              validation_spec=self._validation_spec,
              requests=['my_request'])
        mock_runner_factory.return_value.WaitUntilRunning.assert_called()
        mock_client.WaitUntilModelLoaded.assert_called()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag71')" href="javascript:;">
tfx-1.6.0/tfx/components/infra_validator/executor_test.py: 246-262
</a>
<div class="mid" id="frag71" style="display:none"><pre>
  def testValidateOnce_LoadAndQuery_Succeed(self):
    infra_validator = executor.Executor(self._context)
    with mock.patch.object(self._serving_binary,
                           'MakeClient') as mock_client_factory:
      mock_client = mock_client_factory.return_value
      with mock.patch.object(
          executor, '_create_model_server_runner') as mock_runner_factory:
        infra_validator._ValidateOnce(
            model_path=self._model_path,
            serving_binary=self._serving_binary,
            serving_spec=self._serving_spec,
            validation_spec=self._validation_spec,
            requests=['my_request'])
        mock_runner_factory.return_value.WaitUntilRunning.assert_called()
        mock_client.WaitUntilModelLoaded.assert_called()
        mock_client.SendRequests.assert_called()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag88')" href="javascript:;">
tfx-1.6.0/tfx/components/infra_validator/model_server_clients/tensorflow_serving_client_test.py: 54-71
</a>
<div class="mid" id="frag88" style="display:none"><pre>
  def testGetModelState_ReturnsReady_IfAllAvailable(self):
    # Prepare stub and client.
    self.model_stub.GetModelStatus.return_value = _make_response({
        'model_version_status': [
            {'state': 'AVAILABLE'},
            {'state': 'AVAILABLE'},
            {'state': 'AVAILABLE'}
        ]
    })
    client = tensorflow_serving_client.TensorFlowServingClient(
        'localhost:1234', 'a_model_name')

    # Call.
    result = client._GetServingStatus()

    # Check result.
    self.assertEqual(result, types.ModelServingStatus.READY)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag90')" href="javascript:;">
tfx-1.6.0/tfx/components/infra_validator/model_server_clients/tensorflow_serving_client_test.py: 90-107
</a>
<div class="mid" id="frag90" style="display:none"><pre>
  def testGetModelState_ReturnsUnavailable_IfAnyStateEnded(self):
    # Prepare stub and client.
    self.model_stub.GetModelStatus.return_value = _make_response({
        'model_version_status': [
            {'state': 'AVAILABLE'},
            {'state': 'AVAILABLE'},
            {'state': 'END'}
        ]
    })
    client = tensorflow_serving_client.TensorFlowServingClient(
        'localhost:1234', 'a_model_name')

    # Call.
    result = client._GetServingStatus()

    # Check result.
    self.assertEqual(result, types.ModelServingStatus.UNAVAILABLE)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag89')" href="javascript:;">
tfx-1.6.0/tfx/components/infra_validator/model_server_clients/tensorflow_serving_client_test.py: 72-89
</a>
<div class="mid" id="frag89" style="display:none"><pre>
  def testGetModelState_ReturnsNotReady_IfAnyStateNotAvailable(self):
    # Prepare stub and client.
    self.model_stub.GetModelStatus.return_value = _make_response({
        'model_version_status': [
            {'state': 'AVAILABLE'},
            {'state': 'AVAILABLE'},
            {'state': 'LOADING'}
        ]
    })
    client = tensorflow_serving_client.TensorFlowServingClient(
        'localhost:1234', 'a_model_name')

    # Call.
    result = client._GetServingStatus()

    # Check result.
    self.assertEqual(result, types.ModelServingStatus.NOT_READY)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag98')" href="javascript:;">
tfx-1.6.0/tfx/components/example_validator/component_test.py: 26-43
</a>
<div class="mid" id="frag98" style="display:none"><pre>
  def testConstruct(self):
    statistics_artifact = standard_artifacts.ExampleStatistics()
    statistics_artifact.split_names = artifact_utils.encode_split_names(
        ['train', 'eval'])
    exclude_splits = ['eval']
    example_validator = component.ExampleValidator(
        statistics=channel_utils.as_channel([statistics_artifact]),
        schema=channel_utils.as_channel([standard_artifacts.Schema()]),
        exclude_splits=exclude_splits)
    self.assertEqual(
        standard_artifacts.ExampleAnomalies.TYPE_NAME,
        example_validator.outputs[
            standard_component_specs.ANOMALIES_KEY].type_name)
    self.assertEqual(
        example_validator.spec.exec_properties[
            standard_component_specs.EXCLUDE_SPLITS_KEY], '["eval"]')


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag440')" href="javascript:;">
tfx-1.6.0/tfx/components/schema_gen/component_test.py: 27-43
</a>
<div class="mid" id="frag440" style="display:none"><pre>
  def testConstruct(self):
    statistics_artifact = standard_artifacts.ExampleStatistics()
    statistics_artifact.split_names = artifact_utils.encode_split_names(
        ['train', 'eval'])
    exclude_splits = ['eval']
    schema_gen = component.SchemaGen(
        statistics=channel_utils.as_channel([statistics_artifact]),
        exclude_splits=exclude_splits)
    self.assertEqual(
        standard_artifacts.Schema.TYPE_NAME,
        schema_gen.outputs[standard_component_specs.SCHEMA_KEY].type_name)
    self.assertTrue(schema_gen.spec.exec_properties[
        standard_component_specs.INFER_FEATURE_SHAPE_KEY])
    self.assertEqual(
        schema_gen.spec.exec_properties[
            standard_component_specs.EXCLUDE_SPLITS_KEY], '["eval"]')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag100')" href="javascript:;">
tfx-1.6.0/tfx/components/example_validator/component.py: 65-87
</a>
<div class="mid" id="frag100" style="display:none"><pre>
  def __init__(self,
               statistics: types.BaseChannel,
               schema: types.BaseChannel,
               exclude_splits: Optional[List[str]] = None):
    """Construct an ExampleValidator component.

    Args:
      statistics: A BaseChannel of type `standard_artifacts.ExampleStatistics`.
      schema: A BaseChannel of type `standard_artifacts.Schema`. _required_
      exclude_splits: Names of splits that the example validator should not
        validate. Default behavior (when exclude_splits is set to None) is
        excluding no splits.
    """
    if exclude_splits is None:
      exclude_splits = []
      logging.info('Excluding no splits because exclude_splits is not set.')
    anomalies = types.Channel(type=standard_artifacts.ExampleAnomalies)
    spec = standard_component_specs.ExampleValidatorSpec(
        statistics=statistics,
        schema=schema,
        exclude_splits=json_utils.dumps(exclude_splits),
        anomalies=anomalies)
    super().__init__(spec=spec)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag319')" href="javascript:;">
tfx-1.6.0/tfx/components/statistics_gen/component.py: 53-88
</a>
<div class="mid" id="frag319" style="display:none"><pre>
  def __init__(self,
               examples: types.BaseChannel,
               schema: Optional[types.BaseChannel] = None,
               stats_options: Optional[tfdv.StatsOptions] = None,
               exclude_splits: Optional[List[str]] = None):
    """Construct a StatisticsGen component.

    Args:
      examples: A BaseChannel of `ExamplesPath` type, likely generated by the
        [ExampleGen component](https://www.tensorflow.org/tfx/guide/examplegen).
          This needs to contain two splits labeled `train` and `eval`.
          _required_
      schema: A `Schema` channel to use for automatically configuring the value
        of stats options passed to TFDV.
      stats_options: The StatsOptions instance to configure optional TFDV
        behavior. When stats_options.schema is set, it will be used instead of
        the `schema` channel input. Due to the requirement that stats_options be
        serialized, the slicer functions and custom stats generators are dropped
        and are therefore not usable.
      exclude_splits: Names of splits where statistics and sample should not be
        generated. Default behavior (when exclude_splits is set to None) is
        excluding no splits.
    """
    if exclude_splits is None:
      exclude_splits = []
      logging.info('Excluding no splits because exclude_splits is not set.')
    statistics = types.Channel(type=standard_artifacts.ExampleStatistics)
    # TODO(b/150802589): Move jsonable interface to tfx_bsl and use json_utils.
    stats_options_json = stats_options.to_json() if stats_options else None
    spec = standard_component_specs.StatisticsGenSpec(
        examples=examples,
        schema=schema,
        stats_options_json=stats_options_json,
        exclude_splits=json_utils.dumps(exclude_splits),
        statistics=statistics)
    super().__init__(spec=spec)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 5:</b> &nbsp; 2 fragments, nominal size 27 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag123')" href="javascript:;">
tfx-1.6.0/tfx/components/bulk_inferrer/component.py: 58-101
</a>
<div class="mid" id="frag123" style="display:none"><pre>
  def __init__(
      self,
      examples: types.BaseChannel,
      model: Optional[types.BaseChannel] = None,
      model_blessing: Optional[types.BaseChannel] = None,
      data_spec: Optional[Union[bulk_inferrer_pb2.DataSpec,
                                data_types.RuntimeParameter]] = None,
      model_spec: Optional[Union[bulk_inferrer_pb2.ModelSpec,
                                 data_types.RuntimeParameter]] = None,
      output_example_spec: Optional[Union[bulk_inferrer_pb2.OutputExampleSpec,
                                          data_types.RuntimeParameter]] = None):
    """Construct an BulkInferrer component.

    Args:
      examples: A BaseChannel of type `standard_artifacts.Examples`, usually
        produced by an ExampleGen component. _required_
      model: A BaseChannel of type `standard_artifacts.Model`, usually produced
        by a Trainer component.
      model_blessing: A BaseChannel of type `standard_artifacts.ModelBlessing`,
        usually produced by a ModelValidator component.
      data_spec: bulk_inferrer_pb2.DataSpec instance that describes data
        selection.
      model_spec: bulk_inferrer_pb2.ModelSpec instance that describes model
        specification.
      output_example_spec: bulk_inferrer_pb2.OutputExampleSpec instance, specify
        if you want BulkInferrer to output examples instead of inference result.
    """
    if output_example_spec:
      output_examples = types.Channel(type=standard_artifacts.Examples)
      inference_result = None
    else:
      inference_result = types.Channel(type=standard_artifacts.InferenceResult)
      output_examples = None

    spec = standard_component_specs.BulkInferrerSpec(
        examples=examples,
        model=model,
        model_blessing=model_blessing,
        data_spec=data_spec or bulk_inferrer_pb2.DataSpec(),
        model_spec=model_spec or bulk_inferrer_pb2.ModelSpec(),
        output_example_spec=output_example_spec,
        inference_result=inference_result,
        output_examples=output_examples)
    super().__init__(spec=spec)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag796')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/bulk_inferrer/component.py: 81-130
</a>
<div class="mid" id="frag796" style="display:none"><pre>
  def __init__(
      self,
      examples: types.Channel,
      model: Optional[types.Channel] = None,
      model_blessing: Optional[types.Channel] = None,
      data_spec: Optional[Union[bulk_inferrer_pb2.DataSpec,
                                data_types.RuntimeParameter]] = None,
      output_example_spec: Optional[Union[bulk_inferrer_pb2.OutputExampleSpec,
                                          data_types.RuntimeParameter]] = None,
      custom_config: Optional[Dict[str, Any]] = None):
    """Construct an BulkInferrer component.

    Args:
      examples: A Channel of type `standard_artifacts.Examples`, usually
        produced by an ExampleGen component. _required_
      model: A Channel of type `standard_artifacts.Model`, usually produced by
        a Trainer component.
      model_blessing: A Channel of type `standard_artifacts.ModelBlessing`,
        usually produced by a ModelValidator component.
      data_spec: bulk_inferrer_pb2.DataSpec instance that describes data
        selection.
      output_example_spec: bulk_inferrer_pb2.OutputExampleSpec instance, specify
        if you want BulkInferrer to output examples instead of inference result.
      custom_config: A dict which contains the deployment job parameters to be
        passed to Google Cloud AI Platform.
        custom_config.ai_platform_serving_args need to contain the serving job
        parameters. For the full set of parameters, refer to
        https://cloud.google.com/ml-engine/reference/rest/v1/projects.models

    Raises:
      ValueError: Must not specify inference_result or output_examples depends
        on whether output_example_spec is set or not.
    """
    if output_example_spec:
      output_examples = types.Channel(type=standard_artifacts.Examples)
      inference_result = None
    else:
      inference_result = types.Channel(type=standard_artifacts.InferenceResult)
      output_examples = None

    spec = CloudAIBulkInferrerComponentSpec(
        examples=examples,
        model=model,
        model_blessing=model_blessing,
        data_spec=data_spec or bulk_inferrer_pb2.DataSpec(),
        output_example_spec=output_example_spec,
        custom_config=json_utils.dumps(custom_config),
        inference_result=inference_result,
        output_examples=output_examples)
    super().__init__(spec=spec)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 6:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag137')" href="javascript:;">
tfx-1.6.0/tfx/components/tuner/executor_test.py: 101-113
</a>
<div class="mid" id="frag137" style="display:none"><pre>
  def testDoWithModuleFile(self):
    self._exec_properties[
        standard_component_specs.MODULE_FILE_KEY] = os.path.join(
            self._testdata_dir, 'module_file', 'tuner_module.py')

    tuner = executor.Executor(self._context)
    tuner.Do(
        input_dict=self._input_dict,
        output_dict=self._output_dict,
        exec_properties=self._exec_properties)

    self._verify_output()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag141')" href="javascript:;">
tfx-1.6.0/tfx/components/tuner/executor_test.py: 171-186
</a>
<div class="mid" id="frag141" style="display:none"><pre>
  def testMultipleArtifacts(self):
    self._input_dict[
        standard_component_specs.EXAMPLES_KEY] = self._multiple_artifacts
    self._exec_properties[
        standard_component_specs.MODULE_FILE_KEY] = os.path.join(
            self._testdata_dir, 'module_file', 'tuner_module.py')

    tuner = executor.Executor(self._context)
    tuner.Do(
        input_dict=self._input_dict,
        output_dict=self._output_dict,
        exec_properties=self._exec_properties)

    self._verify_output()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag139')" href="javascript:;">
tfx-1.6.0/tfx/components/tuner/executor_test.py: 126-137
</a>
<div class="mid" id="frag139" style="display:none"><pre>
  def testTuneArgs(self):
    with self.assertRaises(ValueError):
      self._exec_properties[
          standard_component_specs.TUNE_ARGS_KEY] = proto_utils.proto_to_json(
              tuner_pb2.TuneArgs(num_parallel_trials=3))

      tuner = executor.Executor(self._context)
      tuner.Do(
          input_dict=self._input_dict,
          output_dict=self._output_dict,
          exec_properties=self._exec_properties)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 7:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag151')" href="javascript:;">
tfx-1.6.0/tfx/components/model_validator/executor_test.py: 61-78
</a>
<div class="mid" id="frag151" style="display:none"><pre>
  def testDoWithBlessedModel(self):
    # Create exe properties.
    exec_properties = {
        'blessed_model': os.path.join(self._source_data_dir, 'trainer/blessed'),
        'blessed_model_id': 123,
        'current_component_id': self.component_id,
    }

    # Run executor.
    model_validator = executor.Executor(self._context)
    model_validator.Do(self._input_dict, self._output_dict, exec_properties)

    # Check model validator outputs.
    self.assertTrue(fileio.exists(os.path.join(self._tmp_dir)))
    self.assertTrue(
        fileio.exists(
            os.path.join(self._blessing.uri, constants.BLESSED_FILE_NAME)))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag152')" href="javascript:;">
tfx-1.6.0/tfx/components/model_validator/executor_test.py: 79-97
</a>
<div class="mid" id="frag152" style="display:none"><pre>
  def testDoWithoutBlessedModel(self):
    # Create exe properties.
    exec_properties = {
        'blessed_model': None,
        'blessed_model_id': None,
        'current_component_id': self.component_id,
    }

    # Run executor.
    model_validator = executor.Executor(self._context)
    model_validator.Do(self._input_dict, self._output_dict, exec_properties)

    # Check model validator outputs.
    self.assertTrue(fileio.exists(os.path.join(self._tmp_dir)))
    self.assertTrue(
        fileio.exists(
            os.path.join(self._blessing.uri, constants.BLESSED_FILE_NAME)))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 8:</b> &nbsp; 7 fragments, nominal size 11 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag159')" href="javascript:;">
tfx-1.6.0/tfx/components/evaluator/component_test.py: 43-53
</a>
<div class="mid" id="frag159" style="display:none"><pre>
  def testConstructWithBaselineModel(self):
    examples = standard_artifacts.Examples()
    model_exports = standard_artifacts.Model()
    baseline_model = standard_artifacts.Model()
    evaluator = component.Evaluator(
        examples=channel_utils.as_channel([examples]),
        model=channel_utils.as_channel([model_exports]),
        baseline_model=channel_utils.as_channel([baseline_model]))
    self.assertEqual(standard_artifacts.ModelEvaluation.TYPE_NAME,
                     evaluator.outputs['evaluation'].type_name)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag164')" href="javascript:;">
tfx-1.6.0/tfx/components/evaluator/component_test.py: 111-122
</a>
<div class="mid" id="frag164" style="display:none"><pre>
  def testConstructWithModuleFile(self):
    examples = standard_artifacts.Examples()
    model_exports = standard_artifacts.Model()
    evaluator = component.Evaluator(
        examples=channel_utils.as_channel([examples]),
        model=channel_utils.as_channel([model_exports]),
        example_splits=['eval'],
        module_file='path')
    self.assertEqual(standard_artifacts.ModelEvaluation.TYPE_NAME,
                     evaluator.outputs['evaluation'].type_name)
    self.assertEqual('path', evaluator.exec_properties['module_file'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag163')" href="javascript:;">
tfx-1.6.0/tfx/components/evaluator/component_test.py: 98-110
</a>
<div class="mid" id="frag163" style="display:none"><pre>
  def testConstructWithEvalConfig(self):
    examples = standard_artifacts.Examples()
    model_exports = standard_artifacts.Model()
    schema = standard_artifacts.Schema()
    evaluator = component.Evaluator(
        examples=channel_utils.as_channel([examples]),
        model=channel_utils.as_channel([model_exports]),
        eval_config=tfma.EvalConfig(
            slicing_specs=[tfma.SlicingSpec(feature_keys=['trip_start_hour'])]),
        schema=channel_utils.as_channel([schema]),)
    self.assertEqual(standard_artifacts.ModelEvaluation.TYPE_NAME,
                     evaluator.outputs['evaluation'].type_name)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag165')" href="javascript:;">
tfx-1.6.0/tfx/components/evaluator/component_test.py: 123-134
</a>
<div class="mid" id="frag165" style="display:none"><pre>
  def testConstructWithModuleFn(self):
    examples = standard_artifacts.Examples()
    model_exports = standard_artifacts.Model()
    evaluator = component.Evaluator(
        examples=channel_utils.as_channel([examples]),
        model=channel_utils.as_channel([model_exports]),
        example_splits=['eval'],
        module_path='module')
    self.assertEqual(standard_artifacts.ModelEvaluation.TYPE_NAME,
                     evaluator.outputs['evaluation'].type_name)
    self.assertEqual('module', evaluator.exec_properties['module_path'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag160')" href="javascript:;">
tfx-1.6.0/tfx/components/evaluator/component_test.py: 54-66
</a>
<div class="mid" id="frag160" style="display:none"><pre>
  def testConstructWithSliceSpec(self):
    examples = standard_artifacts.Examples()
    model_exports = standard_artifacts.Model()
    evaluator = component.Evaluator(
        examples=channel_utils.as_channel([examples]),
        model=channel_utils.as_channel([model_exports]),
        feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[
            evaluator_pb2.SingleSlicingSpec(
                column_for_slicing=['trip_start_hour'])
        ]))
    self.assertEqual(standard_artifacts.ModelEvaluation.TYPE_NAME,
                     evaluator.outputs['evaluation'].type_name)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag166')" href="javascript:;">
tfx-1.6.0/tfx/components/evaluator/component_test.py: 135-147
</a>
<div class="mid" id="frag166" style="display:none"><pre>
  def testConstructDuplicateUserModule(self):
    examples = standard_artifacts.Examples()
    model_exports = standard_artifacts.Model()

    with self.assertRaises(ValueError):
      _ = component.Evaluator(
          examples=channel_utils.as_channel([examples]),
          model=channel_utils.as_channel([model_exports]),
          example_splits=['eval'],
          module_file='module_file_path',
          module_path='python.path.module')


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag161')" href="javascript:;">
tfx-1.6.0/tfx/components/evaluator/component_test.py: 67-82
</a>
<div class="mid" id="frag161" style="display:none"><pre>
  def testConstructWithFairnessThresholds(self):
    examples = standard_artifacts.Examples()
    model_exports = standard_artifacts.Model()
    evaluator = component.Evaluator(
        examples=channel_utils.as_channel([examples]),
        model=channel_utils.as_channel([model_exports]),
        feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[
            evaluator_pb2.SingleSlicingSpec(
                column_for_slicing=['trip_start_hour'])
        ]),
        fairness_indicator_thresholds=[0.1, 0.3, 0.5, 0.9])
    self.assertEqual(standard_artifacts.ModelEvaluation.TYPE_NAME,
                     evaluator.outputs['evaluation'].type_name)
    self.assertEqual('[0.1, 0.3, 0.5, 0.9]',
                     evaluator.exec_properties['fairness_indicator_thresholds'])

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 9:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag172')" href="javascript:;">
tfx-1.6.0/tfx/components/experimental/data_view/binder_component.py: 70-81
</a>
<div class="mid" id="frag172" style="display:none"><pre>
  def __init__(self,
               input_examples: types.BaseChannel,
               data_view: types.BaseChannel,
               output_examples: Optional[types.Channel] = None):
    if not output_examples:
      output_examples = types.Channel(type=standard_artifacts.Examples)

    spec = _DataViewBinderComponentSpec(
        input_examples=input_examples,
        data_view=data_view,
        output_examples=output_examples)
    super().__init__(spec=spec)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag174')" href="javascript:;">
tfx-1.6.0/tfx/components/experimental/data_view/provider_component.py: 61-88
</a>
<div class="mid" id="frag174" style="display:none"><pre>
  def __init__(self,
               create_decoder_func: str,
               module_file: Optional[str] = None,
               data_view: Optional[types.Channel] = None):
    """Construct a StatisticsGen component.

    Args:
      create_decoder_func: If `module_file` is not None, this should be the name
        of the function in `module_file` that this component need to use to
        create the TfGraphRecordDecoder. Otherwise it should be the path
        (dot-delimited, e.g. "some_package.some_module.some_func") to such
        a function. The function must have the following signature:

        def create_decoder_func() -&gt; tfx_bsl.coder.TfGraphRecordDecoder:
          ...
      module_file: The file path to a python module file, from which the
        function named after `create_decoder_func` will be loaded. If not
        provided, `create_decoder_func` is expected to be a path to a function.
      data_view: Output 'DataView' channel, in which a the decoder will be
        saved.
    """
    if data_view is None:
      data_view = types.Channel(type=standard_artifacts.DataView)
    spec = _TfGraphDataViewProviderSpec(
        module_file=module_file,
        create_decoder_func=create_decoder_func,
        data_view=data_view)
    super().__init__(spec=spec)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 10:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag178')" href="javascript:;">
tfx-1.6.0/tfx/components/experimental/data_view/provider_executor_test.py: 39-56
</a>
<div class="mid" id="frag178" style="display:none"><pre>
  def testExecutorModuleFileProvided(self):
    input_dict = {}
    output = standard_artifacts.DataView()
    output.uri = os.path.join(self._output_data_dir, 'output_data_view')
    output_dict = {'data_view': [output]}
    exec_properties = {
        'module_file':
            os.path.join(self._source_data_dir,
                         'module_file/data_view_module.py'),
        'create_decoder_func':
            'create_simple_decoder',
    }
    executor = provider_executor.TfGraphDataViewProviderExecutor()
    executor.Do(input_dict, output_dict, exec_properties)
    loaded_decoder = tf_graph_record_decoder.load_decoder(output.uri)
    self.assertIsInstance(
        loaded_decoder, tf_graph_record_decoder.LoadedDecoder)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag179')" href="javascript:;">
tfx-1.6.0/tfx/components/experimental/data_view/provider_executor_test.py: 57-74
</a>
<div class="mid" id="frag179" style="display:none"><pre>
  def testExecutorModuleFileNotProvided(self):
    input_dict = {}
    output = standard_artifacts.DataView()
    output.uri = os.path.join(self._output_data_dir, 'output_data_view')
    output_dict = {'data_view': [output]}
    exec_properties = {
        'module_file': None,
        'create_decoder_func':
            '%s.%s' % (data_view_module.create_simple_decoder.__module__,
                       data_view_module.create_simple_decoder.__name__),
    }
    executor = provider_executor.TfGraphDataViewProviderExecutor()
    executor.Do(input_dict, output_dict, exec_properties)
    loaded_decoder = tf_graph_record_decoder.load_decoder(output.uri)
    self.assertIsInstance(
        loaded_decoder, tf_graph_record_decoder.LoadedDecoder)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 11:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag186')" href="javascript:;">
tfx-1.6.0/tfx/components/pusher/component_test.py: 33-43
</a>
<div class="mid" id="frag186" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._model = channel_utils.as_channel([standard_artifacts.Model()])
    self._model_blessing = channel_utils.as_channel(
        [standard_artifacts.ModelBlessing()])
    self._infra_blessing = channel_utils.as_channel(
        [standard_artifacts.InfraBlessing()])
    self._push_destination = pusher_pb2.PushDestination(
        filesystem=pusher_pb2.PushDestination.Filesystem(
            base_directory=self.get_temp_dir()))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag409')" href="javascript:;">
tfx-1.6.0/tfx/components/trainer/component_test.py: 29-40
</a>
<div class="mid" id="frag409" style="display:none"><pre>
  def setUp(self):
    super().setUp()

    self.examples = channel_utils.as_channel([standard_artifacts.Examples()])
    self.transform_graph = channel_utils.as_channel(
        [standard_artifacts.TransformGraph()])
    self.schema = channel_utils.as_channel([standard_artifacts.Schema()])
    self.hyperparameters = channel_utils.as_channel(
        [standard_artifacts.HyperParameters()])
    self.train_args = trainer_pb2.TrainArgs(splits=['train'], num_steps=100)
    self.eval_args = trainer_pb2.EvalArgs(splits=['eval'], num_steps=50)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 12:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag188')" href="javascript:;">
tfx-1.6.0/tfx/components/pusher/component_test.py: 53-64
</a>
<div class="mid" id="frag188" style="display:none"><pre>
  def testConstructWithParameter(self):
    push_dir = data_types.RuntimeParameter(name='push-dir', ptype=str)
    pusher = component.Pusher(
        model=self._model,
        model_blessing=self._model_blessing,
        push_destination={'filesystem': {
            'base_directory': push_dir
        }})
    self.assertEqual(
        standard_artifacts.PushedModel.TYPE_NAME,
        pusher.outputs[standard_component_specs.PUSHED_MODEL_KEY].type_name)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag190')" href="javascript:;">
tfx-1.6.0/tfx/components/pusher/component_test.py: 72-82
</a>
<div class="mid" id="frag190" style="display:none"><pre>
  def testConstructNoDestinationCustomExecutor(self):
    pusher = component.Pusher(
        model=self._model,
        model_blessing=self._model_blessing,
        custom_executor_spec=executor_spec.ExecutorClassSpec(
            self._MyCustomPusherExecutor),
    )
    self.assertEqual(
        standard_artifacts.PushedModel.TYPE_NAME,
        pusher.outputs[standard_component_specs.PUSHED_MODEL_KEY].type_name)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 13:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag203')" href="javascript:;">
tfx-1.6.0/tfx/components/pusher/executor_test.py: 144-159
</a>
<div class="mid" id="frag203" style="display:none"><pre>
  def testDo_NoModelBlessing_InfraBlessed_Pushed(self):
    # Prepare successful InfraBlessing only (without ModelBlessing).
    infra_blessing = standard_artifacts.InfraBlessing()
    infra_blessing.set_int_custom_property('blessed', 1)  # Blessed.
    input_dict = {
        standard_component_specs.MODEL_KEY:
            self._input_dict[standard_component_specs.MODEL_KEY],
        standard_component_specs.INFRA_BLESSING_KEY: [infra_blessing],
    }

    # Run executor
    self._executor.Do(input_dict, self._output_dict, self._exec_properties)

    # Check model is pushed.
    self.assertPushed()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag204')" href="javascript:;">
tfx-1.6.0/tfx/components/pusher/executor_test.py: 160-175
</a>
<div class="mid" id="frag204" style="display:none"><pre>
  def testDo_NoModelBlessing_InfraNotBlessed_NotPushed(self):
    # Prepare unsuccessful InfraBlessing only (without ModelBlessing).
    infra_blessing = standard_artifacts.InfraBlessing()
    infra_blessing.set_int_custom_property('blessed', 0)  # Not blessed.
    input_dict = {
        standard_component_specs.MODEL_KEY:
            self._input_dict[standard_component_specs.MODEL_KEY],
        standard_component_specs.INFRA_BLESSING_KEY: [infra_blessing],
    }

    # Run executor
    self._executor.Do(input_dict, self._output_dict, self._exec_properties)

    # Check model is not pushed.
    self.assertNotPushed()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 14:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag236')" href="javascript:;">
tfx-1.6.0/tfx/components/util/tfxio_utils_test.py: 148-163
</a>
<div class="mid" id="frag236" style="display:none"><pre>
  def decode_record(self, record):
    indices = tf.transpose(
        tf.stack([
            tf.range(tf.size(record), dtype=tf.int64),
            tf.zeros(tf.size(record), dtype=tf.int64)
        ]))

    return {
        'sparse_tensor':
            tf.SparseTensor(
                values=record,
                indices=indices,
                dense_shape=[tf.size(record), 1])
    }


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag322')" href="javascript:;">
tfx-1.6.0/tfx/components/testdata/module_file/data_view_module.py: 24-39
</a>
<div class="mid" id="frag322" style="display:none"><pre>
  def decode_record(self, record: tf.Tensor) -&gt; Dict[str, Any]:
    indices = tf.transpose(
        tf.stack([
            tf.range(tf.size(record), dtype=tf.int64),
            tf.zeros(tf.size(record), dtype=tf.int64)
        ]))

    return {
        "sparse_tensor":
            tf.SparseTensor(
                values=record,
                indices=indices,
                dense_shape=[tf.size(record), 1])
    }


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 15:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag247')" href="javascript:;">
tfx-1.6.0/tfx/components/util/tfxio_utils.py: 140-175
</a>
<div class="mid" id="frag247" style="display:none"><pre>
def get_tf_dataset_factory_from_artifact(
    examples: List[artifact.Artifact],
    telemetry_descriptors: List[str],
) -&gt; Callable[[
    List[str],
    dataset_options.TensorFlowDatasetOptions,
    Optional[schema_pb2.Schema],
], tf.data.Dataset]:
  """Returns a factory function that creates a tf.data.Dataset.

  Args:
    examples: The Examples artifacts that the TFXIO from which the Dataset is
      created from is intended to access.
    telemetry_descriptors: A set of descriptors that identify the component
      that is instantiating the TFXIO. These will be used to construct the
      namespace to contain metrics for profiling and are therefore expected to
      be identifiers of the component itself and not individual instances of
      source use.
  """
  payload_format, data_view_uri = resolve_payload_format_and_data_view_uri(
      examples)

  def dataset_factory(file_pattern: List[str],
                      options: dataset_options.TensorFlowDatasetOptions,
                      schema: Optional[schema_pb2.Schema]) -&gt; tf.data.Dataset:
    return make_tfxio(
        file_pattern=file_pattern,
        telemetry_descriptors=telemetry_descriptors,
        payload_format=payload_format,
        data_view_uri=data_view_uri,
        schema=schema).TensorFlowDataset(
            options)

  return dataset_factory


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag249')" href="javascript:;">
tfx-1.6.0/tfx/components/util/tfxio_utils.py: 176-210
</a>
<div class="mid" id="frag249" style="display:none"><pre>
def get_record_batch_factory_from_artifact(
    examples: List[artifact.Artifact],
    telemetry_descriptors: List[str],
) -&gt; Callable[[
    List[str],
    dataset_options.RecordBatchesOptions,
    Optional[schema_pb2.Schema],
], Iterator[pa.RecordBatch]]:
  """Returns a factory function that creates Iterator[pa.RecordBatch].

  Args:
    examples: The Examples artifacts that the TFXIO from which the Dataset is
      created from is intended to access.
    telemetry_descriptors: A set of descriptors that identify the component that
      is instantiating the TFXIO. These will be used to construct the namespace
      to contain metrics for profiling and are therefore expected to be
      identifiers of the component itself and not individual instances of source
      use.
  """
  payload_format, data_view_uri = resolve_payload_format_and_data_view_uri(
      examples)

  def record_batch_factory(
      file_pattern: List[str], options: dataset_options.RecordBatchesOptions,
      schema: Optional[schema_pb2.Schema]) -&gt; Iterator[pa.RecordBatch]:
    return make_tfxio(
        file_pattern=file_pattern,
        telemetry_descriptors=telemetry_descriptors,
        payload_format=payload_format,
        data_view_uri=data_view_uri,
        schema=schema).RecordBatches(options)

  return record_batch_factory


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 16:</b> &nbsp; 9 fragments, nominal size 12 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag257')" href="javascript:;">
tfx-1.6.0/tfx/components/transform/component_test.py: 85-96
</a>
<div class="mid" id="frag257" style="display:none"><pre>
  def test_construct_from_module_file(self):
    module_file = '/path/to/preprocessing.py'
    transform = component.Transform(
        examples=self.examples,
        schema=self.schema,
        module_file=module_file,
    )
    self._verify_outputs(transform)
    self.assertEqual(
        module_file,
        transform.exec_properties[standard_component_specs.MODULE_FILE_KEY])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag266')" href="javascript:;">
tfx-1.6.0/tfx/components/transform/component_test.py: 189-203
</a>
<div class="mid" id="frag266" style="display:none"><pre>
  def test_construct_with_splits_config(self):
    splits_config = transform_pb2.SplitsConfig(
        analyze=['train'], transform=['eval'])
    module_file = '/path/to/preprocessing.py'
    transform = component.Transform(
        examples=self.examples,
        schema=self.schema,
        module_file=module_file,
        splits_config=splits_config,
    )
    self._verify_outputs(transform)
    self.assertEqual(
        proto_utils.proto_to_json(splits_config),
        transform.exec_properties[standard_component_specs.SPLITS_CONFIG_KEY])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag268')" href="javascript:;">
tfx-1.6.0/tfx/components/transform/component_test.py: 214-226
</a>
<div class="mid" id="frag268" style="display:none"><pre>
  def test_construct_with_force_tf_compat_v1_override(self):
    transform = component.Transform(
        examples=self.examples,
        schema=self.schema,
        preprocessing_fn='my_preprocessing_fn',
        force_tf_compat_v1=True,
    )
    self._verify_outputs(transform)
    self.assertEqual(
        True,
        bool(transform.spec.exec_properties[
            standard_component_specs.FORCE_TF_COMPAT_V1_KEY]))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag415')" href="javascript:;">
tfx-1.6.0/tfx/components/trainer/component_test.py: 109-120
</a>
<div class="mid" id="frag415" style="display:none"><pre>
  def testConstructWithoutTransformOutput(self):
    module_file = '/path/to/module/file'
    trainer = component.Trainer(
        module_file=module_file,
        examples=self.examples,
        train_args=self.train_args,
        eval_args=self.eval_args)
    self._verify_outputs(trainer)
    self.assertEqual(
        module_file,
        trainer.spec.exec_properties[standard_component_specs.MODULE_FILE_KEY])

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag269')" href="javascript:;">
tfx-1.6.0/tfx/components/transform/component_test.py: 227-240
</a>
<div class="mid" id="frag269" style="display:none"><pre>
  def test_construct_with_stats_disabled(self):
    transform = component.Transform(
        examples=self.examples,
        schema=self.schema,
        preprocessing_fn='my_preprocessing_fn',
        disable_statistics=True,
    )
    self._verify_outputs(transform, disable_statistics=True)
    self.assertEqual(
        True,
        bool(transform.spec.exec_properties[
            standard_component_specs.DISABLE_STATISTICS_KEY]))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag258')" href="javascript:;">
tfx-1.6.0/tfx/components/transform/component_test.py: 97-109
</a>
<div class="mid" id="frag258" style="display:none"><pre>
  def test_construct_with_parameter(self):
    module_file = data_types.RuntimeParameter(name='module-file', ptype=str)
    transform = component.Transform(
        examples=self.examples,
        schema=self.schema,
        module_file=module_file,
    )
    self._verify_outputs(transform)
    self.assertJsonEqual(
        str(module_file),
        str(transform.exec_properties[
            standard_component_specs.MODULE_FILE_KEY]))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag414')" href="javascript:;">
tfx-1.6.0/tfx/components/trainer/component_test.py: 94-108
</a>
<div class="mid" id="frag414" style="display:none"><pre>
  def testConstructFromRunFn(self):
    run_fn = 'path.to.my_run_fn'
    trainer = component.Trainer(
        run_fn=run_fn,
        custom_executor_spec=executor_spec.ExecutorClassSpec(
            executor.GenericExecutor),
        examples=self.examples,
        transform_graph=self.transform_graph,
        train_args=self.train_args,
        eval_args=self.eval_args)
    self._verify_outputs(trainer)
    self.assertEqual(
        run_fn,
        trainer.spec.exec_properties[standard_component_specs.RUN_FN_KEY])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag413')" href="javascript:;">
tfx-1.6.0/tfx/components/trainer/component_test.py: 81-93
</a>
<div class="mid" id="frag413" style="display:none"><pre>
  def testConstructFromTrainerFn(self):
    trainer_fn = 'path.to.my_trainer_fn'
    trainer = component.Trainer(
        trainer_fn=trainer_fn,
        examples=self.examples,
        transform_graph=self.transform_graph,
        train_args=self.train_args,
        eval_args=self.eval_args)
    self._verify_outputs(trainer)
    self.assertEqual(
        trainer_fn,
        trainer.spec.exec_properties[standard_component_specs.TRAINER_FN_KEY])

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag411')" href="javascript:;">
tfx-1.6.0/tfx/components/trainer/component_test.py: 49-64
</a>
<div class="mid" id="frag411" style="display:none"><pre>
  def testConstructFromModuleFile(self):
    module_file = '/path/to/module/file'
    trainer = component.Trainer(
        module_file=module_file,
        examples=self.examples,
        transform_graph=self.transform_graph,
        schema=self.schema,
        custom_config={'test': 10})
    self._verify_outputs(trainer)
    self.assertEqual(
        module_file,
        trainer.spec.exec_properties[standard_component_specs.MODULE_FILE_KEY])
    self.assertEqual(
        '{"test": 10}', trainer.spec.exec_properties[
            standard_component_specs.CUSTOM_CONFIG_KEY])

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 17:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag259')" href="javascript:;">
tfx-1.6.0/tfx/components/transform/component_test.py: 110-123
</a>
<div class="mid" id="frag259" style="display:none"><pre>
  def test_construct_from_preprocessing_fn(self):
    preprocessing_fn = 'path.to.my_preprocessing_fn'
    transform = component.Transform(
        examples=self.examples,
        schema=self.schema,
        preprocessing_fn=preprocessing_fn,
    )
    self._verify_outputs(transform)
    self.assertEqual(
        preprocessing_fn, transform.exec_properties[
            standard_component_specs.PREPROCESSING_FN_KEY])
    self.assertIsNone(transform.exec_properties[
        standard_component_specs.STATS_OPTIONS_UPDATER_FN_KEY])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag260')" href="javascript:;">
tfx-1.6.0/tfx/components/transform/component_test.py: 124-139
</a>
<div class="mid" id="frag260" style="display:none"><pre>
  def test_construct_from_preprocessing_fn_with_stats_options_updater_fn(self):
    preprocessing_fn = 'path.to.my_preprocessing_fn'
    stats_options_updater_fn = 'path.to.my.stats_options_updater_fn'
    transform = component.Transform(
        examples=self.examples,
        schema=self.schema,
        preprocessing_fn=preprocessing_fn,
        stats_options_updater_fn=stats_options_updater_fn)
    self._verify_outputs(transform)
    self.assertEqual(
        preprocessing_fn, transform.exec_properties[
            standard_component_specs.PREPROCESSING_FN_KEY])
    self.assertEqual(
        stats_options_updater_fn, transform.exec_properties[
            standard_component_specs.STATS_OPTIONS_UPDATER_FN_KEY])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 18:</b> &nbsp; 3 fragments, nominal size 31 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag316')" href="javascript:;">
tfx-1.6.0/tfx/components/statistics_gen/executor_test.py: 47-94
</a>
<div class="mid" id="frag316" style="display:none"><pre>
  def testDo(self):
    source_data_dir = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), 'testdata')
    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    fileio.makedirs(output_data_dir)

    # Create input dict.
    examples = standard_artifacts.Examples()
    examples.uri = os.path.join(source_data_dir, 'csv_example_gen')
    examples.split_names = artifact_utils.encode_split_names(
        ['train', 'eval', 'test'])

    input_dict = {
        standard_component_specs.EXAMPLES_KEY: [examples],
    }

    exec_properties = {
        # List needs to be serialized before being passed into Do function.
        standard_component_specs.EXCLUDE_SPLITS_KEY:
            json_utils.dumps(['test']),
    }

    # Create output dict.
    stats = standard_artifacts.ExampleStatistics()
    stats.uri = output_data_dir
    output_dict = {
        standard_component_specs.STATISTICS_KEY: [stats],
    }

    # Run executor.
    stats_gen_executor = executor.Executor()
    stats_gen_executor.Do(input_dict, output_dict, exec_properties)

    self.assertEqual(
        artifact_utils.encode_split_names(['train', 'eval']), stats.split_names)

    # Check statistics_gen outputs.
    self._validate_stats_output(
        os.path.join(stats.uri, 'Split-train', 'FeatureStats.pb'))
    self._validate_stats_output(
        os.path.join(stats.uri, 'Split-eval', 'FeatureStats.pb'))

    # Assert 'test' split is excluded.
    self.assertFalse(
        fileio.exists(os.path.join(stats.uri, 'test', 'FeatureStats.pb')))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag317')" href="javascript:;">
tfx-1.6.0/tfx/components/statistics_gen/executor_test.py: 95-139
</a>
<div class="mid" id="frag317" style="display:none"><pre>
  def testDoWithSchemaAndStatsOptions(self):
    source_data_dir = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), 'testdata')
    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    fileio.makedirs(output_data_dir)

    # Create input dict.
    examples = standard_artifacts.Examples()
    examples.uri = os.path.join(source_data_dir, 'csv_example_gen')
    examples.split_names = artifact_utils.encode_split_names(['train', 'eval'])

    schema = standard_artifacts.Schema()
    schema.uri = os.path.join(source_data_dir, 'schema_gen')

    input_dict = {
        standard_component_specs.EXAMPLES_KEY: [examples],
        standard_component_specs.SCHEMA_KEY: [schema]
    }

    exec_properties = {
        standard_component_specs.STATS_OPTIONS_JSON_KEY:
            tfdv.StatsOptions(label_feature='company').to_json(),
        standard_component_specs.EXCLUDE_SPLITS_KEY:
            json_utils.dumps([])
    }

    # Create output dict.
    stats = standard_artifacts.ExampleStatistics()
    stats.uri = output_data_dir
    output_dict = {
        standard_component_specs.STATISTICS_KEY: [stats],
    }

    # Run executor.
    stats_gen_executor = executor.Executor()
    stats_gen_executor.Do(input_dict, output_dict, exec_properties)

    # Check statistics_gen outputs.
    self._validate_stats_output(
        os.path.join(stats.uri, 'Split-train', 'FeatureStats.pb'))
    self._validate_stats_output(
        os.path.join(stats.uri, 'Split-eval', 'FeatureStats.pb'))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag318')" href="javascript:;">
tfx-1.6.0/tfx/components/statistics_gen/executor_test.py: 140-181
</a>
<div class="mid" id="frag318" style="display:none"><pre>
  def testDoWithTwoSchemas(self):
    source_data_dir = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), 'testdata')
    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    fileio.makedirs(output_data_dir)

    # Create input dict.
    examples = standard_artifacts.Examples()
    examples.uri = os.path.join(source_data_dir, 'csv_example_gen')
    examples.split_names = artifact_utils.encode_split_names(['train', 'eval'])

    schema = standard_artifacts.Schema()
    schema.uri = os.path.join(source_data_dir, 'schema_gen')

    input_dict = {
        standard_component_specs.EXAMPLES_KEY: [examples],
        standard_component_specs.SCHEMA_KEY: [schema]
    }

    exec_properties = {
        standard_component_specs.STATS_OPTIONS_JSON_KEY:
            tfdv.StatsOptions(
                label_feature='company', schema=schema_pb2.Schema()).to_json(),
        standard_component_specs.EXCLUDE_SPLITS_KEY:
            json_utils.dumps([])
    }

    # Create output dict.
    stats = standard_artifacts.ExampleStatistics()
    stats.uri = output_data_dir
    output_dict = {
        standard_component_specs.STATISTICS_KEY: [stats],
    }

    # Run executor.
    stats_gen_executor = executor.Executor()
    with self.assertRaises(ValueError):
      stats_gen_executor.Do(input_dict, output_dict, exec_properties)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 19:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag325')" href="javascript:;">
tfx-1.6.0/tfx/components/testdata/module_file/tuner_module.py: 59-86
</a>
<div class="mid" id="frag325" style="display:none"><pre>
def _build_keras_model(hparams: keras_tuner.HyperParameters) -&gt; tf.keras.Model:
  """Creates a DNN Keras model for classifying penguin data.

  Args:
    hparams: Holds HyperParameters for tuning.

  Returns:
    A Keras Model.
  """
  # The model below is built with Functional API, please refer to
  # https://www.tensorflow.org/guide/keras/overview for all API options.
  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]
  d = keras.layers.concatenate(inputs)
  for _ in range(int(hparams.get('num_layers'))):
    d = keras.layers.Dense(8, activation='relu')(d)
  outputs = keras.layers.Dense(3, activation='softmax')(d)

  model = keras.Model(inputs=inputs, outputs=outputs)
  model.compile(
      optimizer=keras.optimizers.Adam(hparams.get('learning_rate')),
      loss='sparse_categorical_crossentropy',
      metrics=[keras.metrics.SparseCategoricalAccuracy()])

  model.summary(print_fn=absl.logging.info)
  return model


# This will be called by TFX Tuner.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3104')" href="javascript:;">
tfx-1.6.0/tfx/examples/penguin/penguin_utils_keras.py: 43-73
</a>
<div class="mid" id="frag3104" style="display:none"><pre>
def _make_keras_model(hparams: keras_tuner.HyperParameters) -&gt; tf.keras.Model:
  """Creates a DNN Keras model for classifying penguin data.

  Args:
    hparams: Holds HyperParameters for tuning.

  Returns:
    A Keras Model.
  """
  # The model below is built with Functional API, please refer to
  # https://www.tensorflow.org/guide/keras/overview for all API options.
  inputs = [
      keras.layers.Input(shape=(1,), name=base.transformed_name(f))
      for f in base.FEATURE_KEYS
  ]
  d = keras.layers.concatenate(inputs)
  for _ in range(int(hparams.get('num_layers'))):
    d = keras.layers.Dense(8, activation='relu')(d)
  outputs = keras.layers.Dense(3, activation='softmax')(d)

  model = keras.Model(inputs=inputs, outputs=outputs)
  model.compile(
      optimizer=keras.optimizers.Adam(hparams.get('learning_rate')),
      loss='sparse_categorical_crossentropy',
      metrics=[keras.metrics.SparseCategoricalAccuracy()])

  model.summary(print_fn=absl.logging.info)
  return model


# TFX Tuner will call this function.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3115')" href="javascript:;">
tfx-1.6.0/tfx/examples/penguin/penguin_utils_cloud_tuner.py: 170-200
</a>
<div class="mid" id="frag3115" style="display:none"><pre>
def _build_keras_model(hparams: keras_tuner.HyperParameters) -&gt; tf.keras.Model:
  """Creates a DNN Keras model for classifying penguin data.

  Args:
    hparams: Holds HyperParameters for tuning.

  Returns:
    A Keras Model.
  """
  # The model below is built with Functional API, please refer to
  # https://www.tensorflow.org/guide/keras/overview for all API options.
  inputs = [
      keras.layers.Input(shape=(1,), name=_transformed_name(f))
      for f in _FEATURE_KEYS
  ]
  d = keras.layers.concatenate(inputs)
  for _ in range(int(hparams.get('num_layers'))):
    d = keras.layers.Dense(8, activation='relu')(d)
  outputs = keras.layers.Dense(3, activation='softmax')(d)

  model = keras.Model(inputs=inputs, outputs=outputs)
  model.compile(
      optimizer=keras.optimizers.Adam(hparams.get('learning_rate')),
      loss='sparse_categorical_crossentropy',
      metrics=[keras.metrics.SparseCategoricalAccuracy()])

  model.summary(print_fn=logging.info)
  return model


# TFX Tuner will call this function.
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 20:</b> &nbsp; 4 fragments, nominal size 26 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag331')" href="javascript:;">
tfx-1.6.0/tfx/components/testdata/module_file/trainer_module.py: 88-133
</a>
<div class="mid" id="frag331" style="display:none"><pre>
def _build_estimator(config, hidden_units=None, warm_start_from=None):
  """Build an estimator for predicting the tipping behavior of taxi riders.

  Args:
    config: tf.estimator.RunConfig defining the runtime environment for the
      estimator (including model_dir).
    hidden_units: [int], the layer sizes of the DNN (input layer first)
    warm_start_from: Optional directory to warm start from.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """
  real_valued_columns = [
      tf.feature_column.numeric_column(key, shape=())
      for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)
  ]
  categorical_columns = [
      tf.feature_column.categorical_column_with_identity(
          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)
      for key in _transformed_names(_VOCAB_FEATURE_KEYS)
  ]
  categorical_columns += [
      tf.feature_column.categorical_column_with_identity(
          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)
      for key in _transformed_names(_BUCKET_FEATURE_KEYS)
  ]
  categorical_columns += [
      tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension
          key,
          num_buckets=num_buckets,
          default_value=0) for key, num_buckets in zip(
              _transformed_names(_CATEGORICAL_FEATURE_KEYS),
              _MAX_CATEGORICAL_FEATURE_VALUES)
  ]
  return tf.estimator.DNNLinearCombinedClassifier(
      config=config,
      linear_feature_columns=categorical_columns,
      dnn_feature_columns=real_valued_columns,
      dnn_hidden_units=hidden_units or [100, 70, 50, 25],
      warm_start_from=warm_start_from)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2951')" href="javascript:;">
tfx-1.6.0/tfx/examples/bigquery_ml/taxi_utils_bqml.py: 148-193
</a>
<div class="mid" id="frag2951" style="display:none"><pre>
def _build_estimator(config, hidden_units=None, warm_start_from=None):
  """Build an estimator for predicting the tipping behavior of taxi riders.

  Args:
    config: tf.estimator.RunConfig defining the runtime environment for the
      estimator (including model_dir).
    hidden_units: [int], the layer sizes of the DNN (input layer first)
    warm_start_from: Optional directory to warm start from.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """
  real_valued_columns = [
      tf.feature_column.numeric_column(key, shape=())
      for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)
  ]
  categorical_columns = [
      tf.feature_column.categorical_column_with_identity(
          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)
      for key in _transformed_names(_VOCAB_FEATURE_KEYS)
  ]
  categorical_columns += [
      tf.feature_column.categorical_column_with_identity(
          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)
      for key in _transformed_names(_BUCKET_FEATURE_KEYS)
  ]
  categorical_columns += [
      tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension
          key,
          num_buckets=num_buckets,
          default_value=0) for key, num_buckets in zip(
              _transformed_names(_CATEGORICAL_FEATURE_KEYS),
              _MAX_CATEGORICAL_FEATURE_VALUES)
  ]
  return tf.estimator.DNNLinearCombinedClassifier(
      config=config,
      linear_feature_columns=categorical_columns,
      dnn_feature_columns=real_valued_columns,
      dnn_hidden_units=hidden_units or [100, 70, 50, 25],
      warm_start_from=warm_start_from)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3080')" href="javascript:;">
tfx-1.6.0/tfx/examples/chicago_taxi_pipeline/taxi_utils.py: 142-187
</a>
<div class="mid" id="frag3080" style="display:none"><pre>
def _build_estimator(config, hidden_units=None, warm_start_from=None):
  """Build an estimator for predicting the tipping behavior of taxi riders.

  Args:
    config: tf.estimator.RunConfig defining the runtime environment for the
      estimator (including model_dir).
    hidden_units: [int], the layer sizes of the DNN (input layer first)
    warm_start_from: Optional directory to warm start from.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """
  real_valued_columns = [
      tf.feature_column.numeric_column(key, shape=())
      for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)
  ]
  categorical_columns = [
      tf.feature_column.categorical_column_with_identity(
          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)
      for key in _transformed_names(_VOCAB_FEATURE_KEYS)
  ]
  categorical_columns += [
      tf.feature_column.categorical_column_with_identity(
          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)
      for key in _transformed_names(_BUCKET_FEATURE_KEYS)
  ]
  categorical_columns += [
      tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension
          key,
          num_buckets=num_buckets,
          default_value=0) for key, num_buckets in zip(
              _transformed_names(_CATEGORICAL_FEATURE_KEYS),
              _MAX_CATEGORICAL_FEATURE_VALUES)
  ]
  return tf.estimator.DNNLinearCombinedClassifier(
      config=config,
      linear_feature_columns=categorical_columns,
      dnn_feature_columns=real_valued_columns,
      dnn_hidden_units=hidden_units or [100, 70, 50, 25],
      warm_start_from=warm_start_from)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3053')" href="javascript:;">
tfx-1.6.0/tfx/examples/custom_components/slack/example/taxi_utils_slack.py: 143-188
</a>
<div class="mid" id="frag3053" style="display:none"><pre>
def _build_estimator(config, hidden_units=None, warm_start_from=None):
  """Build an estimator for predicting the tipping behavior of taxi riders.

  Args:
    config: tf.contrib.learn.RunConfig defining the runtime environment for the
      estimator (including model_dir).
    hidden_units: [int], the layer sizes of the DNN (input layer first)
    warm_start_from: Optional directory to warm start from.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """
  real_valued_columns = [
      tf.feature_column.numeric_column(key, shape=())
      for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)
  ]
  categorical_columns = [
      tf.feature_column.categorical_column_with_identity(
          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)
      for key in _transformed_names(_VOCAB_FEATURE_KEYS)
  ]
  categorical_columns += [
      tf.feature_column.categorical_column_with_identity(
          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)
      for key in _transformed_names(_BUCKET_FEATURE_KEYS)
  ]
  categorical_columns += [
      tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension
          key,
          num_buckets=num_buckets,
          default_value=0) for key, num_buckets in zip(
              _transformed_names(_CATEGORICAL_FEATURE_KEYS),
              _MAX_CATEGORICAL_FEATURE_VALUES)
  ]
  return tf.estimator.DNNLinearCombinedClassifier(
      config=config,
      linear_feature_columns=categorical_columns,
      dnn_feature_columns=real_valued_columns,
      dnn_hidden_units=hidden_units or [100, 70, 50, 25],
      warm_start_from=warm_start_from)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 21:</b> &nbsp; 5 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag332')" href="javascript:;">
tfx-1.6.0/tfx/components/testdata/module_file/trainer_module.py: 134-157
</a>
<div class="mid" id="frag332" style="display:none"><pre>
def _example_serving_receiver_fn(tf_transform_output, schema):
  """Build the serving in inputs.

  Args:
    tf_transform_output: A TFTransformOutput.
    schema: the schema of the input data.

  Returns:
    Tensorflow graph which parses examples, applying tf-transform to them.
  """
  raw_feature_spec = _get_raw_feature_spec(schema)
  raw_feature_spec.pop(_LABEL_KEY)

  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
      raw_feature_spec, default_batch_size=None)
  serving_input_receiver = raw_input_fn()

  transformed_features = tf_transform_output.transform_raw_features(
      serving_input_receiver.features)

  return tf.estimator.export.ServingInputReceiver(
      transformed_features, serving_input_receiver.receiver_tensors)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2952')" href="javascript:;">
tfx-1.6.0/tfx/examples/bigquery_ml/taxi_utils_bqml.py: 194-219
</a>
<div class="mid" id="frag2952" style="display:none"><pre>
def _flat_input_serving_receiver_fn(tf_transform_output, schema):
  """Build the serving function for flat list of Dense tensors as input.

  Args:
    tf_transform_output: A TFTransformOutput.
    schema: the schema of the input data.

  Returns:
    Tensorflow graph which parses examples, applying tf-transform to them.
  """
  raw_feature_spec = _get_raw_feature_spec(schema)
  raw_feature_spec.pop(_LABEL_KEY)

  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
      raw_feature_spec, default_batch_size=None)
  serving_input_receiver = raw_input_fn()

  transformed_features = tf_transform_output.transform_raw_features(
      serving_input_receiver.features)

  # We construct a receiver function that receives flat list of Dense tensors as
  # features. This is as per BigQuery ML serving requirements.
  return tf.estimator.export.ServingInputReceiver(
      transformed_features, serving_input_receiver.features)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3081')" href="javascript:;">
tfx-1.6.0/tfx/examples/chicago_taxi_pipeline/taxi_utils.py: 188-211
</a>
<div class="mid" id="frag3081" style="display:none"><pre>
def _example_serving_receiver_fn(tf_transform_output, schema):
  """Build the serving in inputs.

  Args:
    tf_transform_output: A TFTransformOutput.
    schema: the schema of the input data.

  Returns:
    Tensorflow graph which parses examples, applying tf-transform to them.
  """
  raw_feature_spec = _get_raw_feature_spec(schema)
  raw_feature_spec.pop(_LABEL_KEY)

  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
      raw_feature_spec, default_batch_size=None)
  serving_input_receiver = raw_input_fn()

  transformed_features = tf_transform_output.transform_raw_features(
      serving_input_receiver.features)

  return tf.estimator.export.ServingInputReceiver(
      transformed_features, serving_input_receiver.receiver_tensors)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2762')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/taxi/models/estimator_model/model.py: 95-118
</a>
<div class="mid" id="frag2762" style="display:none"><pre>
def _example_serving_receiver_fn(tf_transform_output, schema):
  """Build the serving in inputs.

  Args:
    tf_transform_output: A TFTransformOutput.
    schema: the schema of the input data.

  Returns:
    Tensorflow graph which parses examples, applying tf-transform to them.
  """
  raw_feature_spec = _get_raw_feature_spec(schema)
  raw_feature_spec.pop(features.LABEL_KEY)

  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
      raw_feature_spec, default_batch_size=None)
  serving_input_receiver = raw_input_fn()

  transformed_features = tf_transform_output.transform_raw_features(
      serving_input_receiver.features)

  return tf.estimator.export.ServingInputReceiver(
      transformed_features, serving_input_receiver.receiver_tensors)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3054')" href="javascript:;">
tfx-1.6.0/tfx/examples/custom_components/slack/example/taxi_utils_slack.py: 189-212
</a>
<div class="mid" id="frag3054" style="display:none"><pre>
def _example_serving_receiver_fn(transform_output, schema):
  """Build the serving in inputs.

  Args:
    transform_output: a `tft.TFTransformOutput` object.
    schema: the schema of the input data.

  Returns:
    Tensorflow graph which parses examples, applying tf-transform to them.
  """
  raw_feature_spec = _get_raw_feature_spec(schema)
  raw_feature_spec.pop(_LABEL_KEY)

  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
      raw_feature_spec, default_batch_size=None)
  serving_input_receiver = raw_input_fn()

  _, transformed_features = transform_output.transform_raw_features(
      serving_input_receiver.features, drop_unused_features=True)

  return tf.estimator.export.ServingInputReceiver(
      transformed_features, serving_input_receiver.receiver_tensors)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 22:</b> &nbsp; 5 fragments, nominal size 14 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag333')" href="javascript:;">
tfx-1.6.0/tfx/components/testdata/module_file/trainer_module.py: 158-200
</a>
<div class="mid" id="frag333" style="display:none"><pre>
def _eval_input_receiver_fn(tf_transform_output, schema):
  """Build everything needed for the tf-model-analysis to run the model.

  Args:
    tf_transform_output: A TFTransformOutput.
    schema: the schema of the input data.

  Returns:
    EvalInputReceiver function, which contains:
      - Tensorflow graph which parses raw untransformed features, applies the
        tf-transform preprocessing operators.
      - Set of raw, untransformed features.
      - Label against which predictions will be compared.
  """
  # Notice that the inputs are raw features, not transformed features here.
  raw_feature_spec = _get_raw_feature_spec(schema)

  serialized_tf_example = tf.compat.v1.placeholder(
      dtype=tf.string, shape=[None], name='input_example_tensor')

  # Add a parse_example operator to the tensorflow graph, which will parse
  # raw, untransformed, tf examples.
  features = tf.io.parse_example(
      serialized=serialized_tf_example, features=raw_feature_spec)

  # Now that we have our raw examples, process them through the tf-transform
  # function computed during the preprocessing step.
  transformed_features = tf_transform_output.transform_raw_features(
      features)

  # The key name MUST be 'examples'.
  receiver_tensors = {'examples': serialized_tf_example}

  # NOTE: Model is driven by transformed features (since training works on the
  # materialized output of TFT, but slicing will happen on raw features.
  features.update(transformed_features)

  return tfma.export.EvalInputReceiver(
      features=features,
      receiver_tensors=receiver_tensors,
      labels=transformed_features[_transformed_name(_LABEL_KEY)])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2953')" href="javascript:;">
tfx-1.6.0/tfx/examples/bigquery_ml/taxi_utils_bqml.py: 220-261
</a>
<div class="mid" id="frag2953" style="display:none"><pre>
def _eval_input_receiver_fn(tf_transform_output, schema):
  """Build everything needed for the tf-model-analysis to run the model.

  Args:
    tf_transform_output: A TFTransformOutput.
    schema: the schema of the input data.

  Returns:
    EvalInputReceiver function, which contains:
      - Tensorflow graph which parses raw untransformed features, applies the
        tf-transform preprocessing operators.
      - Set of raw, untransformed features.
      - Label against which predictions will be compared.
  """
  # Notice that the inputs are raw features, not transformed features here.
  raw_feature_spec = _get_raw_feature_spec(schema)

  serialized_tf_example = tf.compat.v1.placeholder(
      dtype=tf.string, shape=[None], name='input_example_tensor')

  # Add a parse_example operator to the tensorflow graph, which will parse
  # raw, untransformed, tf examples.
  features = tf.io.parse_example(
      serialized=serialized_tf_example, features=raw_feature_spec)

  # Now that we have our raw examples, process them through the tf-transform
  # function computed during the preprocessing step.
  transformed_features = tf_transform_output.transform_raw_features(features)

  # The key name MUST be 'examples'.
  receiver_tensors = {'examples': serialized_tf_example}

  # NOTE: Model is driven by transformed features (since training works on the
  # materialized output of TFT, but slicing will happen on raw features.
  features.update(transformed_features)

  return tfma.export.EvalInputReceiver(
      features=features,
      receiver_tensors=receiver_tensors,
      labels=transformed_features[_transformed_name(_LABEL_KEY)])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3055')" href="javascript:;">
tfx-1.6.0/tfx/examples/custom_components/slack/example/taxi_utils_slack.py: 213-255
</a>
<div class="mid" id="frag3055" style="display:none"><pre>
def _eval_input_receiver_fn(transform_output, schema):
  """Build everything needed for the tf-model-analysis to run the model.

  Args:
    transform_output: a `tft.TFTransformOutput` object.
    schema: the schema of the input data.

  Returns:
    EvalInputReceiver function, which contains:
      - Tensorflow graph which parses raw untransformed features, applies the
        tf-transform preprocessing operators.
      - Set of raw, untransformed features.
      - Label against which predictions will be compared.
  """
  # Notice that the inputs are raw features, not transformed features here.
  raw_feature_spec = _get_raw_feature_spec(schema)

  serialized_tf_example = tf.compat.v1.placeholder(
      dtype=tf.string, shape=[None], name='input_example_tensor')

  # Add a parse_example operator to the tensorflow graph, which will parse
  # raw, untransformed, tf examples.
  features = tf.io.parse_example(
      serialized=serialized_tf_example, features=raw_feature_spec)

  # Now that we have our raw examples, process them through the tf-transform
  # function computed during the preprocessing step.
  _, transformed_features = transform_output.transform_raw_features(
      features, drop_unused_features=True)

  # The key name MUST be 'examples'.
  receiver_tensors = {'examples': serialized_tf_example}

  # NOTE: Model is driven by transformed features (since training works on the
  # materialized output of TFT, but slicing will happen on raw features.
  features.update(transformed_features)

  return tfma.export.EvalInputReceiver(
      features=features,
      receiver_tensors=receiver_tensors,
      labels=transformed_features[_transformed_name(_LABEL_KEY)])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3082')" href="javascript:;">
tfx-1.6.0/tfx/examples/chicago_taxi_pipeline/taxi_utils.py: 212-254
</a>
<div class="mid" id="frag3082" style="display:none"><pre>
def _eval_input_receiver_fn(tf_transform_output, schema):
  """Build everything needed for the tf-model-analysis to run the model.

  Args:
    tf_transform_output: A TFTransformOutput.
    schema: the schema of the input data.

  Returns:
    EvalInputReceiver function, which contains:
      - Tensorflow graph which parses raw untransformed features, applies the
        tf-transform preprocessing operators.
      - Set of raw, untransformed features.
      - Label against which predictions will be compared.
  """
  # Notice that the inputs are raw features, not transformed features here.
  raw_feature_spec = _get_raw_feature_spec(schema)

  serialized_tf_example = tf.compat.v1.placeholder(
      dtype=tf.string, shape=[None], name='input_example_tensor')

  # Add a parse_example operator to the tensorflow graph, which will parse
  # raw, untransformed, tf examples.
  features = tf.io.parse_example(
      serialized=serialized_tf_example, features=raw_feature_spec)

  # Now that we have our raw examples, process them through the tf-transform
  # function computed during the preprocessing step.
  transformed_features = tf_transform_output.transform_raw_features(
      features)

  # The key name MUST be 'examples'.
  receiver_tensors = {'examples': serialized_tf_example}

  # NOTE: Model is driven by transformed features (since training works on the
  # materialized output of TFT, but slicing will happen on raw features.
  features.update(transformed_features)

  return tfma.export.EvalInputReceiver(
      features=features,
      receiver_tensors=receiver_tensors,
      labels=transformed_features[_transformed_name(_LABEL_KEY)])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2763')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/taxi/models/estimator_model/model.py: 119-162
</a>
<div class="mid" id="frag2763" style="display:none"><pre>
def _eval_input_receiver_fn(tf_transform_output, schema):
  """Build everything needed for the tf-model-analysis to run the model.

  Args:
    tf_transform_output: A TFTransformOutput.
    schema: the schema of the input data.

  Returns:
    EvalInputReceiver function, which contains:
      - Tensorflow graph which parses raw untransformed features, applies the
        tf-transform preprocessing operators.
      - Set of raw, untransformed features.
      - Label against which predictions will be compared.
  """
  # Notice that the inputs are raw features, not transformed features here.
  raw_feature_spec = _get_raw_feature_spec(schema)

  serialized_tf_example = tf.compat.v1.placeholder(
      dtype=tf.string, shape=[None], name='input_example_tensor')

  # Add a parse_example operator to the tensorflow graph, which will parse
  # raw, untransformed, tf examples.
  raw_features = tf.io.parse_example(
      serialized=serialized_tf_example, features=raw_feature_spec)

  # Now that we have our raw examples, process them through the tf-transform
  # function computed during the preprocessing step.
  transformed_features = tf_transform_output.transform_raw_features(
      raw_features)

  # The key name MUST be 'examples'.
  receiver_tensors = {'examples': serialized_tf_example}

  # NOTE: Model is driven by transformed features (since training works on the
  # materialized output of TFT, but slicing will happen on raw features.
  raw_features.update(transformed_features)

  return tfma.export.EvalInputReceiver(
      features=raw_features,
      receiver_tensors=receiver_tensors,
      labels=transformed_features[features.transformed_name(
          features.LABEL_KEY)])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 23:</b> &nbsp; 5 fragments, nominal size 46 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag335')" href="javascript:;">
tfx-1.6.0/tfx/components/testdata/module_file/trainer_module.py: 227-314
</a>
<div class="mid" id="frag335" style="display:none"><pre>
def trainer_fn(trainer_fn_args, schema):
  """Build the estimator using the high level API.

  Args:
    trainer_fn_args: Holds args used to train the model as name/value pairs.
    schema: Holds the schema of the training examples.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """
  if trainer_fn_args.hyperparameters:
    hp = trainer_fn_args.hyperparameters
    first_dnn_layer_size = hp.get('first_dnn_layer_size')
    num_dnn_layers = hp.get('num_dnn_layers')
    dnn_decay_factor = hp.get('dnn_decay_factor')
  else:
    # Number of nodes in the first layer of the DNN
    first_dnn_layer_size = 100
    num_dnn_layers = 4
    dnn_decay_factor = 0.7

  train_batch_size = 40
  eval_batch_size = 40

  tf_transform_output = tft.TFTransformOutput(trainer_fn_args.transform_output)

  train_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.train_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=train_batch_size)

  eval_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.eval_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=eval_batch_size)

  train_spec = tf.estimator.TrainSpec(  # pylint: disable=g-long-lambda
      train_input_fn,
      max_steps=trainer_fn_args.train_steps)

  serving_receiver_fn = lambda: _example_serving_receiver_fn(  # pylint: disable=g-long-lambda
      tf_transform_output, schema)

  exporter = tf.estimator.FinalExporter('chicago-taxi', serving_receiver_fn)
  eval_spec = tf.estimator.EvalSpec(
      eval_input_fn,
      steps=trainer_fn_args.eval_steps,
      exporters=[exporter],
      name='chicago-taxi-eval')

  run_config = tf.estimator.RunConfig(
      save_checkpoints_steps=999,
      # keep_checkpoint_max must be more than the number of worker replicas
      # nodes if training distributed, in order to avoid race condition.
      keep_checkpoint_max=5)

  export_dir = path_utils.serving_model_dir(trainer_fn_args.model_run_dir)
  run_config = run_config.replace(model_dir=export_dir)
  warm_start_from = trainer_fn_args.base_model

  estimator = _build_estimator(
      # Construct layers sizes with exponetial decay
      hidden_units=[
          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))
          for i in range(num_dnn_layers)
      ],
      config=run_config,
      warm_start_from=warm_start_from)

  # Create an input receiver for TFMA processing
  receiver_fn = lambda: _eval_input_receiver_fn(  # pylint: disable=g-long-lambda
      tf_transform_output, schema)

  return {
      'estimator': estimator,
      'train_spec': train_spec,
      'eval_spec': eval_spec,
      'eval_input_receiver_fn': receiver_fn
  }


# TFX generic trainer will call this function
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2955')" href="javascript:;">
tfx-1.6.0/tfx/examples/bigquery_ml/taxi_utils_bqml.py: 287-360
</a>
<div class="mid" id="frag2955" style="display:none"><pre>
def trainer_fn(trainer_fn_args, schema):
  """Build the estimator using the high level API.

  Args:
    trainer_fn_args: Holds args used to train the model as name/value pairs.
    schema: Holds the schema of the training examples.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """
  # Number of nodes in the first layer of the DNN
  first_dnn_layer_size = 100
  num_dnn_layers = 4
  dnn_decay_factor = 0.7

  train_batch_size = 40
  eval_batch_size = 40

  tf_transform_output = tft.TFTransformOutput(trainer_fn_args.transform_output)

  train_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.train_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=train_batch_size)

  eval_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.eval_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=eval_batch_size)

  train_spec = tf.estimator.TrainSpec(  # pylint: disable=g-long-lambda
      train_input_fn,
      max_steps=trainer_fn_args.train_steps)

  serving_receiver_fn = lambda: _flat_input_serving_receiver_fn(  # pylint: disable=g-long-lambda
      tf_transform_output, schema)

  exporter = tf.estimator.FinalExporter('chicago-taxi', serving_receiver_fn)
  eval_spec = tf.estimator.EvalSpec(
      eval_input_fn,
      steps=trainer_fn_args.eval_steps,
      exporters=[exporter],
      name='chicago-taxi-eval')

  run_config = tf.estimator.RunConfig(
      save_checkpoints_steps=999, keep_checkpoint_max=1)

  run_config = run_config.replace(model_dir=trainer_fn_args.serving_model_dir)

  estimator = _build_estimator(
      # Construct layers sizes with exponential decay
      hidden_units=[
          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))
          for i in range(num_dnn_layers)
      ],
      config=run_config,
      warm_start_from=trainer_fn_args.base_model)

  # Create an input receiver for TFMA processing
  receiver_fn = lambda: _eval_input_receiver_fn(  # pylint: disable=g-long-lambda
      tf_transform_output, schema)

  return {
      'estimator': estimator,
      'train_spec': train_spec,
      'eval_spec': eval_spec,
      'eval_input_receiver_fn': receiver_fn
  }
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3084')" href="javascript:;">
tfx-1.6.0/tfx/examples/chicago_taxi_pipeline/taxi_utils.py: 280-357
</a>
<div class="mid" id="frag3084" style="display:none"><pre>
def trainer_fn(trainer_fn_args, schema):
  """Build the estimator using the high level API.

  Args:
    trainer_fn_args: Holds args used to train the model as name/value pairs.
    schema: Holds the schema of the training examples.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """
  # Number of nodes in the first layer of the DNN
  first_dnn_layer_size = 100
  num_dnn_layers = 4
  dnn_decay_factor = 0.7

  train_batch_size = 40
  eval_batch_size = 40

  tf_transform_output = tft.TFTransformOutput(trainer_fn_args.transform_output)

  train_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.train_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=train_batch_size)

  eval_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.eval_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=eval_batch_size)

  train_spec = tf.estimator.TrainSpec(  # pylint: disable=g-long-lambda
      train_input_fn,
      max_steps=trainer_fn_args.train_steps)

  serving_receiver_fn = lambda: _example_serving_receiver_fn(  # pylint: disable=g-long-lambda
      tf_transform_output, schema)

  exporter = tf.estimator.FinalExporter('chicago-taxi', serving_receiver_fn)
  eval_spec = tf.estimator.EvalSpec(
      eval_input_fn,
      steps=trainer_fn_args.eval_steps,
      exporters=[exporter],
      name='chicago-taxi-eval')

  # Keep multiple checkpoint files for distributed training, note that
  # keep_max_checkpoint should be greater or equal to the number of replicas to
  # avoid race condition.
  run_config = tf.estimator.RunConfig(
      save_checkpoints_steps=999, keep_checkpoint_max=5)

  run_config = run_config.replace(model_dir=trainer_fn_args.serving_model_dir)
  warm_start_from = trainer_fn_args.base_model

  estimator = _build_estimator(
      # Construct layers sizes with exponetial decay
      hidden_units=[
          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))
          for i in range(num_dnn_layers)
      ],
      config=run_config,
      warm_start_from=warm_start_from)

  # Create an input receiver for TFMA processing
  receiver_fn = lambda: _eval_input_receiver_fn(  # pylint: disable=g-long-lambda
      tf_transform_output, schema)

  return {
      'estimator': estimator,
      'train_spec': train_spec,
      'eval_spec': eval_spec,
      'eval_input_receiver_fn': receiver_fn
  }
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3057')" href="javascript:;">
tfx-1.6.0/tfx/examples/custom_components/slack/example/taxi_utils_slack.py: 281-352
</a>
<div class="mid" id="frag3057" style="display:none"><pre>
def trainer_fn(trainer_fn_args, schema):
  """Build the estimator using the high level API.

  Args:
    trainer_fn_args: Holds args used to train the model as name/value pairs.
    schema: Holds the schema of the training examples.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """
  # Number of nodes in the first layer of the DNN
  first_dnn_layer_size = 100
  num_dnn_layers = 4
  dnn_decay_factor = 0.7

  train_batch_size = 40
  eval_batch_size = 40

  tf_transform_output = tft.TFTransformOutput(trainer_fn_args.transform_output)

  train_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.train_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=train_batch_size)

  eval_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.eval_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=eval_batch_size)

  train_spec = tf.estimator.TrainSpec(
      train_input_fn, max_steps=trainer_fn_args.train_steps)

  serving_receiver_fn = (
      lambda: _example_serving_receiver_fn(tf_transform_output, schema))

  exporter = tf.estimator.FinalExporter('chicago-taxi', serving_receiver_fn)
  eval_spec = tf.estimator.EvalSpec(
      eval_input_fn,
      steps=trainer_fn_args.eval_steps,
      exporters=[exporter],
      name='chicago-taxi-eval')

  run_config = tf.estimator.RunConfig(
      save_checkpoints_steps=999, keep_checkpoint_max=1)

  run_config = run_config.replace(model_dir=trainer_fn_args.serving_model_dir)

  estimator = _build_estimator(
      # Construct layers sizes with exponetial decay
      hidden_units=[
          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))
          for i in range(num_dnn_layers)
      ],
      config=run_config,
      warm_start_from=trainer_fn_args.base_model)

  # Create an input receiver for TFMA processing
  receiver_fn = lambda: _eval_input_receiver_fn(tf_transform_output, schema)

  return {
      'estimator': estimator,
      'train_spec': train_spec,
      'eval_spec': eval_spec,
      'eval_input_receiver_fn': receiver_fn
  }
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2765')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/taxi/models/estimator_model/model.py: 185-248
</a>
<div class="mid" id="frag2765" style="display:none"><pre>
def _create_train_and_eval_spec(trainer_fn_args, schema):
  """Build the estimator using the high level API.

  Args:
    trainer_fn_args: Holds args used to train the model as name/value pairs.
    schema: Holds the schema of the training examples.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """

  tf_transform_output = tft.TFTransformOutput(trainer_fn_args.transform_output)

  train_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.train_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=constants.TRAIN_BATCH_SIZE)

  eval_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.eval_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=constants.EVAL_BATCH_SIZE)

  train_spec = tf.estimator.TrainSpec(  # pylint: disable=g-long-lambda
      train_input_fn,
      max_steps=trainer_fn_args.train_steps)

  serving_receiver_fn = lambda: _example_serving_receiver_fn(  # pylint: disable=g-long-lambda
      tf_transform_output, schema)

  exporter = tf.estimator.FinalExporter('chicago-taxi', serving_receiver_fn)
  eval_spec = tf.estimator.EvalSpec(
      eval_input_fn,
      steps=trainer_fn_args.eval_steps,
      exporters=[exporter],
      name='chicago-taxi-eval')

  run_config = tf.estimator.RunConfig(
      save_checkpoints_steps=999, keep_checkpoint_max=1)

  run_config = run_config.replace(model_dir=trainer_fn_args.serving_model_dir)

  estimator = _build_estimator(
      hidden_units=constants.HIDDEN_UNITS, config=run_config)

  # Create an input receiver for TFMA processing
  receiver_fn = lambda: _eval_input_receiver_fn(  # pylint: disable=g-long-lambda
      tf_transform_output, schema)

  return {
      'estimator': estimator,
      'train_spec': train_spec,
      'eval_spec': eval_spec,
      'eval_input_receiver_fn': receiver_fn
  }


# TFX will call this function
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 24:</b> &nbsp; 4 fragments, nominal size 23 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag340')" href="javascript:;">
tfx-1.6.0/tfx/components/testdata/module_file/transform_module.py: 92-134
</a>
<div class="mid" id="frag340" style="display:none"><pre>
def preprocessing_fn(inputs, custom_config):
  """tf.transform's callback function for preprocessing inputs.

  Args:
    inputs: map from feature keys to raw not-yet-transformed features.
    custom_config: additional properties for pre-processing.

  Returns:
    Map from string feature key to transformed features.
  """
  outputs = {}
  for key in _DENSE_FLOAT_FEATURE_KEYS:
    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.
    outputs[_transformed_name(key)] = tft.scale_to_z_score(
        _fill_in_missing(_identity(inputs[key])))

  for key in _VOCAB_FEATURE_KEYS:
    # Build a vocabulary for this feature.
    outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(
        _fill_in_missing(inputs[key]),
        top_k=custom_config.get('VOCAB_SIZE', _VOCAB_SIZE),
        num_oov_buckets=custom_config.get('OOV_SIZE', _OOV_SIZE))

  for key in _BUCKET_FEATURE_KEYS:
    outputs[_transformed_name(key)] = tft.bucketize(
        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)

  for key in _CATEGORICAL_FEATURE_KEYS:
    outputs[_transformed_name(key)] = _fill_in_missing(inputs[key])

  # Was this passenger a big tipper?
  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])
  tips = _fill_in_missing(inputs[_LABEL_KEY])
  outputs[_transformed_name(_LABEL_KEY)] = tf.compat.v1.where(
      tf.math.is_nan(taxi_fare),
      tf.cast(tf.zeros_like(taxi_fare), tf.int64),
      # Test if the tip was &gt; 20% of the fare.
      tf.cast(
          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))

  return outputs


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3079')" href="javascript:;">
tfx-1.6.0/tfx/examples/chicago_taxi_pipeline/taxi_utils.py: 100-141
</a>
<div class="mid" id="frag3079" style="display:none"><pre>
def preprocessing_fn(inputs):
  """tf.transform's callback function for preprocessing inputs.

  Args:
    inputs: map from feature keys to raw not-yet-transformed features.

  Returns:
    Map from string feature key to transformed feature operations.
  """
  outputs = {}
  for key in _DENSE_FLOAT_FEATURE_KEYS:
    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.
    outputs[_transformed_name(key)] = tft.scale_to_z_score(
        _fill_in_missing(inputs[key]))

  for key in _VOCAB_FEATURE_KEYS:
    # Build a vocabulary for this feature.
    outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(
        _fill_in_missing(inputs[key]),
        top_k=_VOCAB_SIZE,
        num_oov_buckets=_OOV_SIZE)

  for key in _BUCKET_FEATURE_KEYS:
    outputs[_transformed_name(key)] = tft.bucketize(
        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)

  for key in _CATEGORICAL_FEATURE_KEYS:
    outputs[_transformed_name(key)] = _fill_in_missing(inputs[key])

  # Was this passenger a big tipper?
  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])
  tips = _fill_in_missing(inputs[_LABEL_KEY])
  outputs[_transformed_name(_LABEL_KEY)] = tf.compat.v1.where(
      tf.math.is_nan(taxi_fare),
      tf.cast(tf.zeros_like(taxi_fare), tf.int64),
      # Test if the tip was &gt; 20% of the fare.
      tf.cast(
          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))

  return outputs


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3052')" href="javascript:;">
tfx-1.6.0/tfx/examples/custom_components/slack/example/taxi_utils_slack.py: 101-142
</a>
<div class="mid" id="frag3052" style="display:none"><pre>
def preprocessing_fn(inputs):
  """tf.transform's callback function for preprocessing inputs.

  Args:
    inputs: map from feature keys to raw not-yet-transformed features.

  Returns:
    Map from string feature key to transformed feature operations.
  """
  outputs = {}
  for key in _DENSE_FLOAT_FEATURE_KEYS:
    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.
    outputs[_transformed_name(key)] = tft.scale_to_z_score(
        _fill_in_missing(inputs[key]))

  for key in _VOCAB_FEATURE_KEYS:
    # Build a vocabulary for this feature.
    outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(
        _fill_in_missing(inputs[key]),
        top_k=_VOCAB_SIZE,
        num_oov_buckets=_OOV_SIZE)

  for key in _BUCKET_FEATURE_KEYS:
    outputs[_transformed_name(key)] = tft.bucketize(
        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)

  for key in _CATEGORICAL_FEATURE_KEYS:
    outputs[_transformed_name(key)] = _fill_in_missing(inputs[key])

  # Was this passenger a big tipper?
  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])
  tips = _fill_in_missing(inputs[_LABEL_KEY])
  outputs[_transformed_name(_LABEL_KEY)] = tf.compat.v1.where(
      tf.math.is_nan(taxi_fare),
      tf.cast(tf.zeros_like(taxi_fare), tf.int64),
      # Test if the tip was &gt; 20% of the fare.
      tf.cast(
          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))

  return outputs


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2950')" href="javascript:;">
tfx-1.6.0/tfx/examples/bigquery_ml/taxi_utils_bqml.py: 106-147
</a>
<div class="mid" id="frag2950" style="display:none"><pre>
def preprocessing_fn(inputs):
  """tf.transform's callback function for preprocessing inputs.

  Args:
    inputs: map from feature keys to raw not-yet-transformed features.

  Returns:
    Map from string feature key to transformed feature operations.
  """
  outputs = {}
  for key in _DENSE_FLOAT_FEATURE_KEYS:
    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.
    outputs[_transformed_name(key)] = tft.scale_to_z_score(
        _fill_in_missing(inputs[key]))

  for key in _VOCAB_FEATURE_KEYS:
    # Build a vocabulary for this feature.
    outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(
        _fill_in_missing(inputs[key]),
        top_k=_VOCAB_SIZE,
        num_oov_buckets=_OOV_SIZE)

  for key in _BUCKET_FEATURE_KEYS:
    outputs[_transformed_name(key)] = tft.bucketize(
        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)

  for key in _CATEGORICAL_FEATURE_KEYS:
    outputs[_transformed_name(key)] = _fill_in_missing(inputs[key])

  # Was this passenger a big tipper?
  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])
  tips = _fill_in_missing(inputs[_LABEL_KEY])
  outputs[_transformed_name(_LABEL_KEY)] = tf.compat.v1.where(
      tf.math.is_nan(taxi_fare),
      tf.cast(tf.zeros_like(taxi_fare), tf.int64),
      # Test if the tip was &gt; 20% of the fare.
      tf.cast(
          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))

  return outputs


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 25:</b> &nbsp; 4 fragments, nominal size 19 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag344')" href="javascript:;">
tfx-1.6.0/tfx/components/trainer/rewriting/tflite_rewriter_test.py: 61-80
</a>
<div class="mid" id="frag344" style="display:none"><pre>
  def testInvokeTFLiteRewriterNoAssetsSucceeds(self, converter):
    m = self.ConverterMock()
    converter.return_value = m

    src_model, dst_model, _, dst_model_path = self.create_temp_model_template()

    tfrw = tflite_rewriter.TFLiteRewriter(name='myrw', filename='fname')
    tfrw.perform_rewrite(src_model, dst_model)

    converter.assert_called_once_with(
        saved_model_path=mock.ANY,
        quantization_optimizations=[],
        quantization_supported_types=[],
        representative_dataset=None,
        signature_key=None)
    expected_model = os.path.join(dst_model_path, 'fname')
    self.assertTrue(fileio.exists(expected_model))
    with fileio.open(expected_model, 'rb') as f:
      self.assertEqual(f.read(), b'model')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag346')" href="javascript:;">
tfx-1.6.0/tfx/components/trainer/rewriting/tflite_rewriter_test.py: 133-155
</a>
<div class="mid" id="frag346" style="display:none"><pre>
  def testInvokeTFLiteRewriterQuantizationHybridSucceeds(self, converter):
    m = self.ConverterMock()
    converter.return_value = m

    src_model, dst_model, _, dst_model_path = self.create_temp_model_template()

    tfrw = tflite_rewriter.TFLiteRewriter(
        name='myrw',
        filename='fname',
        quantization_optimizations=[tf.lite.Optimize.DEFAULT])
    tfrw.perform_rewrite(src_model, dst_model)

    converter.assert_called_once_with(
        saved_model_path=mock.ANY,
        quantization_optimizations=[tf.lite.Optimize.DEFAULT],
        quantization_supported_types=[],
        representative_dataset=None,
        signature_key=None)
    expected_model = os.path.join(dst_model_path, 'fname')
    self.assertTrue(fileio.exists(expected_model))
    with fileio.open(expected_model, 'rb') as f:
      self.assertEqual(f.read(), b'model')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag347')" href="javascript:;">
tfx-1.6.0/tfx/components/trainer/rewriting/tflite_rewriter_test.py: 158-181
</a>
<div class="mid" id="frag347" style="display:none"><pre>
  def testInvokeTFLiteRewriterQuantizationFloat16Succeeds(self, converter):
    m = self.ConverterMock()
    converter.return_value = m

    src_model, dst_model, _, dst_model_path = self.create_temp_model_template()

    tfrw = tflite_rewriter.TFLiteRewriter(
        name='myrw',
        filename='fname',
        quantization_optimizations=[tf.lite.Optimize.DEFAULT],
        quantization_supported_types=[tf.float16])
    tfrw.perform_rewrite(src_model, dst_model)

    converter.assert_called_once_with(
        saved_model_path=mock.ANY,
        quantization_optimizations=[tf.lite.Optimize.DEFAULT],
        quantization_supported_types=[tf.float16],
        representative_dataset=None,
        signature_key=None)
    expected_model = os.path.join(dst_model_path, 'fname')
    self.assertTrue(fileio.exists(expected_model))
    with fileio.open(expected_model, 'rb') as f:
      self.assertEqual(f.read(), b'model')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag349')" href="javascript:;">
tfx-1.6.0/tfx/components/trainer/rewriting/tflite_rewriter_test.py: 205-233
</a>
<div class="mid" id="frag349" style="display:none"><pre>
  def testInvokeTFLiteRewriterQuantizationFullIntegerSucceeds(self, converter):
    m = self.ConverterMock()
    converter.return_value = m

    src_model, dst_model, _, dst_model_path = self.create_temp_model_template()

    def representative_dataset():
      for i in range(2):
        yield [np.array(i)]

    tfrw = tflite_rewriter.TFLiteRewriter(
        name='myrw',
        filename='fname',
        quantization_optimizations=[tf.lite.Optimize.DEFAULT],
        quantization_enable_full_integer=True,
        representative_dataset=representative_dataset)
    tfrw.perform_rewrite(src_model, dst_model)

    converter.assert_called_once_with(
        saved_model_path=mock.ANY,
        quantization_optimizations=[tf.lite.Optimize.DEFAULT],
        quantization_supported_types=[],
        representative_dataset=representative_dataset,
        signature_key=None)
    expected_model = os.path.join(dst_model_path, 'fname')
    self.assertTrue(fileio.exists(expected_model))
    with fileio.open(expected_model, 'rb') as f:
      self.assertEqual(f.read(), b'model')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 26:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag506')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/custom_executors/avro_component_test.py: 35-53
</a>
<div class="mid" id="frag506" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    # Create input_base.
    input_data_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'testdata')
    self.avro_dir_path = os.path.join(input_data_dir, 'external')

    # Create input_config.
    self.input_config = example_gen_pb2.Input(splits=[
        example_gen_pb2.Input.Split(name='avro', pattern='avro/*.avro'),
    ])

    # Create output_config.
    self.output_config = example_gen_pb2.Output(
        split_config=example_gen_pb2.SplitConfig(splits=[
            example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=2),
            example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)
        ]))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag518')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/custom_executors/parquet_component_test.py: 35-54
</a>
<div class="mid" id="frag518" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    # Create input_base.
    input_data_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'testdata')
    self.parquet_dir_path = os.path.join(input_data_dir, 'external')

    # Create input_config.
    self.input_config = example_gen_pb2.Input(splits=[
        example_gen_pb2.Input.Split(name='parquet',
                                    pattern='parquet/*.parquet'),
    ])

    # Create output_config.
    self.output_config = example_gen_pb2.Output(
        split_config=example_gen_pb2.SplitConfig(splits=[
            example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=2),
            example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)
        ]))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 27:</b> &nbsp; 2 fragments, nominal size 33 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag507')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/custom_executors/avro_component_test.py: 55-97
</a>
<div class="mid" id="frag507" style="display:none"><pre>
  def testRun(self, mock_publisher):
    mock_publisher.return_value.publish_execution.return_value = {}

    example_gen = FileBasedExampleGen(
        custom_executor_spec=executor_spec.ExecutorClassSpec(
            avro_executor.Executor),
        input_base=self.avro_dir_path,
        input_config=self.input_config,
        output_config=self.output_config).with_id('AvroExampleGen')

    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    pipeline_root = os.path.join(output_data_dir, 'Test')
    fileio.makedirs(pipeline_root)
    pipeline_info = data_types.PipelineInfo(
        pipeline_name='Test', pipeline_root=pipeline_root, run_id='123')

    driver_args = data_types.DriverArgs(enable_cache=True)

    connection_config = metadata_store_pb2.ConnectionConfig()
    connection_config.sqlite.SetInParent()
    metadata_connection = metadata.Metadata(connection_config)

    launcher = in_process_component_launcher.InProcessComponentLauncher.create(
        component=example_gen,
        pipeline_info=pipeline_info,
        driver_args=driver_args,
        metadata_connection=metadata_connection,
        beam_pipeline_args=[],
        additional_pipeline_args={})
    self.assertEqual(
        launcher._component_info.component_type,
        '.'.join([FileBasedExampleGen.__module__,
                  FileBasedExampleGen.__name__]))

    launcher.launch()
    mock_publisher.return_value.publish_execution.assert_called_once()

    # Check output paths.
    self.assertTrue(fileio.exists(os.path.join(pipeline_root, example_gen.id)))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag519')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/custom_executors/parquet_component_test.py: 56-98
</a>
<div class="mid" id="frag519" style="display:none"><pre>
  def testRun(self, mock_publisher):
    mock_publisher.return_value.publish_execution.return_value = {}

    example_gen = FileBasedExampleGen(
        custom_executor_spec=executor_spec.ExecutorClassSpec(
            parquet_executor.Executor),
        input_base=self.parquet_dir_path,
        input_config=self.input_config,
        output_config=self.output_config).with_id('ParquetExampleGen')

    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    pipeline_root = os.path.join(output_data_dir, 'Test')
    fileio.makedirs(pipeline_root)
    pipeline_info = data_types.PipelineInfo(
        pipeline_name='Test', pipeline_root=pipeline_root, run_id='123')

    driver_args = data_types.DriverArgs(enable_cache=True)

    connection_config = metadata_store_pb2.ConnectionConfig()
    connection_config.sqlite.SetInParent()
    metadata_connection = metadata.Metadata(connection_config)

    launcher = in_process_component_launcher.InProcessComponentLauncher.create(
        component=example_gen,
        pipeline_info=pipeline_info,
        driver_args=driver_args,
        metadata_connection=metadata_connection,
        beam_pipeline_args=[],
        additional_pipeline_args={})
    self.assertEqual(
        launcher._component_info.component_type,
        '.'.join([FileBasedExampleGen.__module__,
                  FileBasedExampleGen.__name__]))

    launcher.launch()
    mock_publisher.return_value.publish_execution.assert_called_once()

    # Check output paths.
    self.assertTrue(fileio.exists(os.path.join(pipeline_root, example_gen.id)))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 28:</b> &nbsp; 6 fragments, nominal size 13 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag509')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/custom_executors/parquet_executor_test.py: 38-55
</a>
<div class="mid" id="frag509" style="display:none"><pre>
  def testParquetToExample(self):
    with beam.Pipeline() as pipeline:
      examples = (
          pipeline
          | 'ToTFExample' &gt;&gt; parquet_executor._ParquetToExample(
              exec_properties={
                  standard_component_specs.INPUT_BASE_KEY: self._input_data_dir
              },
              split_pattern='parquet/*'))

      def check_result(got):
        # We use Python assertion here to avoid Beam serialization error in
        # pickling tf.test.TestCase.
        assert (10000 == len(got)), 'Unexpected example count'
        assert (18 == len(got[0].features.feature)), 'Example not match'

      util.assert_that(examples, check_result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag591')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/csv_example_gen/executor_test.py: 38-55
</a>
<div class="mid" id="frag591" style="display:none"><pre>
  def testCsvToExample(self):
    with beam.Pipeline() as pipeline:
      examples = (
          pipeline
          | 'ToTFExample' &gt;&gt; executor._CsvToExample(
              exec_properties={
                  standard_component_specs.INPUT_BASE_KEY: self._input_data_dir
              },
              split_pattern='csv/*'))

      def check_results(results):
        # We use Python assertion here to avoid Beam serialization error.
        assert (15000 == len(results)), 'Unexpected example count {}.'.format(
            len(results))
        assert (18 == len(results[0].features.feature)), 'Example not match.'

      util.assert_that(examples, check_results)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag515')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/custom_executors/avro_executor_test.py: 38-55
</a>
<div class="mid" id="frag515" style="display:none"><pre>
  def testAvroToExample(self):
    with beam.Pipeline() as pipeline:
      examples = (
          pipeline
          | 'ToTFExample' &gt;&gt; avro_executor._AvroToExample(
              exec_properties={
                  standard_component_specs.INPUT_BASE_KEY: self._input_data_dir
              },
              split_pattern='avro/*.avro'))

      def check_result(got):
        # We use Python assertion here to avoid Beam serialization error in
        # pickling tf.test.TestCase.
        assert (10000 == len(got)), 'Unexpected example count'
        assert (18 == len(got[0].features.feature)), 'Example not match'

      util.assert_that(examples, check_result)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag550')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/import_example_gen/executor_test.py: 51-69
</a>
<div class="mid" id="frag550" style="display:none"><pre>
  def testImportExample(self):
    with beam.Pipeline() as pipeline:
      examples = (
          pipeline
          | 'ToSerializedRecord' &gt;&gt; executor._ImportSerializedRecord(
              exec_properties={
                  standard_component_specs.INPUT_BASE_KEY: self._input_data_dir
              },
              split_pattern='tfrecord/*')
          | 'ToTFExample' &gt;&gt; beam.Map(tf.train.Example.FromString))

      def check_result(got):
        # We use Python assertion here to avoid Beam serialization error in
        # pickling tf.test.TestCase.
        assert (15000 == len(got)), 'Unexpected example count'
        assert (18 == len(got[0].features.feature)), 'Example not match'

      util.assert_that(examples, check_result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag593')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/csv_example_gen/executor_test.py: 56-81
</a>
<div class="mid" id="frag593" style="display:none"><pre>
  def testCsvToExampleWithEmptyColumn(self):
    with beam.Pipeline() as pipeline:
      examples = (
          pipeline
          | 'ToTFExample' &gt;&gt; executor._CsvToExample(
              exec_properties={
                  standard_component_specs.INPUT_BASE_KEY: self._input_data_dir
              },
              split_pattern='csv_empty/*'))

      def check_results(results):
        # We use Python assertion here to avoid Beam serialization error.
        assert (3 == len(results)), 'Unexpected example count {}.'.format(
            len(results))
        for example in results:
          assert (example.features.feature['A'].HasField('int64_list')
                 ), 'Column A should be int64 type.'
          assert (not example.features.feature['B'].WhichOneof('kind')
                 ), 'Column B should be empty.'
          assert (example.features.feature['C'].HasField('bytes_list')
                 ), 'Column C should be byte type.'
          assert (example.features.feature['D'].HasField('float_list')
                 ), 'Column D should be float type.'

      util.assert_that(examples, check_results)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag595')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/csv_example_gen/executor_test.py: 82-104
</a>
<div class="mid" id="frag595" style="display:none"><pre>
  def testCsvToExampleMultiLineString(self):
    with beam.Pipeline() as pipeline:
      examples = (
          pipeline
          | 'ToTFExample' &gt;&gt; executor._CsvToExample(
              exec_properties={
                  standard_component_specs.INPUT_BASE_KEY: self._input_data_dir
              },
              split_pattern='csv_multi_line_string/*'))

      def check_results(results):
        # We use Python assertion here to avoid Beam serialization error.
        assert (3 == len(results)), 'Unexpected example count: {}.'.format(
            len(results))
        instance = results[1]
        assert (instance.features.feature['B'].HasField('bytes_list')
               ), 'Column B should be bytes type. '
        value = instance.features.feature['B'].bytes_list.value
        assert (value ==
                [b'"2,\n"3",\n4\n5"']), 'Unexpected value: {}.'.format(value)

      util.assert_that(examples, check_results)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 29:</b> &nbsp; 4 fragments, nominal size 38 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag511')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/custom_executors/parquet_executor_test.py: 56-106
</a>
<div class="mid" id="frag511" style="display:none"><pre>
  def testDo(self):
    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    # Create output dict.
    examples = standard_artifacts.Examples()
    examples.uri = output_data_dir
    output_dict = {standard_component_specs.EXAMPLES_KEY: [examples]}

    # Create exec proterties.
    exec_properties = {
        standard_component_specs.INPUT_BASE_KEY:
            self._input_data_dir,
        standard_component_specs.INPUT_CONFIG_KEY:
            proto_utils.proto_to_json(
                example_gen_pb2.Input(splits=[
                    example_gen_pb2.Input.Split(
                        name='parquet', pattern='parquet/*'),
                ])),
        standard_component_specs.OUTPUT_CONFIG_KEY:
            proto_utils.proto_to_json(
                example_gen_pb2.Output(
                    split_config=example_gen_pb2.SplitConfig(splits=[
                        example_gen_pb2.SplitConfig.Split(
                            name='train', hash_buckets=2),
                        example_gen_pb2.SplitConfig.Split(
                            name='eval', hash_buckets=1)
                    ])))
    }

    # Run executor.
    parquet_example_gen = parquet_executor.Executor()
    parquet_example_gen.Do({}, output_dict, exec_properties)

    self.assertEqual(
        artifact_utils.encode_split_names(['train', 'eval']),
        examples.split_names)

    # Check Parquet example gen outputs.
    train_output_file = os.path.join(examples.uri, 'Split-train',
                                     'data_tfrecord-00000-of-00001.gz')
    eval_output_file = os.path.join(examples.uri, 'Split-eval',
                                    'data_tfrecord-00000-of-00001.gz')
    self.assertTrue(fileio.exists(train_output_file))
    self.assertTrue(fileio.exists(eval_output_file))
    self.assertGreater(
        fileio.open(train_output_file).size(),
        fileio.open(eval_output_file).size())


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag780')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_big_query/example_gen/executor_test.py: 117-172
</a>
<div class="mid" id="frag780" style="display:none"><pre>
  def testDo(self, mock_client):
    # Mock query result schema for _BigQueryConverter.
    mock_client.return_value.query.return_value.result.return_value.schema = self._schema

    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    # Create output dict.
    examples = standard_artifacts.Examples()
    examples.uri = output_data_dir
    output_dict = {'examples': [examples]}

    # Create exe properties.
    exec_properties = {
        'input_config':
            proto_utils.proto_to_json(
                example_gen_pb2.Input(splits=[
                    example_gen_pb2.Input.Split(
                        name='bq', pattern='SELECT i, b, f, s FROM `fake`'),
                ])),
        'output_config':
            proto_utils.proto_to_json(
                example_gen_pb2.Output(
                    split_config=example_gen_pb2.SplitConfig(splits=[
                        example_gen_pb2.SplitConfig.Split(
                            name='train', hash_buckets=2),
                        example_gen_pb2.SplitConfig.Split(
                            name='eval', hash_buckets=1)
                    ])))
    }

    # Run executor.
    big_query_example_gen = executor.Executor(
        base_beam_executor.BaseBeamExecutor.Context(
            beam_pipeline_args=['--project=test-project']))
    big_query_example_gen.Do({}, output_dict, exec_properties)

    mock_client.assert_called_with(project='test-project')

    self.assertEqual(
        artifact_utils.encode_split_names(['train', 'eval']),
        examples.split_names)

    # Check BigQuery example gen outputs.
    train_output_file = os.path.join(examples.uri, 'Split-train',
                                     'data_tfrecord-00000-of-00001.gz')
    eval_output_file = os.path.join(examples.uri, 'Split-eval',
                                    'data_tfrecord-00000-of-00001.gz')
    self.assertTrue(fileio.exists(train_output_file))
    self.assertTrue(fileio.exists(eval_output_file))
    self.assertGreater(
        fileio.open(train_output_file).size(),
        fileio.open(eval_output_file).size())


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag597')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/csv_example_gen/executor_test.py: 105-154
</a>
<div class="mid" id="frag597" style="display:none"><pre>
  def testDo(self):
    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.create_tempdir()),
        self._testMethodName)

    # Create output dict.
    examples = standard_artifacts.Examples()
    examples.uri = output_data_dir
    output_dict = {standard_component_specs.EXAMPLES_KEY: [examples]}

    # Create exec proterties.
    exec_properties = {
        standard_component_specs.INPUT_BASE_KEY:
            self._input_data_dir,
        standard_component_specs.INPUT_CONFIG_KEY:
            proto_utils.proto_to_json(
                example_gen_pb2.Input(splits=[
                    example_gen_pb2.Input.Split(name='csv', pattern='csv/*'),
                ])),
        standard_component_specs.OUTPUT_CONFIG_KEY:
            proto_utils.proto_to_json(
                example_gen_pb2.Output(
                    split_config=example_gen_pb2.SplitConfig(splits=[
                        example_gen_pb2.SplitConfig.Split(
                            name='train', hash_buckets=2),
                        example_gen_pb2.SplitConfig.Split(
                            name='eval', hash_buckets=1)
                    ])))
    }

    # Run executor.
    csv_example_gen = executor.Executor()
    csv_example_gen.Do({}, output_dict, exec_properties)

    self.assertEqual(
        artifact_utils.encode_split_names(['train', 'eval']),
        examples.split_names)

    # Check CSV example gen outputs.
    train_output_file = os.path.join(examples.uri, 'Split-train',
                                     'data_tfrecord-00000-of-00001.gz')
    eval_output_file = os.path.join(examples.uri, 'Split-eval',
                                    'data_tfrecord-00000-of-00001.gz')
    self.assertTrue(fileio.exists(train_output_file))
    self.assertTrue(fileio.exists(eval_output_file))
    self.assertGreater(
        fileio.open(train_output_file).size(),
        fileio.open(eval_output_file).size())


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag517')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/custom_executors/avro_executor_test.py: 56-106
</a>
<div class="mid" id="frag517" style="display:none"><pre>
  def testDo(self):
    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    # Create output dict.
    examples = standard_artifacts.Examples()
    examples.uri = output_data_dir
    output_dict = {standard_component_specs.EXAMPLES_KEY: [examples]}

    # Create exec proterties.
    exec_properties = {
        standard_component_specs.INPUT_BASE_KEY:
            self._input_data_dir,
        standard_component_specs.INPUT_CONFIG_KEY:
            proto_utils.proto_to_json(
                example_gen_pb2.Input(splits=[
                    example_gen_pb2.Input.Split(
                        name='avro', pattern='avro/*.avro'),
                ])),
        standard_component_specs.OUTPUT_CONFIG_KEY:
            proto_utils.proto_to_json(
                example_gen_pb2.Output(
                    split_config=example_gen_pb2.SplitConfig(splits=[
                        example_gen_pb2.SplitConfig.Split(
                            name='train', hash_buckets=2),
                        example_gen_pb2.SplitConfig.Split(
                            name='eval', hash_buckets=1)
                    ])))
    }

    # Run executor.
    avro_example_gen = avro_executor.Executor()
    avro_example_gen.Do({}, output_dict, exec_properties)

    self.assertEqual(
        artifact_utils.encode_split_names(['train', 'eval']),
        examples.split_names)

    # Check Avro example gen outputs.
    train_output_file = os.path.join(examples.uri, 'Split-train',
                                     'data_tfrecord-00000-of-00001.gz')
    eval_output_file = os.path.join(examples.uri, 'Split-eval',
                                    'data_tfrecord-00000-of-00001.gz')
    self.assertTrue(fileio.exists(train_output_file))
    self.assertTrue(fileio.exists(eval_output_file))
    self.assertGreater(
        fileio.open(train_output_file).size(),
        fileio.open(eval_output_file).size())


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 30:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag529')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/component_test.py: 158-176
</a>
<div class="mid" id="frag529" style="display:none"><pre>
  def testConstructWithOutputConfig(self):
    output_config = example_gen_pb2.Output(
        split_config=example_gen_pb2.SplitConfig(splits=[
            example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=2),
            example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1),
            example_gen_pb2.SplitConfig.Split(name='test', hash_buckets=1)
        ]))
    example_gen = TestFileBasedExampleGenComponent(
        input_base='path', output_config=output_config)
    self.assertEqual(
        standard_artifacts.Examples.TYPE_NAME,
        example_gen.outputs[standard_component_specs.EXAMPLES_KEY].type_name)

    stored_output_config = example_gen_pb2.Output()
    proto_utils.json_to_proto(
        example_gen.exec_properties[standard_component_specs.OUTPUT_CONFIG_KEY],
        stored_output_config)
    self.assertEqual(output_config, stored_output_config)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag530')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/component_test.py: 177-194
</a>
<div class="mid" id="frag530" style="display:none"><pre>
  def testConstructWithInputConfig(self):
    input_config = example_gen_pb2.Input(splits=[
        example_gen_pb2.Input.Split(name='train', pattern='train/*'),
        example_gen_pb2.Input.Split(name='eval', pattern='eval/*'),
        example_gen_pb2.Input.Split(name='test', pattern='test/*')
    ])
    example_gen = TestFileBasedExampleGenComponent(
        input_base='path', input_config=input_config)
    self.assertEqual(
        standard_artifacts.Examples.TYPE_NAME,
        example_gen.outputs[standard_component_specs.EXAMPLES_KEY].type_name)

    stored_input_config = example_gen_pb2.Input()
    proto_utils.json_to_proto(
        example_gen.exec_properties[standard_component_specs.INPUT_CONFIG_KEY],
        stored_input_config)
    self.assertEqual(input_config, stored_input_config)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 31:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag531')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/component_test.py: 195-208
</a>
<div class="mid" id="frag531" style="display:none"><pre>
  def testConstructWithCustomConfig(self):
    custom_config = example_gen_pb2.CustomConfig(custom_config=any_pb2.Any())
    example_gen = component.FileBasedExampleGen(
        input_base='path',
        custom_config=custom_config,
        custom_executor_spec=executor_spec.BeamExecutorSpec(
            TestExampleGenExecutor))

    stored_custom_config = example_gen_pb2.CustomConfig()
    proto_utils.json_to_proto(
        example_gen.exec_properties[standard_component_specs.CUSTOM_CONFIG_KEY],
        stored_custom_config)
    self.assertEqual(custom_config, stored_custom_config)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag532')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/component_test.py: 209-224
</a>
<div class="mid" id="frag532" style="display:none"><pre>
  def testConstructWithStaticRangeConfig(self):
    range_config = range_config_pb2.RangeConfig(
        static_range=range_config_pb2.StaticRange(
            start_span_number=1, end_span_number=1))
    example_gen = component.FileBasedExampleGen(
        input_base='path',
        range_config=range_config,
        custom_executor_spec=executor_spec.BeamExecutorSpec(
            TestExampleGenExecutor))
    stored_range_config = range_config_pb2.RangeConfig()
    proto_utils.json_to_proto(
        example_gen.exec_properties[standard_component_specs.RANGE_CONFIG_KEY],
        stored_range_config)
    self.assertEqual(range_config, stored_range_config)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 32:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag541')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/write_split_test.py: 34-58
</a>
<div class="mid" id="frag541" style="display:none"><pre>
  def testWriteSplitCounter_WithFormatUnspecified(self):
    count = 10

    def Pipeline(root):
      data = [tf.train.Example()] * count
      _ = (
          root
          | beam.Create(data)
          | write_split.WriteSplit(self._output_data_dir,
                                   example_gen_pb2.FILE_FORMAT_UNSPECIFIED))

    run_result = direct_runner.DirectRunner().run(Pipeline)
    run_result.wait_until_finish()

    num_instances = run_result.metrics().query(
        MetricsFilter().with_name('num_instances'))

    self.assertTrue(
        fileio.exists(
            os.path.join(self._output_data_dir,
                         'data_tfrecord-00000-of-00001.gz')))
    self.assertTrue(num_instances['counters'])
    self.assertEqual(len(num_instances['counters']), 1)
    self.assertEqual(num_instances['counters'][0].result, count)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag543')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/write_split_test.py: 59-84
</a>
<div class="mid" id="frag543" style="display:none"><pre>
  def testWriteSplitCounter_WithTFRECORDS_GZIP(self):
    count = 10

    def Pipeline(root):
      data = [tf.train.Example()] * count
      _ = (
          root
          | beam.Create(data)
          | write_split.WriteSplit(self._output_data_dir,
                                   example_gen_pb2.FORMAT_TFRECORDS_GZIP))

    run_result = direct_runner.DirectRunner().run(Pipeline)
    run_result.wait_until_finish()

    num_instances = run_result.metrics().query(
        MetricsFilter().with_name('num_instances'))

    self.assertTrue(
        fileio.exists(
            os.path.join(self._output_data_dir,
                         'data_tfrecord-00000-of-00001.gz')))
    self.assertTrue(num_instances['counters'])
    self.assertEqual(len(num_instances['counters']), 1)
    self.assertEqual(num_instances['counters'][0].result, count)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 33:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag556')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/import_example_gen/component.py: 42-74
</a>
<div class="mid" id="frag556" style="display:none"><pre>
  def __init__(
      self,
      input_base: Optional[str] = None,
      input_config: Optional[Union[example_gen_pb2.Input,
                                   data_types.RuntimeParameter]] = None,
      output_config: Optional[Union[example_gen_pb2.Output,
                                    data_types.RuntimeParameter]] = None,
      range_config: Optional[Union[range_config_pb2.RangeConfig,
                                   data_types.RuntimeParameter]] = None,
      payload_format: Optional[int] = example_gen_pb2.FORMAT_TF_EXAMPLE):
    """Construct an ImportExampleGen component.

    Args:
      input_base: an external directory containing the TFRecord files.
      input_config: An example_gen_pb2.Input instance, providing input
        configuration. If unset, the files under input_base will be treated as a
        single split.
      output_config: An example_gen_pb2.Output instance, providing output
        configuration. If unset, default splits will be 'train' and 'eval' with
        size 2:1.
      range_config: An optional range_config_pb2.RangeConfig instance,
        specifying the range of span values to consider. If unset, driver will
        default to searching for latest span with no restrictions.
      payload_format: Payload format of input data. Should be one of
        example_gen_pb2.PayloadFormat enum. Note that payload format of output
        data is the same as input.
    """
    super().__init__(
        input_base=input_base,
        input_config=input_config,
        output_config=output_config,
        range_config=range_config,
        output_data_format=payload_format)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag781')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_big_query/example_gen/component.py: 40-76
</a>
<div class="mid" id="frag781" style="display:none"><pre>
  def __init__(
      self,
      query: Optional[str] = None,
      input_config: Optional[Union[example_gen_pb2.Input,
                                   data_types.RuntimeParameter]] = None,
      output_config: Optional[Union[example_gen_pb2.Output,
                                    data_types.RuntimeParameter]] = None,
      range_config: Optional[Union[range_config_pb2.RangeConfig,
                                   data_types.RuntimeParameter]] = None):
    """Constructs a BigQueryExampleGen component.

    Args:
      query: BigQuery sql string, query result will be treated as a single
        split, can be overwritten by input_config.
      input_config: An example_gen_pb2.Input instance with Split.pattern as
        BigQuery sql string. If set, it overwrites the 'query' arg, and allows
        different queries per split. If any field is provided as a
        RuntimeParameter, input_config should be constructed as a dict with the
        same field names as Input proto message.
      output_config: An example_gen_pb2.Output instance, providing output
        configuration. If unset, default splits will be 'train' and 'eval' with
        size 2:1. If any field is provided as a RuntimeParameter,
        input_config should be constructed as a dict with the same field names
        as Output proto message.
      range_config: An optional range_config_pb2.RangeConfig instance,
        specifying the range of span values to consider.

    Raises:
      RuntimeError: Only one of query and input_config should be set.
    """
    if bool(query) == bool(input_config):
      raise RuntimeError('Exactly one of query and input_config should be set.')
    input_config = input_config or utils.make_default_input_config(query)
    super().__init__(
        input_config=input_config,
        output_config=output_config,
        range_config=range_config)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag598')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/csv_example_gen/component.py: 64-91
</a>
<div class="mid" id="frag598" style="display:none"><pre>
  def __init__(
      self,
      input_base: Optional[str] = None,
      input_config: Optional[Union[example_gen_pb2.Input,
                                   data_types.RuntimeParameter]] = None,
      output_config: Optional[Union[example_gen_pb2.Output,
                                    data_types.RuntimeParameter]] = None,
      range_config: Optional[Union[range_config_pb2.RangeConfig,
                                   data_types.RuntimeParameter]] = None):
    """Construct a CsvExampleGen component.

    Args:
      input_base: an external directory containing the CSV files.
      input_config: An example_gen_pb2.Input instance, providing input
        configuration. If unset, the files under input_base will be treated as a
        single split.
      output_config: An example_gen_pb2.Output instance, providing output
        configuration. If unset, default splits will be 'train' and 'eval' with
        size 2:1.
      range_config: An optional range_config_pb2.RangeConfig instance,
        specifying the range of span values to consider. If unset, driver will
        default to searching for latest span with no restrictions.
    """
    super().__init__(
        input_base=input_base,
        input_config=input_config,
        output_config=output_config,
        range_config=range_config)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 34:</b> &nbsp; 2 fragments, nominal size 28 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag569')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/component.py: 58-115
</a>
<div class="mid" id="frag569" style="display:none"><pre>
  def __init__(
      self,
      input_config: Union[example_gen_pb2.Input, data_types.RuntimeParameter],
      output_config: Optional[Union[example_gen_pb2.Output,
                                    data_types.RuntimeParameter]] = None,
      custom_config: Optional[Union[example_gen_pb2.CustomConfig,
                                    data_types.RuntimeParameter]] = None,
      range_config: Optional[Union[range_config_pb2.RangeConfig,
                                   data_types.RuntimeParameter]] = None,
      output_data_format: Optional[int] = example_gen_pb2.FORMAT_TF_EXAMPLE,
      output_file_format: Optional[int] = example_gen_pb2.FORMAT_TFRECORDS_GZIP,
  ):
    """Construct a QueryBasedExampleGen component.

    Args:
      input_config: An
        [example_gen_pb2.Input](https://github.com/tensorflow/tfx/blob/master/tfx/proto/example_gen.proto)
        instance, providing input configuration. _required_
      output_config: An
        [example_gen_pb2.Output](https://github.com/tensorflow/tfx/blob/master/tfx/proto/example_gen.proto)
        instance, providing output configuration. If unset, the default splits
        will be labeled as 'train' and 'eval' with a distribution ratio of 2:1.
      custom_config: An
        [example_gen_pb2.CustomConfig](https://github.com/tensorflow/tfx/blob/master/tfx/proto/example_gen.proto)
        instance, providing custom configuration for ExampleGen.
      range_config: An optional range_config_pb2.RangeConfig instance,
        specifying the range of span values to consider.
      output_data_format: Payload format of generated data in output artifact,
        one of example_gen_pb2.PayloadFormat enum.
      output_file_format: File format of generated data in output artifact,
          one of example_gen_pb2.FileFormat enum.

    Raises:
      ValueError: The output_data_format, output_file_format value
        must be defined in the example_gen_pb2.PayloadFormat proto.
    """
    # Configure outputs.
    output_config = output_config or utils.make_default_output_config(
        input_config)
    example_artifacts = types.Channel(type=standard_artifacts.Examples)
    if output_data_format not in example_gen_pb2.PayloadFormat.values():
      raise ValueError('The value of output_data_format must be defined in'
                       'the example_gen_pb2.PayloadFormat proto.')
    if output_file_format not in example_gen_pb2.FileFormat.values():
      raise ValueError('The value of output_file_format must be defined in'
                       'the example_gen_pb2.FileFormat proto.')

    spec = standard_component_specs.QueryBasedExampleGenSpec(
        input_config=input_config,
        output_config=output_config,
        range_config=range_config,
        output_data_format=output_data_format,
        output_file_format=output_file_format,
        custom_config=custom_config,
        examples=example_artifacts)
    super().__init__(spec=spec)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag570')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/component.py: 145-196
</a>
<div class="mid" id="frag570" style="display:none"><pre>
  def __init__(
      self,
      input_base: Optional[str] = None,
      input_config: Optional[Union[example_gen_pb2.Input,
                                   data_types.RuntimeParameter]] = None,
      output_config: Optional[Union[example_gen_pb2.Output,
                                    data_types.RuntimeParameter]] = None,
      custom_config: Optional[Union[example_gen_pb2.CustomConfig,
                                    data_types.RuntimeParameter]] = None,
      range_config: Optional[Union[range_config_pb2.RangeConfig,
                                   data_types.RuntimeParameter]] = None,
      output_data_format: Optional[int] = example_gen_pb2.FORMAT_TF_EXAMPLE,
      output_file_format: Optional[int] = example_gen_pb2.FORMAT_TFRECORDS_GZIP,
      custom_executor_spec: Optional[executor_spec.ExecutorSpec] = None):
    """Construct a FileBasedExampleGen component.

    Args:
      input_base: an external directory containing the data files.
      input_config: An
        [`example_gen_pb2.Input`](https://github.com/tensorflow/tfx/blob/master/tfx/proto/example_gen.proto)
          instance, providing input configuration. If unset, input files will be
          treated as a single split.
      output_config: An example_gen_pb2.Output instance, providing the output
        configuration. If unset, default splits will be 'train' and
        'eval' with size 2:1.
      custom_config: An optional example_gen_pb2.CustomConfig instance,
        providing custom configuration for executor.
      range_config: An optional range_config_pb2.RangeConfig instance,
        specifying the range of span values to consider. If unset, driver will
        default to searching for latest span with no restrictions.
      output_data_format: Payload format of generated data in output artifact,
        one of example_gen_pb2.PayloadFormat enum.
      output_file_format: File format of generated data in output artifact,
        one of example_gen_pb2.FileFormat enum.
      custom_executor_spec: Optional custom executor spec overriding the default
        executor spec specified in the component attribute.
    """
    # Configure inputs and outputs.
    input_config = input_config or utils.make_default_input_config()
    output_config = output_config or utils.make_default_output_config(
        input_config)
    example_artifacts = types.Channel(type=standard_artifacts.Examples)
    spec = standard_component_specs.FileBasedExampleGenSpec(
        input_base=input_base,
        input_config=input_config,
        output_config=output_config,
        custom_config=custom_config,
        range_config=range_config,
        output_data_format=output_data_format,
        output_file_format=output_file_format,
        examples=example_artifacts)
    super().__init__(spec=spec, custom_executor_spec=custom_executor_spec)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 35:</b> &nbsp; 7 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag610')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/utils_test.py: 267-279
</a>
<div class="mid" id="frag610" style="display:none"><pre>
  def testVersionWrongFormat(self):
    wrong_version = os.path.join(self._input_base_path, 'span01', 'versionx',
                                 'split1', 'data')
    io_utils.write_string_file(wrong_version, 'testing_wrong_version')

    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='span{SPAN}/version{VERSION}/split1/*')
    ]
    with self.assertRaisesRegex(ValueError, 'Cannot find version number'):
      utils.calculate_splits_fingerprint_span_and_version(
          self._input_base_path, splits)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag619')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/utils_test.py: 399-413
</a>
<div class="mid" id="frag619" style="display:none"><pre>
  def testDateBadFormat(self):
    # Test improperly formed date.
    split1 = os.path.join(self._input_base_path, 'yyyymmdd', 'split1', 'data')
    io_utils.write_string_file(split1, 'testing')

    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='{YYYY}{MM}{DD}/split1/*')
    ]

    with self.assertRaisesRegex(ValueError,
                                'Cannot find span number using date'):
      utils.calculate_splits_fingerprint_span_and_version(
          self._input_base_path, splits)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag621')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/utils_test.py: 427-441
</a>
<div class="mid" id="frag621" style="display:none"><pre>
  def testHaveDateNoVersion(self):
    # Test specific behavior when Date spec is present but Version is not.
    split1 = os.path.join(self._input_base_path, '19700102', 'split1', 'data')
    io_utils.write_string_file(split1, 'testing')

    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='{YYYY}{MM}{DD}/split1/*')
    ]

    _, span, version = utils.calculate_splits_fingerprint_span_and_version(
        self._input_base_path, splits)
    self.assertEqual(span, 1)
    self.assertIsNone(version)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag627')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/utils_test.py: 528-541
</a>
<div class="mid" id="frag627" style="display:none"><pre>
  def testSpanVersionWidthNoSeperator(self):
    split1 = os.path.join(self._input_base_path, '1234', 'split1', 'data')
    io_utils.write_string_file(split1, 'testing')

    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='{SPAN:2}{VERSION:2}/split1/*')
    ]

    _, span, version = utils.calculate_splits_fingerprint_span_and_version(
        self._input_base_path, splits)
    self.assertEqual(span, 12)
    self.assertEqual(version, 34)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag622')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/utils_test.py: 442-457
</a>
<div class="mid" id="frag622" style="display:none"><pre>
  def testHaveDateAndVersion(self):
    # Test specific behavior when both Date and Version are present.
    split1 = os.path.join(self._input_base_path, '19700102', 'ver1', 'split1',
                          'data')
    io_utils.write_string_file(split1, 'testing')

    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='{YYYY}{MM}{DD}/ver{VERSION}/split1/*')
    ]

    _, span, version = utils.calculate_splits_fingerprint_span_and_version(
        self._input_base_path, splits)
    self.assertEqual(span, 1)
    self.assertEqual(version, 1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag613')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/utils_test.py: 320-335
</a>
<div class="mid" id="frag613" style="display:none"><pre>
  def testHaveSpanAndVersion(self):
    # Test specific behavior when both Span and Version are present.
    split1 = os.path.join(self._input_base_path, 'span1', 'version1', 'split1',
                          'data')
    io_utils.write_string_file(split1, 'testing')

    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='span{SPAN}/version{VERSION}/split1/*')
    ]

    _, span, version = utils.calculate_splits_fingerprint_span_and_version(
        self._input_base_path, splits)
    self.assertEqual(span, 1)
    self.assertEqual(version, 1)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag616')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/utils_test.py: 360-378
</a>
<div class="mid" id="frag616" style="display:none"><pre>
  def testNewSpanWithOlderVersionAlign(self):
    # Test specific behavior when a newer Span has older Version.
    span1_ver2 = os.path.join(self._input_base_path, 'span1', 'ver2', 'split1',
                              'data')
    io_utils.write_string_file(span1_ver2, 'testing')
    span2_ver1 = os.path.join(self._input_base_path, 'span2', 'ver1', 'split1',
                              'data')
    io_utils.write_string_file(span2_ver1, 'testing')

    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='span{SPAN}/ver{VERSION}/split1/*')
    ]

    _, span, version = utils.calculate_splits_fingerprint_span_and_version(
        self._input_base_path, splits)
    self.assertEqual(span, 2)
    self.assertEqual(version, 1)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 36:</b> &nbsp; 2 fragments, nominal size 48 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag628')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/utils_test.py: 542-602
</a>
<div class="mid" id="frag628" style="display:none"><pre>
  def testCalculateSplitsFingerprintSpanAndVersionWithSpan(self):
    # Test align of span and version numbers.
    span1_v1_split1 = os.path.join(self._input_base_path, 'span01', 'ver01',
                                   'split1', 'data')
    io_utils.write_string_file(span1_v1_split1, 'testing11')
    span1_v1_split2 = os.path.join(self._input_base_path, 'span01', 'ver01',
                                   'split2', 'data')
    io_utils.write_string_file(span1_v1_split2, 'testing12')
    span2_v1_split1 = os.path.join(self._input_base_path, 'span02', 'ver01',
                                   'split1', 'data')
    io_utils.write_string_file(span2_v1_split1, 'testing21')

    # Test if error raised when span does not align.
    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='span{SPAN}/ver{VERSION}/split1/*'),
        example_gen_pb2.Input.Split(
            name='s2', pattern='span{SPAN}/ver{VERSION}/split2/*')
    ]
    with self.assertRaisesRegex(
        ValueError, 'Latest span should be the same for each split'):
      utils.calculate_splits_fingerprint_span_and_version(
          self._input_base_path, splits)

    span2_v1_split2 = os.path.join(self._input_base_path, 'span02', 'ver01',
                                   'split2', 'data')
    io_utils.write_string_file(span2_v1_split2, 'testing22')
    span2_v2_split1 = os.path.join(self._input_base_path, 'span02', 'ver02',
                                   'split1', 'data')
    io_utils.write_string_file(span2_v2_split1, 'testing21')

    # Test if error raised when span aligns but version does not.
    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='span{SPAN}/ver{VERSION}/split1/*'),
        example_gen_pb2.Input.Split(
            name='s2', pattern='span{SPAN}/ver{VERSION}/split2/*')
    ]
    with self.assertRaisesRegex(
        ValueError, 'Latest version should be the same for each split'):
      utils.calculate_splits_fingerprint_span_and_version(
          self._input_base_path, splits)

    span2_v2_split2 = os.path.join(self._input_base_path, 'span02', 'ver02',
                                   'split2', 'data')
    io_utils.write_string_file(span2_v2_split2, 'testing22')

    # Test if latest span and version is selected when aligned for each split.
    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='span{SPAN}/ver{VERSION}/split1/*'),
        example_gen_pb2.Input.Split(
            name='s2', pattern='span{SPAN}/ver{VERSION}/split2/*')
    ]
    _, span, version = utils.calculate_splits_fingerprint_span_and_version(
        self._input_base_path, splits)
    self.assertEqual(span, 2)
    self.assertEqual(version, 2)
    self.assertEqual(splits[0].pattern, 'span02/ver02/split1/*')
    self.assertEqual(splits[1].pattern, 'span02/ver02/split2/*')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag629')" href="javascript:;">
tfx-1.6.0/tfx/components/example_gen/utils_test.py: 603-662
</a>
<div class="mid" id="frag629" style="display:none"><pre>
  def testCalculateSplitsFingerprintSpanAndVersionWithDate(self):
    # Test align of span and version numbers.
    span1_v1_split1 = os.path.join(self._input_base_path, '19700102', 'ver01',
                                   'split1', 'data')
    io_utils.write_string_file(span1_v1_split1, 'testing11')
    span1_v1_split2 = os.path.join(self._input_base_path, '19700102', 'ver01',
                                   'split2', 'data')
    io_utils.write_string_file(span1_v1_split2, 'testing12')
    span2_v1_split1 = os.path.join(self._input_base_path, '19700103', 'ver01',
                                   'split1', 'data')
    io_utils.write_string_file(span2_v1_split1, 'testing21')

    # Test if error raised when date does not align.
    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='{YYYY}{MM}{DD}/ver{VERSION}/split1/*'),
        example_gen_pb2.Input.Split(
            name='s2', pattern='{YYYY}{MM}{DD}/ver{VERSION}/split2/*')
    ]
    with self.assertRaisesRegex(
        ValueError, 'Latest span should be the same for each split'):
      utils.calculate_splits_fingerprint_span_and_version(
          self._input_base_path, splits)

    span2_v1_split2 = os.path.join(self._input_base_path, '19700103', 'ver01',
                                   'split2', 'data')
    io_utils.write_string_file(span2_v1_split2, 'testing22')
    span2_v2_split1 = os.path.join(self._input_base_path, '19700103', 'ver02',
                                   'split1', 'data')
    io_utils.write_string_file(span2_v2_split1, 'testing21')

    # Test if error raised when date aligns but version does not.
    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='{YYYY}{MM}{DD}/ver{VERSION}/split1/*'),
        example_gen_pb2.Input.Split(
            name='s2', pattern='{YYYY}{MM}{DD}/ver{VERSION}/split2/*')
    ]
    with self.assertRaisesRegex(
        ValueError, 'Latest version should be the same for each split'):
      utils.calculate_splits_fingerprint_span_and_version(
          self._input_base_path, splits)
    span2_v2_split2 = os.path.join(self._input_base_path, '19700103', 'ver02',
                                   'split2', 'data')
    io_utils.write_string_file(span2_v2_split2, 'testing22')

    # Test if latest span and version is selected when aligned for each split.
    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='{YYYY}{MM}{DD}/ver{VERSION}/split1/*'),
        example_gen_pb2.Input.Split(
            name='s2', pattern='{YYYY}{MM}{DD}/ver{VERSION}/split2/*')
    ]
    _, span, version = utils.calculate_splits_fingerprint_span_and_version(
        self._input_base_path, splits)
    self.assertEqual(span, 2)
    self.assertEqual(version, 2)
    self.assertEqual(splits[0].pattern, '19700103/ver02/split1/*')
    self.assertEqual(splits[1].pattern, '19700103/ver02/split2/*')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 37:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag678')" href="javascript:;">
tfx-1.6.0/tfx/types/artifact_test.py: 947-963
</a>
<div class="mid" id="frag678" style="display:none"><pre>
  def testTypeAnnotationIsNone(self):
    my_artifact_1 = _MyArtifact()
    self.assertIsNone(my_artifact_1.TYPE_ANNOTATION)
    self.assertEqual(my_artifact_1.artifact_type.base_type,
                     metadata_store_pb2.ArtifactType.UNSET)

    # _MyArtifact2/_MyArtifact3 classes are created from _ArtifactType method.
    my_artifact_2 = _MyArtifact2()
    self.assertIsNone(my_artifact_2.TYPE_ANNOTATION)
    self.assertEqual(my_artifact_2.artifact_type.base_type,
                     metadata_store_pb2.ArtifactType.UNSET)

    my_artifact_3 = _MyArtifact3()
    self.assertIsNone(my_artifact_3.TYPE_ANNOTATION)
    self.assertEqual(my_artifact_3.artifact_type.base_type,
                     metadata_store_pb2.ArtifactType.UNSET)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag679')" href="javascript:;">
tfx-1.6.0/tfx/types/artifact_test.py: 964-977
</a>
<div class="mid" id="frag679" style="display:none"><pre>
  def testArtifactTypeWithTypeAnnotation(self):
    my_artifact_with_annotation_1 = _MyArtifact4()
    self.assertEqual(my_artifact_with_annotation_1.artifact_type.base_type,
                     metadata_store_pb2.ArtifactType.DATASET)

    # _MyArtifact5/_MyArtifact6 classes are created from _ArtifactType method.
    my_artifact_with_annotation_2 = _MyArtifact5()
    self.assertEqual(my_artifact_with_annotation_2.artifact_type.base_type,
                     metadata_store_pb2.ArtifactType.DATASET)

    my_artifact_with_annotation_3 = _MyArtifact6()
    self.assertEqual(my_artifact_with_annotation_3.artifact_type.base_type,
                     metadata_store_pb2.ArtifactType.DATASET)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 38:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag755')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_big_query/experimental/elwc_example_gen/component/component.py: 35-78
</a>
<div class="mid" id="frag755" style="display:none"><pre>
  def __init__(self,
               query: Optional[str] = None,
               elwc_config: Optional[elwc_config_pb2.ElwcConfig] = None,
               input_config: Optional[example_gen_pb2.Input] = None,
               output_config: Optional[example_gen_pb2.Output] = None):
    """Constructs a BigQueryElwcExampleGen component.

    Args:
      query: BigQuery sql string, query result will be treated as a single
        split, can be overwritten by input_config.
      elwc_config: The elwc config contains a list of context feature fields.
        The fields are used to build context feature. Examples with the same
        context feature will be converted to an ELWC(ExampleListWithContext)
        instance. For example, when there are two examples with the same context
        field, the two examples will be intergrated to a ELWC instance.
      input_config: An example_gen_pb2.Input instance with Split.pattern as
        BigQuery sql string. If set, it overwrites the 'query' arg, and allows
        different queries per split. If any field is provided as a
        RuntimeParameter, input_config should be constructed as a dict with the
        same field names as Input proto message.
      output_config: An example_gen_pb2.Output instance, providing output
        configuration. If unset, default splits will be 'train' and 'eval' with
        size 2:1. If any field is provided as a RuntimeParameter, input_config
          should be constructed as a dict with the same field names as Output
          proto message.

    Raises:
      RuntimeError: Only one of query and input_config should be set and
        elwc_config is required.
    """

    if bool(query) == bool(input_config):
      raise RuntimeError('Exactly one of query and input_config should be set.')
    if not elwc_config:
      raise RuntimeError(
          'elwc_config is required for BigQueryToElwcExampleGen.')
    input_config = input_config or utils.make_default_input_config(query)
    packed_custom_config = example_gen_pb2.CustomConfig()
    packed_custom_config.custom_config.Pack(elwc_config)
    super().__init__(
        input_config=input_config,
        output_config=output_config,
        output_data_format=example_gen_pb2.FORMAT_PROTO,
        custom_config=packed_custom_config)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3033')" href="javascript:;">
tfx-1.6.0/tfx/examples/custom_components/presto_example_gen/presto_component/component.py: 39-78
</a>
<div class="mid" id="frag3033" style="display:none"><pre>
  def __init__(self,
               conn_config: presto_config_pb2.PrestoConnConfig,
               query: Optional[str] = None,
               input_config: Optional[example_gen_pb2.Input] = None,
               output_config: Optional[example_gen_pb2.Output] = None):
    """Constructs a PrestoExampleGen component.

    Args:
      conn_config: Parameters for Presto connection client.
      query: Presto sql string, query result will be treated as a single split,
        can be overwritten by input_config.
      input_config: An example_gen_pb2.Input instance with Split.pattern as
        Presto sql string. If set, it overwrites the 'query' arg, and allows
        different queries per split.
      output_config: An example_gen_pb2.Output instance, providing output
        configuration. If unset, default splits will be 'train' and 'eval' with
        size 2:1.

    Raises:
      RuntimeError: Only one of query and input_config should be set. Or
      required host field in connection_config should be set.
    """
    if bool(query) == bool(input_config):
      raise RuntimeError('Exactly one of query and input_config should be set.')
    if not bool(conn_config.host):
      raise RuntimeError(
          'Required host field in connection config should be set.')

    input_config = input_config or utils.make_default_input_config(query)

    packed_custom_config = example_gen_pb2.CustomConfig()
    packed_custom_config.custom_config.Pack(conn_config)

    output_config = output_config or utils.make_default_output_config(
        input_config)

    super().__init__(
        input_config=input_config,
        output_config=output_config,
        custom_config=packed_custom_config)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 39:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag765')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_big_query/pusher/component.py: 34-57
</a>
<div class="mid" id="frag765" style="display:none"><pre>
  def __init__(self,
               model: Optional[types.Channel] = None,
               model_blessing: Optional[types.Channel] = None,
               infra_blessing: Optional[types.Channel] = None,
               custom_config: Optional[Dict[str, Any]] = None):
    """Construct a Pusher component.

    Args:
      model: An optional Channel of type `standard_artifacts.Model`, usually
        produced by a Trainer component.
      model_blessing: An optional Channel of type
        `standard_artifacts.ModelBlessing`, usually produced from an Evaluator
        component.
      infra_blessing: An optional Channel of type
        `standard_artifacts.InfraBlessing`, usually produced from an
        InfraValidator component.
      custom_config: A dict which contains the deployment job parameters to be
        passed to Cloud platforms.
    """
    super().__init__(
        model=model,
        model_blessing=model_blessing,
        infra_blessing=infra_blessing,
        custom_config=custom_config)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag883')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/pusher/component.py: 29-53
</a>
<div class="mid" id="frag883" style="display:none"><pre>
  def __init__(self,
               model: Optional[types.Channel] = None,
               model_blessing: Optional[types.Channel] = None,
               infra_blessing: Optional[types.Channel] = None,
               custom_config: Optional[Dict[str, Any]] = None):
    """Construct a Pusher component.

    Args:
      model: An optional Channel of type `standard_artifacts.Model`, usually
        produced by a Trainer component, representing the model used for
        training.
      model_blessing: An optional Channel of type
        `standard_artifacts.ModelBlessing`, usually produced from an Evaluator
        component, containing the blessing model.
      infra_blessing: An optional Channel of type
        `standard_artifacts.InfraBlessing`, usually produced from an
        InfraValidator component, containing the validation result.
      custom_config: A dict which contains the deployment job parameters to be
        passed to Cloud platforms.
    """
    super().__init__(
        model=model,
        model_blessing=model_blessing,
        infra_blessing=infra_blessing,
        custom_config=custom_config)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 40:</b> &nbsp; 2 fragments, nominal size 57 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag793')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/bulk_inferrer/executor_test.py: 76-139
</a>
<div class="mid" id="frag793" style="display:none"><pre>
  def testDoWithBlessedModel(self, mock_runner, mock_run_model_inference, _):
    input_dict = {
        'examples': [self._examples],
        'model': [self._model],
        'model_blessing': [self._model_blessing],
    }
    output_dict = {
        'inference_result': [self._inference_result],
    }
    ai_platform_serving_args = {
        'model_name': 'model_name',
        'project_id': 'project_id'
    }
    # Create exe properties.
    exec_properties = {
        'data_spec':
            proto_utils.proto_to_json(bulk_inferrer_pb2.DataSpec()),
        'custom_config':
            json_utils.dumps({
                executor.SERVING_ARGS_KEY:
                    ai_platform_serving_args,
                constants.ENDPOINT_ARGS_KEY:
                    'https://us-central1-ml.googleapis.com',
            }),
    }
    mock_runner.get_service_name_and_api_version.return_value = ('ml', 'v1')
    mock_runner.create_model_for_aip_prediction_if_not_exist.return_value = True

    # Run executor.
    bulk_inferrer = executor.Executor(self._context)
    bulk_inferrer.Do(input_dict, output_dict, exec_properties)

    ai_platform_prediction_model_spec = (
        model_spec_pb2.AIPlatformPredictionModelSpec(
            project_id='project_id',
            model_name='model_name',
            version_name=self._model_version))
    ai_platform_prediction_model_spec.use_serialization_config = True
    inference_endpoint = model_spec_pb2.InferenceSpecType()
    inference_endpoint.ai_platform_prediction_model_spec.CopyFrom(
        ai_platform_prediction_model_spec)
    mock_run_model_inference.assert_called_once_with(mock.ANY, mock.ANY,
                                                     mock.ANY, mock.ANY,
                                                     mock.ANY,
                                                     inference_endpoint)
    executor_class_path = '%s.%s' % (bulk_inferrer.__class__.__module__,
                                     bulk_inferrer.__class__.__name__)
    with telemetry_utils.scoped_labels(
        {telemetry_utils.LABEL_TFX_EXECUTOR: executor_class_path}):
      job_labels = telemetry_utils.make_labels_dict()
    mock_runner.deploy_model_for_aip_prediction.assert_called_once_with(
        serving_path=path_utils.serving_model_path(self._model.uri),
        model_version_name=mock.ANY,
        ai_platform_serving_args=ai_platform_serving_args,
        labels=job_labels,
        api=mock.ANY,
        skip_model_endpoint_creation=True,
        set_default=False)
    mock_runner.delete_model_from_aip_if_exists.assert_called_once_with(
        model_version_name=mock.ANY,
        ai_platform_serving_args=ai_platform_serving_args,
        api=mock.ANY,
        delete_model_endpoint=True)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag794')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/bulk_inferrer/executor_test.py: 145-205
</a>
<div class="mid" id="frag794" style="display:none"><pre>
  def testDoSkippedModelCreation(self, mock_runner, mock_run_model_inference,
                                 _):
    input_dict = {
        'examples': [self._examples],
        'model': [self._model],
        'model_blessing': [self._model_blessing],
    }
    output_dict = {
        'inference_result': [self._inference_result],
    }
    ai_platform_serving_args = {
        'model_name': 'model_name',
        'project_id': 'project_id'
    }
    # Create exe properties.
    exec_properties = {
        'data_spec':
            proto_utils.proto_to_json(bulk_inferrer_pb2.DataSpec()),
        'custom_config':
            json_utils.dumps(
                {executor.SERVING_ARGS_KEY: ai_platform_serving_args}),
    }
    mock_runner.get_service_name_and_api_version.return_value = ('ml', 'v1')
    mock_runner.create_model_for_aip_prediction_if_not_exist.return_value = False

    # Run executor.
    bulk_inferrer = executor.Executor(self._context)
    bulk_inferrer.Do(input_dict, output_dict, exec_properties)

    ai_platform_prediction_model_spec = (
        model_spec_pb2.AIPlatformPredictionModelSpec(
            project_id='project_id',
            model_name='model_name',
            version_name=self._model_version))
    ai_platform_prediction_model_spec.use_serialization_config = True
    inference_endpoint = model_spec_pb2.InferenceSpecType()
    inference_endpoint.ai_platform_prediction_model_spec.CopyFrom(
        ai_platform_prediction_model_spec)
    mock_run_model_inference.assert_called_once_with(mock.ANY, mock.ANY,
                                                     mock.ANY, mock.ANY,
                                                     mock.ANY,
                                                     inference_endpoint)
    executor_class_path = '%s.%s' % (bulk_inferrer.__class__.__module__,
                                     bulk_inferrer.__class__.__name__)
    with telemetry_utils.scoped_labels(
        {telemetry_utils.LABEL_TFX_EXECUTOR: executor_class_path}):
      job_labels = telemetry_utils.make_labels_dict()
    mock_runner.deploy_model_for_aip_prediction.assert_called_once_with(
        serving_path=path_utils.serving_model_path(self._model.uri),
        model_version_name=mock.ANY,
        ai_platform_serving_args=ai_platform_serving_args,
        labels=job_labels,
        api=mock.ANY,
        skip_model_endpoint_creation=True,
        set_default=False)
    mock_runner.delete_model_from_aip_if_exists.assert_called_once_with(
        model_version_name=mock.ANY,
        ai_platform_serving_args=ai_platform_serving_args,
        api=mock.ANY,
        delete_model_endpoint=False)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 41:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag799')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/tuner/component_test.py: 41-52
</a>
<div class="mid" id="frag799" style="display:none"><pre>
  def testConstructWithCustomConfig(self):
    tuner = component.Tuner(
        examples=self.examples,
        schema=self.schema,
        train_args=self.train_args,
        eval_args=self.eval_args,
        tune_args=self.tune_args,
        module_file='/path/to/module/file',
        custom_config=self.custom_config,
    )
    self._verify_output(tuner)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag800')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/tuner/component_test.py: 53-64
</a>
<div class="mid" id="frag800" style="display:none"><pre>
  def testConstructWithoutCustomConfig(self):
    tuner = component.Tuner(
        examples=self.examples,
        schema=self.schema,
        train_args=self.train_args,
        eval_args=self.eval_args,
        tune_args=self.tune_args,
        module_file='/path/to/module/file',
    )
    self._verify_output(tuner)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 42:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag801')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/tuner/executor_test.py: 33-63
</a>
<div class="mid" id="frag801" style="display:none"><pre>
  def setUp(self):
    super().setUp()

    self._output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    self._job_dir = os.path.join(self._output_data_dir, 'jobDir')
    self._project_id = '12345'
    self._job_id = 'fake_job_id'
    self._inputs = {}
    self._outputs = {}
    # Dict format of exec_properties. custom_config needs to be serialized
    # before being passed into Do function.
    self._exec_properties = {
        'custom_config': {
            ai_platform_trainer_executor.JOB_ID_KEY: self._job_id,
            ai_platform_tuner_executor.TUNING_ARGS_KEY: {
                'project': self._project_id,
                'jobDir': self._job_dir,
            },
        },
    }
    self._executor_class_path = '%s.%s' % (
        ai_platform_tuner_executor._WorkerExecutor.__module__,
        ai_platform_tuner_executor._WorkerExecutor.__name__)

    self.addCleanup(mock.patch.stopall)
    self.mock_runner = mock.patch(
        'tfx.extensions.google_cloud_ai_platform.tuner.executor.runner').start(
        )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag895')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/trainer/executor_test.py: 31-62
</a>
<div class="mid" id="frag895" style="display:none"><pre>
  def setUp(self):
    super().setUp()

    self._output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    self._job_dir = os.path.join(self._output_data_dir, 'jobDir')
    self._project_id = '12345'
    self._inputs = {}
    self._outputs = {}
    # Dict format of exec_properties. custom_config needs to be serialized
    # before being passed into Do function.
    self._exec_properties = {
        standard_component_specs.CUSTOM_CONFIG_KEY: {
            ai_platform_trainer_executor.TRAINING_ARGS_KEY: {
                'project': self._project_id,
                'jobDir': self._job_dir,
            },
        },
    }
    self._executor_class_path = '%s.%s' % (
        tfx_trainer_executor.Executor.__module__,
        tfx_trainer_executor.Executor.__name__)
    self._generic_executor_class_path = '%s.%s' % (
        tfx_trainer_executor.GenericExecutor.__module__,
        tfx_trainer_executor.GenericExecutor.__name__)

    self.addCleanup(mock.patch.stopall)
    self.mock_runner = mock.patch(
        'tfx.extensions.google_cloud_ai_platform.trainer.executor.runner'
    ).start()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 43:</b> &nbsp; 5 fragments, nominal size 14 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag807')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/tuner/executor_test.py: 134-150
</a>
<div class="mid" id="frag807" style="display:none"><pre>
  def testDoWithEnableVertexOverride(self):
    executor = ai_platform_tuner_executor.Executor()
    enable_vertex = True
    vertex_region = 'us-central2'
    self._exec_properties[standard_component_specs.CUSTOM_CONFIG_KEY][
        constants.ENABLE_VERTEX_KEY] = enable_vertex
    self._exec_properties[standard_component_specs.CUSTOM_CONFIG_KEY][
        constants.VERTEX_REGION_KEY] = vertex_region
    executor.Do(self._inputs, self._outputs,
                self._serialize_custom_config_under_test())
    self.mock_runner.start_cloud_training.assert_called_with(
        self._inputs, self._outputs, self._serialize_custom_config_under_test(),
        self._executor_class_path, {
            'project': self._project_id,
            'jobDir': self._job_dir,
        }, self._job_id, enable_vertex, vertex_region)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag900')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/trainer/executor_test.py: 106-123
</a>
<div class="mid" id="frag900" style="display:none"><pre>
  def testDoWithEnableVertexOverride(self):
    executor = ai_platform_trainer_executor.Executor()
    enable_vertex = True
    vertex_region = 'us-central2'
    self._exec_properties[standard_component_specs.CUSTOM_CONFIG_KEY][
        ai_platform_trainer_executor.ENABLE_VERTEX_KEY] = enable_vertex
    self._exec_properties[standard_component_specs.CUSTOM_CONFIG_KEY][
        ai_platform_trainer_executor.VERTEX_REGION_KEY] = vertex_region
    executor.Do(self._inputs, self._outputs,
                self._serialize_custom_config_under_test())
    self.mock_runner.start_cloud_training.assert_called_with(
        self._inputs, self._outputs, self._serialize_custom_config_under_test(),
        self._executor_class_path, {
            'project': self._project_id,
            'jobDir': self._job_dir,
        }, None, enable_vertex, vertex_region)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag898')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/trainer/executor_test.py: 81-94
</a>
<div class="mid" id="frag898" style="display:none"><pre>
  def testDoWithJobIdOverride(self):
    executor = ai_platform_trainer_executor.Executor()
    job_id = 'overridden_job_id'
    self._exec_properties[standard_component_specs.CUSTOM_CONFIG_KEY][
        ai_platform_trainer_executor.JOB_ID_KEY] = job_id
    executor.Do(self._inputs, self._outputs,
                self._serialize_custom_config_under_test())
    self.mock_runner.start_cloud_training.assert_called_with(
        self._inputs, self._outputs, self._serialize_custom_config_under_test(),
        self._executor_class_path, {
            'project': self._project_id,
            'jobDir': self._job_dir,
        }, job_id, False, None)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag899')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/trainer/executor_test.py: 95-105
</a>
<div class="mid" id="frag899" style="display:none"><pre>
  def testDoWithGenericExecutorClass(self):
    executor = ai_platform_trainer_executor.GenericExecutor()
    executor.Do(self._inputs, self._outputs,
                self._serialize_custom_config_under_test())
    self.mock_runner.start_cloud_training.assert_called_with(
        self._inputs, self._outputs, self._serialize_custom_config_under_test(),
        self._generic_executor_class_path, {
            'project': self._project_id,
            'jobDir': self._job_dir,
        }, None, False, None)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag897')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/trainer/executor_test.py: 70-80
</a>
<div class="mid" id="frag897" style="display:none"><pre>
  def testDo(self):
    executor = ai_platform_trainer_executor.Executor()
    executor.Do(self._inputs, self._outputs,
                self._serialize_custom_config_under_test())
    self.mock_runner.start_cloud_training.assert_called_with(
        self._inputs, self._outputs, self._serialize_custom_config_under_test(),
        self._executor_class_path, {
            'project': self._project_id,
            'jobDir': self._job_dir,
        }, None, False, None)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 44:</b> &nbsp; 2 fragments, nominal size 28 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag833')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 100-136
</a>
<div class="mid" id="frag833" style="display:none"><pre>
  def testStartCloudTraining(self, mock_discovery):
    mock_discovery.build.return_value = self._mock_api_client
    self._setUpTrainingMocks()

    class_path = 'foo.bar.class'

    runner.start_cloud_training(self._inputs, self._outputs,
                                self._serialize_custom_config_under_test(),
                                class_path, self._training_inputs, None)

    self._mock_create.assert_called_with(
        body=mock.ANY, parent='projects/{}'.format(self._project_id))
    kwargs = self._mock_create.call_args[1]
    body = kwargs['body']

    default_image = 'gcr.io/tfx-oss-public/tfx:{}'.format(
        version_utils.get_image_version())
    self.assertDictContainsSubset(
        {
            'masterConfig': {
                'imageUri':
                    default_image,
                'containerCommand':
                    runner._CONTAINER_COMMAND + [
                        '--executor_class_path', class_path, '--inputs', '{}',
                        '--outputs', '{}', '--exec-properties',
                        ('{"custom_config": '
                         '"{\\"ai_platform_training_args\\": {\\"project\\": \\"12345\\"'
                         '}}"}')
                    ],
            },
        }, body['training_input'])
    self.assertNotIn('project', body['training_input'])
    self.assertStartsWith(body['job_id'], 'tfx_')
    self._mock_get.execute.assert_called_with()
    self._mock_create_request.execute.assert_called_with()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag834')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 139-175
</a>
<div class="mid" id="frag834" style="display:none"><pre>
  def testStartCloudTrainingWithUserContainer(self, mock_discovery):
    mock_discovery.build.return_value = self._mock_api_client
    self._setUpTrainingMocks()

    class_path = 'foo.bar.class'

    self._training_inputs['masterConfig'] = {'imageUri': 'my-custom-image'}
    self._exec_properties['custom_config'][executor.JOB_ID_KEY] = self._job_id
    runner.start_cloud_training(self._inputs, self._outputs,
                                self._serialize_custom_config_under_test(),
                                class_path, self._training_inputs, self._job_id)

    self._mock_create.assert_called_with(
        body=mock.ANY, parent='projects/{}'.format(self._project_id))
    kwargs = self._mock_create.call_args[1]
    body = kwargs['body']
    self.assertDictContainsSubset(
        {
            'masterConfig': {
                'imageUri':
                    'my-custom-image',
                'containerCommand':
                    runner._CONTAINER_COMMAND + [
                        '--executor_class_path', class_path, '--inputs', '{}',
                        '--outputs', '{}', '--exec-properties',
                        ('{"custom_config": '
                         '"{\\"ai_platform_training_args\\": '
                         '{\\"masterConfig\\": {\\"imageUri\\": \\"my-custom-image\\"}, '
                         '\\"project\\": \\"12345\\"}, '
                         '\\"ai_platform_training_job_id\\": \\"my_jobid\\"}"}')
                    ],
            }
        }, body['training_input'])
    self.assertEqual(body['job_id'], 'my_jobid')
    self._mock_get.execute.assert_called_with()
    self._mock_create_request.execute.assert_called_with()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 45:</b> &nbsp; 3 fragments, nominal size 35 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag835')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 177-217
</a>
<div class="mid" id="frag835" style="display:none"><pre>
  def testStartCloudTraining_Vertex(self, mock_gapic):
    mock_gapic.JobServiceClient.return_value = self._mock_api_client
    self._setUpVertexTrainingMocks()

    class_path = 'foo.bar.class'
    region = 'us-central1'

    runner.start_cloud_training(self._inputs, self._outputs,
                                self._serialize_custom_config_under_test(),
                                class_path, self._training_inputs, None, True,
                                region)

    self._mock_create.assert_called_with(
        parent='projects/{}/locations/{}'.format(self._project_id, region),
        custom_job=mock.ANY)
    kwargs = self._mock_create.call_args[1]
    body = kwargs['custom_job']

    default_image = 'gcr.io/tfx-oss-public/tfx:{}'.format(
        version_utils.get_image_version())
    self.assertDictContainsSubset(
        {
            'worker_pool_specs': [{
                'container_spec': {
                    'image_uri':
                        default_image,
                    'command':
                        runner._CONTAINER_COMMAND + [
                            '--executor_class_path', class_path, '--inputs',
                            '{}', '--outputs', '{}', '--exec-properties',
                            ('{"custom_config": '
                             '"{\\"ai_platform_training_args\\": '
                             '{\\"project\\": \\"12345\\"'
                             '}}"}')
                        ],
                },
            },],
        }, body['job_spec'])
    self.assertStartsWith(body['display_name'], 'tfx_')
    self._mock_get.assert_called_with(name='vertex_job_study_id')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag836')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 219-266
</a>
<div class="mid" id="frag836" style="display:none"><pre>
  def testStartCloudTrainingWithUserContainer_Vertex(self, mock_gapic):
    mock_gapic.JobServiceClient.return_value = self._mock_api_client
    self._setUpVertexTrainingMocks()

    class_path = 'foo.bar.class'

    self._training_inputs['worker_pool_specs'] = [{
        'container_spec': {
            'image_uri': 'my-custom-image'
        }
    }]
    self._exec_properties['custom_config'][executor.JOB_ID_KEY] = self._job_id
    region = 'us-central2'
    runner.start_cloud_training(self._inputs, self._outputs,
                                self._serialize_custom_config_under_test(),
                                class_path, self._training_inputs, self._job_id,
                                True, region)

    self._mock_create.assert_called_with(
        parent='projects/{}/locations/{}'.format(self._project_id, region),
        custom_job=mock.ANY)
    kwargs = self._mock_create.call_args[1]
    body = kwargs['custom_job']
    self.assertDictContainsSubset(
        {
            'worker_pool_specs': [{
                'container_spec': {
                    'image_uri':
                        'my-custom-image',
                    'command':
                        runner._CONTAINER_COMMAND + [
                            '--executor_class_path', class_path, '--inputs',
                            '{}', '--outputs', '{}', '--exec-properties',
                            ('{"custom_config": '
                             '"{\\"ai_platform_training_args\\": '
                             '{\\"project\\": \\"12345\\", '
                             '\\"worker_pool_specs\\": '
                             '[{\\"container_spec\\": '
                             '{\\"image_uri\\": \\"my-custom-image\\"}}]}, '
                             '\\"ai_platform_training_job_id\\": '
                             '\\"my_jobid\\"}"}')
                        ],
                },
            },],
        }, body['job_spec'])
    self.assertEqual(body['display_name'], 'my_jobid')
    self._mock_get.assert_called_with(name='vertex_job_study_id')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag837')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 268-334
</a>
<div class="mid" id="frag837" style="display:none"><pre>
  def testStartCloudTrainingWithVertexCustomJob(self, mock_gapic):
    mock_gapic.JobServiceClient.return_value = self._mock_api_client
    self._setUpVertexTrainingMocks()

    class_path = 'foo.bar.class'
    expected_encryption_spec = {
        'kms_key_name': 'my_kmskey',
    }
    user_provided_labels = {
        'l1': 'v1',
        'l2': 'v2',
    }

    self._training_inputs['display_name'] = 'valid_name'
    self._training_inputs['job_spec'] = {
        'worker_pool_specs': [{
            'container_spec': {
                'image_uri': 'my-custom-image'
            }
        }]
    }
    self._training_inputs['labels'] = user_provided_labels
    self._training_inputs['encryption_spec'] = expected_encryption_spec
    self._exec_properties['custom_config'][executor.JOB_ID_KEY] = self._job_id
    region = 'us-central2'
    runner.start_cloud_training(self._inputs, self._outputs,
                                self._serialize_custom_config_under_test(),
                                class_path, self._training_inputs, self._job_id,
                                True, region)

    self._mock_create.assert_called_with(
        parent='projects/{}/locations/{}'.format(self._project_id, region),
        custom_job=mock.ANY)
    kwargs = self._mock_create.call_args[1]
    body = kwargs['custom_job']
    self.assertDictContainsSubset(
        {
            'worker_pool_specs': [{
                'container_spec': {
                    'image_uri':
                        'my-custom-image',
                    'command':
                        runner._CONTAINER_COMMAND + [
                            '--executor_class_path', class_path, '--inputs',
                            '{}', '--outputs', '{}', '--exec-properties',
                            ('{"custom_config": '
                             '"{\\"ai_platform_training_args\\": '
                             '{\\"display_name\\": \\"valid_name\\", '
                             '\\"encryption_spec\\": {\\"kms_key_name\\": '
                             '\\"my_kmskey\\"}, \\"job_spec\\": '
                             '{\\"worker_pool_specs\\": '
                             '[{\\"container_spec\\": '
                             '{\\"image_uri\\": \\"my-custom-image\\"}}]}, '
                             '\\"labels\\": {\\"l1\\": \\"v1\\", '
                             '\\"l2\\": \\"v2\\"}, '
                             '\\"project\\": \\"12345\\"}, '
                             '\\"ai_platform_training_job_id\\": '
                             '\\"my_jobid\\"}"}')
                        ],
                },
            },],
        }, body['job_spec'])
    self.assertEqual(body['display_name'], 'valid_name')
    self.assertDictEqual(body['encryption_spec'], expected_encryption_spec)
    self.assertDictContainsSubset(user_provided_labels, body['labels'])
    self._mock_get.assert_called_with(name='vertex_job_study_id')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 46:</b> &nbsp; 3 fragments, nominal size 34 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag839')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 367-414
</a>
<div class="mid" id="frag839" style="display:none"><pre>
  def _setUpVertexPredictionMocks(self):
    importlib.reload(initializer)
    importlib.reload(aiplatform)

    self._serving_container_image_uri = 'gcr.io/path/to/container'
    self._serving_path = os.path.join(self._output_data_dir, 'serving_path')
    self._endpoint_name = 'endpoint-name'
    self._endpoint_region = 'us-central1'
    self._deployed_model_id = 'model_id'

    self._mock_create_client = mock.Mock()
    initializer.global_config.create_client = self._mock_create_client
    self._mock_create_client.return_value = mock.Mock(
        spec=endpoint_service_client.EndpointServiceClient)

    self._mock_get_endpoint = mock.Mock()
    endpoint_service_client.EndpointServiceClient.get_endpoint = self._mock_get_endpoint
    self._mock_get_endpoint.return_value = endpoint.Endpoint(
        display_name=self._endpoint_name,)

    aiplatform.init(
        project=self._project_id,
        location=None,
        credentials=mock.Mock(spec=auth_credentials.AnonymousCredentials()))

    self._mock_endpoint = aiplatform.Endpoint(
        endpoint_name='projects/{}/locations/us-central1/endpoints/1234'.format(
            self._project_id))

    self._mock_endpoint_create = mock.Mock()
    aiplatform.Endpoint.create = self._mock_endpoint_create
    self._mock_endpoint_create.return_value = self._mock_endpoint

    self._mock_endpoint_list = mock.Mock()
    aiplatform.Endpoint.list = self._mock_endpoint_list
    self._mock_endpoint_list.return_value = []

    self._mock_model_upload = mock.Mock()
    aiplatform.Model.upload = self._mock_model_upload

    self._mock_model_deploy = mock.Mock()
    self._mock_model_upload.return_value.deploy = self._mock_model_deploy

    self._ai_platform_serving_args_vertex = {
        'endpoint_name': self._endpoint_name,
        'project_id': self._project_id,
    }

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag861')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 832-876
</a>
<div class="mid" id="frag861" style="display:none"><pre>
  def _setUpDeleteVertexModelMocks(self):
    importlib.reload(initializer)
    importlib.reload(aiplatform)

    self._endpoint_name = 'endpoint_name'
    self._deployed_model_id = 'model_id'

    self._mock_create_client = mock.Mock()
    initializer.global_config.create_client = self._mock_create_client
    self._mock_create_client.return_value = mock.Mock(
        spec=endpoint_service_client.EndpointServiceClient)

    self._mock_get_endpoint = mock.Mock()
    endpoint_service_client.EndpointServiceClient.get_endpoint = self._mock_get_endpoint
    self._mock_get_endpoint.return_value = endpoint.Endpoint(
        display_name=self._endpoint_name)

    aiplatform.init(
        project=self._project_id,
        location=None,
        credentials=mock.Mock(spec=auth_credentials.AnonymousCredentials()))

    self._mock_endpoint = aiplatform.Endpoint(
        endpoint_name='projects/{}/locations/us-central1/endpoints/1234'.format(
            self._project_id))

    self._mock_endpoint_list = mock.Mock()
    aiplatform.Endpoint.list = self._mock_endpoint_list
    self._mock_endpoint_list.return_value = [self._mock_endpoint]

    self._mock_model_delete = mock.Mock()
    self._mock_endpoint.undeploy = self._mock_model_delete

    self._mock_list_models = mock.Mock()
    self._mock_list_models.return_value = [
        endpoint.DeployedModel(
            display_name=self._model_name, id=self._deployed_model_id)
    ]
    self._mock_endpoint.list_models = self._mock_list_models

    self._ai_platform_serving_args_vertex = {
        'endpoint_name': self._endpoint_name,
        'project_id': self._project_id,
    }

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag864')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 891-928
</a>
<div class="mid" id="frag864" style="display:none"><pre>
  def _setUpDeleteVertexEndpointMocks(self):
    importlib.reload(initializer)
    importlib.reload(aiplatform)

    self._endpoint_name = 'endpoint_name'

    self._mock_create_client = mock.Mock()
    initializer.global_config.create_client = self._mock_create_client
    self._mock_create_client.return_value = mock.Mock(
        spec=endpoint_service_client.EndpointServiceClient)

    self._mock_get_endpoint = mock.Mock()
    endpoint_service_client.EndpointServiceClient.get_endpoint = (
        self._mock_get_endpoint)
    self._mock_get_endpoint.return_value = endpoint.Endpoint(
        display_name=self._endpoint_name,)

    aiplatform.init(
        project=self._project_id,
        location=None,
        credentials=mock.Mock(spec=auth_credentials.AnonymousCredentials()))

    self._mock_endpoint = aiplatform.Endpoint(
        endpoint_name='projects/{}/locations/us-central1/endpoints/1234'.format(
            self._project_id))

    self._mock_endpoint_list = mock.Mock()
    aiplatform.Endpoint.list = self._mock_endpoint_list
    self._mock_endpoint_list.return_value = [self._mock_endpoint]

    self._mock_endpoint_delete = mock.Mock()
    self._mock_endpoint.delete = self._mock_endpoint_delete

    self._ai_platform_serving_args_vertex = {
        'endpoint_name': self._endpoint_name,
        'project_id': self._project_id,
    }

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 47:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag842')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 498-515
</a>
<div class="mid" id="frag842" style="display:none"><pre>
  def testDeployModelForAIPPrediction(self):
    self._setUpPredictionMocks()

    runner.deploy_model_for_aip_prediction(
        serving_path=self._serving_path,
        model_version_name=self._model_version,
        ai_platform_serving_args=self._ai_platform_serving_args,
        labels=self._job_labels,
        api=self._mock_api_client)

    expected_models_create_body = {
        'name': self._model_name,
        'regions': [],
        'labels': self._job_labels
    }
    self._assertDeployModelMockCalls(
        expected_models_create_body=expected_models_create_body)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag847')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 584-604
</a>
<div class="mid" id="frag847" style="display:none"><pre>
  def testDeployModelForAIPPredictionWithCustomRuntime(self):
    self._setUpPredictionMocks()

    self._ai_platform_serving_args['runtime_version'] = '1.23.45'
    runner.deploy_model_for_aip_prediction(
        serving_path=self._serving_path,
        model_version_name=self._model_version,
        ai_platform_serving_args=self._ai_platform_serving_args,
        labels=self._job_labels,
        api=self._mock_api_client)

    expected_versions_create_body = {
        'name': self._model_version,
        'deployment_uri': self._serving_path,
        'runtime_version': '1.23.45',
        'python_version': '3.7',
        'labels': self._job_labels,
    }
    self._assertDeployModelMockCalls(
        expected_versions_create_body=expected_versions_create_body)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag846')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 565-583
</a>
<div class="mid" id="frag846" style="display:none"><pre>
  def testDeployModelForAIPPredictionWithCustomRegion(self):
    self._setUpPredictionMocks()

    self._ai_platform_serving_args['regions'] = ['custom-region']
    runner.deploy_model_for_aip_prediction(
        serving_path=self._serving_path,
        model_version_name=self._model_version,
        ai_platform_serving_args=self._ai_platform_serving_args,
        labels=self._job_labels,
        api=self._mock_api_client)

    expected_models_create_body = {
        'name': self._model_name,
        'regions': ['custom-region'],
        'labels': self._job_labels
    }
    self._assertDeployModelMockCalls(
        expected_models_create_body=expected_models_create_body)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 48:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag849')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 633-648
</a>
<div class="mid" id="frag849" style="display:none"><pre>
  def _setUpDeleteModelVersionMocks(self):
    self._model_version = 'model_version'

    self._mock_models_version_delete = mock.Mock()
    self._mock_api_client.projects().models().versions().delete = (
        self._mock_models_version_delete)
    self._mock_models_version_delete.return_value.execute.return_value = {
        'name': 'version_delete_op_name'
    }
    self._mock_get = mock.Mock()
    self._mock_api_client.projects().operations().get = self._mock_get

    self._mock_get.return_value.execute.return_value = {
        'done': True,
    }

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag852')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 668-680
</a>
<div class="mid" id="frag852" style="display:none"><pre>
  def _setUpDeleteModelMocks(self):
    self._mock_models_delete = mock.Mock()
    self._mock_api_client.projects().models().delete = (
        self._mock_models_delete)
    self._mock_models_delete.return_value.execute.return_value = {
        'name': 'model_delete_op_name'
    }
    self._mock_get = mock.Mock()
    self._mock_api_client.projects().operations().get = self._mock_get
    self._mock_get.return_value.execute.return_value = {
        'done': True,
    }

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 49:</b> &nbsp; 2 fragments, nominal size 30 lines, similarity 89%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag855')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 699-730
</a>
<div class="mid" id="frag855" style="display:none"><pre>
  def testDeployModelForVertexPrediction(self):
    self._setUpVertexPredictionMocks()
    self._mock_endpoint_list.side_effect = [[], [self._mock_endpoint]]

    runner.deploy_model_for_aip_prediction(
        serving_path=self._serving_path,
        model_version_name=self._model_name,
        ai_platform_serving_args=self._ai_platform_serving_args_vertex,
        labels=self._job_labels,
        serving_container_image_uri=self._serving_container_image_uri,
        endpoint_region=self._endpoint_region,
        enable_vertex=True)

    expected_endpoint_create_body = {
        'display_name': self._endpoint_name,
        'labels': self._job_labels,
    }
    expected_model_upload_body = {
        'display_name': self._model_name,
        'artifact_uri': self._serving_path,
        'serving_container_image_uri': self._serving_container_image_uri,
    }
    expected_model_deploy_body = {
        'endpoint': self._mock_endpoint,
        'traffic_percentage': 100,
    }

    self._assertDeployModelMockCallsVertex(
        expected_endpoint_create_body=expected_endpoint_create_body,
        expected_model_upload_body=expected_model_upload_body,
        expected_model_deploy_body=expected_model_deploy_body)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag856')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 731-766
</a>
<div class="mid" id="frag856" style="display:none"><pre>
  def testDeployModelForVertexPredictionError(self):
    self._setUpVertexPredictionMocks()
    self._mock_endpoint_list.side_effect = [[], [self._mock_endpoint]]

    self._mock_model_deploy.side_effect = errors.HttpError(
        httplib2.Response(info={'status': 429}), b'')

    with self.assertRaises(RuntimeError):
      runner.deploy_model_for_aip_prediction(
          serving_path=self._serving_path,
          model_version_name=self._model_name,
          ai_platform_serving_args=self._ai_platform_serving_args_vertex,
          labels=self._job_labels,
          serving_container_image_uri=self._serving_container_image_uri,
          endpoint_region=self._endpoint_region,
          enable_vertex=True)

    expected_endpoint_create_body = {
        'display_name': self._endpoint_name,
        'labels': self._job_labels,
    }
    expected_model_upload_body = {
        'display_name': self._model_name,
        'artifact_uri': self._serving_path,
        'serving_container_image_uri': self._serving_container_image_uri,
    }
    expected_model_deploy_body = {
        'endpoint': self._mock_endpoint,
        'traffic_percentage': 100,
    }

    self._assertDeployModelMockCallsVertex(
        expected_endpoint_create_body=expected_endpoint_create_body,
        expected_model_upload_body=expected_model_upload_body,
        expected_model_deploy_body=expected_model_deploy_body)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 50:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag859')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 786-808
</a>
<div class="mid" id="frag859" style="display:none"><pre>
  def testDeployModelForVertexPredictionWithCustomRegion(self):
    self._setUpVertexPredictionMocks()
    self._mock_endpoint_list.side_effect = [[], [self._mock_endpoint]]

    self._mock_init = mock.Mock()
    aiplatform.init = self._mock_init

    self._endpoint_region = 'custom-region'
    runner.deploy_model_for_aip_prediction(
        serving_path=self._serving_path,
        model_version_name=self._model_name,
        ai_platform_serving_args=self._ai_platform_serving_args_vertex,
        labels=self._job_labels,
        serving_container_image_uri=self._serving_container_image_uri,
        endpoint_region=self._endpoint_region,
        enable_vertex=True)

    expected_init_body = {
        'project': self._project_id,
        'location': 'custom-region',
    }
    self._mock_init.assert_called_with(**expected_init_body)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag860')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/runner_test.py: 809-831
</a>
<div class="mid" id="frag860" style="display:none"><pre>
  def testDeployModelForVertexPredictionWithCustomMachineType(self):
    self._setUpVertexPredictionMocks()
    self._mock_endpoint_list.side_effect = [[], [self._mock_endpoint]]

    self._ai_platform_serving_args_vertex[
        'machine_type'] = 'custom_machine_type'
    runner.deploy_model_for_aip_prediction(
        serving_path=self._serving_path,
        model_version_name=self._model_name,
        ai_platform_serving_args=self._ai_platform_serving_args_vertex,
        labels=self._job_labels,
        serving_container_image_uri=self._serving_container_image_uri,
        endpoint_region=self._endpoint_region,
        enable_vertex=True)

    expected_model_deploy_body = {
        'endpoint': self._mock_endpoint,
        'traffic_percentage': 100,
        'machine_type': 'custom_machine_type',
    }
    self._assertDeployModelMockCallsVertex(
        expected_model_deploy_body=expected_model_deploy_body)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 51:</b> &nbsp; 3 fragments, nominal size 27 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag876')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/pusher/executor_test.py: 118-145
</a>
<div class="mid" id="frag876" style="display:none"><pre>
  def testDoBlessed(self, mock_runner, _):
    self._model_blessing.uri = os.path.join(self._source_data_dir,
                                            'model_validator/blessed')
    self._model_blessing.set_int_custom_property('blessed', 1)
    mock_runner.get_service_name_and_api_version.return_value = ('ml', 'v1')
    version = self._model_push.get_string_custom_property('pushed_version')
    mock_runner.deploy_model_for_aip_prediction.return_value = (
        'projects/project_id/models/model_name/versions/{}'.format(version))

    self._executor.Do(self._input_dict, self._output_dict,
                      self._serialize_custom_config_under_test())
    executor_class_path = '%s.%s' % (self._executor.__class__.__module__,
                                     self._executor.__class__.__name__)
    with telemetry_utils.scoped_labels(
        {telemetry_utils.LABEL_TFX_EXECUTOR: executor_class_path}):
      job_labels = telemetry_utils.make_labels_dict()
    mock_runner.deploy_model_for_aip_prediction.assert_called_once_with(
        serving_path=self._model_push.uri,
        model_version_name=mock.ANY,
        ai_platform_serving_args=mock.ANY,
        api=mock.ANY,
        labels=job_labels,
    )
    self.assertPushed()
    self.assertEqual(
        self._model_push.get_string_custom_property('pushed_destination'),
        'projects/project_id/models/model_name/versions/{}'.format(version))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag880')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/pusher/executor_test.py: 224-250
</a>
<div class="mid" id="frag880" style="display:none"><pre>
  def testDoBlessed_Vertex(self, mock_runner):
    endpoint_uri = 'projects/project_id/locations/us-central1/endpoints/12345'
    mock_runner.deploy_model_for_aip_prediction.return_value = endpoint_uri
    self._model_blessing.uri = os.path.join(self._source_data_dir,
                                            'model_validator/blessed')
    self._model_blessing.set_int_custom_property('blessed', 1)
    self._executor.Do(self._input_dict, self._output_dict,
                      self._serialize_custom_config_under_test_vertex())
    executor_class_path = '%s.%s' % (self._executor.__class__.__module__,
                                     self._executor.__class__.__name__)
    with telemetry_utils.scoped_labels(
        {telemetry_utils.LABEL_TFX_EXECUTOR: executor_class_path}):
      job_labels = telemetry_utils.make_labels_dict()
    mock_runner.deploy_model_for_aip_prediction.assert_called_once_with(
        serving_container_image_uri=self._container_image_uri_vertex,
        model_version_name=mock.ANY,
        ai_platform_serving_args=mock.ANY,
        labels=job_labels,
        serving_path=self._model_push.uri,
        endpoint_region='us-central1',
        enable_vertex=True,
    )
    self.assertPushed()
    self.assertEqual(
        self._model_push.get_string_custom_property('pushed_destination'),
        endpoint_uri)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag879')" href="javascript:;">
tfx-1.6.0/tfx/extensions/google_cloud_ai_platform/pusher/executor_test.py: 186-222
</a>
<div class="mid" id="frag879" style="display:none"><pre>
  def testDoBlessedOnRegionalEndpoint(self, mock_runner, _):
    self._exec_properties = {
        'custom_config': {
            constants.SERVING_ARGS_KEY: {
                'model_name': 'model_name',
                'project_id': 'project_id'
            },
            constants.ENDPOINT_ARGS_KEY: 'https://ml-us-west1.googleapis.com',
        },
    }
    self._model_blessing.uri = os.path.join(self._source_data_dir,
                                            'model_validator/blessed')
    self._model_blessing.set_int_custom_property('blessed', 1)
    mock_runner.get_service_name_and_api_version.return_value = ('ml', 'v1')
    version = self._model_push.get_string_custom_property('pushed_version')
    mock_runner.deploy_model_for_aip_prediction.return_value = (
        'projects/project_id/models/model_name/versions/{}'.format(version))

    self._executor.Do(self._input_dict, self._output_dict,
                      self._serialize_custom_config_under_test())
    executor_class_path = '%s.%s' % (self._executor.__class__.__module__,
                                     self._executor.__class__.__name__)
    with telemetry_utils.scoped_labels(
        {telemetry_utils.LABEL_TFX_EXECUTOR: executor_class_path}):
      job_labels = telemetry_utils.make_labels_dict()
    mock_runner.deploy_model_for_aip_prediction.assert_called_once_with(
        serving_path=self._model_push.uri,
        model_version_name=mock.ANY,
        ai_platform_serving_args=mock.ANY,
        api=mock.ANY,
        labels=job_labels,
    )
    self.assertPushed()
    self.assertEqual(
        self._model_push.get_string_custom_property('pushed_destination'),
        'projects/project_id/models/model_name/versions/{}'.format(version))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 52:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag911')" href="javascript:;">
tfx-1.6.0/tfx/dsl/components/common/resolver_test.py: 31-48
</a>
<div class="mid" id="frag911" style="display:none"><pre>
  def testResolverDefinition(self):
    channel_to_resolve = types.Channel(type=standard_artifacts.Examples)
    rnode = resolver.Resolver(
        strategy_class=latest_artifact_strategy.LatestArtifactStrategy,
        config={'desired_num_of_artifacts': 5},
        channel_to_resolve=channel_to_resolve)
    self.assertDictEqual(
        rnode.exec_properties, {
            resolver.RESOLVER_STRATEGY_CLASS:
                latest_artifact_strategy.LatestArtifactStrategy,
            resolver.RESOLVER_CONFIG: {
                'desired_num_of_artifacts': 5
            }
        })
    self.assertEqual(rnode.inputs['channel_to_resolve'], channel_to_resolve)
    self.assertEqual(rnode.outputs['channel_to_resolve'].type_name,
                     channel_to_resolve.type_name)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag912')" href="javascript:;">
tfx-1.6.0/tfx/dsl/components/common/resolver_test.py: 49-68
</a>
<div class="mid" id="frag912" style="display:none"><pre>
  def testResolverUnionChannel(self):
    one_channel = types.Channel(type=standard_artifacts.Examples)
    another_channel = types.Channel(type=standard_artifacts.Examples)
    unioned_channel = channel.union([one_channel, another_channel])
    rnode = resolver.Resolver(
        strategy_class=latest_artifact_strategy.LatestArtifactStrategy,
        config={'desired_num_of_artifacts': 5},
        unioned_channel=unioned_channel)
    self.assertDictEqual(
        rnode.exec_properties, {
            resolver.RESOLVER_STRATEGY_CLASS:
                latest_artifact_strategy.LatestArtifactStrategy,
            resolver.RESOLVER_CONFIG: {
                'desired_num_of_artifacts': 5
            }
        })
    self.assertEqual(rnode.inputs['unioned_channel'], unioned_channel)
    self.assertEqual(rnode.outputs['unioned_channel'].type_name,
                     unioned_channel.type_name)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 53:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag917')" href="javascript:;">
tfx-1.6.0/tfx/dsl/components/common/resolver_test.py: 107-122
</a>
<div class="mid" id="frag917" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self.connection_config = metadata_store_pb2.ConnectionConfig()
    self.connection_config.sqlite.SetInParent()
    self.pipeline_info = data_types.PipelineInfo(
        pipeline_name='p_name', pipeline_root='p_root', run_id='run_id')
    self.component_info = data_types.ComponentInfo(
        component_type='c_type',
        component_id='c_id',
        pipeline_info=self.pipeline_info)
    self.driver_args = data_types.DriverArgs(enable_cache=True)
    self.source_channel_key = 'source_channel'
    self.source_channels = {
        self.source_channel_key: types.Channel(type=standard_artifacts.Examples)
    }

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1109')" href="javascript:;">
tfx-1.6.0/tfx/dsl/input_resolution/strategies/conditional_strategy_test.py: 91-104
</a>
<div class="mid" id="frag1109" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._connection_config = metadata_store_pb2.ConnectionConfig()
    self._connection_config.sqlite.SetInParent()
    self._metadata = self.enter_context(
        metadata.Metadata(connection_config=self._connection_config))
    self._store = self._metadata.store
    self._pipeline_info = data_types.PipelineInfo(
        pipeline_name='my_pipeline', pipeline_root='/tmp', run_id='my_run_id')
    self._component_info = data_types.ComponentInfo(
        component_type='a.b.c',
        component_id='my_component',
        pipeline_info=self._pipeline_info)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 54:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag941')" href="javascript:;">
tfx-1.6.0/tfx/dsl/components/base/executor_spec_test.py: 36-49
</a>
<div class="mid" id="frag941" style="display:none"><pre>
  def testExecutorClassSpecCopy(self):
    class _NestedExecutor(base_executor.BaseExecutor):
      pass
    spec = executor_spec.ExecutorClassSpec(_NestedExecutor)
    spec.add_extra_flags('a')
    spec_copy = spec.copy()
    del spec
    self.assertProtoEquals(
        """
        class_path: "__main__._NestedExecutor"
        extra_flags: "a"
        """,
        spec_copy.encode())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag942')" href="javascript:;">
tfx-1.6.0/tfx/dsl/components/base/executor_spec_test.py: 50-68
</a>
<div class="mid" id="frag942" style="display:none"><pre>
  def testBeamExecutorSpecCopy(self):

    class _NestedExecutor(base_executor.BaseExecutor):
      pass

    spec = executor_spec.BeamExecutorSpec(_NestedExecutor)
    spec.add_extra_flags('a')
    spec.add_beam_pipeline_args('b')
    spec_copy = spec.copy()
    del spec
    self.assertProtoEquals(
        """
        python_executor_spec: {
            class_path: "__main__._NestedExecutor"
            extra_flags: "a"
        }
        beam_pipeline_args: "b"
        """, spec_copy.encode())

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 55:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1004')" href="javascript:;">
tfx-1.6.0/tfx/dsl/components/base/base_component_test.py: 90-115
</a>
<div class="mid" id="frag1004" style="display:none"><pre>
  def testComponentSpecClass(self):

    class MissingSpecComponent(base_component.BaseComponent):

      EXECUTOR_SPEC = executor_spec.ExecutorClassSpec(
          base_executor.BaseExecutor)

    with self.assertRaisesRegex(TypeError, "Can't instantiate abstract class"):
      MissingSpecComponent(spec=object())  # pytype: disable=wrong-arg-types

    with self.assertRaisesRegex(
        TypeError, "expects SPEC_CLASS property to be a subclass of "
        "types.ComponentSpec"):
      MissingSpecComponent._validate_component_class()

    class InvalidSpecComponent(base_component.BaseComponent):

      SPEC_CLASSES = object()
      EXECUTOR_SPEC = executor_spec.ExecutorClassSpec(
          base_executor.BaseExecutor)

    with self.assertRaisesRegex(
        TypeError, "expects SPEC_CLASS property to be a subclass of "
        "types.ComponentSpec"):
      InvalidSpecComponent._validate_component_class()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1006')" href="javascript:;">
tfx-1.6.0/tfx/dsl/components/base/base_component_test.py: 168-191
</a>
<div class="mid" id="frag1006" style="display:none"><pre>
  def testComponentExecutorClass(self):

    class MissingExecutorComponent(base_component.BaseComponent):

      SPEC_CLASS = _BasicComponentSpec

    with self.assertRaisesRegex(TypeError, "Can't instantiate abstract class"):
      MissingExecutorComponent(spec=object())  # pytype: disable=wrong-arg-types

    with self.assertRaisesRegex(
        TypeError, "expects EXECUTOR_SPEC property to be an instance of "
        "ExecutorSpec"):
      MissingExecutorComponent._validate_component_class()

    class InvalidExecutorComponent(base_component.BaseComponent):

      SPEC_CLASS = _BasicComponentSpec
      EXECUTOR_SPEC = object()

    with self.assertRaisesRegex(
        TypeError, "expects EXECUTOR_SPEC property to be an instance of "
        "ExecutorSpec"):
      InvalidExecutorComponent._validate_component_class()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 56:</b> &nbsp; 4 fragments, nominal size 22 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1046')" href="javascript:;">
tfx-1.6.0/tfx/dsl/component/experimental/decorators_test.py: 179-202
</a>
<div class="mid" id="frag1046" style="display:none"><pre>
  def testBeamExecutionSuccess(self):
    """Test execution with return values; success case."""
    instance_1 = _injector_1(foo=9, bar='secret')
    instance_2 = _simple_component(
        a=instance_1.outputs['a'],
        b=instance_1.outputs['b'],
        c=instance_1.outputs['c'],
        d=instance_1.outputs['d'])
    instance_3 = _verify(
        e=instance_2.outputs['e'],
        f=instance_2.outputs['f'],
        g=instance_2.outputs['g'],
        h=instance_2.outputs['h'])  # pylint: disable=assignment-from-no-return

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    test_pipeline = pipeline.Pipeline(
        pipeline_name='test_pipeline_1',
        pipeline_root=self._test_dir,
        metadata_connection_config=metadata_config,
        components=[instance_1, instance_2, instance_3])

    beam_dag_runner.BeamDagRunner().run(test_pipeline)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1047')" href="javascript:;">
tfx-1.6.0/tfx/dsl/component/experimental/decorators_test.py: 203-229
</a>
<div class="mid" id="frag1047" style="display:none"><pre>
  def testBeamExecutionFailure(self):
    """Test execution with return values; failure case."""
    instance_1 = _injector_1(foo=9, bar='secret')
    instance_2 = _simple_component(
        a=instance_1.outputs['a'],
        b=instance_1.outputs['b'],
        c=instance_1.outputs['c'],
        d=instance_1.outputs['d'])
    # Swapped 'e' and 'f'.
    instance_3 = _verify(
        e=instance_2.outputs['f'],
        f=instance_2.outputs['e'],
        g=instance_2.outputs['g'],
        h=instance_2.outputs['h'])  # pylint: disable=assignment-from-no-return

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    test_pipeline = pipeline.Pipeline(
        pipeline_name='test_pipeline_1',
        pipeline_root=self._test_dir,
        metadata_connection_config=metadata_config,
        components=[instance_1, instance_2, instance_3])

    with self.assertRaisesRegex(
        RuntimeError, r'AssertionError: \(220.0, 32.0, \'OK\', None\)'):
      beam_dag_runner.BeamDagRunner().run(test_pipeline)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1049')" href="javascript:;">
tfx-1.6.0/tfx/dsl/component/experimental/decorators_test.py: 257-285
</a>
<div class="mid" id="frag1049" style="display:none"><pre>
  def testBeamExecutionNonNullableReturnError(self):
    """Test failure when None used for non-optional primitive return value."""
    instance_1 = _injector_3()  # pylint: disable=no-value-for-parameter
    self.assertEqual(1, len(instance_1.outputs['examples'].get()))
    instance_2 = _optionalarg_component(  # pylint: disable=assignment-from-no-return
        foo=9,
        bar='secret',
        examples=instance_1.outputs['examples'],
        a=instance_1.outputs['a'],
        b=instance_1.outputs['b'],
        c=instance_1.outputs['c'],
        d=instance_1.outputs['d'],
        e1=instance_1.outputs['e'],
        e2=instance_1.outputs['e'],
        g=999.0,
        optional_examples_1=instance_1.outputs['examples'])

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    test_pipeline = pipeline.Pipeline(
        pipeline_name='test_pipeline_1',
        pipeline_root=self._test_dir,
        metadata_connection_config=metadata_config,
        components=[instance_1, instance_2])
    with self.assertRaisesRegex(
        ValueError, 'Non-nullable output \'e\' received None return value'):
      beam_dag_runner.BeamDagRunner().run(test_pipeline)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1048')" href="javascript:;">
tfx-1.6.0/tfx/dsl/component/experimental/decorators_test.py: 230-256
</a>
<div class="mid" id="frag1048" style="display:none"><pre>
  def testBeamExecutionOptionalInputsAndParameters(self):
    """Test execution with optional inputs and parameters."""
    instance_1 = _injector_2()  # pylint: disable=no-value-for-parameter
    self.assertEqual(1, len(instance_1.outputs['examples'].get()))
    instance_2 = _optionalarg_component(  # pylint: disable=assignment-from-no-return
        foo=9,
        bar='secret',
        examples=instance_1.outputs['examples'],
        a=instance_1.outputs['a'],
        b=instance_1.outputs['b'],
        c=instance_1.outputs['c'],
        d=instance_1.outputs['d'],
        e1=instance_1.outputs['e'],
        e2=instance_1.outputs['e'],
        g=999.0,
        optional_examples_1=instance_1.outputs['examples'])

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    test_pipeline = pipeline.Pipeline(
        pipeline_name='test_pipeline_1',
        pipeline_root=self._test_dir,
        metadata_connection_config=metadata_config,
        components=[instance_1, instance_2])

    beam_dag_runner.BeamDagRunner().run(test_pipeline)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 57:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1110')" href="javascript:;">
tfx-1.6.0/tfx/dsl/input_resolution/strategies/conditional_strategy_test.py: 105-123
</a>
<div class="mid" id="frag1110" style="display:none"><pre>
  def testStrategy_IrMode_PredicateTrue(self):
    artifact_1 = standard_artifacts.Integer()
    artifact_1.uri = self.create_tempfile().full_path
    artifact_1.value = 0
    artifact_2 = standard_artifacts.Integer()
    artifact_2.uri = self.create_tempfile().full_path
    artifact_2.value = 1

    strategy = conditional_strategy.ConditionalStrategy([
        text_format.Parse(_TEST_PREDICATE_1,
                          placeholder_pb2.PlaceholderExpression()),
        text_format.Parse(_TEST_PREDICATE_2,
                          placeholder_pb2.PlaceholderExpression())
    ])
    input_dict = {'channel_1_key': [artifact_1], 'channel_2_key': [artifact_2]}
    result = strategy.resolve_artifacts(self._store, input_dict)
    self.assertIsNotNone(result)
    self.assertEqual(result, input_dict)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1111')" href="javascript:;">
tfx-1.6.0/tfx/dsl/input_resolution/strategies/conditional_strategy_test.py: 124-141
</a>
<div class="mid" id="frag1111" style="display:none"><pre>
  def testStrategy_IrMode_PredicateFalse(self):
    artifact_1 = standard_artifacts.Integer()
    artifact_1.uri = self.create_tempfile().full_path
    artifact_1.value = 0
    artifact_2 = standard_artifacts.Integer()
    artifact_2.uri = self.create_tempfile().full_path
    artifact_2.value = 42

    strategy = conditional_strategy.ConditionalStrategy([
        text_format.Parse(_TEST_PREDICATE_1,
                          placeholder_pb2.PlaceholderExpression()),
        text_format.Parse(_TEST_PREDICATE_2,
                          placeholder_pb2.PlaceholderExpression())
    ])
    input_dict = {'channel_1_key': [artifact_1], 'channel_2_key': [artifact_2]}
    with self.assertRaises(exceptions.SkipSignal):
      strategy.resolve_artifacts(self._store, input_dict)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 58:</b> &nbsp; 2 fragments, nominal size 84 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1138')" href="javascript:;">
tfx-1.6.0/tfx/dsl/compiler/testdata/iris_pipeline_async.py: 38-141
</a>
<div class="mid" id="frag1138" style="display:none"><pre>
def create_test_pipeline():
  """Builds an Iris example pipeline with slight changes."""
  pipeline_name = "iris"
  iris_root = "iris_root"
  serving_model_dir = os.path.join(iris_root, "serving_model", pipeline_name)
  tfx_root = "tfx_root"
  data_path = os.path.join(tfx_root, "data_path")
  pipeline_root = os.path.join(tfx_root, "pipelines", pipeline_name)

  example_gen = CsvExampleGen(input_base=data_path)

  statistics_gen = StatisticsGen(examples=example_gen.outputs["examples"])

  importer = ImporterNode(
      source_uri="m/y/u/r/i",
      properties={
          "split_names": "['train', 'eval']",
      },
      custom_properties={
          "int_custom_property": 42,
          "str_custom_property": "42",
      },
      artifact_type=standard_artifacts.Examples).with_id("my_importer")

  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs["statistics"], infer_feature_shape=True)

  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs["statistics"],
      schema=schema_gen.outputs["schema"])

  trainer = Trainer(
      # Use RuntimeParameter as module_file to test out RuntimeParameter in
      # compiler.
      module_file=data_types.RuntimeParameter(
          name="module_file",
          default=os.path.join(iris_root, "iris_utils.py"),
          ptype=str),
      custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),
      examples=example_gen.outputs["examples"],
      schema=schema_gen.outputs["schema"],
      train_args=trainer_pb2.TrainArgs(num_steps=2000),
      # Attaching `TrainerArgs` as platform config is not sensible practice,
      # but is only for testing purpose.
      eval_args=trainer_pb2.EvalArgs(num_steps=5)).with_platform_config(
          config=trainer_pb2.TrainArgs(num_steps=2000))

  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_strategy.LatestBlessedModelStrategy,
      baseline_model=Channel(
          type=standard_artifacts.Model, producer_component_id="Trainer"),
      # Cannot add producer_component_id="Evaluator" for model_blessing as it
      # raises "producer component should have already been compiled" error.
      model_blessing=Channel(type=standard_artifacts.ModelBlessing)).with_id(
          "latest_blessed_model_resolver")

  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(signature_name="eval")],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(
              thresholds={
                  "sparse_categorical_accuracy":
                      tfma.MetricThreshold(
                          value_threshold=tfma.GenericValueThreshold(
                              lower_bound={"value": 0.6}),
                          change_threshold=tfma.GenericChangeThreshold(
                              direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                              absolute={"value": -1e-10}))
              })
      ])
  evaluator = Evaluator(
      examples=example_gen.outputs["examples"],
      model=trainer.outputs["model"],
      baseline_model=model_resolver.outputs["baseline_model"],
      eval_config=eval_config)

  pusher = Pusher(
      model=trainer.outputs["model"],
      model_blessing=evaluator.outputs["blessing"],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen,
          statistics_gen,
          importer,
          schema_gen,
          example_validator,
          trainer,
          model_resolver,
          evaluator,
          pusher,
      ],
      enable_cache=False,
      beam_pipeline_args=["--my_testing_beam_pipeline_args=bar"],
      # Attaching `TrainerArgs` as platform config is not sensible practice,
      # but is only for testing purpose.
      platform_config=trainer_pb2.TrainArgs(num_steps=2000),
      execution_mode=pipeline.ExecutionMode.ASYNC)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1139')" href="javascript:;">
tfx-1.6.0/tfx/dsl/compiler/testdata/iris_pipeline_sync.py: 38-141
</a>
<div class="mid" id="frag1139" style="display:none"><pre>
def create_test_pipeline():
  """Builds an Iris example pipeline with slight changes."""
  pipeline_name = "iris"
  iris_root = "iris_root"
  serving_model_dir = os.path.join(iris_root, "serving_model", pipeline_name)
  tfx_root = "tfx_root"
  data_path = os.path.join(tfx_root, "data_path")
  pipeline_root = os.path.join(tfx_root, "pipelines", pipeline_name)

  example_gen = CsvExampleGen(input_base=data_path)

  statistics_gen = StatisticsGen(examples=example_gen.outputs["examples"])

  importer = ImporterNode(
      source_uri="m/y/u/r/i",
      properties={
          "split_names": "['train', 'eval']",
      },
      custom_properties={
          "int_custom_property": 42,
          "str_custom_property": "42",
      },
      artifact_type=standard_artifacts.Examples).with_id("my_importer")
  another_statistics_gen = StatisticsGen(
      examples=importer.outputs["result"]).with_id("another_statistics_gen")

  schema_gen = SchemaGen(statistics=statistics_gen.outputs["statistics"])

  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs["statistics"],
      schema=schema_gen.outputs["schema"])

  trainer = Trainer(
      # Use RuntimeParameter as module_file to test out RuntimeParameter in
      # compiler.
      module_file=data_types.RuntimeParameter(
          name="module_file",
          default=os.path.join(iris_root, "iris_utils.py"),
          ptype=str),
      custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),
      examples=example_gen.outputs["examples"],
      schema=schema_gen.outputs["schema"],
      train_args=trainer_pb2.TrainArgs(num_steps=2000),
      # Attaching `TrainerArgs` as platform config is not sensible practice,
      # but is only for testing purpose.
      eval_args=trainer_pb2.EvalArgs(num_steps=5)).with_platform_config(
          config=trainer_pb2.TrainArgs(num_steps=2000))

  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_strategy.LatestBlessedModelStrategy,
      model=Channel(
          type=standard_artifacts.Model, producer_component_id=trainer.id),
      model_blessing=Channel(type=standard_artifacts.ModelBlessing)).with_id(
          "latest_blessed_model_resolver")

  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(signature_name="eval")],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(
              thresholds={
                  "sparse_categorical_accuracy":
                      tfma.MetricThreshold(
                          value_threshold=tfma.GenericValueThreshold(
                              lower_bound={"value": 0.6}),
                          change_threshold=tfma.GenericChangeThreshold(
                              direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                              absolute={"value": -1e-10}))
              })
      ])
  evaluator = Evaluator(
      examples=example_gen.outputs["examples"],
      model=trainer.outputs["model"],
      baseline_model=model_resolver.outputs["model"],
      eval_config=eval_config)

  pusher = Pusher(
      model=trainer.outputs["model"],
      model_blessing=evaluator.outputs["blessing"],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen,
          statistics_gen,
          another_statistics_gen,
          importer,
          schema_gen,
          example_validator,
          trainer,
          model_resolver,
          evaluator,
          pusher,
      ],
      enable_cache=True,
      beam_pipeline_args=["--my_testing_beam_pipeline_args=foo"],
      # Attaching `TrainerArgs` as platform config is not sensible practice,
      # but is only for testing purpose.
      platform_config=trainer_pb2.TrainArgs(num_steps=2000),
      execution_mode=pipeline.ExecutionMode.SYNC)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 59:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1142')" href="javascript:;">
tfx-1.6.0/tfx/dsl/compiler/placeholder_utils_test.py: 257-287
</a>
<div class="mid" id="frag1142" style="display:none"><pre>
  def testArtifactProperty(self):
    placeholder_expression = """
      operator {
        artifact_property_op {
          expression {
            operator {
              index_op{
                expression {
                  placeholder {
                    type: INPUT_ARTIFACT
                    key: "examples"
                  }
                }
                index: 0
              }
            }
          }
          key: "version"
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())
    self.assertEqual(
        placeholder_utils.resolve_placeholder_expression(
            pb, self._resolution_context), 42)

    self.assertEqual(
        placeholder_utils.debug_str(pb),
        "input(\"examples\")[0].property(\"version\")")

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1143')" href="javascript:;">
tfx-1.6.0/tfx/dsl/compiler/placeholder_utils_test.py: 288-319
</a>
<div class="mid" id="frag1143" style="display:none"><pre>
  def testArtifactCustomProperty(self):
    placeholder_expression = """
      operator {
        artifact_property_op {
          expression {
            operator {
              index_op{
                expression {
                  placeholder {
                    type: INPUT_ARTIFACT
                    key: "examples"
                  }
                }
                index: 0
              }
            }
          }
          key: "custom_key"
          is_custom_property: True
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())
    self.assertEqual(
        placeholder_utils.resolve_placeholder_expression(
            pb, self._resolution_context), "custom_value")

    self.assertEqual(
        placeholder_utils.debug_str(pb),
        "input(\"examples\")[0].custom_property(\"custom_key\")")

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 60:</b> &nbsp; 8 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1146')" href="javascript:;">
tfx-1.6.0/tfx/dsl/compiler/placeholder_utils_test.py: 385-423
</a>
<div class="mid" id="frag1146" style="display:none"><pre>
  def testProtoExecPropertyPrimitiveField(self):
    # Access a non-message type proto field
    placeholder_expression = """
      operator {
        index_op {
          expression {
            operator {
              proto_op {
                expression {
                  placeholder {
                    type: EXEC_PROPERTY
                    key: "proto_property"
                  }
                }
                proto_schema {
                  message_type: "tfx.components.infra_validator.ServingSpec"
                }
                proto_field_path: ".tensorflow_serving"
                proto_field_path: ".tags"
              }
            }
          }
          index: 1
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())

    # Prepare FileDescriptorSet
    fd = descriptor_pb2.FileDescriptorProto()
    infra_validator_pb2.ServingSpec().DESCRIPTOR.file.CopyToProto(fd)
    pb.operator.index_op.expression.operator.proto_op.proto_schema.file_descriptors.file.append(
        fd)

    self.assertEqual(
        placeholder_utils.resolve_placeholder_expression(
            pb, self._resolution_context), "1.15.0-gpu")

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1157')" href="javascript:;">
tfx-1.6.0/tfx/dsl/compiler/placeholder_utils_test.py: 714-752
</a>
<div class="mid" id="frag1157" style="display:none"><pre>
  def testProtoSerializationJSON(self):
    placeholder_expression = """
      operator {
        proto_op {
          expression {
            placeholder {
              type: EXEC_PROPERTY
              key: "proto_property"
            }
          }
          proto_schema {
            message_type: "tfx.components.infra_validator.ServingSpec"
          }
          serialization_format: JSON
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())

    # Prepare FileDescriptorSet
    fd = descriptor_pb2.FileDescriptorProto()
    infra_validator_pb2.ServingSpec().DESCRIPTOR.file.CopyToProto(fd)
    pb.operator.proto_op.proto_schema.file_descriptors.file.append(fd)

    expected_json_serialization = """\
{
  "tensorflow_serving": {
    "tags": [
      "latest",
      "1.15.0-gpu"
    ]
  }
}"""

    self.assertEqual(
        placeholder_utils.resolve_placeholder_expression(
            pb, self._resolution_context), expected_json_serialization)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1156')" href="javascript:;">
tfx-1.6.0/tfx/dsl/compiler/placeholder_utils_test.py: 683-713
</a>
<div class="mid" id="frag1156" style="display:none"><pre>
  def testProtoRuntimeInfoNoneAccess(self):
    # Access a missing platform config.
    placeholder_expression = """
      operator {
        proto_op {
          expression {
            placeholder {
              type: RUNTIME_INFO
              key: "platform_config"
            }
          }
          proto_schema {
            message_type: "tfx.components.infra_validator.ServingSpec"
          }
          proto_field_path: ".tensorflow_serving"
          proto_field_path: ".tags"
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())

    # Prepare FileDescriptorSet
    fd = descriptor_pb2.FileDescriptorProto()
    infra_validator_pb2.ServingSpec().DESCRIPTOR.file.CopyToProto(fd)
    pb.operator.proto_op.proto_schema.file_descriptors.file.append(fd)

    self.assertIsNone(
        placeholder_utils.resolve_placeholder_expression(
            pb, self._none_resolution_context))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1151')" href="javascript:;">
tfx-1.6.0/tfx/dsl/compiler/placeholder_utils_test.py: 558-588
</a>
<div class="mid" id="frag1151" style="display:none"><pre>
  def testProtoExecPropertyRepeatedField(self):
    # Access a repeated field.
    placeholder_expression = """
      operator {
        proto_op {
          expression {
            placeholder {
              type: EXEC_PROPERTY
              key: "proto_property"
            }
          }
          proto_schema {
            message_type: "tfx.components.infra_validator.ServingSpec"
          }
          proto_field_path: ".tensorflow_serving"
          proto_field_path: ".tags"
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())

    # Prepare FileDescriptorSet
    fd = descriptor_pb2.FileDescriptorProto()
    infra_validator_pb2.ServingSpec().DESCRIPTOR.file.CopyToProto(fd)
    pb.operator.proto_op.proto_schema.file_descriptors.file.append(fd)

    self.assertEqual(
        placeholder_utils.resolve_placeholder_expression(
            pb, self._resolution_context), ["latest", "1.15.0-gpu"])

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1153')" href="javascript:;">
tfx-1.6.0/tfx/dsl/compiler/placeholder_utils_test.py: 619-649
</a>
<div class="mid" id="frag1153" style="display:none"><pre>
  def testProtoExecPropertyNoneAccess(self):
    # Access a missing optional exec property.
    placeholder_expression = """
      operator {
        proto_op {
          expression {
            placeholder {
              type: EXEC_PROPERTY
              key: "proto_property"
            }
          }
          proto_schema {
            message_type: "tfx.components.infra_validator.ServingSpec"
          }
          proto_field_path: ".tensorflow_serving"
          proto_field_path: ".tags"
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())

    # Prepare FileDescriptorSet
    fd = descriptor_pb2.FileDescriptorProto()
    infra_validator_pb2.ServingSpec().DESCRIPTOR.file.CopyToProto(fd)
    pb.operator.proto_op.proto_schema.file_descriptors.file.append(fd)

    self.assertIsNone(
        placeholder_utils.resolve_placeholder_expression(
            pb, self._none_resolution_context))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1150')" href="javascript:;">
tfx-1.6.0/tfx/dsl/compiler/placeholder_utils_test.py: 525-557
</a>
<div class="mid" id="frag1150" style="display:none"><pre>
  def testProtoExecPropertyMessageFieldTextFormat(self):
    # Access a message type proto field
    placeholder_expression = """
      operator {
        proto_op {
          expression {
            placeholder {
              type: EXEC_PROPERTY
              key: "proto_property"
            }
          }
          proto_schema {
            message_type: "tfx.components.infra_validator.ServingSpec"
          }
          proto_field_path: ".tensorflow_serving"
          serialization_format: TEXT_FORMAT
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())

    fd = descriptor_pb2.FileDescriptorProto()
    infra_validator_pb2.ServingSpec().DESCRIPTOR.file.CopyToProto(fd)
    pb.operator.proto_op.proto_schema.file_descriptors.file.append(fd)

    # If proto_field_path points to a message type field, the message will
    # be rendered using text_format.
    self.assertEqual(
        placeholder_utils.resolve_placeholder_expression(
            pb, self._resolution_context),
        "tags: \"latest\"\ntags: \"1.15.0-gpu\"\n")

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1158')" href="javascript:;">
tfx-1.6.0/tfx/dsl/compiler/placeholder_utils_test.py: 753-780
</a>
<div class="mid" id="frag1158" style="display:none"><pre>
  def testProtoWithoutSerializationFormat(self):
    placeholder_expression = """
      operator {
        proto_op {
          expression {
            placeholder {
              type: EXEC_PROPERTY
              key: "proto_property"
            }
          }
          proto_schema {
            message_type: "tfx.components.infra_validator.ServingSpec"
          }
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())

    # Prepare FileDescriptorSet
    fd = descriptor_pb2.FileDescriptorProto()
    infra_validator_pb2.ServingSpec().DESCRIPTOR.file.CopyToProto(fd)
    pb.operator.proto_op.proto_schema.file_descriptors.file.append(fd)

    with self.assertRaises(ValueError):
      placeholder_utils.resolve_placeholder_expression(pb,
                                                       self._resolution_context)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1152')" href="javascript:;">
tfx-1.6.0/tfx/dsl/compiler/placeholder_utils_test.py: 589-618
</a>
<div class="mid" id="frag1152" style="display:none"><pre>
  def testProtoExecPropertyInvalidField(self):
    # Access a repeated field.
    placeholder_expression = """
      operator {
        proto_op {
          expression {
            placeholder {
              type: EXEC_PROPERTY
              key: "proto_property"
            }
          }
          proto_schema {
            message_type: "tfx.components.infra_validator.ServingSpec"
          }
          proto_field_path: ".some_invalid_field"
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())

    # Prepare FileDescriptorSet
    fd = descriptor_pb2.FileDescriptorProto()
    infra_validator_pb2.ServingSpec().DESCRIPTOR.file.CopyToProto(fd)
    pb.operator.proto_op.proto_schema.file_descriptors.file.append(fd)

    with self.assertRaises(ValueError):
      placeholder_utils.resolve_placeholder_expression(pb,
                                                       self._resolution_context)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 61:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1184')" href="javascript:;">
tfx-1.6.0/tfx/dsl/context_managers/context_manager_test.py: 102-115
</a>
<div class="mid" id="frag1184" style="display:none"><pre>
  def testRegistry_AllContexts(self):
    registry = self.reset_registry()
    bg = registry.background_context

    self.assertEqual(registry.all_contexts, [bg])
    with _FakeContextManager() as c1:
      self.assertEqual(registry.all_contexts, [bg, c1])
      with _FakeContextManager() as c2:
        self.assertEqual(registry.all_contexts, [bg, c1, c2])
        with _FakeContextManager() as c3:
          self.assertEqual(registry.all_contexts, [bg, c1, c2, c3])
      with _FakeContextManager() as c4:
        self.assertEqual(registry.all_contexts, [bg, c1, c2, c3, c4])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1185')" href="javascript:;">
tfx-1.6.0/tfx/dsl/context_managers/context_manager_test.py: 116-129
</a>
<div class="mid" id="frag1185" style="display:none"><pre>
  def testRegistry_ActiveContexts(self):
    registry = self.reset_registry()
    bg = registry.background_context

    self.assertEqual(registry.active_contexts, [bg])
    with _FakeContextManager() as c1:
      self.assertEqual(registry.active_contexts, [bg, c1])
      with _FakeContextManager() as c2:
        self.assertEqual(registry.active_contexts, [bg, c1, c2])
        with _FakeContextManager() as c3:
          self.assertEqual(registry.active_contexts, [bg, c1, c2, c3])
      with _FakeContextManager() as c4:
        self.assertEqual(registry.active_contexts, [bg, c1, c4])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 62:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1232')" href="javascript:;">
tfx-1.6.0/tfx/dsl/io/plugins/local_test.py: 26-62
</a>
<div class="mid" id="frag1232" style="display:none"><pre>
  def testNotFound(self):
    temp_dir = tempfile.mkdtemp()

    with self.assertRaises(NotFoundError):
      LocalFilesystem.open(os.path.join(temp_dir, 'foo')).read()

    with self.assertRaises(NotFoundError):
      LocalFilesystem.copy(
          os.path.join(temp_dir, 'foo'), os.path.join(temp_dir, 'bar'))

    # No exception raised.
    self.assertEqual(LocalFilesystem.glob(os.path.join(temp_dir, 'foo/*')), [])

    # No exception raised.
    self.assertEqual(
        LocalFilesystem.isdir(os.path.join(temp_dir, 'foo/bar')), False)

    with self.assertRaises(NotFoundError):
      LocalFilesystem.listdir(os.path.join(temp_dir, 'foo'))

    with self.assertRaises(NotFoundError):
      LocalFilesystem.mkdir(os.path.join(temp_dir, 'foo/bar'))

    with self.assertRaises(NotFoundError):
      LocalFilesystem.remove(os.path.join(temp_dir, 'foo'))

    with self.assertRaises(NotFoundError):
      LocalFilesystem.rmtree(os.path.join(temp_dir, 'foo'))

    with self.assertRaises(NotFoundError):
      LocalFilesystem.stat(os.path.join(temp_dir, 'foo'))

    # No exception raised.
    self.assertEqual(
        list(LocalFilesystem.walk(os.path.join(temp_dir, 'foo'))), [])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1233')" href="javascript:;">
tfx-1.6.0/tfx/dsl/io/plugins/tensorflow_gfile_test.py: 27-65
</a>
<div class="mid" id="frag1233" style="display:none"><pre>
  def testNotFound(self):
    temp_dir = tempfile.mkdtemp()

    # Because the GFile implementation delays I/O until necessary, we cannot
    # catch `NotFoundError` here, so this does not raise an error.
    TensorflowFilesystem.open(os.path.join(temp_dir, 'foo'))

    with self.assertRaises(NotFoundError):
      TensorflowFilesystem.copy(
          os.path.join(temp_dir, 'foo'), os.path.join(temp_dir, 'bar'))

    # No exception raised.
    self.assertEqual(
        TensorflowFilesystem.glob(os.path.join(temp_dir, 'foo/*')), [])

    # No exception raised.
    self.assertEqual(
        TensorflowFilesystem.isdir(os.path.join(temp_dir, 'foo/bar')), False)

    with self.assertRaises(NotFoundError):
      TensorflowFilesystem.listdir(os.path.join(temp_dir, 'foo'))

    with self.assertRaises(NotFoundError):
      TensorflowFilesystem.mkdir(os.path.join(temp_dir, 'foo/bar'))

    with self.assertRaises(NotFoundError):
      TensorflowFilesystem.remove(os.path.join(temp_dir, 'foo'))

    with self.assertRaises(NotFoundError):
      TensorflowFilesystem.rmtree(os.path.join(temp_dir, 'foo'))

    with self.assertRaises(NotFoundError):
      TensorflowFilesystem.stat(os.path.join(temp_dir, 'foo'))

    # No exception raised.
    self.assertEqual(
        list(TensorflowFilesystem.walk(os.path.join(temp_dir, 'foo'))), [])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 63:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1301')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/kubeflow/v2/kubeflow_v2_dag_runner_test.py: 67-82
</a>
<div class="mid" id="frag1301" style="display:none"><pre>
  def testCompileTwoStepPipeline(self, fake_now, fake_sys_version):
    fake_now.return_value = datetime.date(2020, 1, 1)
    fake_sys_version.major = 3
    fake_sys_version.minor = 7
    runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(
        output_dir=_TEST_DIR,
        output_filename=_TEST_FILE_NAME,
        config=kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(
            display_name='my-pipeline',
            default_image='gcr.io/my-tfx:latest'))

    self._compare_against_testdata(
        runner=runner,
        pipeline=test_utils.two_step_pipeline(),
        golden_file='expected_two_step_pipeline_job.json')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1302')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/kubeflow/v2/kubeflow_v2_dag_runner_test.py: 86-102
</a>
<div class="mid" id="frag1302" style="display:none"><pre>
  def testCompileFullTaxiPipeline(self, fake_now, fake_sys_version):
    fake_now.return_value = datetime.date(2020, 1, 1)
    fake_sys_version.major = 3
    fake_sys_version.minor = 7
    runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(
        output_dir=_TEST_DIR,
        output_filename=_TEST_FILE_NAME,
        config=kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(
            display_name='my-pipeline',
            default_image='tensorflow/tfx:latest'))

    self._compare_against_testdata(
        runner=runner,
        pipeline=test_utils.full_taxi_pipeline(),
        golden_file='expected_full_taxi_pipeline_job.json')


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 64:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1315')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/kubeflow/v2/e2e_tests/bigquery_integration_test.py: 51-76
</a>
<div class="mid" id="frag1315" style="display:none"><pre>
  def testSimpleEnd2EndPipeline(self):
    """End-to-End test for a simple pipeline."""
    pipeline_name = 'kubeflow-v2-bqeg-test-{}'.format(test_utils.random_id())

    components = kubeflow_v2_test_utils.create_pipeline_components(
        pipeline_root=self._pipeline_root(pipeline_name),
        transform_module=self._MODULE_FILE,
        trainer_module=self._MODULE_FILE,
        bigquery_query=_BIGQUERY_QUERY)

    beam_pipeline_args = [
        '--temp_location=' +
        os.path.join(self._pipeline_root(pipeline_name), 'dataflow', 'temp'),
        '--project={}'.format(self._GCP_PROJECT_ID),
        # TODO(b/171733562): Remove `use_runner_v2` once it is the default for
        # Dataflow.
        '--experiments=use_runner_v2',
        '--worker_harness_container_image=%s' % self.container_image,
    ]

    pipeline = self._create_pipeline(pipeline_name, components,
                                     beam_pipeline_args)

    self._run_pipeline(pipeline)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1317')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/kubeflow/v2/e2e_tests/csv_example_gen_integration_test.py: 31-52
</a>
<div class="mid" id="frag1317" style="display:none"><pre>
  def testSimpleEnd2EndPipeline(self):
    """End-to-End test for a simple pipeline."""
    pipeline_name = 'kubeflow-v2-fbeg-test-{}'.format(test_utils.random_id())

    components = kubeflow_v2_test_utils.create_pipeline_components(
        pipeline_root=self._pipeline_root(pipeline_name),
        transform_module=self._MODULE_FILE,
        trainer_module=self._MODULE_FILE,
        csv_input_location=_TEST_DATA_ROOT)

    beam_pipeline_args = [
        '--temp_location=' +
        os.path.join(self._pipeline_root(pipeline_name), 'dataflow', 'temp'),
        '--project={}'.format(self._GCP_PROJECT_ID)
    ]

    pipeline = self._create_pipeline(pipeline_name, components,
                                     beam_pipeline_args)

    self._run_pipeline(pipeline)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 65:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1391')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/kubeflow/container_entrypoint.py: 260-289
</a>
<div class="mid" id="frag1391" style="display:none"><pre>
  def _dump_input_populated_artifacts(
      node_inputs: MutableMapping[str, pipeline_pb2.InputSpec],
      name_to_artifacts: Dict[str, List[artifact.Artifact]]) -&gt; List[str]:
    """Dump artifacts markdown string for inputs.

    Args:
      node_inputs: maps from input name to input sepc proto.
      name_to_artifacts: maps from input key to list of populated artifacts.

    Returns:
      A list of dumped markdown string, each of which represents a channel.
    """
    rendered_list = []
    for name, spec in node_inputs.items():
      # Need to look for materialized artifacts in the execution decision.
      rendered_artifacts = ''.join([
          _render_artifact_as_mdstr(single_artifact)
          for single_artifact in name_to_artifacts.get(name, [])
      ])
      # There must be at least a channel in a input, and all channels in a input
      # share the same artifact type.
      artifact_type = spec.channels[0].artifact_query.type.name
      rendered_list.append(
          '## {name}\n\n**Type**: {channel_type}\n\n{artifacts}'.format(
              name=_sanitize_underscore(name),
              channel_type=_sanitize_underscore(artifact_type),
              artifacts=rendered_artifacts))

    return rendered_list

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1392')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/kubeflow/container_entrypoint.py: 290-319
</a>
<div class="mid" id="frag1392" style="display:none"><pre>
  def _dump_output_populated_artifacts(
      node_outputs: MutableMapping[str, pipeline_pb2.OutputSpec],
      name_to_artifacts: Dict[str, List[artifact.Artifact]]) -&gt; List[str]:
    """Dump artifacts markdown string for outputs.

    Args:
      node_outputs: maps from output name to output sepc proto.
      name_to_artifacts: maps from output key to list of populated artifacts.

    Returns:
      A list of dumped markdown string, each of which represents a channel.
    """
    rendered_list = []
    for name, spec in node_outputs.items():
      # Need to look for materialized artifacts in the execution decision.
      rendered_artifacts = ''.join([
          _render_artifact_as_mdstr(single_artifact)
          for single_artifact in name_to_artifacts.get(name, [])
      ])
      # There must be at least a channel in a input, and all channels in a input
      # share the same artifact type.
      artifact_type = spec.artifact_spec.type.name
      rendered_list.append(
          '## {name}\n\n**Type**: {channel_type}\n\n{artifacts}'.format(
              name=_sanitize_underscore(name),
              channel_type=_sanitize_underscore(artifact_type),
              artifacts=rendered_artifacts))

    return rendered_list

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 66:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1400')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/kubeflow/e2e_tests/kubeflow_dataflow_integration_test.py: 77-89
</a>
<div class="mid" id="frag1400" style="display:none"><pre>
  def testTransformOnDataflowRunner(self):
    """Transform-only test pipeline on DataflowRunner."""
    pipeline_name = 'kubeflow-transform-dataflow-test-{}'.format(
        test_utils.random_id())
    pipeline = self._create_dataflow_pipeline(pipeline_name, [
        self.raw_examples_importer, self.schema_importer,
        Transform(
            examples=self.raw_examples_importer.outputs['result'],
            schema=self.schema_importer.outputs['result'],
            module_file=self._transform_module)
    ])
    self._compile_and_run_pipeline(pipeline)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1401')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/kubeflow/e2e_tests/kubeflow_dataflow_integration_test.py: 90-106
</a>
<div class="mid" id="frag1401" style="display:none"><pre>
  def testEvaluatorOnDataflowRunner(self):
    """Evaluator-only test pipeline on DataflowRunner."""
    pipeline_name = 'kubeflow-evaluator-dataflow-test-{}'.format(
        test_utils.random_id())
    pipeline = self._create_dataflow_pipeline(pipeline_name, [
        self.raw_examples_importer, self.model_1_importer,
        Evaluator(
            examples=self.raw_examples_importer.outputs['result'],
            model=self.model_1_importer.outputs['result'],
            feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[
                evaluator_pb2.SingleSlicingSpec(
                    column_for_slicing=['trip_start_hour'])
            ]))
    ])
    self._compile_and_run_pipeline(pipeline)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 67:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1420')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/kubeflow/e2e_tests/kubeflow_gcp_integration_test.py: 177-200
</a>
<div class="mid" id="frag1420" style="display:none"><pre>
  def testAIPlatformTrainerPipeline(self):
    """Trainer-only test pipeline on AI Platform Training."""
    pipeline_name = self._make_unique_pipeline_name('kubeflow-aip-trainer')
    pipeline = self._create_pipeline(pipeline_name, [
        self.schema_importer, self.transformed_examples_importer,
        self.transform_graph_importer,
        Trainer(
            custom_executor_spec=executor_spec.ExecutorClassSpec(
                ai_platform_trainer_executor.Executor),
            module_file=self._trainer_module,
            transformed_examples=self.transformed_examples_importer
            .outputs['result'],
            schema=self.schema_importer.outputs['result'],
            transform_graph=self.transform_graph_importer.outputs['result'],
            train_args=trainer_pb2.TrainArgs(num_steps=10),
            eval_args=trainer_pb2.EvalArgs(num_steps=5),
            custom_config={
                ai_platform_trainer_executor.TRAINING_ARGS_KEY:
                    self._getCaipTrainingArgsForDistributed(pipeline_name)
            })
    ])
    self._compile_and_run_pipeline(pipeline)
    self._assertNumberOfTrainerOutputIsOne(pipeline_name)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1421')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/kubeflow/e2e_tests/kubeflow_gcp_integration_test.py: 201-229
</a>
<div class="mid" id="frag1421" style="display:none"><pre>
  def testAIPlatformGenericTrainerPipeline(self):
    """Trainer-only pipeline on AI Platform Training with GenericTrainer."""
    pipeline_name = self._make_unique_pipeline_name(
        'kubeflow-aip-generic-trainer')
    pipeline = self._create_pipeline(pipeline_name, [
        self.schema_importer, self.transformed_examples_importer,
        self.transform_graph_importer,
        Trainer(
            custom_executor_spec=executor_spec.ExecutorClassSpec(
                ai_platform_trainer_executor.GenericExecutor),
            module_file=self._trainer_module,
            transformed_examples=self.transformed_examples_importer
            .outputs['result'],
            schema=self.schema_importer.outputs['result'],
            transform_graph=self.transform_graph_importer.outputs['result'],
            train_args=trainer_pb2.TrainArgs(num_steps=10),
            eval_args=trainer_pb2.EvalArgs(num_steps=5),
            custom_config={
                ai_platform_trainer_executor.TRAINING_ARGS_KEY:
                    self._getCaipTrainingArgs(pipeline_name)
            })
    ])
    self._compile_and_run_pipeline(pipeline)
    self._assertNumberOfTrainerOutputIsOne(pipeline_name)

  # TODO(b/150661783): Add tests using distributed training with a generic
  #  trainer.
  # TODO(b/150576271): Add Trainer tests using Keras models.

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 68:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1423')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/kubeflow/e2e_tests/kubeflow_gcp_integration_test.py: 243-271
</a>
<div class="mid" id="frag1423" style="display:none"><pre>
  def testVertexDistributedTunerPipeline(self):
    """Tuner-only pipeline for distributed Tuner flock on Vertex AI Training."""
    pipeline_name = self._make_unique_pipeline_name(
        'kubeflow-vertex-dist-tuner')
    pipeline = self._create_pipeline(
        pipeline_name,
        [
            self.penguin_examples_importer,
            self.penguin_schema_importer,
            ai_platform_tuner_component.Tuner(
                examples=self.penguin_examples_importer.outputs['result'],
                module_file=self._penguin_tuner_module,
                schema=self.penguin_schema_importer.outputs['result'],
                train_args=trainer_pb2.TrainArgs(num_steps=10),
                eval_args=trainer_pb2.EvalArgs(num_steps=5),
                # 3 worker parallel tuning.
                tune_args=tuner_pb2.TuneArgs(num_parallel_trials=3),
                custom_config={
                    ai_platform_tuner_executor.TUNING_ARGS_KEY:
                        self._getVertexTrainingArgs(pipeline_name),
                    constants.ENABLE_VERTEX_KEY:
                        True,
                    constants.VERTEX_REGION_KEY:
                        self._GCP_REGION
                })
        ])
    self._compile_and_run_pipeline(pipeline)
    self._assertHyperparametersAreWritten(pipeline_name)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1424')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/kubeflow/e2e_tests/kubeflow_gcp_integration_test.py: 272-295
</a>
<div class="mid" id="frag1424" style="display:none"><pre>
  def testAIPlatformDistributedTunerPipeline(self):
    """Tuner-only pipeline for distributed Tuner flock on AIP Training."""
    pipeline_name = self._make_unique_pipeline_name('kubeflow-aip-dist-tuner')
    pipeline = self._create_pipeline(
        pipeline_name,
        [
            self.penguin_examples_importer,
            self.penguin_schema_importer,
            ai_platform_tuner_component.Tuner(
                examples=self.penguin_examples_importer.outputs['result'],
                module_file=self._penguin_tuner_module,
                schema=self.penguin_schema_importer.outputs['result'],
                train_args=trainer_pb2.TrainArgs(num_steps=10),
                eval_args=trainer_pb2.EvalArgs(num_steps=5),
                # 3 worker parallel tuning.
                tune_args=tuner_pb2.TuneArgs(num_parallel_trials=3),
                custom_config={
                    ai_platform_tuner_executor.TUNING_ARGS_KEY:
                        self._getCaipTrainingArgs(pipeline_name)
                })
        ])
    self._compile_and_run_pipeline(pipeline)
    self._assertHyperparametersAreWritten(pipeline_name)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 69:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1427')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/kubeflow/e2e_tests/kubeflow_gcp_integration_test.py: 323-338
</a>
<div class="mid" id="frag1427" style="display:none"><pre>
    def _pusher(model_importer, model_blessing_importer, bigquery_dataset_id):
      return Pusher(
          custom_executor_spec=executor_spec.ExecutorClassSpec(
              bigquery_pusher_executor.Executor),
          model=model_importer.outputs['result'],
          model_blessing=model_blessing_importer.outputs['result'],
          custom_config={
              bigquery_pusher_executor.SERVING_ARGS_KEY: {
                  'bq_dataset_id': bigquery_dataset_id,
                  'model_name': pipeline_name,
                  'project_id': self._GCP_PROJECT_ID,
              }
          },
      )

    # The model list should be empty
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1431')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/kubeflow/e2e_tests/kubeflow_gcp_integration_test.py: 378-393
</a>
<div class="mid" id="frag1431" style="display:none"><pre>
    def _pusher(model_importer, model_blessing_importer):
      return Pusher(
          custom_executor_spec=executor_spec.ExecutorClassSpec(
              ai_platform_pusher_executor.Executor),
          model=model_importer.outputs['result'],
          model_blessing=model_blessing_importer.outputs['result'],
          custom_config={
              tfx.extensions.google_cloud_ai_platform.experimental
              .PUSHER_SERVING_ARGS_KEY: {
                  'model_name': model_name,
                  'project_id': self._GCP_PROJECT_ID,
              }
          },
      )

    # Use default service_name / api_version.
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 70:</b> &nbsp; 2 fragments, nominal size 77 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1458')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/launcher/kubernetes_component_launcher.py: 52-162
</a>
<div class="mid" id="frag1458" style="display:none"><pre>
  def _run_executor(self, execution_id: int,
                    input_dict: Dict[str, List[types.Artifact]],
                    output_dict: Dict[str, List[types.Artifact]],
                    exec_properties: Dict[str, Any]) -&gt; None:
    """Execute underlying component implementation.

    Runs executor container in a Kubernetes Pod and wait until it goes into
    `Succeeded` or `Failed` state.

    Args:
      execution_id: The ID of the execution.
      input_dict: Input dict from input key to a list of Artifacts. These are
        often outputs of another component in the pipeline and passed to the
        component by the orchestration system.
      output_dict: Output dict from output key to a list of Artifacts. These are
        often consumed by a dependent component.
      exec_properties: A dict of execution properties. These are inputs to
        pipeline with primitive types (int, string, float) and fully
        materialized when a pipeline is constructed. No dependency to other
        component or later injection from orchestration systems is necessary or
        possible on these values.

    Raises:
      RuntimeError: when the pod is in `Failed` state or unexpected failure from
      Kubernetes API.

    """

    container_spec = cast(executor_spec.ExecutorContainerSpec,
                          self._component_executor_spec)

    # Replace container spec with jinja2 template.
    container_spec = container_common.resolve_container_template(
        container_spec, input_dict, output_dict, exec_properties)
    pod_name = self._build_pod_name(execution_id)
    # TODO(hongyes): replace the default value from component config.
    try:
      namespace = kube_utils.get_kfp_namespace()
    except RuntimeError:
      namespace = 'kubeflow'

    pod_manifest = self._build_pod_manifest(pod_name, container_spec)
    core_api = kube_utils.make_core_v1_api()

    if kube_utils.is_inside_kfp():
      launcher_pod = kube_utils.get_current_kfp_pod(core_api)
      pod_manifest['spec']['serviceAccount'] = launcher_pod.spec.service_account
      pod_manifest['spec'][
          'serviceAccountName'] = launcher_pod.spec.service_account_name
      pod_manifest['metadata'][
          'ownerReferences'] = container_common.to_swagger_dict(
              launcher_pod.metadata.owner_references)
    else:
      pod_manifest['spec']['serviceAccount'] = kube_utils.TFX_SERVICE_ACCOUNT
      pod_manifest['spec'][
          'serviceAccountName'] = kube_utils.TFX_SERVICE_ACCOUNT

    logging.info('Looking for pod "%s:%s".', namespace, pod_name)
    resp = kube_utils.get_pod(core_api, pod_name, namespace)
    if not resp:
      logging.info('Pod "%s:%s" does not exist. Creating it...',
                   namespace, pod_name)
      logging.info('Pod manifest: %s', pod_manifest)
      try:
        resp = core_api.create_namespaced_pod(
            namespace=namespace, body=pod_manifest)
      except client.rest.ApiException as e:
        raise RuntimeError(
            'Failed to created container executor pod!\nReason: %s\nBody: %s' %
            (e.reason, e.body))

    # Wait up to 300 seconds for the pod to move from pending to another status.
    logging.info('Waiting for pod "%s:%s" to start.', namespace, pod_name)
    kube_utils.wait_pod(
        core_api,
        pod_name,
        namespace,
        exit_condition_lambda=kube_utils.pod_is_not_pending,
        condition_description='non-pending status',
        timeout_sec=300)

    logging.info('Start log streaming for pod "%s:%s".', namespace, pod_name)
    try:
      logs = core_api.read_namespaced_pod_log(
          name=pod_name,
          namespace=namespace,
          container=kube_utils.ARGO_MAIN_CONTAINER_NAME,
          follow=True,
          _preload_content=False).stream()
    except client.rest.ApiException as e:
      raise RuntimeError(
          'Failed to stream the logs from the pod!\nReason: %s\nBody: %s' %
          (e.reason, e.body))

    for log in logs:
      logging.info(log.decode().rstrip('\n'))

    # Wait indefinitely for the pod to complete.
    resp = kube_utils.wait_pod(
        core_api,
        pod_name,
        namespace,
        exit_condition_lambda=kube_utils.pod_is_done,
        condition_description='done state')

    if resp.status.phase == kube_utils.PodPhase.FAILED.value:
      raise RuntimeError('Pod "%s:%s" failed with status "%s".' %
                         (namespace, pod_name, resp.status))

    logging.info('Pod "%s:%s" is done.', namespace, pod_name)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1882')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/kubernetes_executor_operator.py: 46-161
</a>
<div class="mid" id="frag1882" style="display:none"><pre>
  def run_executor(
      self, execution_info: data_types.ExecutionInfo
  ) -&gt; execution_result_pb2.ExecutorOutput:
    """Execute underlying component implementation.

    Runs executor container in a Kubernetes Pod and wait until it goes into
    `Succeeded` or `Failed` state.

    Args:
      execution_info: All the information that the launcher provides.

    Raises:
      RuntimeError: when the pod is in `Failed` state or unexpected failure from
      Kubernetes API.

    Returns:
      An ExecutorOutput instance

    """

    context = placeholder_utils.ResolutionContext(
        exec_info=execution_info,
        executor_spec=self._executor_spec,
        platform_config=self._platform_config)

    container_spec = executor_specs.TemplatedExecutorContainerSpec(
        image=self._container_executor_spec.image,
        command=[
            placeholder_utils.resolve_placeholder_expression(cmd, context)
            for cmd in self._container_executor_spec.commands
        ] or None,
        args=[
            placeholder_utils.resolve_placeholder_expression(arg, context)
            for arg in self._container_executor_spec.args
        ] or None,
    )

    pod_name = self._build_pod_name(execution_info)
    # TODO(hongyes): replace the default value from component config.
    try:
      namespace = kube_utils.get_kfp_namespace()
    except RuntimeError:
      namespace = 'kubeflow'

    pod_manifest = self._build_pod_manifest(pod_name, container_spec)
    core_api = kube_utils.make_core_v1_api()

    if kube_utils.is_inside_kfp():
      launcher_pod = kube_utils.get_current_kfp_pod(core_api)
      pod_manifest['spec']['serviceAccount'] = launcher_pod.spec.service_account
      pod_manifest['spec'][
          'serviceAccountName'] = launcher_pod.spec.service_account_name
      pod_manifest['metadata'][
          'ownerReferences'] = container_common.to_swagger_dict(
              launcher_pod.metadata.owner_references)
    else:
      pod_manifest['spec']['serviceAccount'] = kube_utils.TFX_SERVICE_ACCOUNT
      pod_manifest['spec'][
          'serviceAccountName'] = kube_utils.TFX_SERVICE_ACCOUNT

    logging.info('Looking for pod "%s:%s".', namespace, pod_name)
    resp = kube_utils.get_pod(core_api, pod_name, namespace)
    if not resp:
      logging.info('Pod "%s:%s" does not exist. Creating it...',
                   namespace, pod_name)
      logging.info('Pod manifest: %s', pod_manifest)
      try:
        resp = core_api.create_namespaced_pod(
            namespace=namespace, body=pod_manifest)
      except client.rest.ApiException as e:
        raise RuntimeError(
            'Failed to created container executor pod!\nReason: %s\nBody: %s' %
            (e.reason, e.body))

    # Wait up to 300 seconds for the pod to move from pending to another status.
    logging.info('Waiting for pod "%s:%s" to start.', namespace, pod_name)
    kube_utils.wait_pod(
        core_api,
        pod_name,
        namespace,
        exit_condition_lambda=kube_utils.pod_is_not_pending,
        condition_description='non-pending status',
        timeout_sec=300)

    logging.info('Start log streaming for pod "%s:%s".', namespace, pod_name)
    try:
      logs = core_api.read_namespaced_pod_log(
          name=pod_name,
          namespace=namespace,
          container=kube_utils.ARGO_MAIN_CONTAINER_NAME,
          follow=True,
          _preload_content=False).stream()
    except client.rest.ApiException as e:
      raise RuntimeError(
          'Failed to stream the logs from the pod!\nReason: %s\nBody: %s' %
          (e.reason, e.body))

    for log in logs:
      logging.info(log.decode().rstrip('\n'))

    # Wait indefinitely for the pod to complete.
    resp = kube_utils.wait_pod(
        core_api,
        pod_name,
        namespace,
        exit_condition_lambda=kube_utils.pod_is_done,
        condition_description='done state')

    if resp.status.phase == kube_utils.PodPhase.FAILED.value:
      raise RuntimeError('Pod "%s:%s" failed with status "%s".' %
                         (namespace, pod_name, resp.status))

    logging.info('Pod "%s:%s" is done.', namespace, pod_name)

    return execution_result_pb2.ExecutorOutput()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 71:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1472')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/launcher/docker_component_launcher_test.py: 104-138
</a>
<div class="mid" id="frag1472" style="display:none"><pre>
  def _create_launcher_context(self, component_config=None):
    test_dir = self.get_temp_dir()

    connection_config = metadata_store_pb2.ConnectionConfig()
    connection_config.sqlite.SetInParent()
    metadata_connection = metadata.Metadata(connection_config)

    pipeline_root = os.path.join(test_dir, 'Test')

    input_artifact = test_utils._InputArtifact()
    input_artifact.uri = os.path.join(test_dir, 'input')

    component = test_utils._FakeComponent(
        name='FakeComponent',
        input_channel=channel_utils.as_channel([input_artifact]),
        custom_executor_spec=executor_spec.ExecutorContainerSpec(
            image='gcr://test', args=['{{input_dict["input"][0].uri}}']))

    pipeline_info = data_types.PipelineInfo(
        pipeline_name='Test', pipeline_root=pipeline_root, run_id='123')

    driver_args = data_types.DriverArgs(enable_cache=True)

    launcher = docker_component_launcher.DockerComponentLauncher.create(
        component=component,
        pipeline_info=pipeline_info,
        driver_args=driver_args,
        metadata_connection=metadata_connection,
        beam_pipeline_args=[],
        additional_pipeline_args={},
        component_config=component_config)

    return {'launcher': launcher, 'input_artifact': input_artifact}


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1488')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/launcher/kubernetes_component_launcher_test.py: 256-289
</a>
<div class="mid" id="frag1488" style="display:none"><pre>
  def _create_launcher_context(self, component_config=None):
    test_dir = self.get_temp_dir()

    connection_config = metadata_store_pb2.ConnectionConfig()
    connection_config.sqlite.SetInParent()
    metadata_connection = metadata.Metadata(connection_config)

    pipeline_root = os.path.join(test_dir, 'Test')

    input_artifact = test_utils._InputArtifact()
    input_artifact.uri = os.path.join(test_dir, 'input')

    component = test_utils._FakeComponent(
        name='FakeComponent',
        input_channel=channel_utils.as_channel([input_artifact]),
        custom_executor_spec=executor_spec.ExecutorContainerSpec(
            image='gcr://test', args=['{{input_dict["input"][0].uri}}']))

    pipeline_info = data_types.PipelineInfo(
        pipeline_name='Test', pipeline_root=pipeline_root, run_id='123')

    driver_args = data_types.DriverArgs(enable_cache=True)

    launcher = kubernetes_component_launcher.KubernetesComponentLauncher.create(
        component=component,
        pipeline_info=pipeline_info,
        driver_args=driver_args,
        metadata_connection=metadata_connection,
        beam_pipeline_args=[],
        additional_pipeline_args={},
        component_config=component_config)

    return {'launcher': launcher, 'input_artifact': input_artifact}

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 72:</b> &nbsp; 5 fragments, nominal size 48 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1485')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/launcher/kubernetes_component_launcher_test.py: 65-119
</a>
<div class="mid" id="frag1485" style="display:none"><pre>
  def testLaunch_loadInClusterSucceed(self, mock_core_api_cls,
                                      mock_incluster_config, mock_publisher,
                                      mock_is_inside_kfp, mock_kfp_namespace):
    mock_publisher.return_value.publish_execution.return_value = {}
    core_api = mock_core_api_cls.return_value
    core_api.read_namespaced_pod.side_effect = [
        self._mock_launcher_pod(),
        client.rest.ApiException(status=404),  # Mock no existing pod state.
        self._mock_executor_pod(
            'Pending'),  # Mock pending state after creation.
        self._mock_executor_pod('Running'),  # Mock running state after pending.
        self._mock_executor_pod('Succeeded'),  # Mock Succeeded state.
    ]
    # Mock successful pod creation.
    core_api.create_namespaced_pod.return_value = client.V1Pod()
    core_api.read_namespaced_pod_log.return_value.stream.return_value = [
        b'log-1'
    ]
    context = self._create_launcher_context()

    context['launcher'].launch()

    self.assertEqual(5, core_api.read_namespaced_pod.call_count)
    core_api.create_namespaced_pod.assert_called_once()
    core_api.read_namespaced_pod_log.assert_called_once()
    _, mock_kwargs = core_api.create_namespaced_pod.call_args
    self.assertEqual(_KFP_NAMESPACE, mock_kwargs['namespace'])
    pod_manifest = mock_kwargs['body']
    self.assertDictEqual(
        {
            'apiVersion': 'v1',
            'kind': 'Pod',
            'metadata': {
                'name':
                    'test-123-fakecomponent-fakecomponent-123',
                'ownerReferences': [{
                    'apiVersion': 'argoproj.io/v1alpha1',
                    'kind': 'Workflow',
                    'name': 'wf-1',
                    'uid': 'wf-uid-1'
                }]
            },
            'spec': {
                'restartPolicy': 'Never',
                'containers': [{
                    'name': 'main',
                    'image': 'gcr://test',
                    'command': None,
                    'args': [context['input_artifact'].uri],
                }],
                'serviceAccount': 'sa-1',
                'serviceAccountName': None
            }
        }, pod_manifest)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1487')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/launcher/kubernetes_component_launcher_test.py: 187-255
</a>
<div class="mid" id="frag1487" style="display:none"><pre>
  def testLaunch_withComponentConfig(self, mock_core_api_cls,
                                     mock_incluster_config, mock_publisher,
                                     mock_is_inside_kfp, mock_kfp_namespace):
    mock_publisher.return_value.publish_execution.return_value = {}
    core_api = mock_core_api_cls.return_value
    core_api.read_namespaced_pod.side_effect = [
        self._mock_launcher_pod(),
        client.rest.ApiException(status=404),  # Mock no existing pod state.
        self._mock_executor_pod(
            'Pending'),  # Mock pending state after creation.
        self._mock_executor_pod('Running'),  # Mock running state after pending.
        self._mock_executor_pod('Succeeded'),  # Mock Succeeded state.
    ]
    # Mock successful pod creation.
    core_api.create_namespaced_pod.return_value = client.V1Pod()
    core_api.read_namespaced_pod_log.return_value.stream.return_value = [
        b'log-1'
    ]
    component_config = kubernetes_component_config.KubernetesComponentConfig(
        client.V1Pod(
            spec=client.V1PodSpec(containers=[
                client.V1Container(
                    name='main', resources={'limits': {
                        'memory': '200mi'
                    }})
            ])))
    context = self._create_launcher_context(component_config)

    context['launcher'].launch()

    self.assertEqual(5, core_api.read_namespaced_pod.call_count)
    core_api.create_namespaced_pod.assert_called_once()
    core_api.read_namespaced_pod_log.assert_called_once()
    _, mock_kwargs = core_api.create_namespaced_pod.call_args
    self.assertEqual(_KFP_NAMESPACE, mock_kwargs['namespace'])
    pod_manifest = mock_kwargs['body']
    print(pod_manifest)
    self.assertDictEqual(
        {
            'apiVersion': 'v1',
            'kind': 'Pod',
            'metadata': {
                'name':
                    'test-123-fakecomponent-fakecomponent-123',
                'ownerReferences': [{
                    'apiVersion': 'argoproj.io/v1alpha1',
                    'kind': 'Workflow',
                    'name': 'wf-1',
                    'uid': 'wf-uid-1'
                }]
            },
            'spec': {
                'restartPolicy': 'Never',
                'containers': [{
                    'name': 'main',
                    'image': 'gcr://test',
                    'command': None,
                    'args': [context['input_artifact'].uri],
                    'resources': {
                        'limits': {
                            'memory': '200mi'
                        }
                    }
                }],
                'serviceAccount': 'sa-1',
                'serviceAccountName': None
            }
        }, pod_manifest)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1486')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/launcher/kubernetes_component_launcher_test.py: 124-175
</a>
<div class="mid" id="frag1486" style="display:none"><pre>
  def testLaunch_loadKubeConfigSucceed(self, mock_core_api_cls,
                                       mock_kube_config, mock_incluster_config,
                                       mock_publisher):
    mock_publisher.return_value.publish_execution.return_value = {}
    mock_incluster_config.side_effect = config.config_exception.ConfigException(
    )
    core_api = mock_core_api_cls.return_value
    core_api.read_namespaced_pod.side_effect = [
        client.rest.ApiException(status=404),  # Mock no existing pod state.
        self._mock_executor_pod(
            'Pending'),  # Mock pending state after creation.
        self._mock_executor_pod('Running'),  # Mock running state after pending.
        self._mock_executor_pod('Succeeded'),  # Mock Succeeded state.
    ]
    # Mock successful pod creation.
    core_api.create_namespaced_pod.return_value = client.V1Pod()
    core_api.read_namespaced_pod_log.return_value.stream.return_value = [
        b'log-1'
    ]
    context = self._create_launcher_context()

    context['launcher'].launch()

    self.assertEqual(4, core_api.read_namespaced_pod.call_count)
    core_api.create_namespaced_pod.assert_called_once()
    core_api.read_namespaced_pod_log.assert_called_once()
    _, mock_kwargs = core_api.create_namespaced_pod.call_args
    self.assertEqual('kubeflow', mock_kwargs['namespace'])
    pod_manifest = mock_kwargs['body']
    self.assertDictEqual(
        {
            'apiVersion': 'v1',
            'kind': 'Pod',
            'metadata': {
                'name': 'test-123-fakecomponent-fakecomponent-123',
            },
            'spec': {
                'restartPolicy':
                    'Never',
                'serviceAccount':
                    'tfx-service-account',
                'serviceAccountName':
                    'tfx-service-account',
                'containers': [{
                    'name': 'main',
                    'image': 'gcr://test',
                    'command': None,
                    'args': [context['input_artifact'].uri],
                }],
            }
        }, pod_manifest)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1886')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/kubernetes_executor_operator_test.py: 85-141
</a>
<div class="mid" id="frag1886" style="display:none"><pre>
  def testLaunch_loadInClusterSucceed(self, mock_core_api_cls,
                                      mock_incluster_config, mock_publisher,
                                      mock_is_inside_kfp, mock_kfp_namespace):
    mock_publisher.return_value.publish_execution.return_value = {}
    core_api = mock_core_api_cls.return_value
    core_api.read_namespaced_pod.side_effect = [
        self._mock_launcher_pod(),
        client.rest.ApiException(status=404),  # Mock no existing pod state.
        self._mock_executor_pod(
            'Pending'),  # Mock pending state after creation.
        self._mock_executor_pod('Running'),  # Mock running state after pending.
        self._mock_executor_pod('Succeeded'),  # Mock Succeeded state.
    ]
    # Mock successful pod creation.
    core_api.create_namespaced_pod.return_value = client.V1Pod()
    core_api.read_namespaced_pod_log.return_value.stream.return_value = [
        b'log-1'
    ]
    context = self._create_launcher_context()

    execution_info = self._set_up_test_execution_info(
        input_dict={'input': [context['input_artifact']]})
    context['operator'].run_executor(execution_info)

    self.assertEqual(5, core_api.read_namespaced_pod.call_count)
    core_api.create_namespaced_pod.assert_called_once()
    core_api.read_namespaced_pod_log.assert_called_once()
    _, mock_kwargs = core_api.create_namespaced_pod.call_args
    self.assertEqual(_KFP_NAMESPACE, mock_kwargs['namespace'])
    pod_manifest = mock_kwargs['body']
    self.assertDictEqual(
        {
            'apiVersion': 'v1',
            'kind': 'Pod',
            'metadata': {
                'name':
                    'test-123-fakecomponent-fakecomponent-123',
                'ownerReferences': [{
                    'apiVersion': 'argoproj.io/v1alpha1',
                    'kind': 'Workflow',
                    'name': 'wf-1',
                    'uid': 'wf-uid-1'
                }]
            },
            'spec': {
                'restartPolicy': 'Never',
                'containers': [{
                    'name': 'main',
                    'image': 'gcr://test',
                    'command': [],
                    'args': [context['input_artifact'].uri],
                }],
                'serviceAccount': 'sa-1',
                'serviceAccountName': None
            }
        }, pod_manifest)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1887')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/kubernetes_executor_operator_test.py: 146-199
</a>
<div class="mid" id="frag1887" style="display:none"><pre>
  def testLaunch_loadKubeConfigSucceed(self, mock_core_api_cls,
                                       mock_kube_config, mock_incluster_config,
                                       mock_publisher):
    mock_publisher.return_value.publish_execution.return_value = {}
    mock_incluster_config.side_effect = config.config_exception.ConfigException(
    )
    core_api = mock_core_api_cls.return_value
    core_api.read_namespaced_pod.side_effect = [
        client.rest.ApiException(status=404),  # Mock no existing pod state.
        self._mock_executor_pod(
            'Pending'),  # Mock pending state after creation.
        self._mock_executor_pod('Running'),  # Mock running state after pending.
        self._mock_executor_pod('Succeeded'),  # Mock Succeeded state.
    ]
    # Mock successful pod creation.
    core_api.create_namespaced_pod.return_value = client.V1Pod()
    core_api.read_namespaced_pod_log.return_value.stream.return_value = [
        b'log-1'
    ]
    context = self._create_launcher_context()

    execution_info = self._set_up_test_execution_info(
        input_dict={'input': [context['input_artifact']]})
    context['operator'].run_executor(execution_info)

    self.assertEqual(4, core_api.read_namespaced_pod.call_count)
    core_api.create_namespaced_pod.assert_called_once()
    core_api.read_namespaced_pod_log.assert_called_once()
    _, mock_kwargs = core_api.create_namespaced_pod.call_args
    self.assertEqual('kubeflow', mock_kwargs['namespace'])
    pod_manifest = mock_kwargs['body']
    self.assertDictEqual(
        {
            'apiVersion': 'v1',
            'kind': 'Pod',
            'metadata': {
                'name': 'test-123-fakecomponent-fakecomponent-123',
            },
            'spec': {
                'restartPolicy':
                    'Never',
                'serviceAccount':
                    'tfx-service-account',
                'serviceAccountName':
                    'tfx-service-account',
                'containers': [{
                    'name': 'main',
                    'image': 'gcr://test',
                    'command': [],
                    'args': [context['input_artifact'].uri],
                }],
            }
        }, pod_manifest)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 73:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1492')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/launcher/docker_component_launcher_e2e_test.py: 51-69
</a>
<div class="mid" id="frag1492" style="display:none"><pre>
def _create_pipeline(
    pipeline_name,
    pipeline_root,
    metadata_path,
    name,
):
  hello_world = _HelloWorldComponent(name=name)

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[hello_world],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      additional_pipeline_args={},
  )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1958')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/docker_executor_operator_e2e_test.py: 51-69
</a>
<div class="mid" id="frag1958" style="display:none"><pre>
def _create_pipeline(
    pipeline_name,
    pipeline_root,
    metadata_path,
    name,
):
  hello_world = _HelloWorldComponent(name=name)

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[hello_world],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      additional_pipeline_args={},
  )


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 74:</b> &nbsp; 13 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1493')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/launcher/docker_component_launcher_e2e_test.py: 72-83
</a>
<div class="mid" id="frag1493" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'docker_e2e_test'
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2902')" href="javascript:;">
tfx-1.6.0/tfx/examples/bert/mrpc/bert_mrpc_pipeline_e2e_test.py: 31-46
</a>
<div class="mid" id="frag2902" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'keras_test'
    self._data_root = os.path.join(os.path.dirname(__file__), 'data')
    self._module_file = os.path.join(
        os.path.dirname(__file__), 'bert_mrpc_utils.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2995')" href="javascript:;">
tfx-1.6.0/tfx/examples/imdb/imdb_pipeline_native_keras_e2e_test.py: 31-46
</a>
<div class="mid" id="frag2995" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'keras_test'
    self._data_root = os.path.join(os.path.dirname(__file__), 'data')
    self._module_file = os.path.join(
        os.path.dirname(__file__), 'imdb_utils_native_keras.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3014')" href="javascript:;">
tfx-1.6.0/tfx/examples/tfjs_next_page_prediction/tfjs_next_page_prediction_e2e_test.py: 28-43
</a>
<div class="mid" id="frag3014" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'page_prediction_test'
    self._data_root = os.path.join(os.path.dirname(__file__), 'data')
    self._module_file = os.path.join(
        os.path.dirname(__file__), 'tfjs_next_page_prediction_util.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1959')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/docker_executor_operator_e2e_test.py: 72-83
</a>
<div class="mid" id="frag1959" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'docker_e2e_test'
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3036')" href="javascript:;">
tfx-1.6.0/tfx/examples/custom_components/hello_world/example/taxi_pipeline_hello_e2e_test.py: 27-39
</a>
<div class="mid" id="frag3036" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'hello_test'
    self._data_root = os.path.join(os.path.dirname(__file__), '..', 'data')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3085')" href="javascript:;">
tfx-1.6.0/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_local_e2e_test.py: 28-42
</a>
<div class="mid" id="frag3085" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'beam_test'
    self._data_root = os.path.join(os.path.dirname(__file__), 'data', 'simple')
    self._module_file = os.path.join(os.path.dirname(__file__), 'taxi_utils.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2882')" href="javascript:;">
tfx-1.6.0/tfx/examples/bert/cola/bert_cola_pipeline_e2e_test.py: 31-46
</a>
<div class="mid" id="frag2882" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'keras_test'
    self._data_root = os.path.join(os.path.dirname(__file__), 'data')
    self._module_file = os.path.join(
        os.path.dirname(__file__), 'bert_cola_utils.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2922')" href="javascript:;">
tfx-1.6.0/tfx/examples/cifar10/cifar10_pipeline_native_keras_e2e_test.py: 28-45
</a>
<div class="mid" id="frag2922" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'keras_test'
    self._data_root = os.path.join(os.path.dirname(__file__), 'data')
    self._module_file = os.path.join(
        os.path.dirname(__file__), 'cifar10_utils_native_keras.py')
    self._serving_model_dir_lite = os.path.join(self._test_dir,
                                                'serving_model_lite')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')
    self._labels_path = os.path.join(self._data_root, 'labels.txt')

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2937')" href="javascript:;">
tfx-1.6.0/tfx/examples/mnist/mnist_pipeline_native_keras_e2e_test.py: 31-50
</a>
<div class="mid" id="frag2937" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'keras_test'
    self._data_root = os.path.join(os.path.dirname(__file__), 'data')
    self._module_file = os.path.join(
        os.path.dirname(__file__), 'mnist_utils_native_keras.py')
    self._module_file_lite = os.path.join(
        os.path.dirname(__file__), 'mnist_utils_native_keras_lite.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._serving_model_dir_lite = os.path.join(
        self._test_dir, 'serving_model_lite')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3069')" href="javascript:;">
tfx-1.6.0/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_native_keras_e2e_test.py: 32-48
</a>
<div class="mid" id="frag3069" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'native_keras_test'
    self._data_root = os.path.join(
        os.path.dirname(__file__), 'data', 'simple')
    self._module_file = os.path.join(
        os.path.dirname(__file__), 'taxi_utils_native_keras.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3118')" href="javascript:;">
tfx-1.6.0/tfx/examples/penguin/penguin_pipeline_local_infraval_e2e_test.py: 41-58
</a>
<div class="mid" id="frag3118" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'penguin_test'
    self._data_root = os.path.join(os.path.dirname(__file__), 'data')
    self._schema_path = os.path.join(
        os.path.dirname(__file__), 'schema', 'user_provided', 'schema.pbtxt')
    self._module_file = os.path.join(
        os.path.dirname(__file__), 'penguin_utils_keras.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3136')" href="javascript:;">
tfx-1.6.0/tfx/examples/penguin/experimental/penguin_pipeline_sklearn_local_e2e_test.py: 29-48
</a>
<div class="mid" id="frag3136" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    self._experimental_root = os.path.dirname(__file__)
    self._penguin_root = os.path.dirname(self._experimental_root)

    self._pipeline_name = 'sklearn_test'
    self._data_root = os.path.join(self._penguin_root, 'data')
    self._trainer_module_file = os.path.join(
        self._experimental_root, 'penguin_utils_sklearn.py')
    self._evaluator_module_file = os.path.join(
        self._experimental_root, 'sklearn_predict_extractor.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 75:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1494')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/launcher/docker_component_launcher_e2e_test.py: 84-98
</a>
<div class="mid" id="frag1494" style="display:none"><pre>
  def testDockerComponentLauncherInBeam(self):

    beam_dag_runner.BeamDagRunner().run(
        _create_pipeline(
            pipeline_name=self._pipeline_name,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            name='docker_e2e_test_in_beam'))

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      self.assertEqual(1, len(m.store.get_executions()))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1960')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/docker_executor_operator_e2e_test.py: 84-98
</a>
<div class="mid" id="frag1960" style="display:none"><pre>
  def testDockerComponentLauncherInBeam(self):

    beam_dag_runner.BeamDagRunner().run(
                _create_pipeline(
                    pipeline_name=self._pipeline_name,
                    pipeline_root=self._pipeline_root,
                    metadata_path=self._metadata_path,
                    name='docker_e2e_test_in_beam'))

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      self.assertEqual(1, len(m.store.get_executions()))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 76:</b> &nbsp; 2 fragments, nominal size 34 lines, similarity 79%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1497')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/beam/legacy/beam_dag_runner_test.py: 125-163
</a>
<div class="mid" id="frag1497" style="display:none"><pre>
  def testRun(self):
    component_a = _FakeComponent(
        _FakeComponentSpecA(output=types.Channel(type=_ArtifactTypeA)))
    component_b = _FakeComponent(
        _FakeComponentSpecB(
            a=component_a.outputs['output'],
            output=types.Channel(type=_ArtifactTypeB)))
    component_c = _FakeComponent(
        _FakeComponentSpecC(
            a=component_a.outputs['output'],
            output=types.Channel(type=_ArtifactTypeC)))
    component_c.add_upstream_node(component_b)
    component_d = _FakeComponent(
        _FakeComponentSpecD(
            b=component_b.outputs['output'],
            c=component_c.outputs['output'],
            output=types.Channel(type=_ArtifactTypeD)))
    component_e = _FakeComponent(
        _FakeComponentSpecE(
            a=component_a.outputs['output'],
            b=component_b.outputs['output'],
            d=component_d.outputs['output'],
            output=types.Channel(type=_ArtifactTypeE)))

    test_pipeline = pipeline.Pipeline(
        pipeline_name='x',
        pipeline_root='y',
        metadata_connection_config=metadata_store_pb2.ConnectionConfig(),
        components=[
            component_d, component_c, component_a, component_b, component_e
        ])

    beam_dag_runner.BeamDagRunner().run(test_pipeline)
    self.assertEqual(_executed_components, [
        '_FakeComponent.A', '_FakeComponent.B', '_FakeComponent.C',
        '_FakeComponent.D', '_FakeComponent.E'
    ])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2026')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/local/legacy/local_dag_runner_test.py: 124-160
</a>
<div class="mid" id="frag2026" style="display:none"><pre>
  def _getTestPipeline(self):  # pylint: disable=invalid-name
    component_a = _get_fake_component(
        _FakeComponentSpecA(output=types.Channel(type=_ArtifactTypeA)))
    component_b = _get_fake_component(
        _FakeComponentSpecB(
            a=component_a.outputs['output'],
            output=types.Channel(type=_ArtifactTypeB)))
    component_c = _get_fake_component(
        _FakeComponentSpecC(
            a=component_a.outputs['output'],
            output=types.Channel(type=_ArtifactTypeC)))
    component_c.add_upstream_node(component_b)
    component_d = _get_fake_component(
        _FakeComponentSpecD(
            b=component_b.outputs['output'],
            c=component_c.outputs['output'],
            output=types.Channel(type=_ArtifactTypeD)))
    component_e = _get_fake_component(
        _FakeComponentSpecE(
            a=component_a.outputs['output'],
            b=component_b.outputs['output'],
            d=component_d.outputs['output'],
            output=types.Channel(type=_ArtifactTypeE)))

    temp_path = tempfile.mkdtemp()
    pipeline_root_path = os.path.join(temp_path, 'pipeline_root')
    metadata_path = os.path.join(temp_path, 'metadata.db')
    test_pipeline = pipeline.Pipeline(
        pipeline_name='test_pipeline',
        pipeline_root=pipeline_root_path,
        metadata_connection_config=sqlite_metadata_connection_config(
            metadata_path),
        components=[
            component_d, component_c, component_a, component_b, component_e
        ])
    return test_pipeline

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 77:</b> &nbsp; 2 fragments, nominal size 49 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1501')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/beam/beam_dag_runner_test.py: 193-244
</a>
<div class="mid" id="frag1501" style="display:none"><pre>
  def testRunWithLocalDeploymentConfig(self):
    self._pipeline.deployment_config.Pack(_LOCAL_DEPLOYMENT_CONFIG)
    beam_dag_runner.BeamDagRunner().run_with_ir(self._pipeline)
    self.assertEqual(
        _component_executors, {
            'my_example_gen':
                text_format.Parse(
                    'class_path: "tfx.components.example_gen_executor"',
                    _PythonClassExecutableSpec()),
            'my_transform':
                text_format.Parse(
                    'class_path: "tfx.components.transform_executor"',
                    _PythonClassExecutableSpec()),
            'my_trainer':
                text_format.Parse('image: "path/to/docker/image"',
                                  _ContainerExecutableSpec()),
            'my_importer':
                None,
        })
    self.assertEqual(
        _component_drivers, {
            'my_example_gen':
                text_format.Parse(
                    'class_path: "tfx.components.example_gen_driver"',
                    _PythonClassExecutableSpec()),
            'my_transform':
                None,
            'my_trainer':
                None,
            'my_importer':
                None,
        })
    self.assertEqual(
        _component_platform_configs, {
            'my_example_gen':
                None,
            'my_transform':
                None,
            'my_trainer':
                text_format.Parse('docker_server_url: "docker/server/url"',
                                  _DockerPlatformConfig()),
            'my_importer':
                None,
        })
    # 'my_importer' has no upstream and can be executed in any order.
    self.assertIn('my_importer', _executed_components)
    _executed_components.remove('my_importer')
    self.assertEqual(_executed_components,
                     ['my_example_gen', 'my_transform', 'my_trainer'])
    # Verifies that every component gets a not-None pipeline_run.
    self.assertTrue(all(_component_to_pipeline_run.values()))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1502')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/beam/beam_dag_runner_test.py: 249-300
</a>
<div class="mid" id="frag1502" style="display:none"><pre>
  def testRunWithIntermediateDeploymentConfig(self):
    self._pipeline.deployment_config.Pack(_INTERMEDIATE_DEPLOYMENT_CONFIG)
    beam_dag_runner.BeamDagRunner().run_with_ir(self._pipeline)
    self.assertEqual(
        _component_executors, {
            'my_example_gen':
                text_format.Parse(
                    'class_path: "tfx.components.example_gen_executor"',
                    _PythonClassExecutableSpec()),
            'my_transform':
                text_format.Parse(
                    'class_path: "tfx.components.transform_executor"',
                    _PythonClassExecutableSpec()),
            'my_trainer':
                text_format.Parse('image: "path/to/docker/image"',
                                  _ContainerExecutableSpec()),
            'my_importer':
                None,
        })
    self.assertEqual(
        _component_drivers, {
            'my_example_gen':
                text_format.Parse(
                    'class_path: "tfx.components.example_gen_driver"',
                    _PythonClassExecutableSpec()),
            'my_transform':
                None,
            'my_trainer':
                None,
            'my_importer':
                None,
        })
    self.assertEqual(
        _component_platform_configs, {
            'my_example_gen':
                None,
            'my_transform':
                None,
            'my_trainer':
                text_format.Parse('docker_server_url: "docker/server/url"',
                                  _DockerPlatformConfig()),
            'my_importer':
                None,
        })
    # 'my_importer' has no upstream and can be executed in any order.
    self.assertIn('my_importer', _executed_components)
    _executed_components.remove('my_importer')
    self.assertEqual(_executed_components,
                     ['my_example_gen', 'my_transform', 'my_trainer'])
    # Verifies that every component gets a not-None pipeline_run.
    self.assertTrue(all(_component_to_pipeline_run.values()))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 78:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1503')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/beam/beam_dag_runner_test.py: 305-315
</a>
<div class="mid" id="frag1503" style="display:none"><pre>
  def testPartialRunWithLocalDeploymentConfig(self):
    self._pipeline.deployment_config.Pack(_LOCAL_DEPLOYMENT_CONFIG)
    pr_opts = pipeline_pb2.PartialRun()
    pr_opts.from_nodes.append('my_trainer')
    pr_opts.to_nodes.append('my_trainer')
    pr_opts.snapshot_settings.latest_pipeline_run_strategy.SetInParent()
    beam_dag_runner.BeamDagRunner().run_with_ir(
        self._pipeline,
        run_options=pipeline_pb2.RunOptions(partial_run=pr_opts))
    self.assertEqual(_executed_components, ['my_trainer'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1504')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/beam/beam_dag_runner_test.py: 320-330
</a>
<div class="mid" id="frag1504" style="display:none"><pre>
  def testPartialRunWithIntermediateDeploymentConfig(self):
    self._pipeline.deployment_config.Pack(_INTERMEDIATE_DEPLOYMENT_CONFIG)
    pr_opts = pipeline_pb2.PartialRun()
    pr_opts.from_nodes.append('my_trainer')
    pr_opts.to_nodes.append('my_trainer')
    pr_opts.snapshot_settings.latest_pipeline_run_strategy.SetInParent()
    beam_dag_runner.BeamDagRunner().run_with_ir(
        self._pipeline,
        run_options=pipeline_pb2.RunOptions(partial_run=pr_opts))
    self.assertEqual(_executed_components, ['my_trainer'])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 79:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1556')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/pipeline_test.py: 304-322
</a>
<div class="mid" id="frag1556" style="display:none"><pre>
  def testPipelineWithBeamPipelineArgs(self):
    expected_args = [
        '--my_first_beam_pipeline_args=foo',
        '--my_second_beam_pipeline_args=bar'
    ]
    p = pipeline.Pipeline(
        pipeline_name='a',
        pipeline_root='b',
        log_root='c',
        components=[
            _make_fake_component_instance(
                'component_a', _OutputTypeA, {}, {},
                with_beam=True).with_beam_pipeline_args([expected_args[1]])
        ],
        beam_pipeline_args=[expected_args[0]],
        metadata_connection_config=self._metadata_connection_config)
    self.assertEqual(expected_args,
                     p.components[0].executor_spec.beam_pipeline_args)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1557')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/pipeline_test.py: 323-342
</a>
<div class="mid" id="frag1557" style="display:none"><pre>
  def testComponentsSetAfterCreationWithBeamPipelineArgs(self):
    expected_args = [
        '--my_first_beam_pipeline_args=foo',
        '--my_second_beam_pipeline_args=bar'
    ]
    p = pipeline.Pipeline(
        pipeline_name='a',
        pipeline_root='b',
        log_root='c',
        beam_pipeline_args=[expected_args[0]],
        metadata_connection_config=self._metadata_connection_config)
    p.components = [
        _make_fake_component_instance(
            'component_a', _OutputTypeA, {}, {},
            with_beam=True).with_beam_pipeline_args([expected_args[1]])
    ]
    self.assertEqual(expected_args,
                     p.components[0].executor_spec.beam_pipeline_args)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 80:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1563')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/data_types_utils.py: 73-87
</a>
<div class="mid" id="frag1563" style="display:none"><pre>
def build_metadata_value_dict(
    value_dict: Mapping[str, types.ExecPropertyTypes]
) -&gt; Dict[str, metadata_store_pb2.Value]:
  """Converts plain value dict into MLMD value dict."""
  result = {}
  if not value_dict:
    return result
  for k, v in value_dict.items():
    if v is None:
      continue
    value = metadata_store_pb2.Value()
    result[k] = set_metadata_value(value, v)
  return result


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1564')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/data_types_utils.py: 88-102
</a>
<div class="mid" id="frag1564" style="display:none"><pre>
def build_pipeline_value_dict(
    value_dict: Dict[str, types.ExecPropertyTypes]
) -&gt; Dict[str, pipeline_pb2.Value]:
  """Converts plain value dict into pipeline_pb2.Value dict."""
  result = {}
  if not value_dict:
    return result
  for k, v in value_dict.items():
    if v is None:
      continue
    value = pipeline_pb2.Value()
    result[k] = set_parameter_value(value, v)
  return result


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 81:</b> &nbsp; 10 fragments, nominal size 72 lines, similarity 74%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1588')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/experimental/kubernetes/examples/taxi_pipeline_kubernetes.py: 69-168
</a>
<div class="mid" id="frag1588" style="display:none"><pre>
def create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                    module_file: str, serving_model_dir: str,
                    beam_pipeline_args: List[str]) -&gt; pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'],
      infer_feature_shape=False)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that implements a model.
  trainer = Trainer(
      module_file=module_file,
      transformed_examples=transform.outputs['transformed_examples'],
      schema=schema_gen.outputs['schema'],
      transform_graph=transform.outputs['transform_graph'],
      train_args=trainer_pb2.TrainArgs(num_steps=10000),
      eval_args=trainer_pb2.EvalArgs(num_steps=5000))

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute a evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(signature_name='eval')],
      slicing_specs=[
          tfma.SlicingSpec(),
          tfma.SlicingSpec(feature_keys=['trip_start_hour'])
      ],
      metrics_specs=[
          tfma.MetricsSpec(
              thresholds={
                  'accuracy':
                      tfma.MetricThreshold(
                          value_threshold=tfma.GenericValueThreshold(
                              lower_bound={'value': 0.6}),
                          # Change threshold will be ignored if there is no
                          # baseline model resolved from MLMD (first run).
                          change_threshold=tfma.GenericChangeThreshold(
                              direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                              absolute={'value': -1e-10}))
              })
      ])
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  config = kubernetes_dag_runner.get_default_kubernetes_metadata_config()
  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen,
          statistics_gen,
          schema_gen,
          example_validator,
          transform,
          trainer,
          model_resolver,
          evaluator,
          pusher,
      ],
      enable_cache=False,
      metadata_connection_config=config,
      beam_pipeline_args=beam_pipeline_args)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2894')" href="javascript:;">
tfx-1.6.0/tfx/examples/bert/mrpc/bert_mrpc_pipeline.py: 70-176
</a>
<div class="mid" id="frag2894" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir: str,
                     metadata_path: str,
                     beam_pipeline_args: List[str]) -&gt; pipeline.Pipeline:
  """Implements the Bert classication on mrpc dataset pipline with TFX."""
  input_config = example_gen_pb2.Input(splits=[
      example_gen_pb2.Input.Split(name='train', pattern='train/*'),
      example_gen_pb2.Input.Split(name='eval', pattern='validation/*')
  ])

  # Brings data into the pipline
  example_gen = CsvExampleGen(input_base=data_root, input_config=input_config)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that trains a model.
  trainer = Trainer(
      module_file=module_file,
      examples=transform.outputs['transformed_examples'],
      transform_graph=transform.outputs['transform_graph'],
      schema=schema_gen.outputs['schema'],
      # Adjust these steps when training on the full dataset.
      train_args=trainer_pb2.TrainArgs(num_steps=1),
      eval_args=trainer_pb2.EvalArgs(num_steps=1))

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(label_key='label')],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='SparseCategoricalAccuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          # Adjust the threshold when training on the
                          # full dataset.
                          lower_bound={'value': 0.5}),
                      # Change threshold will be ignored if there is no
                      # baseline model resolved from MLMD (first run).
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-2})))
          ])
      ])
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  components = [
      example_gen,
      statistics_gen,
      schema_gen,
      example_validator,
      transform,
      trainer,
      model_resolver,
      evaluator,
      pusher,
  ]

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=components,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      enable_cache=True,
      beam_pipeline_args=beam_pipeline_args,
  )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2960')" href="javascript:;">
tfx-1.6.0/tfx/examples/airflow_workshop/setup/dags/taxi_pipeline_solution.py: 81-179
</a>
<div class="mid" id="frag2960" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir: str,
                     metadata_path: str,
                     beam_pipeline_args: List[str]) -&gt; pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""
  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  infer_schema = SchemaGen(
      statistics=statistics_gen.outputs['statistics'],
      infer_feature_shape=False)

  # Performs anomaly detection based on statistics and data schema.
  validate_stats = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=infer_schema.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=infer_schema.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that implements a model.
  trainer = Trainer(
      module_file=module_file,
      custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),
      examples=transform.outputs['transformed_examples'],
      transform_graph=transform.outputs['transform_graph'],
      schema=infer_schema.outputs['schema'],
      train_args=trainer_pb2.TrainArgs(num_steps=10000),
      eval_args=trainer_pb2.EvalArgs(num_steps=5000))

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute a evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(label_key='tips')],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='BinaryAccuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          lower_bound={'value': 0.6}),
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-10})))
          ])
      ])

  model_analyzer = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      # Change threshold will be ignored if there is no baseline (first run).
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=model_analyzer.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen,
          statistics_gen,
          infer_schema,
          validate_stats,
          transform,
          trainer,
          model_resolver,
          model_analyzer,
          pusher,
      ],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      beam_pipeline_args=beam_pipeline_args)


# 'DAG' below need to be kept for Airflow to detect dag.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3073')" href="javascript:;">
tfx-1.6.0/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_simple.py: 82-183
</a>
<div class="mid" id="frag3073" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir: str,
                     metadata_path: str,
                     beam_pipeline_args: List[str]) -&gt; pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""
  # Parametrize data root so it can be replaced on runtime. See the
  # "Passing Parameters when triggering dags" section of
  # https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html
  # for more details.
  data_root_runtime = data_types.RuntimeParameter(
      'data_root', ptype=str, default=data_root)

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root_runtime)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'],
      infer_feature_shape=False)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that implements a model.
  trainer = Trainer(
      module_file=module_file,
      custom_executor_spec=executor_spec.ExecutorClassSpec(Executor),
      transformed_examples=transform.outputs['transformed_examples'],
      schema=schema_gen.outputs['schema'],
      transform_graph=transform.outputs['transform_graph'],
      train_args=trainer_pb2.TrainArgs(num_steps=10000),
      eval_args=trainer_pb2.EvalArgs(num_steps=5000))

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute a evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(signature_name='eval')],
      slicing_specs=[
          tfma.SlicingSpec(),
          tfma.SlicingSpec(feature_keys=['trip_start_hour'])
      ],
      metrics_specs=[
          tfma.MetricsSpec(
              thresholds={
                  'accuracy':
                      tfma.MetricThreshold(
                          value_threshold=tfma.GenericValueThreshold(
                              lower_bound={'value': 0.6}),
                          # Change threshold will be ignored if there is no
                          # baseline model resolved from MLMD (first run).
                          change_threshold=tfma.GenericChangeThreshold(
                              direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                              absolute={'value': -1e-10}))
              })
      ])
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen, statistics_gen, schema_gen, example_validator, transform,
          trainer, model_resolver, evaluator, pusher
      ],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      beam_pipeline_args=beam_pipeline_args)


# 'DAG' below need to be kept for Airflow to detect dag.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3074')" href="javascript:;">
tfx-1.6.0/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_local.py: 75-184
</a>
<div class="mid" id="frag3074" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir: str,
                     metadata_path: str,
                     beam_pipeline_args: List[str]) -&gt; pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'],
      infer_feature_shape=False)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Get the latest model so that we can warm start from the model.
  latest_model_resolver = resolver.Resolver(
      strategy_class=latest_artifacts_resolver.LatestArtifactsResolver,
      latest_model=Channel(type=Model)).with_id('latest_model_resolver')

  # Uses user-provided Python function that implements a model.
  trainer = Trainer(
      module_file=module_file,
      custom_executor_spec=executor_spec.ExecutorClassSpec(Executor),
      transformed_examples=transform.outputs['transformed_examples'],
      schema=schema_gen.outputs['schema'],
      base_model=latest_model_resolver.outputs['latest_model'],
      transform_graph=transform.outputs['transform_graph'],
      train_args=trainer_pb2.TrainArgs(num_steps=10000),
      eval_args=trainer_pb2.EvalArgs(num_steps=5000))

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute a evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(signature_name='eval')],
      slicing_specs=[
          tfma.SlicingSpec(),
          tfma.SlicingSpec(feature_keys=['trip_start_hour'])
      ],
      metrics_specs=[
          tfma.MetricsSpec(
              thresholds={
                  'accuracy':
                      tfma.MetricThreshold(
                          value_threshold=tfma.GenericValueThreshold(
                              lower_bound={'value': 0.6}),
                          change_threshold=tfma.GenericChangeThreshold(
                              direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                              absolute={'value': -1e-10}))
              })
      ])
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      # Change threshold will be ignored if there is no baseline (first run).
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen,
          statistics_gen,
          schema_gen,
          example_validator,
          transform,
          latest_model_resolver,
          trainer,
          model_resolver,
          evaluator,
          pusher,
      ],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      beam_pipeline_args=beam_pipeline_args)


# To run this pipeline from the python CLI:
#   $python taxi_pipeline_beam.py
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3093')" href="javascript:;">
tfx-1.6.0/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_native_keras.py: 72-174
</a>
<div class="mid" id="frag3093" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir: str,
                     metadata_path: str,
                     beam_pipeline_args: List[str]) -&gt; pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""
  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'],
      infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that implements a model.
  trainer = Trainer(
      module_file=module_file,
      examples=transform.outputs['transformed_examples'],
      transform_graph=transform.outputs['transform_graph'],
      schema=schema_gen.outputs['schema'],
      train_args=trainer_pb2.TrainArgs(num_steps=1000),
      eval_args=trainer_pb2.EvalArgs(num_steps=150))

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute a evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[
          tfma.ModelSpec(
              signature_name='serving_default', label_key='tips_xf',
              preprocessing_function_names=['transform_features'])
      ],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='BinaryAccuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          lower_bound={'value': 0.6}),
                      # Change threshold will be ignored if there is no
                      # baseline model resolved from MLMD (first run).
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-10})))
          ])
      ])
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen,
          statistics_gen,
          schema_gen,
          example_validator,
          transform,
          trainer,
          model_resolver,
          evaluator,
          pusher,
      ],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      beam_pipeline_args=beam_pipeline_args)


# To run this pipeline from the python CLI:
#   $python taxi_pipeline_native_keras.py
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2994')" href="javascript:;">
tfx-1.6.0/tfx/examples/imdb/imdb_pipeline_native_keras.py: 72-179
</a>
<div class="mid" id="frag2994" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir: str,
                     metadata_path: str,
                     beam_pipeline_args: List[str]) -&gt; pipeline.Pipeline:
  """Implements the imdb sentiment analysis pipline with TFX."""
  output = example_gen_pb2.Output(
      split_config=example_gen_pb2.SplitConfig(splits=[
          example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=9),
          example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)
      ]))

  # Brings data in to the pipline
  example_gen = CsvExampleGen(input_base=data_root, output_config=output)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that trains a model.
  trainer = Trainer(
      module_file=module_file,
      examples=transform.outputs['transformed_examples'],
      transform_graph=transform.outputs['transform_graph'],
      schema=schema_gen.outputs['schema'],
      train_args=trainer_pb2.TrainArgs(num_steps=500),
      eval_args=trainer_pb2.EvalArgs(num_steps=200))

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(label_key='label')],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='BinaryAccuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          # Increase this threshold when training on complete
                          # dataset.
                          lower_bound={'value': 0.01}),
                      # Change threshold will be ignored if there is no
                      # baseline model resolved from MLMD (first run).
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-2})))
          ])
      ])

  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  components = [
      example_gen,
      statistics_gen,
      schema_gen,
      example_validator,
      transform,
      trainer,
      model_resolver,
      evaluator,
      pusher,
  ]
  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=components,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      enable_cache=True,
      beam_pipeline_args=beam_pipeline_args)


# To run this pipeline from the python CLI:
# $python imdb_pipeline_native_keras.py
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2886')" href="javascript:;">
tfx-1.6.0/tfx/examples/bert/cola/bert_cola_pipeline.py: 70-176
</a>
<div class="mid" id="frag2886" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir: str,
                     metadata_path: str,
                     beam_pipeline_args: List[str]) -&gt; pipeline.Pipeline:
  """Implements the Bert classication on Cola dataset pipline with TFX."""
  input_config = example_gen_pb2.Input(splits=[
      example_gen_pb2.Input.Split(name='train', pattern='train/*'),
      example_gen_pb2.Input.Split(name='eval', pattern='validation/*')
  ])

  # Brings data into the pipline
  example_gen = CsvExampleGen(input_base=data_root, input_config=input_config)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that trains a model.
  trainer = Trainer(
      module_file=module_file,
      examples=transform.outputs['transformed_examples'],
      transform_graph=transform.outputs['transform_graph'],
      schema=schema_gen.outputs['schema'],
      # Adjust these steps when training on the full dataset.
      train_args=trainer_pb2.TrainArgs(num_steps=2),
      eval_args=trainer_pb2.EvalArgs(num_steps=1))

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(label_key='label')],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='SparseCategoricalAccuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          # Adjust the threshold when training on the
                          # full dataset.
                          lower_bound={'value': 0.5}),
                      # Change threshold will be ignored if there is no
                      # baseline model resolved from MLMD (first run).
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-2})))
          ])
      ])
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  components = [
      example_gen,
      statistics_gen,
      schema_gen,
      example_validator,
      transform,
      trainer,
      model_resolver,
      evaluator,
      pusher,
  ]

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=components,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      enable_cache=True,
      beam_pipeline_args=beam_pipeline_args,
  )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3018')" href="javascript:;">
tfx-1.6.0/tfx/examples/tfjs_next_page_prediction/tfjs_next_page_prediction_pipeline.py: 71-186
</a>
<div class="mid" id="frag3018" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir: str,
                     metadata_path: str,
                     beam_pipeline_args: List[str]) -&gt; dsl.Pipeline:
  """Implements the page prediction pipline with TFX."""
  input_config = proto.Input(
      splits=[proto.Input.Split(name='input', pattern='*.tfrecord.gz')])
  output_config = proto.Output(
      split_config=proto.SplitConfig(splits=[
          proto.SplitConfig.Split(name='train', hash_buckets=9),
          proto.SplitConfig.Split(name='eval', hash_buckets=1)
      ]))

  # Brings data in to the pipline
  example_gen = ImportExampleGen(
      input_base=data_root,
      input_config=input_config,
      output_config=output_config)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(
      examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that trains a model.
  trainer = Trainer(
      module_file=module_file,
      examples=transform.outputs['transformed_examples'],
      transform_graph=transform.outputs['transform_graph'],
      schema=schema_gen.outputs['schema'],
      train_args=proto.TrainArgs(num_steps=100000),
      eval_args=proto.EvalArgs(num_steps=200))

  # Get the latest blessed model for model validation.
  model_resolver = dsl.Resolver(
      strategy_class=dsl.experimental.LatestBlessedModelStrategy,
      model=dsl.Channel(type=types.standard_artifacts.Model),
      model_blessing=dsl.Channel(
          type=types.standard_artifacts.ModelBlessing)).with_id(
              'latest_blessed_model_resolver')

  # Uses TFMA to compute evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      # Directly evaluates the tfjs model.
      model_specs=[tfma.ModelSpec(label_key='label', model_type='tf_js')],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='SparseCategoricalAccuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          # Increase this threshold when training on complete
                          # dataset.
                          lower_bound={'value': 0.01}),
                      # Change threshold will be ignored if there is no
                      # baseline model resolved from MLMD (first run).
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-2})))
          ])
      ])

  evaluator = Evaluator(
      examples=transform.outputs['transformed_examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=proto.PushDestination(
          filesystem=proto.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  components = [
      example_gen,
      statistics_gen,
      schema_gen,
      example_validator,
      transform,
      trainer,
      model_resolver,
      evaluator,
      pusher,
  ]
  return dsl.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=components,
      metadata_connection_config=orchestration.metadata
      .sqlite_metadata_connection_config(metadata_path),
      enable_cache=True,
      beam_pipeline_args=beam_pipeline_args)


# To run this pipeline from the python CLI:
# $python imdb_pipeline_native_keras.py
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2926')" href="javascript:;">
tfx-1.6.0/tfx/examples/cifar10/cifar10_pipeline_native_keras.py: 82-200
</a>
<div class="mid" id="frag2926" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir_lite: str,
                     metadata_path: str, labels_path: str,
                     beam_pipeline_args: List[str]) -&gt; pipeline.Pipeline:
  """Implements the CIFAR10 image classification pipeline using TFX."""
  # This is needed for datasets with pre-defined splits
  # Change the pattern argument to train_whole/* and test_whole/* to train
  # on the whole CIFAR-10 dataset
  input_config = example_gen_pb2.Input(splits=[
      example_gen_pb2.Input.Split(name='train', pattern='train/*'),
      example_gen_pb2.Input.Split(name='eval', pattern='test/*')
  ])

  # Brings data into the pipeline.
  example_gen = ImportExampleGen(
      input_base=data_root, input_config=input_config)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that trains a model.
  # When traning on the whole dataset, use 18744 for train steps, 156 for eval
  # steps. 18744 train steps correspond to 24 epochs on the whole train set, and
  # 156 eval steps correspond to 1 epoch on the whole test set. The
  # configuration below is for training on the dataset we provided in the data
  # folder, which has 128 train and 128 test samples. The 160 train steps
  # correspond to 40 epochs on this tiny train set, and 4 eval steps correspond
  # to 1 epoch on this tiny test set.
  trainer = Trainer(
      module_file=module_file,
      examples=transform.outputs['transformed_examples'],
      transform_graph=transform.outputs['transform_graph'],
      schema=schema_gen.outputs['schema'],
      train_args=trainer_pb2.TrainArgs(num_steps=160),
      eval_args=trainer_pb2.EvalArgs(num_steps=4),
      custom_config={'labels_path': labels_path})

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compare to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(label_key='label_xf', model_type='tf_lite')],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='SparseCategoricalAccuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          lower_bound={'value': 0.55}),
                      # Change threshold will be ignored if there is no
                      # baseline model resolved from MLMD (first run).
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-3})))
          ])
      ])

  # Uses TFMA to compute the evaluation statistics over features of a model.
  # We evaluate using the materialized examples that are output by Transform
  # because
  # 1. the decoding_png function currently performed within Transform are not
  # compatible with TFLite.
  # 2. MLKit requires deserialized (float32) tensor image inputs
  # Note that for deployment, the same logic that is performed within Transform
  # must be reproduced client-side.
  evaluator = Evaluator(
      examples=transform.outputs['transformed_examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir_lite)))

  components = [
      example_gen, statistics_gen, schema_gen, example_validator, transform,
      trainer, model_resolver, evaluator, pusher
  ]

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=components,
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      beam_pipeline_args=beam_pipeline_args)


# To run this pipeline from the python CLI:
#   $python cifar_pipeline_native_keras.py
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 82:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1602')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/experimental/interactive/standard_visualizations.py: 35-51
</a>
<div class="mid" id="frag1602" style="display:none"><pre>
  def display(self, artifact: types.Artifact):
    from IPython.core.display import display  # pylint: disable=g-import-not-at-top
    from IPython.core.display import HTML  # pylint: disable=g-import-not-at-top
    for split in artifact_utils.decode_split_names(artifact.split_names):
      display(HTML('&lt;div&gt;&lt;b&gt;%r split:&lt;/b&gt;&lt;/div&gt;&lt;br/&gt;' % split))
      anomalies_path = io_utils.get_only_uri_in_dir(
          artifact_utils.get_split_uri([artifact], split))
      if artifact_utils.is_artifact_version_older_than(
          artifact, artifact_utils._ARTIFACT_VERSION_FOR_ANOMALIES_UPDATE):  # pylint: disable=protected-access
        anomalies = tfdv.load_anomalies_text(anomalies_path)
      else:
        anomalies = anomalies_pb2.Anomalies()
        anomalies_bytes = io_utils.read_bytes_file(anomalies_path)
        anomalies.ParseFromString(anomalies_bytes)
      tfdv.display_anomalies(anomalies)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1603')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/experimental/interactive/standard_visualizations.py: 57-71
</a>
<div class="mid" id="frag1603" style="display:none"><pre>
  def display(self, artifact: types.Artifact):
    from IPython.core.display import display  # pylint: disable=g-import-not-at-top
    from IPython.core.display import HTML  # pylint: disable=g-import-not-at-top
    for split in artifact_utils.decode_split_names(artifact.split_names):
      display(HTML('&lt;div&gt;&lt;b&gt;%r split:&lt;/b&gt;&lt;/div&gt;&lt;br/&gt;' % split))
      stats_path = io_utils.get_only_uri_in_dir(
          artifact_utils.get_split_uri([artifact], split))
      if artifact_utils.is_artifact_version_older_than(
          artifact, artifact_utils._ARTIFACT_VERSION_FOR_STATS_UPDATE):  # pylint: disable=protected-access
        stats = tfdv.load_statistics(stats_path)
      else:
        stats = tfdv.load_stats_binary(stats_path)
      tfdv.visualize_statistics(stats)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 83:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1631')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/experimental/interactive/interactive_context_test.py: 100-126
</a>
<div class="mid" id="frag1631" style="display:none"><pre>
  def testBasicRun(self):

    class _FakeComponentSpec(types.ComponentSpec):
      PARAMETERS = {}
      INPUTS = {}
      OUTPUTS = {}

    class _FakeExecutor(base_executor.BaseExecutor):
      CALLED = False

      def Do(self, input_dict: Dict[str, List[types.Artifact]],
             output_dict: Dict[str, List[types.Artifact]],
             exec_properties: Dict[str, Any]) -&gt; None:
        _FakeExecutor.CALLED = True

    class _FakeComponent(base_component.BaseComponent):
      SPEC_CLASS = _FakeComponentSpec
      EXECUTOR_SPEC = executor_spec.ExecutorClassSpec(_FakeExecutor)

      def __init__(self, spec: types.ComponentSpec):
        super().__init__(spec=spec)

    c = interactive_context.InteractiveContext()
    component = _FakeComponent(_FakeComponentSpec())
    c.run(component)
    self.assertTrue(_FakeExecutor.CALLED)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1635')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/experimental/interactive/interactive_context_test.py: 133-164
</a>
<div class="mid" id="frag1635" style="display:none"><pre>
  def testUnresolvedChannel(self):

    class _FakeComponentSpec(types.ComponentSpec):
      PARAMETERS = {}
      INPUTS = {
          'input':
              component_spec.ChannelParameter(type=standard_artifacts.Examples)
      }
      OUTPUTS = {}

    class _FakeExecutor(base_executor.BaseExecutor):
      CALLED = False

      def Do(self, input_dict: Dict[str, List[types.Artifact]],
             output_dict: Dict[str, List[types.Artifact]],
             exec_properties: Dict[str, Any]) -&gt; None:
        _FakeExecutor.CALLED = True

    class _FakeComponent(base_component.BaseComponent):
      SPEC_CLASS = _FakeComponentSpec
      EXECUTOR_SPEC = executor_spec.ExecutorClassSpec(_FakeExecutor)

      def __init__(self, spec: types.ComponentSpec):
        super().__init__(spec=spec)

    c = interactive_context.InteractiveContext()
    foo = types.Channel(type=standard_artifacts.Examples).set_artifacts(
        [standard_artifacts.Examples()])
    component = _FakeComponent(_FakeComponentSpec(input=foo))
    with self.assertRaisesRegex(ValueError, 'Unresolved input channel'):
      c.run(component)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 84:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1660')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/experimental/core/mlmd_state_test.py: 80-91
</a>
<div class="mid" id="frag1660" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    pipeline_root = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self.id())
    metadata_path = os.path.join(pipeline_root, 'metadata', 'metadata.db')
    connection_config = metadata.sqlite_metadata_connection_config(
        metadata_path)
    connection_config.sqlite.SetInParent()
    self._mlmd_connection = metadata.Metadata(
        connection_config=connection_config)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1757')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/experimental/core/task_schedulers/manual_task_scheduler_test.py: 39-55
</a>
<div class="mid" id="frag1757" style="display:none"><pre>
  def setUp(self):
    super().setUp()

    pipeline_root = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self.id())

    metadata_path = os.path.join(pipeline_root, 'metadata', 'metadata.db')
    connection_config = metadata.sqlite_metadata_connection_config(
        metadata_path)
    connection_config.sqlite.SetInParent()
    self._mlmd_connection = metadata.Metadata(
        connection_config=connection_config)

    self._pipeline = self._make_pipeline(pipeline_root, str(uuid.uuid4()))
    self._manual_node = self._pipeline.nodes[0].pipeline_node

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1762')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/experimental/core/task_schedulers/resolver_task_scheduler_test.py: 38-57
</a>
<div class="mid" id="frag1762" style="display:none"><pre>
  def setUp(self):
    super().setUp()

    pipeline_root = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self.id())

    metadata_path = os.path.join(pipeline_root, 'metadata', 'metadata.db')
    connection_config = metadata.sqlite_metadata_connection_config(
        metadata_path)
    connection_config.sqlite.SetInParent()
    self._mlmd_connection = metadata.Metadata(
        connection_config=connection_config)

    pipeline = self._make_pipeline(pipeline_root, str(uuid.uuid4()))
    self._pipeline = pipeline
    self._trainer = self._pipeline.nodes[0].pipeline_node
    self._resolver_node = self._pipeline.nodes[1].pipeline_node
    self._consumer_node = self._pipeline.nodes[2].pipeline_node

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 85:</b> &nbsp; 2 fragments, nominal size 37 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1687')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/experimental/core/async_pipeline_task_gen_test.py: 382-426
</a>
<div class="mid" id="frag1687" style="display:none"><pre>
  def test_triggering_upon_exec_properties_change(self):
    test_utils.fake_example_gen_run(self._mlmd_connection, self._example_gen, 1,
                                    1)

    [exec_transform_task] = self._generate_and_test(
        False,
        num_initial_executions=1,
        num_tasks_generated=1,
        num_new_executions=1,
        num_active_executions=1,
        expected_exec_nodes=[self._transform],
        ignore_update_node_state_tasks=True)

    # Fail the registered execution.
    with self._mlmd_connection as m:
      with mlmd_state.mlmd_execution_atomic_op(
          m, exec_transform_task.execution_id) as execution:
        execution.last_known_state = metadata_store_pb2.Execution.FAILED

    # Try to generate with same execution properties. This should not trigger
    # as there are no changes since last run.
    self._generate_and_test(
        False,
        num_initial_executions=2,
        num_tasks_generated=0,
        num_new_executions=0,
        num_active_executions=0,
        ignore_update_node_state_tasks=True)

    # Change execution properties of last run.
    with self._mlmd_connection as m:
      with mlmd_state.mlmd_execution_atomic_op(
          m, exec_transform_task.execution_id) as execution:
        execution.custom_properties['a_param'].int_value = 20

    # Generating with different execution properties should trigger.
    self._generate_and_test(
        False,
        num_initial_executions=2,
        num_tasks_generated=1,
        num_new_executions=1,
        num_active_executions=1,
        expected_exec_nodes=[self._transform],
        ignore_update_node_state_tasks=True)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1688')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/experimental/core/async_pipeline_task_gen_test.py: 427-475
</a>
<div class="mid" id="frag1688" style="display:none"><pre>
  def test_triggering_upon_executor_spec_change(self):
    test_utils.fake_example_gen_run(self._mlmd_connection, self._example_gen, 1,
                                    1)

    with mock.patch.object(task_gen_utils,
                           'get_executor_spec') as mock_get_executor_spec:
      mock_get_executor_spec.side_effect = _fake_executor_spec(1)
      [exec_transform_task] = self._generate_and_test(
          False,
          num_initial_executions=1,
          num_tasks_generated=1,
          num_new_executions=1,
          num_active_executions=1,
          expected_exec_nodes=[self._transform],
          ignore_update_node_state_tasks=True)

    # Fail the registered execution.
    with self._mlmd_connection as m:
      with mlmd_state.mlmd_execution_atomic_op(
          m, exec_transform_task.execution_id) as execution:
        execution.last_known_state = metadata_store_pb2.Execution.FAILED

    # Try to generate with same executor spec. This should not trigger as
    # there are no changes since last run.
    with mock.patch.object(task_gen_utils,
                           'get_executor_spec') as mock_get_executor_spec:
      mock_get_executor_spec.side_effect = _fake_executor_spec(1)
      self._generate_and_test(
          False,
          num_initial_executions=2,
          num_tasks_generated=0,
          num_new_executions=0,
          num_active_executions=0,
          ignore_update_node_state_tasks=True)

    # Generating with a different executor spec should trigger.
    with mock.patch.object(task_gen_utils,
                           'get_executor_spec') as mock_get_executor_spec:
      mock_get_executor_spec.side_effect = _fake_executor_spec(2)
      self._generate_and_test(
          False,
          num_initial_executions=2,
          num_tasks_generated=1,
          num_new_executions=1,
          num_active_executions=1,
          expected_exec_nodes=[self._transform],
          ignore_update_node_state_tasks=True)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 86:</b> &nbsp; 2 fragments, nominal size 39 lines, similarity 74%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1713')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/experimental/core/pipeline_ops_test.py: 178-225
</a>
<div class="mid" id="frag1713" style="display:none"><pre>
  def test_initiate_pipeline_start_with_partial_run(self, mock_snapshot):
    with self._mlmd_connection as m:
      pipeline = _test_pipeline('test_pipeline', pipeline_pb2.Pipeline.SYNC)
      node_example_gen = pipeline.nodes.add().pipeline_node
      node_example_gen.node_info.id = 'ExampleGen'
      node_example_gen.downstream_nodes.extend(['Transform'])
      node_transform = pipeline.nodes.add().pipeline_node
      node_transform.node_info.id = 'Transform'
      node_transform.upstream_nodes.extend(['ExampleGen'])
      node_transform.downstream_nodes.extend(['Trainer'])
      node_trainer = pipeline.nodes.add().pipeline_node
      node_trainer.node_info.id = 'Trainer'
      node_trainer.upstream_nodes.extend(['Transform'])

      latest_pipeline_snapshot_settings = pipeline_pb2.SnapshotSettings()
      latest_pipeline_snapshot_settings.latest_pipeline_run_strategy.SetInParent(
      )

      incorrect_partial_run_option = pipeline_pb2.PartialRun(
          from_nodes=['InvalidaNode'],
          to_nodes=['Trainer'],
          snapshot_settings=latest_pipeline_snapshot_settings)
      with self.assertRaisesRegex(
          status_lib.StatusNotOkError,
          'specified in from_nodes/to_nodes are not present in the pipeline.'):
        pipeline_ops.initiate_pipeline_start(
            m, pipeline, partial_run_option=incorrect_partial_run_option)

      expected_pipeline = copy.deepcopy(pipeline)
      expected_pipeline.runtime_spec.snapshot_settings.latest_pipeline_run_strategy.SetInParent(
      )
      expected_pipeline.nodes[
          0].pipeline_node.execution_options.skip.reuse_artifacts = True
      expected_pipeline.nodes[
          1].pipeline_node.execution_options.run.perform_snapshot = True
      expected_pipeline.nodes[
          1].pipeline_node.execution_options.run.depends_on_snapshot = True
      expected_pipeline.nodes[
          2].pipeline_node.execution_options.run.SetInParent()

      partial_run_option = pipeline_pb2.PartialRun(
          from_nodes=['Transform'],
          to_nodes=['Trainer'],
          snapshot_settings=latest_pipeline_snapshot_settings)
      with pipeline_ops.initiate_pipeline_start(
          m, pipeline, partial_run_option=partial_run_option) as pipeline_state:
        self.assertEqual(expected_pipeline, pipeline_state.pipeline)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1714')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/experimental/core/pipeline_ops_test.py: 227-264
</a>
<div class="mid" id="frag1714" style="display:none"><pre>
  def test_initiate_pipeline_start_with_partial_run_default_to_nodes(
      self, mock_snapshot):
    with self._mlmd_connection as m:
      pipeline = _test_pipeline('test_pipeline', pipeline_pb2.Pipeline.SYNC)
      node_example_gen = pipeline.nodes.add().pipeline_node
      node_example_gen.node_info.id = 'ExampleGen'
      node_example_gen.downstream_nodes.extend(['Transform'])
      node_transform = pipeline.nodes.add().pipeline_node
      node_transform.node_info.id = 'Transform'
      node_transform.upstream_nodes.extend(['ExampleGen'])
      node_transform.downstream_nodes.extend(['Trainer'])
      node_trainer = pipeline.nodes.add().pipeline_node
      node_trainer.node_info.id = 'Trainer'
      node_trainer.upstream_nodes.extend(['Transform'])

      latest_pipeline_snapshot_settings = pipeline_pb2.SnapshotSettings()
      latest_pipeline_snapshot_settings.latest_pipeline_run_strategy.SetInParent(
      )

      expected_pipeline = copy.deepcopy(pipeline)
      expected_pipeline.runtime_spec.snapshot_settings.latest_pipeline_run_strategy.SetInParent(
      )
      expected_pipeline.nodes[
          0].pipeline_node.execution_options.skip.reuse_artifacts = True
      expected_pipeline.nodes[
          1].pipeline_node.execution_options.run.perform_snapshot = True
      expected_pipeline.nodes[
          1].pipeline_node.execution_options.run.depends_on_snapshot = True
      expected_pipeline.nodes[
          2].pipeline_node.execution_options.run.SetInParent()

      partial_run_option = pipeline_pb2.PartialRun(
          from_nodes=['Transform'],
          snapshot_settings=latest_pipeline_snapshot_settings)
      with pipeline_ops.initiate_pipeline_start(
          m, pipeline, partial_run_option=partial_run_option) as pipeline_state:
        self.assertEqual(expected_pipeline, pipeline_state.pipeline)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 87:</b> &nbsp; 2 fragments, nominal size 43 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1734')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/experimental/core/pipeline_ops_test.py: 863-920
</a>
<div class="mid" id="frag1734" style="display:none"><pre>
  def test_executor_node_stop_then_start_flow(self, pipeline,
                                              mock_async_task_gen,
                                              mock_sync_task_gen):
    service_job_manager = service_jobs.DummyServiceJobManager()
    with self._mlmd_connection as m:
      pipeline_uid = task_lib.PipelineUid.from_pipeline(pipeline)
      pipeline.nodes.add().pipeline_node.node_info.id = 'Trainer'
      trainer_node_uid = task_lib.NodeUid.from_pipeline_node(
          pipeline, pipeline.nodes[0].pipeline_node)

      # Start pipeline and stop trainer.
      pipeline_ops.initiate_pipeline_start(m, pipeline)
      with pstate.PipelineState.load(m, pipeline_uid) as pipeline_state:
        with pipeline_state.node_state_update_context(
            trainer_node_uid) as node_state:
          node_state.update(pstate.NodeState.STOPPING,
                            status_lib.Status(code=status_lib.Code.CANCELLED))

      task_queue = tq.TaskQueue()

      # Simulate ExecNodeTask for trainer already present in the task queue.
      trainer_task = test_utils.create_exec_node_task(node_uid=trainer_node_uid)
      task_queue.enqueue(trainer_task)

      pipeline_ops.orchestrate(m, task_queue, service_job_manager)

      # Dequeue pre-existing trainer task.
      task = task_queue.dequeue()
      task_queue.task_done(task)
      self.assertEqual(trainer_task, task)

      # Dequeue CancelNodeTask for trainer.
      task = task_queue.dequeue()
      task_queue.task_done(task)
      self.assertTrue(task_lib.is_cancel_node_task(task))
      self.assertEqual(trainer_node_uid, task.node_uid)

      self.assertTrue(task_queue.is_empty())

      with pstate.PipelineState.load(m, pipeline_uid) as pipeline_state:
        node_state = pipeline_state.get_node_state(trainer_node_uid)
        self.assertEqual(pstate.NodeState.STOPPING, node_state.state)
        self.assertEqual(status_lib.Code.CANCELLED, node_state.status.code)

      pipeline_ops.orchestrate(m, task_queue, service_job_manager)

      with pstate.PipelineState.load(m, pipeline_uid) as pipeline_state:
        node_state = pipeline_state.get_node_state(trainer_node_uid)
        self.assertEqual(pstate.NodeState.STOPPED, node_state.state)
        self.assertEqual(status_lib.Code.CANCELLED, node_state.status.code)

      pipeline_ops.initiate_node_start(m, trainer_node_uid)
      pipeline_ops.orchestrate(m, task_queue, service_job_manager)

      with pstate.PipelineState.load(m, pipeline_uid) as pipeline_state:
        node_state = pipeline_state.get_node_state(trainer_node_uid)
        self.assertEqual(pstate.NodeState.STARTED, node_state.state)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1736')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/experimental/core/pipeline_ops_test.py: 970-1036
</a>
<div class="mid" id="frag1736" style="display:none"><pre>
  def test_mixed_service_node_stop_then_start_flow(self, pipeline,
                                                   mock_async_task_gen,
                                                   mock_sync_task_gen):
    with self._mlmd_connection as m:
      pipeline_uid = task_lib.PipelineUid.from_pipeline(pipeline)
      pipeline.nodes.add().pipeline_node.node_info.id = 'Transform'

      transform_node_uid = task_lib.NodeUid.from_pipeline_node(
          pipeline, pipeline.nodes[0].pipeline_node)

      pipeline_ops.initiate_pipeline_start(m, pipeline)
      with pstate.PipelineState.load(
          m, task_lib.PipelineUid.from_pipeline(pipeline)) as pipeline_state:
        # Stop Transform.
        with pipeline_state.node_state_update_context(
            transform_node_uid) as node_state:
          node_state.update(pstate.NodeState.STOPPING,
                            status_lib.Status(code=status_lib.Code.CANCELLED))

      task_queue = tq.TaskQueue()

      # Simulate ExecNodeTask for Transform already present in the task queue.
      transform_task = test_utils.create_exec_node_task(
          node_uid=transform_node_uid)
      task_queue.enqueue(transform_task)

      pipeline_ops.orchestrate(m, task_queue, self._mock_service_job_manager)

      # stop_node_services should not be called as there was an active
      # ExecNodeTask for Transform which is a mixed service node.
      self._mock_service_job_manager.stop_node_services.assert_not_called()

      # Dequeue pre-existing transform task.
      task = task_queue.dequeue()
      task_queue.task_done(task)
      self.assertEqual(transform_task, task)

      # Dequeue CancelNodeTask for transform.
      task = task_queue.dequeue()
      task_queue.task_done(task)
      self.assertTrue(task_lib.is_cancel_node_task(task))
      self.assertEqual(transform_node_uid, task.node_uid)

      with pstate.PipelineState.load(m, pipeline_uid) as pipeline_state:
        node_state = pipeline_state.get_node_state(transform_node_uid)
        self.assertEqual(pstate.NodeState.STOPPING, node_state.state)
        self.assertEqual(status_lib.Code.CANCELLED, node_state.status.code)

      pipeline_ops.orchestrate(m, task_queue, self._mock_service_job_manager)

      # stop_node_services should be called for Transform which is a mixed
      # service node and corresponding ExecNodeTask has been dequeued.
      self._mock_service_job_manager.stop_node_services.assert_called_once_with(
          mock.ANY, 'Transform')

      with pstate.PipelineState.load(m, pipeline_uid) as pipeline_state:
        node_state = pipeline_state.get_node_state(transform_node_uid)
        self.assertEqual(pstate.NodeState.STOPPED, node_state.state)
        self.assertEqual(status_lib.Code.CANCELLED, node_state.status.code)

      pipeline_ops.initiate_node_start(m, transform_node_uid)
      pipeline_ops.orchestrate(m, task_queue, self._mock_service_job_manager)

      with pstate.PipelineState.load(m, pipeline_uid) as pipeline_state:
        node_state = pipeline_state.get_node_state(transform_node_uid)
        self.assertEqual(pstate.NodeState.STARTED, node_state.state)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 88:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1781')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/experimental/core/task_scheduler_test.py: 58-71
</a>
<div class="mid" id="frag1781" style="display:none"><pre>
  def test_register_using_executor_spec_type_url(self):
    # Register a fake task scheduler.
    ts.TaskSchedulerRegistry.register(self._spec_type_url, _FakeTaskScheduler)

    # Create a task and verify that the correct scheduler is instantiated.
    task = test_utils.create_exec_node_task(
        node_uid=task_lib.NodeUid(
            pipeline_uid=task_lib.PipelineUid(pipeline_id='pipeline'),
            node_id='Trainer'),
        pipeline=self._pipeline)
    task_scheduler = ts.TaskSchedulerRegistry.create_task_scheduler(
        mock.Mock(), self._pipeline, task)
    self.assertIsInstance(task_scheduler, _FakeTaskScheduler)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1782')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/experimental/core/task_scheduler_test.py: 72-86
</a>
<div class="mid" id="frag1782" style="display:none"><pre>
  def test_register_using_node_type_name(self):
    # Register a fake task scheduler.
    ts.TaskSchedulerRegistry.register(constants.IMPORTER_NODE_TYPE,
                                      _FakeTaskScheduler)

    # Create a task and verify that the correct scheduler is instantiated.
    task = test_utils.create_exec_node_task(
        node_uid=task_lib.NodeUid(
            pipeline_uid=task_lib.PipelineUid(pipeline_id='pipeline'),
            node_id='Importer'),
        pipeline=self._pipeline)
    task_scheduler = ts.TaskSchedulerRegistry.create_task_scheduler(
        mock.Mock(), self._pipeline, task)
    self.assertIsInstance(task_scheduler, _FakeTaskScheduler)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 89:</b> &nbsp; 3 fragments, nominal size 30 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1861')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/execution_publish_utils_test.py: 55-106
</a>
<div class="mid" id="frag1861" style="display:none"><pre>
  def testRegisterExecution(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      contexts = self._generate_contexts(m)
      input_example = standard_artifacts.Examples()
      execution_publish_utils.register_execution(
          m,
          self._execution_type,
          contexts,
          input_artifacts={'examples': [input_example]},
          exec_properties={
              'p1': 1,
          })
      [execution] = m.store.get_executions()
      self.assertProtoPartiallyEquals(
          """
          id: 1
          custom_properties {
            key: 'p1'
            value {int_value: 1}
          }
          last_known_state: RUNNING
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])
      [event] = m.store.get_events_by_execution_ids([execution.id])
      self.assertProtoPartiallyEquals(
          """
          artifact_id: 1
          execution_id: 1
          path {
            steps {
              key: 'examples'
            }
            steps {
              index: 0
            }
          }
          type: INPUT
          """,
          event,
          ignored_fields=['milliseconds_since_epoch'])
      # Verifies the context-execution edges are set up.
      self.assertCountEqual(
          [c.id for c in contexts],
          [c.id for c in m.store.get_contexts_by_execution(execution.id)])
      self.assertCountEqual(
          [c.id for c in contexts],
          [c.id for c in m.store.get_contexts_by_artifact(input_example.id)])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1862')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/execution_publish_utils_test.py: 107-153
</a>
<div class="mid" id="frag1862" style="display:none"><pre>
  def testPublishCachedExecution(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      contexts = self._generate_contexts(m)
      execution_id = execution_publish_utils.register_execution(
          m, self._execution_type, contexts).id
      output_example = standard_artifacts.Examples()
      execution_publish_utils.publish_cached_execution(
          m,
          contexts,
          execution_id,
          output_artifacts={'examples': [output_example]})
      [execution] = m.store.get_executions()
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: CACHED
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])
      [event] = m.store.get_events_by_execution_ids([execution.id])
      self.assertProtoPartiallyEquals(
          """
          artifact_id: 1
          execution_id: 1
          path {
            steps {
              key: 'examples'
            }
            steps {
              index: 0
            }
          }
          type: OUTPUT
          """,
          event,
          ignored_fields=['milliseconds_since_epoch'])
      # Verifies the context-execution edges are set up.
      self.assertCountEqual(
          [c.id for c in contexts],
          [c.id for c in m.store.get_contexts_by_execution(execution.id)])
      self.assertCountEqual(
          [c.id for c in contexts],
          [c.id for c in m.store.get_contexts_by_artifact(output_example.id)])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1872')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/execution_publish_utils_test.py: 559-606
</a>
<div class="mid" id="frag1872" style="display:none"><pre>
  def testPublishInternalExecution(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      contexts = self._generate_contexts(m)
      execution_id = execution_publish_utils.register_execution(
          m, self._execution_type, contexts).id
      output_example = standard_artifacts.Examples()
      execution_publish_utils.publish_internal_execution(
          m,
          contexts,
          execution_id,
          output_artifacts={'examples': [output_example]})
      [execution] = m.store.get_executions()
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: COMPLETE
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])
      [event] = m.store.get_events_by_execution_ids([execution.id])
      self.assertProtoPartiallyEquals(
          """
          artifact_id: 1
          execution_id: 1
          path {
            steps {
              key: 'examples'
            }
            steps {
              index: 0
            }
          }
          type: INTERNAL_OUTPUT
          """,
          event,
          ignored_fields=['milliseconds_since_epoch'])
      # Verifies the context-execution edges are set up.
      self.assertCountEqual(
          [c.id for c in contexts],
          [c.id for c in m.store.get_contexts_by_execution(execution.id)])
      self.assertCountEqual(
          [c.id for c in contexts],
          [c.id for c in m.store.get_contexts_by_artifact(output_example.id)])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 90:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1864')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/execution_publish_utils_test.py: 227-239
</a>
<div class="mid" id="frag1864" style="display:none"><pre>
  def testPublishSuccessExecutionFailNewKey(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      contexts = self._generate_contexts(m)
      execution_id = execution_publish_utils.register_execution(
          m, self._execution_type, contexts).id
      executor_output = execution_result_pb2.ExecutorOutput()
      executor_output.output_artifacts['new_key'].artifacts.add()

      with self.assertRaisesRegex(RuntimeError, 'contains more keys'):
        execution_publish_utils.publish_succeeded_execution(
            m, execution_id, contexts,
            {'examples': [standard_artifacts.Examples()]}, executor_output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1866')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/execution_publish_utils_test.py: 359-371
</a>
<div class="mid" id="frag1866" style="display:none"><pre>
  def testPublishSuccessExecutionFailChangedType(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      contexts = self._generate_contexts(m)
      execution_id = execution_publish_utils.register_execution(
          m, self._execution_type, contexts).id
      executor_output = execution_result_pb2.ExecutorOutput()
      executor_output.output_artifacts['examples'].artifacts.add().type_id = 10

      with self.assertRaisesRegex(RuntimeError, 'change artifact type'):
        execution_publish_utils.publish_succeeded_execution(
            m, execution_id, contexts,
            {'examples': [standard_artifacts.Examples(),]}, executor_output)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 91:</b> &nbsp; 4 fragments, nominal size 18 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1868')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/execution_publish_utils_test.py: 407-452
</a>
<div class="mid" id="frag1868" style="display:none"><pre>
  def testPublishSuccessExecutionUpdatesCustomProperties(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      executor_output = text_format.Parse(
          """
          execution_properties {
          key: "int"
          value {
            int_value: 1
          }
          }
          execution_properties {
            key: "string"
            value {
              string_value: "string_value"
            }
          }
           """, execution_result_pb2.ExecutorOutput())
      contexts = self._generate_contexts(m)
      execution_id = execution_publish_utils.register_execution(
          m, self._execution_type, contexts).id
      execution_publish_utils.publish_succeeded_execution(
          m, execution_id, contexts, {}, executor_output)
      [execution] = m.store.get_executions_by_id([execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: COMPLETE
          custom_properties {
            key: "int"
            value {
              int_value: 1
            }
          }
          custom_properties {
            key: "string"
            value {
              string_value: "string_value"
            }
          }
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1870')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/execution_publish_utils_test.py: 493-518
</a>
<div class="mid" id="frag1870" style="display:none"><pre>
  def testPublishSuccessExecutionDropsEmptyResult(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      executor_output = text_format.Parse(
          """
        execution_result {
          code: 0
         }
      """, execution_result_pb2.ExecutorOutput())
      contexts = self._generate_contexts(m)
      execution_id = execution_publish_utils.register_execution(
          m, self._execution_type, contexts).id
      execution_publish_utils.publish_failed_execution(m, contexts,
                                                       execution_id,
                                                       executor_output)
      [execution] = m.store.get_executions_by_id([execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: FAILED
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1869')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/execution_publish_utils_test.py: 453-492
</a>
<div class="mid" id="frag1869" style="display:none"><pre>
  def testPublishSuccessExecutionRecordExecutionResult(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      executor_output = text_format.Parse(
          """
        execution_result {
          code: 0
          result_message: 'info message.'
         }
      """, execution_result_pb2.ExecutorOutput())
      contexts = self._generate_contexts(m)
      execution_id = execution_publish_utils.register_execution(
          m, self._execution_type, contexts).id
      execution_publish_utils.publish_failed_execution(m, contexts,
                                                       execution_id,
                                                       executor_output)
      [execution] = m.store.get_executions_by_id([execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: FAILED
          custom_properties {
            key: '__execution_result__'
            value {
              string_value: '{\\n  "resultMessage": "info message."\\n}'
            }
          }
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])
      # No events because there is no artifact published.
      events = m.store.get_events_by_execution_ids([execution.id])
      self.assertEmpty(events)
      # Verifies the context-execution edges are set up.
      self.assertCountEqual(
          [c.id for c in contexts],
          [c.id for c in m.store.get_contexts_by_execution(execution.id)])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1871')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/execution_publish_utils_test.py: 519-558
</a>
<div class="mid" id="frag1871" style="display:none"><pre>
  def testPublishFailedExecution(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      executor_output = text_format.Parse(
          """
        execution_result {
          code: 1
          result_message: 'error message.'
         }
      """, execution_result_pb2.ExecutorOutput())
      contexts = self._generate_contexts(m)
      execution_id = execution_publish_utils.register_execution(
          m, self._execution_type, contexts).id
      execution_publish_utils.publish_failed_execution(m, contexts,
                                                       execution_id,
                                                       executor_output)
      [execution] = m.store.get_executions_by_id([execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: FAILED
          custom_properties {
            key: '__execution_result__'
            value {
              string_value: '{\\n  "resultMessage": "error message.",\\n  "code": 1\\n}'
            }
          }
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])
      # No events because there is no artifact published.
      events = m.store.get_events_by_execution_ids([execution.id])
      self.assertEmpty(events)
      # Verifies the context-execution edges are set up.
      self.assertCountEqual(
          [c.id for c in contexts],
          [c.id for c in m.store.get_contexts_by_execution(execution.id)])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 92:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1873')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/docker_executor_operator_test.py: 103-117
</a>
<div class="mid" id="frag1873" style="display:none"><pre>
  def _set_up_test_execution_info(self,
                                  input_dict=None,
                                  output_dict=None,
                                  exec_properties=None):
    return data_types.ExecutionInfo(
        input_dict=input_dict or {},
        output_dict=output_dict or {},
        exec_properties=exec_properties or {},
        execution_output_uri='/testing/executor/output/',
        stateful_working_dir='/testing/stateful/dir',
        pipeline_node=pipeline_pb2.PipelineNode(
            node_info=pipeline_pb2.NodeInfo(
                type=metadata_store_pb2.ExecutionType(name='Docker_executor'))),
        pipeline_info=pipeline_pb2.PipelineInfo(id='test_pipeline_id'))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1891')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/kubernetes_executor_operator_test.py: 227-242
</a>
<div class="mid" id="frag1891" style="display:none"><pre>
  def _set_up_test_execution_info(self,
                                  input_dict=None,
                                  output_dict=None,
                                  exec_properties=None):
    return data_types.ExecutionInfo(
        execution_id=123,
        input_dict=input_dict or {},
        output_dict=output_dict or {},
        exec_properties=exec_properties or {},
        execution_output_uri='/testing/executor/output/',
        stateful_working_dir='/testing/stateful/dir',
        pipeline_node=pipeline_pb2.PipelineNode(
            node_info=pipeline_pb2.NodeInfo(id='fakecomponent-fakecomponent')),
        pipeline_info=pipeline_pb2.PipelineInfo(id='Test'),
        pipeline_run_id='123')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 93:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1904')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/python_executor_operator_test.py: 91-117
</a>
<div class="mid" id="frag1904" style="display:none"><pre>
  def testRunExecutor_with_InprocessExecutor(self):
    executor_sepc = text_format.Parse(
        """
      class_path: "tfx.orchestration.portable.python_executor_operator_test.InprocessExecutor"
    """, executable_spec_pb2.PythonClassExecutableSpec())
    operator = python_executor_operator.PythonExecutorOperator(executor_sepc)
    input_dict = {'input_key': [standard_artifacts.Examples()]}
    output_dict = {'output_key': [standard_artifacts.Model()]}
    exec_properties = {'key': 'value'}
    executor_output = operator.run_executor(
        self._get_execution_info(input_dict, output_dict, exec_properties))
    self.assertProtoPartiallyEquals(
        """
          execution_properties {
            key: "key"
            value {
              string_value: "value"
            }
          }
          output_artifacts {
            key: "output_key"
            value {
              artifacts {
              }
            }
          }""", executor_output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1905')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/python_executor_operator_test.py: 118-144
</a>
<div class="mid" id="frag1905" style="display:none"><pre>
  def testRunExecutor_with_NotInprocessExecutor(self):
    executor_sepc = text_format.Parse(
        """
      class_path: "tfx.orchestration.portable.python_executor_operator_test.NotInprocessExecutor"
    """, executable_spec_pb2.PythonClassExecutableSpec())
    operator = python_executor_operator.PythonExecutorOperator(executor_sepc)
    input_dict = {'input_key': [standard_artifacts.Examples()]}
    output_dict = {'output_key': [standard_artifacts.Model()]}
    exec_properties = {'key': 'value'}
    executor_output = operator.run_executor(
        self._get_execution_info(input_dict, output_dict, exec_properties))
    self.assertProtoPartiallyEquals(
        """
          execution_properties {
            key: "key"
            value {
              string_value: "value"
            }
          }
          output_artifacts {
            key: "output_key"
            value {
              artifacts {
              }
            }
          }""", executor_output)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 94:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1911')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/resolver_node_handler_test.py: 37-71
</a>
<div class="mid" id="frag1911" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    pipeline_root = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self.id())

    # Makes sure multiple connections within a test always connect to the same
    # MLMD instance.
    metadata_path = os.path.join(pipeline_root, 'metadata', 'metadata.db')
    connection_config = metadata.sqlite_metadata_connection_config(
        metadata_path)
    connection_config.sqlite.SetInParent()
    self._mlmd_connection = metadata.Metadata(
        connection_config=connection_config)
    self._testdata_dir = os.path.join(os.path.dirname(__file__), 'testdata')

    # Sets up pipelines
    pipeline = pipeline_pb2.Pipeline()
    self.load_proto_from_text(
        os.path.join(
            os.path.dirname(__file__), 'testdata',
            'pipeline_for_resolver_test.pbtxt'), pipeline)
    self._pipeline_info = pipeline.pipeline_info
    self._pipeline_runtime_spec = pipeline.runtime_spec
    runtime_parameter_utils.substitute_runtime_parameter(
        pipeline, {
            constants.PIPELINE_RUN_ID_PARAMETER_NAME: 'my_pipeline_run',
        })

    # Extracts components
    self._my_trainer = pipeline.nodes[0].pipeline_node
    self._my_resolver = pipeline.nodes[1].pipeline_node
    self._model_type = (
        self._my_trainer.outputs.outputs['model'].artifact_spec.type)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1919')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/importer_node_handler_test.py: 29-62
</a>
<div class="mid" id="frag1919" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    pipeline_root = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self.id())

    # Makes sure multiple connections within a test always connect to the same
    # MLMD instance.
    metadata_path = os.path.join(pipeline_root, 'metadata', 'metadata.db')
    connection_config = metadata.sqlite_metadata_connection_config(
        metadata_path)
    connection_config.sqlite.SetInParent()
    self._mlmd_connection = metadata.Metadata(
        connection_config=connection_config)
    self._testdata_dir = os.path.join(os.path.dirname(__file__), 'testdata')

    # Sets up pipelines
    pipeline = pipeline_pb2.Pipeline()
    self.load_proto_from_text(
        os.path.join(
            os.path.dirname(__file__), 'testdata',
            'pipeline_for_launcher_test.pbtxt'), pipeline)
    self._pipeline_info = pipeline.pipeline_info
    self._pipeline_runtime_spec = pipeline.runtime_spec
    runtime_parameter_utils.substitute_runtime_parameter(
        pipeline, {
            constants.PIPELINE_RUN_ID_PARAMETER_NAME: 'my_pipeline_run',
        })

    # Extracts components
    self._importer = pipeline.nodes[3].pipeline_node
    # Fake tfx_version for tests.
    tfx_version.__version__ = '0.123.4.dev'

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 95:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1914')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/resolver_node_handler_test.py: 173-195
</a>
<div class="mid" id="frag1914" style="display:none"><pre>
  def testRun_InputResolutionError_ExecutionFailed(self, mock_resolve):
    mock_resolve.side_effect = exceptions.InputResolutionError('Meh')
    handler = resolver_node_handler.ResolverNodeHandler()

    execution_info = handler.run(
        mlmd_connection=self._mlmd_connection,
        pipeline_node=self._my_resolver,
        pipeline_info=self._pipeline_info,
        pipeline_runtime_spec=self._pipeline_runtime_spec)

    with self._mlmd_connection as m:
      self.assertTrue(execution_info.execution_id)
      [execution] = m.store.get_executions_by_id([execution_info.execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: FAILED
          """,
          execution,
          ignored_fields=['type_id', 'custom_properties',
                          'create_time_since_epoch',
                          'last_update_time_since_epoch'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1915')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/resolver_node_handler_test.py: 197-223
</a>
<div class="mid" id="frag1915" style="display:none"><pre>
  def testRun_MultipleInputs_ExecutionFailed(self, mock_resolve):
    mock_resolve.return_value = inputs_utils.Trigger([
        {'model': [self._create_model_artifact(uri='/tmp/model/1')]},
        {'model': [self._create_model_artifact(uri='/tmp/model/2')]},
    ])
    handler = resolver_node_handler.ResolverNodeHandler()

    execution_info = handler.run(
        mlmd_connection=self._mlmd_connection,
        pipeline_node=self._my_resolver,
        pipeline_info=self._pipeline_info,
        pipeline_runtime_spec=self._pipeline_runtime_spec)

    with self._mlmd_connection as m:
      self.assertTrue(execution_info.execution_id)
      [execution] = m.store.get_executions_by_id([execution_info.execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: FAILED
          """,
          execution,
          ignored_fields=['type_id', 'custom_properties',
                          'create_time_since_epoch',
                          'last_update_time_since_epoch'])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 96:</b> &nbsp; 2 fragments, nominal size 40 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1920')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/importer_node_handler_test.py: 63-183
</a>
<div class="mid" id="frag1920" style="display:none"><pre>
  def testLauncher_importer_mode_reimport_enabled(self):
    handler = importer_node_handler.ImporterNodeHandler()
    execution_info = handler.run(
        mlmd_connection=self._mlmd_connection,
        pipeline_node=self._importer,
        pipeline_info=self._pipeline_info,
        pipeline_runtime_spec=self._pipeline_runtime_spec)

    with self._mlmd_connection as m:
      [artifact] = m.store.get_artifacts_by_type('Schema')
      self.assertProtoPartiallyEquals(
          """
          id: 1
          uri: "my_url"
          custom_properties {
            key: "int_custom_property"
            value {
              int_value: 123
            }
          }
          custom_properties {
            key: "str_custom_property"
            value {
              string_value: "abc"
            }
          }
          custom_properties {
            key: "tfx_version"
            value {
              string_value: "0.123.4.dev"
            }
          }
          state: LIVE""",
          artifact,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])
      [execution] = m.store.get_executions_by_id([execution_info.execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: COMPLETE
          custom_properties {
            key: "artifact_uri"
            value {
              string_value: "my_url"
            }
          }
          custom_properties {
            key: "reimport"
            value {
              int_value: 1
            }
          }
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])

    execution_info = handler.run(
        mlmd_connection=self._mlmd_connection,
        pipeline_node=self._importer,
        pipeline_info=self._pipeline_info,
        pipeline_runtime_spec=self._pipeline_runtime_spec)
    with self._mlmd_connection as m:
      new_artifact = m.store.get_artifacts_by_type('Schema')[1]
      self.assertProtoPartiallyEquals(
          """
          id: 2
          uri: "my_url"
          custom_properties {
            key: "int_custom_property"
            value {
              int_value: 123
            }
          }
          custom_properties {
            key: "str_custom_property"
            value {
              string_value: "abc"
            }
          }
          custom_properties {
            key: "tfx_version"
            value {
              string_value: "0.123.4.dev"
            }
          }
          state: LIVE""",
          new_artifact,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])
      [execution] = m.store.get_executions_by_id([execution_info.execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 2
          last_known_state: COMPLETE
          custom_properties {
            key: "artifact_uri"
            value {
              string_value: "my_url"
            }
          }
          custom_properties {
            key: "reimport"
            value {
              int_value: 1
            }
          }
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1921')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/importer_node_handler_test.py: 184-281
</a>
<div class="mid" id="frag1921" style="display:none"><pre>
  def testLauncher_importer_mode_reimport_disabled(self):
    self._importer.parameters.parameters['reimport'].field_value.int_value = 0
    handler = importer_node_handler.ImporterNodeHandler()
    execution_info = handler.run(
        mlmd_connection=self._mlmd_connection,
        pipeline_node=self._importer,
        pipeline_info=self._pipeline_info,
        pipeline_runtime_spec=self._pipeline_runtime_spec)

    with self._mlmd_connection as m:
      [artifact] = m.store.get_artifacts_by_type('Schema')
      self.assertProtoPartiallyEquals(
          """
          id: 1
          uri: "my_url"
          custom_properties {
            key: "int_custom_property"
            value {
              int_value: 123
            }
          }
          custom_properties {
            key: "str_custom_property"
            value {
              string_value: "abc"
            }
          }
          custom_properties {
            key: "tfx_version"
            value {
              string_value: "0.123.4.dev"
            }
          }
          state: LIVE""",
          artifact,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])
      [execution] = m.store.get_executions_by_id([execution_info.execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: COMPLETE
          custom_properties {
            key: "artifact_uri"
            value {
              string_value: "my_url"
            }
          }
          custom_properties {
            key: "reimport"
            value {
              int_value: 0
            }
          }
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])

    # Run the 2nd execution. Since the reimport is disabled, no new schema
    # is imported and the corresponding execution is published as CACHED.
    execution_info = handler.run(
        mlmd_connection=self._mlmd_connection,
        pipeline_node=self._importer,
        pipeline_info=self._pipeline_info,
        pipeline_runtime_spec=self._pipeline_runtime_spec)
    with self._mlmd_connection as m:
      # No new Schema is produced.
      self.assertLen(m.store.get_artifacts_by_type('Schema'), 1)
      [execution] = m.store.get_executions_by_id([execution_info.execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 2
          last_known_state: CACHED
          custom_properties {
            key: "artifact_uri"
            value {
              string_value: "my_url"
            }
          }
          custom_properties {
            key: "reimport"
            value {
              int_value: 0
            }
          }
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 97:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1924')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/runtime_parameter_utils_test.py: 34-55
</a>
<div class="mid" id="frag1924" style="display:none"><pre>
  def testFullySubstituteRuntimeParameter(self):
    pipeline = pipeline_pb2.Pipeline()
    expected = pipeline_pb2.Pipeline()
    self.load_proto_from_text(
        os.path.join(self._testdata_dir,
                     'pipeline_with_runtime_parameter.pbtxt'), pipeline)
    self.load_proto_from_text(
        os.path.join(self._testdata_dir,
                     'pipeline_with_runtime_parameter_substituted.pbtxt'),
        expected)
    parameters = runtime_parameter_utils.substitute_runtime_parameter(
        pipeline, {
            'context_name_rp': 'my_context',
            'prop_one_rp': 2,
            'prop_two_rp': 'X'
        })
    self.assertProtoEquals(pipeline, expected)
    self.assertEqual(len(parameters), 3)
    self.assertEqual(parameters['context_name_rp'], 'my_context')
    self.assertEqual(parameters['prop_one_rp'], 2)
    self.assertEqual(parameters['prop_two_rp'], 'X')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1925')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/runtime_parameter_utils_test.py: 56-76
</a>
<div class="mid" id="frag1925" style="display:none"><pre>
  def testPartiallySubstituteRuntimeParameter(self):
    pipeline = pipeline_pb2.Pipeline()
    expected = pipeline_pb2.Pipeline()
    self.load_proto_from_text(
        os.path.join(self._testdata_dir,
                     'pipeline_with_runtime_parameter.pbtxt'), pipeline)
    self.load_proto_from_text(
        os.path.join(
            self._testdata_dir,
            'pipeline_with_runtime_parameter_partially_substituted.pbtxt'),
        expected)
    parameters = runtime_parameter_utils.substitute_runtime_parameter(
        pipeline, {
            'context_name_rp': 'my_context',
        })
    self.assertProtoEquals(pipeline, expected)
    self.assertEqual(len(parameters), 3)
    self.assertEqual(parameters['context_name_rp'], 'my_context')
    self.assertEqual(parameters['prop_one_rp'], 1)
    self.assertIsNone(parameters['prop_two_rp'])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 98:</b> &nbsp; 3 fragments, nominal size 33 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2001')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/inputs_utils_test.py: 465-503
</a>
<div class="mid" id="frag2001" style="display:none"><pre>
  def testLatestUnprocessedArtifacts(self):
    pipeline = self.load_pipeline_proto(
        'pipeline_for_input_resolver_test.pbtxt')
    my_example_gen = pipeline.nodes[0].pipeline_node
    my_transform = pipeline.nodes[2].pipeline_node

    step1_pb = my_transform.inputs.resolver_config.resolver_steps.add()
    step1_pb.class_path = (
        'tfx.dsl.resolvers.unprocessed_artifacts_resolver'
        '.UnprocessedArtifactsResolver')
    step1_pb.config_json = '{"execution_type_name": "Transform"}'
    step2_pb = my_transform.inputs.resolver_config.resolver_steps.add()
    step2_pb.class_path = (
        'tfx.dsl.input_resolution.strategies.latest_artifact_strategy'
        '.LatestArtifactStrategy')
    step2_pb.config_json = '{}'

    with self.get_metadata() as m:
      ex1 = self.make_examples(uri='a')
      ex2 = self.make_examples(uri='b')
      output_artifacts = self.fake_execute(
          m,
          my_example_gen,
          input_map=None,
          output_map={'output_examples': [ex1]})
      ex1 = output_artifacts['output_examples'][0]
      output_artifacts = self.fake_execute(
          m,
          my_example_gen,
          input_map=None,
          output_map={'output_examples': [ex2]})
      ex2 = output_artifacts['output_examples'][0]

      result = inputs_utils.resolve_input_artifacts(
          metadata_handler=m,
          node_inputs=my_transform.inputs)

    self.assertArtifactMapEqual({'examples': [ex2]}, result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2002')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/inputs_utils_test.py: 506-546
</a>
<div class="mid" id="frag2002" style="display:none"><pre>
  def testLatestUnprocessedArtifacts_IgnoreAlreadyProcessed(self):
    pipeline = self.load_pipeline_proto(
        'pipeline_for_input_resolver_test.pbtxt')
    my_example_gen = pipeline.nodes[0].pipeline_node
    my_transform = pipeline.nodes[2].pipeline_node

    step1_pb = my_transform.inputs.resolver_config.resolver_steps.add()
    step1_pb.class_path = (
        'tfx.dsl.resolvers.unprocessed_artifacts_resolver'
        '.UnprocessedArtifactsResolver')
    step1_pb.config_json = '{"execution_type_name": "Transform"}'
    step2_pb = my_transform.inputs.resolver_config.resolver_steps.add()
    step2_pb.class_path = (
        'tfx.dsl.input_resolution.strategies.latest_artifact_strategy'
        '.LatestArtifactStrategy')
    step2_pb.config_json = '{}'

    with self.get_metadata() as m:
      ex1 = self.make_examples(uri='a')
      ex2 = self.make_examples(uri='b')
      output_artifacts = self.fake_execute(
          m,
          my_example_gen,
          input_map=None,
          output_map={'output_examples': [ex1]})
      ex1 = output_artifacts['output_examples'][0]
      output_artifacts = self.fake_execute(
          m,
          my_example_gen,
          input_map=None,
          output_map={'output_examples': [ex2]})
      ex2 = output_artifacts['output_examples'][0]
      self.fake_execute(
          m, my_transform, input_map={'examples': [ex2]}, output_map=None)

      result = inputs_utils.resolve_input_artifacts(
          metadata_handler=m,
          node_inputs=my_transform.inputs)

    self.assertArtifactMapEqual({'examples': [ex1]}, result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2003')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/portable/inputs_utils_test.py: 549-593
</a>
<div class="mid" id="frag2003" style="display:none"><pre>
  def testLatestUnprocessedArtifacts_NoneIfEverythingProcessed(self):
    pipeline = self.load_pipeline_proto(
        'pipeline_for_input_resolver_test.pbtxt')
    my_example_gen = pipeline.nodes[0].pipeline_node
    my_transform = pipeline.nodes[2].pipeline_node

    step1_pb = my_transform.inputs.resolver_config.resolver_steps.add()
    step1_pb.class_path = (
        'tfx.dsl.resolvers.unprocessed_artifacts_resolver'
        '.UnprocessedArtifactsResolver')
    step1_pb.config_json = '{"execution_type_name": "Transform"}'
    step2_pb = my_transform.inputs.resolver_config.resolver_steps.add()
    step2_pb.class_path = (
        'tfx.dsl.input_resolution.strategies.latest_artifact_strategy'
        '.LatestArtifactStrategy')
    step2_pb.config_json = '{}'

    with self.get_metadata() as m:
      ex1 = self.make_examples(uri='a')
      ex2 = self.make_examples(uri='b')
      output_artifacts = self.fake_execute(
          m,
          my_example_gen,
          input_map=None,
          output_map={'output_examples': [ex1]})
      ex1 = output_artifacts['output_examples'][0]
      output_artifacts = self.fake_execute(
          m,
          my_example_gen,
          input_map=None,
          output_map={'output_examples': [ex2]})
      ex2 = output_artifacts['output_examples'][0]
      self.fake_execute(m, my_transform,
                        input_map={'examples': [ex1]},
                        output_map=None)
      self.fake_execute(m, my_transform,
                        input_map={'examples': [ex2]},
                        output_map=None)

      result = inputs_utils.resolve_input_artifacts(
          metadata_handler=m,
          node_inputs=my_transform.inputs)

    self.assertIsNone(result)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 99:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2059')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/local/runner_utils.py: 46-64
</a>
<div class="mid" id="frag2059" style="display:none"><pre>
def _build_executable_spec(
    node_id: str,
    spec: any_pb2.Any) -&gt; local_deployment_config_pb2.ExecutableSpec:
  """Builds ExecutableSpec given the any proto from IntermediateDeploymentConfig."""
  result = local_deployment_config_pb2.ExecutableSpec()
  if spec.Is(result.python_class_executable_spec.DESCRIPTOR):
    spec.Unpack(result.python_class_executable_spec)
  elif spec.Is(result.container_executable_spec.DESCRIPTOR):
    spec.Unpack(result.container_executable_spec)
  elif spec.Is(result.beam_executable_spec.DESCRIPTOR):
    spec.Unpack(result.beam_executable_spec)
  else:
    raise ValueError(
        'Executor spec of {} is expected to be of one of the '
        'types of tfx.orchestration.deployment_config.ExecutableSpec.spec '
        'but got type {}'.format(node_id, spec.type_url))
  return result


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2060')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/local/runner_utils.py: 65-79
</a>
<div class="mid" id="frag2060" style="display:none"><pre>
def _build_local_platform_config(
    node_id: str,
    spec: any_pb2.Any) -&gt; local_deployment_config_pb2.LocalPlatformConfig:
  """Builds LocalPlatformConfig given the any proto from IntermediateDeploymentConfig."""
  result = local_deployment_config_pb2.LocalPlatformConfig()
  if spec.Is(result.docker_platform_config.DESCRIPTOR):
    spec.Unpack(result.docker_platform_config)
  else:
    raise ValueError(
        'Platform config of {} is expected to be of one of the types of '
        'tfx.orchestration.deployment_config.LocalPlatformConfig.config '
        'but got type {}'.format(node_id, spec.type_url))
  return result


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 100:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2128')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/airflow/airflow_dag_runner_test.py: 199-221
</a>
<div class="mid" id="frag2128" style="display:none"><pre>
  def testRuntimeParam(self):
    param = RuntimeParameter('name', str, 'tf"x')
    component_f = _FakeComponent(_FakeComponentSpecF(a=param))
    airflow_config = {
        'schedule_interval': '* * * * *',
        'start_date': datetime.datetime(2019, 1, 1)
    }
    test_pipeline = pipeline.Pipeline(
        pipeline_name='x',
        pipeline_root='y',
        metadata_connection_config=None,
        components=[component_f])

    runner = airflow_dag_runner.AirflowDagRunner(
        airflow_dag_runner.AirflowPipelineConfig(
            airflow_dag_config=airflow_config))
    dag = runner.run(test_pipeline)
    task = dag.tasks[0]
    self.assertDictEqual(
        {'exec_properties': {
            'a': '{{ dag_run.conf.get("name", "tf\\"x") }}'
        }}, task.op_kwargs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2129')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/airflow/airflow_dag_runner_test.py: 222-246
</a>
<div class="mid" id="frag2129" style="display:none"><pre>
  def testRuntimeParamTemplated(self):
    param = RuntimeParameter('a', str, '{{execution_date}}')
    component_f = _FakeComponent(_FakeComponentSpecF(a=param))
    airflow_config = {
        'schedule_interval': '* * * * *',
        'start_date': datetime.datetime(2019, 1, 1)
    }
    test_pipeline = pipeline.Pipeline(
        pipeline_name='x',
        pipeline_root='y',
        metadata_connection_config=None,
        components=[component_f])

    runner = airflow_dag_runner.AirflowDagRunner(
        airflow_dag_runner.AirflowPipelineConfig(
            airflow_dag_config=airflow_config))
    dag = runner.run(test_pipeline)
    task = dag.tasks[0]
    self.assertDictEqual(
        {
            'exec_properties': {
                'a': '{{ dag_run.conf.get("a", execution_date) }}'
            }
        }, task.op_kwargs)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 101:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2138')" href="javascript:;">
tfx-1.6.0/tfx/orchestration/test_pipelines/download_grep_print_pipeline.py: 118-136
</a>
<div class="mid" id="frag2138" style="display:none"><pre>
def create_pipeline_component_instances(text_url: str, pattern: str):
  """Creates tasks for the download_grep_print pipeline."""

  downloader_task = downloader_component(url=text_url)
  grep_task = grep_component(
      text=downloader_task.outputs['data'],
      pattern=pattern,
  )
  print_task = print_component(
      text=grep_task.outputs['filtered_text'],
  )

  component_instances = [
      downloader_task,
      grep_task,
      print_task,
  ]

  return component_instances
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3028')" href="javascript:;">
tfx-1.6.0/tfx/examples/custom_components/container_components/download_grep_print_pipeline.py: 118-136
</a>
<div class="mid" id="frag3028" style="display:none"><pre>
def create_pipeline_component_instances(text_url: str, pattern: str):
  """Creates tasks for the download_grep_print pipeline."""

  downloader_task = downloader_component(url=text_url)
  grep_task = grep_component(
      text=downloader_task.outputs['data'],
      pattern=pattern,
  )
  print_task = print_component(
      text=grep_task.outputs['filtered_text'],
  )

  component_instances = [
      downloader_task,
      grep_task,
      print_task,
  ]

  return component_instances
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 102:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2339')" href="javascript:;">
tfx-1.6.0/tfx/utils/deprecation_utils_test.py: 44-69
</a>
<div class="mid" id="frag2339" style="display:none"><pre>
  def testDeprecated(self):
    # By default, we warn once across all calls.
    my_function_1 = self._mock_function(name='my_function_1')
    deprecated_func_1 = deprecation_utils.deprecated(
        '2099-01-02', 'Please change to new_my_function_1')(
            my_function_1)
    deprecated_func_1()
    deprecated_func_1()
    self._assertDeprecatedWarningRegex(
        r'From .*: my_function_1 \(from .*\) is deprecated and will be '
        r'removed after 2099-01-02. Instructions for updating:\n'
        r'Please change to new_my_function_1')
    self.assertEqual(my_function_1.call_count, 2)
    self._mock_warn.reset_mock()

    # If `warn_once=False`, we warn once for each call.
    my_function_2 = self._mock_function()
    deprecated_func_2 = deprecation_utils.deprecated(
        '2099-01-02', 'Please change to new_my_function_2', warn_once=False)(
            my_function_2)
    deprecated_func_2()
    deprecated_func_2()
    deprecated_func_2()
    self.assertEqual(self._mock_warn.call_count, 3)
    self.assertEqual(my_function_2.call_count, 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2340')" href="javascript:;">
tfx-1.6.0/tfx/utils/deprecation_utils_test.py: 70-92
</a>
<div class="mid" id="frag2340" style="display:none"><pre>
  def testDeprecationAliasFunction(self):
    # By default, we warn once across all calls.
    my_function_1 = self._mock_function(name='my_function_1')
    deprecation_alias_1 = deprecation_utils.deprecated_alias(
        'deprecation_alias_1', 'my_function_1', my_function_1)
    deprecation_alias_1()
    deprecation_alias_1()
    self._assertDeprecatedWarningRegex(
        'From .*: The name deprecation_alias_1 is deprecated. Please use '
        'my_function_1 instead.')
    self.assertEqual(my_function_1.call_count, 2)
    self._mock_warn.reset_mock()

    # If `warn_once=False`, we warn once for each call.
    my_function_2 = self._mock_function()
    deprecation_alias_2 = deprecation_utils.deprecated_alias(
        'deprecation_alias_2', 'my_function_2', my_function_2, warn_once=False)
    deprecation_alias_2()
    deprecation_alias_2()
    deprecation_alias_2()
    self.assertEqual(self._mock_warn.call_count, 3)
    self.assertEqual(my_function_2.call_count, 3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 103:</b> &nbsp; 3 fragments, nominal size 17 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2371')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/kubeflow_dag_runner_patcher_test.py: 32-52
</a>
<div class="mid" id="frag2371" style="display:none"><pre>
  def testPatcher(self):
    given_image_name = 'foo/bar'
    built_image_name = 'foo/bar@sha256:1234567890'

    mock_build_image_fn = mock.MagicMock(return_value=built_image_name)
    patcher = kubeflow_dag_runner_patcher.KubeflowDagRunnerPatcher(
        call_real_run=True,
        build_image_fn=mock_build_image_fn,
        use_temporary_output_file=True)
    runner_config = kubeflow_dag_runner.KubeflowDagRunnerConfig(
        tfx_image=given_image_name)
    runner = kubeflow_dag_runner.KubeflowDagRunner(config=runner_config)
    pipeline = tfx_pipeline.Pipeline('dummy', 'dummy_root')
    with patcher.patch() as context:
      runner.run(pipeline)
    self.assertTrue(context[patcher.USE_TEMPORARY_OUTPUT_FILE])
    self.assertIn(patcher.OUTPUT_FILE_PATH, context)

    mock_build_image_fn.assert_called_once_with(given_image_name)
    self.assertEqual(runner_config.tfx_image, built_image_name)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2372')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/kubeflow_dag_runner_patcher_test.py: 53-69
</a>
<div class="mid" id="frag2372" style="display:none"><pre>
  def testPatcherWithOutputFile(self):
    output_filename = 'foo.tar.gz'
    patcher = kubeflow_dag_runner_patcher.KubeflowDagRunnerPatcher(
        call_real_run=False,
        build_image_fn=None,
        use_temporary_output_file=True)
    runner = kubeflow_dag_runner.KubeflowDagRunner(
        output_filename=output_filename)
    pipeline = tfx_pipeline.Pipeline('dummy', 'dummy_root')
    with patcher.patch() as context:
      runner.run(pipeline)
    self.assertFalse(context[patcher.USE_TEMPORARY_OUTPUT_FILE])
    self.assertEqual(
        os.path.basename(context[patcher.OUTPUT_FILE_PATH]), output_filename)
    self.assertEqual(runner._output_filename, output_filename)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2422')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/kubeflow_v2_dag_runner_patcher_test.py: 32-49
</a>
<div class="mid" id="frag2422" style="display:none"><pre>
  def testPatcherBuildImageFn(self):
    given_image_name = 'foo/bar'
    built_image_name = 'foo/bar@sha256:1234567890'

    mock_build_image_fn = mock.MagicMock(return_value=built_image_name)
    patcher = kubeflow_v2_dag_runner_patcher.KubeflowV2DagRunnerPatcher(
        call_real_run=True, build_image_fn=mock_build_image_fn)
    runner_config = kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(
        default_image=given_image_name)
    runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(config=runner_config)
    pipeline = tfx_pipeline.Pipeline('dummy', 'dummy_root')
    with patcher.patch() as context:
      runner.run(pipeline)
    self.assertIn(patcher.OUTPUT_FILE_PATH, context)

    mock_build_image_fn.assert_called_once_with(given_image_name)
    self.assertEqual(runner_config.default_image, built_image_name)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 104:</b> &nbsp; 3 fragments, nominal size 21 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2373')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/local_handler_test.py: 32-56
</a>
<div class="mid" id="frag2373" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self.chicago_taxi_pipeline_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'testdata')
    self._home = self.tmp_dir
    self.enter_context(test_case_utils.change_working_dir(self.tmp_dir))
    self.enter_context(test_case_utils.override_env_var('HOME', self._home))
    self._local_home = os.path.join(os.environ['HOME'], 'local')
    self.enter_context(
        test_case_utils.override_env_var('LOCAL_HOME', self._local_home))

    # Flags for handler.
    self.engine = 'local'
    self.pipeline_path = os.path.join(self.chicago_taxi_pipeline_dir,
                                      'test_pipeline_local_1.py')
    self.pipeline_name = 'chicago_taxi_local'
    self.pipeline_root = os.path.join(self._home, 'tfx', 'pipelines',
                                      self.pipeline_name)
    self.run_id = 'dummyID'

    self.pipeline_args = {
        labels.PIPELINE_NAME: self.pipeline_name,
        labels.PIPELINE_DSL_PATH: self.pipeline_path,
    }

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2529')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/beam_handler_test.py: 32-56
</a>
<div class="mid" id="frag2529" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self.chicago_taxi_pipeline_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'testdata')
    self._home = self.tmp_dir
    self.enter_context(test_case_utils.change_working_dir(self.tmp_dir))
    self.enter_context(test_case_utils.override_env_var('HOME', self._home))
    self._beam_home = os.path.join(os.environ['HOME'], 'beam')
    self.enter_context(
        test_case_utils.override_env_var('BEAM_HOME', self._beam_home))

    # Flags for handler.
    self.engine = 'beam'
    self.pipeline_path = os.path.join(self.chicago_taxi_pipeline_dir,
                                      'test_pipeline_beam_1.py')
    self.pipeline_name = 'chicago_taxi_beam'
    self.pipeline_root = os.path.join(self._home, 'tfx', 'pipelines',
                                      self.pipeline_name)
    self.run_id = 'dummyID'

    self.pipeline_args = {
        labels.PIPELINE_NAME: self.pipeline_name,
        labels.PIPELINE_DSL_PATH: self.pipeline_path,
    }

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2552')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/vertex_handler_test.py: 36-64
</a>
<div class="mid" id="frag2552" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self.chicago_taxi_pipeline_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'testdata')

    self._home = self.tmp_dir
    self.enter_context(test_case_utils.change_working_dir(self.tmp_dir))
    self.enter_context(test_case_utils.override_env_var('HOME', self._home))
    self._vertex_home = os.path.join(self._home, 'vertex')
    self.enter_context(
        test_case_utils.override_env_var('VERTEX_HOME', self._vertex_home))

    # Flags for handler.
    self.engine = 'vertex'
    self.pipeline_path = os.path.join(self.chicago_taxi_pipeline_dir,
                                      'test_pipeline_kubeflow_v2_1.py')
    self.pipeline_name = _TEST_PIPELINE_NAME
    self.pipeline_root = os.path.join(self._home, 'tfx', 'pipelines',
                                      self.pipeline_name)
    self.run_id = 'dummyID'
    self.project = 'gcp_project_1'
    self.region = 'us-central1'

    self.runtime_parameter = {'a': '1', 'b': '2'}

    # Setting up Mock for API client, so that this Python test is hermetic.
    # subprocess Mock will be setup per-test.
    self.addCleanup(mock.patch.stopall)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 105:</b> &nbsp; 16 fragments, nominal size 11 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2377')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/local_handler_test.py: 90-103
</a>
<div class="mid" id="frag2377" style="display:none"><pre>
  def testCreatePipelineExistentPipeline(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = local_handler.LocalHandler(flags_dict)
    handler.create_pipeline()
    # Run create_pipeline again to test.
    with self.assertRaises(SystemExit) as err:
      handler.create_pipeline()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" already exists.'.format(
            self.pipeline_args[labels.PIPELINE_NAME]))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2525')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/airflow_handler_test.py: 419-432
</a>
<div class="mid" id="frag2525" style="display:none"><pre>
  def testGetRunWrongPipeline(self):
    # Run pipeline without creating one.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.RUN_ID: self.run_id,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.get_run()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2379')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/local_handler_test.py: 127-139
</a>
<div class="mid" id="frag2379" style="display:none"><pre>
  def testUpdatePipelineNoPipeline(self):
    # Update pipeline without creating one.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = local_handler.LocalHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.update_pipeline()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            self.pipeline_args[labels.PIPELINE_NAME]))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2383')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/local_handler_test.py: 178-189
</a>
<div class="mid" id="frag2383" style="display:none"><pre>
  def testDeletePipelineNonExistentPipeline(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = local_handler.LocalHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.delete_pipeline()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2554')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/vertex_handler_test.py: 76-89
</a>
<div class="mid" id="frag2554" style="display:none"><pre>
  def testCreatePipelineExistentPipeline(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = vertex_handler.VertexHandler(flags_dict)
    handler.create_pipeline()
    # Run create_pipeline again to test.
    with self.assertRaises(SystemExit) as err:
      handler.create_pipeline()
    self.assertEqual(
        str(err.exception),
        'Pipeline "{}" already exists.'.format(self.pipeline_name))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2532')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/beam_handler_test.py: 78-91
</a>
<div class="mid" id="frag2532" style="display:none"><pre>
  def testCreatePipelineExistentPipeline(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = beam_handler.BeamHandler(flags_dict)
    handler.create_pipeline()
    # Run create_pipeline again to test.
    with self.assertRaises(SystemExit) as err:
      handler.create_pipeline()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" already exists.'.format(
            self.pipeline_args[labels.PIPELINE_NAME]))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2556')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/vertex_handler_test.py: 114-126
</a>
<div class="mid" id="frag2556" style="display:none"><pre>
  def testUpdatePipelineNoPipeline(self):
    # Update pipeline without creating one.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = vertex_handler.VertexHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.update_pipeline()
    self.assertEqual(
        str(err.exception),
        'Pipeline "{}" does not exist.'.format(self.pipeline_name))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2390')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/local_handler_test.py: 310-322
</a>
<div class="mid" id="frag2390" style="display:none"><pre>
  def testCreateRunNoPipeline(self):
    # Run pipeline without creating one.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = local_handler.LocalHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.create_run()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2508')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/airflow_handler_test.py: 131-142
</a>
<div class="mid" id="frag2508" style="display:none"><pre>
  def testCreatePipelineExistentPipeline(self):
    flags_dict = {labels.ENGINE_FLAG: self.engine,
                  labels.PIPELINE_DSL_PATH: self.pipeline_path}
    handler = airflow_handler.AirflowHandler(flags_dict)
    handler.create_pipeline()
    # Run create_pipeline again to test.
    with self.assertRaises(SystemExit) as err:
      handler.create_pipeline()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" already exists.'.format(
            self.pipeline_args[labels.PIPELINE_NAME]))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2521')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/airflow_handler_test.py: 345-356
</a>
<div class="mid" id="frag2521" style="display:none"><pre>
  def testCreateRunNoPipeline(self):
    # Run pipeline without creating one.
    flags_dict = {labels.ENGINE_FLAG: self.engine,
                  labels.PIPELINE_NAME: self.pipeline_name,
                  labels.RUNTIME_PARAMETER: self.runtime_parameter}
    handler = airflow_handler.AirflowHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.create_run()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2545')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/beam_handler_test.py: 298-310
</a>
<div class="mid" id="frag2545" style="display:none"><pre>
  def testCreateRunNoPipeline(self):
    # Run pipeline without creating one.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = beam_handler.BeamHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.create_run()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2538')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/beam_handler_test.py: 166-177
</a>
<div class="mid" id="frag2538" style="display:none"><pre>
  def testDeletePipelineNonExistentPipeline(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = beam_handler.BeamHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.delete_pipeline()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2534')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/beam_handler_test.py: 115-127
</a>
<div class="mid" id="frag2534" style="display:none"><pre>
  def testUpdatePipelineNoPipeline(self):
    # Update pipeline without creating one.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = beam_handler.BeamHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.update_pipeline()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            self.pipeline_args[labels.PIPELINE_NAME]))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2561')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/vertex_handler_test.py: 183-194
</a>
<div class="mid" id="frag2561" style="display:none"><pre>
  def testDeletePipelineNonExistentPipeline(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = vertex_handler.VertexHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.delete_pipeline()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2523')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/airflow_handler_test.py: 382-394
</a>
<div class="mid" id="frag2523" style="display:none"><pre>
  def testListRunsWrongPipeline(self):
    # Run pipeline without creating one.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: 'chicago_taxi'
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.list_runs()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2588')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/base_handler_test.py: 109-120
</a>
<div class="mid" id="frag2588" style="display:none"><pre>
  def testCheckPipelineExistenceRequired(self):
    flags_dict = {
        labels.ENGINE_FLAG: 'beam',
        labels.PIPELINE_NAME: 'chicago_taxi_beam'
    }
    handler = FakeHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler._check_pipeline_existence(flags_dict[labels.PIPELINE_NAME])
    self.assertTrue(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 106:</b> &nbsp; 3 fragments, nominal size 17 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2378')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/local_handler_test.py: 104-126
</a>
<div class="mid" id="frag2378" style="display:none"><pre>
  def testUpdatePipeline(self):
    # First create pipeline with test_pipeline.py
    pipeline_path_1 = os.path.join(self.chicago_taxi_pipeline_dir,
                                   'test_pipeline_local_1.py')
    flags_dict_1 = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: pipeline_path_1
    }
    handler = local_handler.LocalHandler(flags_dict_1)
    handler.create_pipeline()

    # Update test_pipeline and run update_pipeline
    pipeline_path_2 = os.path.join(self.chicago_taxi_pipeline_dir,
                                   'test_pipeline_local_2.py')
    flags_dict_2 = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: pipeline_path_2
    }
    handler = local_handler.LocalHandler(flags_dict_2)
    handler.update_pipeline()
    self.assertTrue(
        fileio.exists(handler._get_pipeline_args_path(self.pipeline_name)))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2555')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/vertex_handler_test.py: 90-113
</a>
<div class="mid" id="frag2555" style="display:none"><pre>
  def testUpdatePipeline(self):
    # First create pipeline with test_pipeline.py
    pipeline_path_1 = os.path.join(self.chicago_taxi_pipeline_dir,
                                   'test_pipeline_kubeflow_v2_1.py')
    flags_dict_1 = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: pipeline_path_1
    }
    handler = vertex_handler.VertexHandler(flags_dict_1)
    handler.create_pipeline()

    # Update test_pipeline and run update_pipeline
    pipeline_path_2 = os.path.join(self.chicago_taxi_pipeline_dir,
                                   'test_pipeline_kubeflow_v2_2.py')
    flags_dict_2 = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: pipeline_path_2
    }
    handler = vertex_handler.VertexHandler(flags_dict_2)
    handler.update_pipeline()
    self.assertTrue(
        fileio.exists(
            handler._get_pipeline_definition_path(self.pipeline_name)))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2533')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/beam_handler_test.py: 92-114
</a>
<div class="mid" id="frag2533" style="display:none"><pre>
  def testUpdatePipeline(self):
    # First create pipeline with test_pipeline.py
    pipeline_path_1 = os.path.join(self.chicago_taxi_pipeline_dir,
                                   'test_pipeline_beam_1.py')
    flags_dict_1 = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: pipeline_path_1
    }
    handler = beam_handler.BeamHandler(flags_dict_1)
    handler.create_pipeline()

    # Update test_pipeline and run update_pipeline
    pipeline_path_2 = os.path.join(self.chicago_taxi_pipeline_dir,
                                   'test_pipeline_beam_2.py')
    flags_dict_2 = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: pipeline_path_2
    }
    handler = beam_handler.BeamHandler(flags_dict_2)
    handler.update_pipeline()
    self.assertTrue(
        fileio.exists(handler._get_pipeline_args_path(self.pipeline_name)))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 107:</b> &nbsp; 9 fragments, nominal size 15 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2382')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/local_handler_test.py: 159-177
</a>
<div class="mid" id="frag2382" style="display:none"><pre>
  def testDeletePipeline(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = local_handler.LocalHandler(flags_dict)
    handler.create_pipeline()

    # Now delete the pipeline created aand check if pipeline folder is deleted.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = local_handler.LocalHandler(flags_dict)
    handler.delete_pipeline()
    self.assertFalse(
        fileio.exists(handler._get_pipeline_info_path(self.pipeline_name)))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2537')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/beam_handler_test.py: 147-165
</a>
<div class="mid" id="frag2537" style="display:none"><pre>
  def testDeletePipeline(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = beam_handler.BeamHandler(flags_dict)
    handler.create_pipeline()

    # Now delete the pipeline created aand check if pipeline folder is deleted.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = beam_handler.BeamHandler(flags_dict)
    handler.delete_pipeline()
    self.assertFalse(
        fileio.exists(handler._get_pipeline_info_path(self.pipeline_name)))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2517')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/airflow_handler_test.py: 248-267
</a>
<div class="mid" id="frag2517" style="display:none"><pre>
  def testPipelineSchemaNoPipelineRoot(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.get_schema()
    self.assertEqual(
        str(err.exception),
        'Create a run before inferring schema. If pipeline is already running, then wait for it to successfully finish.'
    )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2386')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/local_handler_test.py: 215-234
</a>
<div class="mid" id="frag2386" style="display:none"><pre>
  def testPipelineSchemaNoPipelineRoot(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = local_handler.LocalHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = local_handler.LocalHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.get_schema()
    self.assertEqual(
        str(err.exception),
        'Create a run before inferring schema. If pipeline is already running, then wait for it to successfully finish.'
    )

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2560')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/vertex_handler_test.py: 163-182
</a>
<div class="mid" id="frag2560" style="display:none"><pre>
  def testDeletePipeline(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = vertex_handler.VertexHandler(flags_dict)
    handler.create_pipeline()

    # Now delete the pipeline created aand check if pipeline folder is deleted.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = vertex_handler.VertexHandler(flags_dict)
    handler.delete_pipeline()
    handler_pipeline_path = os.path.join(handler._handler_home_dir,
                                         self.pipeline_name)
    self.assertFalse(fileio.exists(handler_pipeline_path))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2541')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/beam_handler_test.py: 203-222
</a>
<div class="mid" id="frag2541" style="display:none"><pre>
  def testPipelineSchemaNoPipelineRoot(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = beam_handler.BeamHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = beam_handler.BeamHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.get_schema()
    self.assertEqual(
        str(err.exception),
        'Create a run before inferring schema. If pipeline is already running, then wait for it to successfully finish.'
    )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2518')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/airflow_handler_test.py: 268-289
</a>
<div class="mid" id="frag2518" style="display:none"><pre>
  def testPipelineSchemaNoSchemaGenOutput(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    fileio.makedirs(self.pipeline_root)
    with self.assertRaises(SystemExit) as err:
      handler.get_schema()
    self.assertEqual(
        str(err.exception),
        'Either SchemaGen component does not exist or pipeline is still running. If pipeline is running, then wait for it to successfully finish.'
    )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2387')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/local_handler_test.py: 235-256
</a>
<div class="mid" id="frag2387" style="display:none"><pre>
  def testPipelineSchemaNoSchemaGenOutput(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = local_handler.LocalHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = local_handler.LocalHandler(flags_dict)
    fileio.makedirs(self.pipeline_root)
    with self.assertRaises(SystemExit) as err:
      handler.get_schema()
    self.assertEqual(
        str(err.exception),
        'Either SchemaGen component does not exist or pipeline is still running. If pipeline is running, then wait for it to successfully finish.'
    )

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2542')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/beam_handler_test.py: 223-244
</a>
<div class="mid" id="frag2542" style="display:none"><pre>
  def testPipelineSchemaNoSchemaGenOutput(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = beam_handler.BeamHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = beam_handler.BeamHandler(flags_dict)
    fileio.makedirs(self.pipeline_root)
    with self.assertRaises(SystemExit) as err:
      handler.get_schema()
    self.assertEqual(
        str(err.exception),
        'Either SchemaGen component does not exist or pipeline is still running. If pipeline is running, then wait for it to successfully finish.'
    )

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 108:</b> &nbsp; 4 fragments, nominal size 13 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2384')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/local_handler_test.py: 190-207
</a>
<div class="mid" id="frag2384" style="display:none"><pre>
  def testListPipelinesNonEmpty(self):
    # First create two pipelines in the dags folder.
    handler_pipeline_path_1 = os.path.join(os.environ['LOCAL_HOME'],
                                           'pipeline_1')
    handler_pipeline_path_2 = os.path.join(os.environ['LOCAL_HOME'],
                                           'pipeline_2')
    fileio.makedirs(handler_pipeline_path_1)
    fileio.makedirs(handler_pipeline_path_2)

    # Now, list the pipelines
    flags_dict = {labels.ENGINE_FLAG: self.engine}
    handler = local_handler.LocalHandler(flags_dict)

    with self.captureWritesToStream(sys.stdout) as captured:
      handler.list_pipelines()
    self.assertIn('pipeline_1', captured.contents())
    self.assertIn('pipeline_2', captured.contents())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2513')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/airflow_handler_test.py: 202-221
</a>
<div class="mid" id="frag2513" style="display:none"><pre>
  def testListPipelinesNonEmpty(self):
    # First create two pipelines in the dags folder.
    handler_pipeline_path_1 = os.path.join(os.environ['AIRFLOW_HOME'],
                                           'dags',
                                           'pipeline_1')
    handler_pipeline_path_2 = os.path.join(os.environ['AIRFLOW_HOME'],
                                           'dags',
                                           'pipeline_2')
    fileio.makedirs(handler_pipeline_path_1)
    fileio.makedirs(handler_pipeline_path_2)

    # Now, list the pipelines
    flags_dict = {labels.ENGINE_FLAG: self.engine}
    handler = airflow_handler.AirflowHandler(flags_dict)

    with self.captureWritesToStream(sys.stdout) as captured:
      handler.list_pipelines()
    self.assertIn('pipeline_1', captured.contents())
    self.assertIn('pipeline_2', captured.contents())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2539')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/beam_handler_test.py: 178-195
</a>
<div class="mid" id="frag2539" style="display:none"><pre>
  def testListPipelinesNonEmpty(self):
    # First create two pipelines in the dags folder.
    handler_pipeline_path_1 = os.path.join(os.environ['BEAM_HOME'],
                                           'pipeline_1')
    handler_pipeline_path_2 = os.path.join(os.environ['BEAM_HOME'],
                                           'pipeline_2')
    fileio.makedirs(handler_pipeline_path_1)
    fileio.makedirs(handler_pipeline_path_2)

    # Now, list the pipelines
    flags_dict = {labels.ENGINE_FLAG: self.engine}
    handler = beam_handler.BeamHandler(flags_dict)

    with self.captureWritesToStream(sys.stdout) as captured:
      handler.list_pipelines()
    self.assertIn('pipeline_1', captured.contents())
    self.assertIn('pipeline_2', captured.contents())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2558')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/vertex_handler_test.py: 138-155
</a>
<div class="mid" id="frag2558" style="display:none"><pre>
  def testListPipelinesNonEmpty(self):
    # First create two pipelines in the dags folder.
    handler_pipeline_path_1 = os.path.join(os.environ['VERTEX_HOME'],
                                           'pipeline_1')
    handler_pipeline_path_2 = os.path.join(os.environ['VERTEX_HOME'],
                                           'pipeline_2')
    fileio.makedirs(handler_pipeline_path_1)
    fileio.makedirs(handler_pipeline_path_2)

    # Now, list the pipelines
    flags_dict = {labels.ENGINE_FLAG: labels.VERTEX_ENGINE}
    handler = vertex_handler.VertexHandler(flags_dict)

    with self.captureWritesToStream(sys.stdout) as captured:
      handler.list_pipelines()
    self.assertIn('pipeline_1', captured.contents())
    self.assertIn('pipeline_2', captured.contents())

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 109:</b> &nbsp; 3 fragments, nominal size 26 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2388')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/local_handler_test.py: 257-288
</a>
<div class="mid" id="frag2388" style="display:none"><pre>
  def testPipelineSchemaSuccessfulRun(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = local_handler.LocalHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = local_handler.LocalHandler(flags_dict)
    # Create fake schema in pipeline root.
    component_output_dir = os.path.join(self.pipeline_root, 'SchemaGen')
    schema_path = base_driver._generate_output_uri(  # pylint: disable=protected-access
        component_output_dir, 'schema', 3)

    fileio.makedirs(schema_path)
    with open(os.path.join(schema_path, 'schema.pbtxt'), 'w') as f:
      f.write('SCHEMA')
    with self.captureWritesToStream(sys.stdout) as captured:
      handler.get_schema()
      curr_dir_path = os.path.abspath('schema.pbtxt')
      self.assertIn('Path to schema: {}'.format(curr_dir_path),
                    captured.contents())
      self.assertIn(
          '*********SCHEMA FOR {}**********'.format(self.pipeline_name.upper()),
          captured.contents())
      self.assertTrue(fileio.exists(curr_dir_path))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2543')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/beam_handler_test.py: 245-276
</a>
<div class="mid" id="frag2543" style="display:none"><pre>
  def testPipelineSchemaSuccessfulRun(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = beam_handler.BeamHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = beam_handler.BeamHandler(flags_dict)
    # Create fake schema in pipeline root.
    component_output_dir = os.path.join(self.pipeline_root, 'SchemaGen')
    schema_path = base_driver._generate_output_uri(  # pylint: disable=protected-access
        component_output_dir, 'schema', 3)

    fileio.makedirs(schema_path)
    with open(os.path.join(schema_path, 'schema.pbtxt'), 'w') as f:
      f.write('SCHEMA')
    with self.captureWritesToStream(sys.stdout) as captured:
      handler.get_schema()
      curr_dir_path = os.path.abspath('schema.pbtxt')
      self.assertIn('Path to schema: {}'.format(curr_dir_path),
                    captured.contents())
      self.assertIn(
          '*********SCHEMA FOR {}**********'.format(self.pipeline_name.upper()),
          captured.contents())
      self.assertTrue(fileio.exists(curr_dir_path))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2519')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/airflow_handler_test.py: 290-320
</a>
<div class="mid" id="frag2519" style="display:none"><pre>
  def testPipelineSchemaSuccessfulRun(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    # Create fake schema in pipeline root.
    component_output_dir = os.path.join(self.pipeline_root, 'SchemaGen')
    schema_path = base_driver._generate_output_uri(  # pylint: disable=protected-access
        component_output_dir, 'schema', 3)
    fileio.makedirs(schema_path)
    with open(os.path.join(schema_path, 'schema.pbtxt'), 'w') as f:
      f.write('SCHEMA')
    with self.captureWritesToStream(sys.stdout) as captured:
      handler.get_schema()
      curr_dir_path = os.path.abspath('schema.pbtxt')
      self.assertIn('Path to schema: {}'.format(curr_dir_path),
                    captured.contents())
      self.assertIn(
          '*********SCHEMA FOR {}**********'.format(self.pipeline_name.upper()),
          captured.contents())
      self.assertTrue(fileio.exists(curr_dir_path))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 110:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2389')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/local_handler_test.py: 290-309
</a>
<div class="mid" id="frag2389" style="display:none"><pre>
  def testCreateRun(self, mock_call):
    # Create a pipeline in dags folder.
    handler_pipeline_path = os.path.join(
        os.environ['LOCAL_HOME'], self.pipeline_args[labels.PIPELINE_NAME])
    fileio.makedirs(handler_pipeline_path)

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = local_handler.LocalHandler(flags_dict)
    with open(handler._get_pipeline_args_path(self.pipeline_name), 'w') as f:
      json.dump(self.pipeline_args, f)

    # Now run the pipeline
    handler.create_run()

    mock_call.assert_called_once()
    self.assertIn(self.pipeline_path, mock_call.call_args[0][0])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2544')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/beam_handler_test.py: 278-297
</a>
<div class="mid" id="frag2544" style="display:none"><pre>
  def testCreateRun(self, mock_call):
    # Create a pipeline in dags folder.
    handler_pipeline_path = os.path.join(
        os.environ['BEAM_HOME'], self.pipeline_args[labels.PIPELINE_NAME])
    fileio.makedirs(handler_pipeline_path)

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = beam_handler.BeamHandler(flags_dict)
    with open(handler._get_pipeline_args_path(self.pipeline_name), 'w') as f:
      json.dump(self.pipeline_args, f)

    # Now run the pipeline
    handler.create_run()

    mock_call.assert_called_once()
    self.assertIn(self.pipeline_path, mock_call.call_args[0][0])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 111:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2522')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/airflow_handler_test.py: 362-381
</a>
<div class="mid" id="frag2522" style="display:none"><pre>
  def testListRuns(self, mock_check_output):
    # Create a pipeline in dags folder.
    handler_pipeline_path = os.path.join(
        os.environ['AIRFLOW_HOME'], 'dags',
        self.pipeline_args[labels.PIPELINE_NAME])
    fileio.makedirs(handler_pipeline_path)

    # Now run the pipeline
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    with self.captureWritesToStream(sys.stdout) as captured:
      handler.list_runs()
    mock_check_output.assert_called_once()

    # Just check the run_id is succesfully parsed.
    self.assertIn(self.run_id, captured.contents())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2524')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/handler/airflow_handler_test.py: 400-418
</a>
<div class="mid" id="frag2524" style="display:none"><pre>
  def testGetRun(self, mock_check_output):
    # Create a pipeline in dags folder.
    handler_pipeline_path = os.path.join(
        os.environ['AIRFLOW_HOME'], 'dags',
        self.pipeline_args[labels.PIPELINE_NAME])
    fileio.makedirs(handler_pipeline_path)

    # Now run the pipeline
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.RUN_ID: self.run_id,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    with self.captureWritesToStream(sys.stdout) as captured:
      handler.get_run()
    self.assertIn(self.run_id, captured.contents())
    self.assertIn('running', captured.contents())

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 112:</b> &nbsp; 3 fragments, nominal size 31 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2594')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_local_e2e_test.py: 32-72
</a>
<div class="mid" id="frag2594" style="display:none"><pre>
  def setUp(self):
    super().setUp()

    # Change the encoding for Click since Python 3 is configured to use ASCII as
    # encoding for the environment.
    if codecs.lookup(locale.getpreferredencoding()).name == 'ascii':
      os.environ['LANG'] = 'en_US.utf-8'

    # Setup local_home in a temp directory
    self._home = self.tmp_dir
    self._local_home = os.path.join(self._home, 'local')
    self.enter_context(
        test_case_utils.override_env_var('LOCAL_HOME', self._local_home))
    self.enter_context(
        test_case_utils.override_env_var('HOME', self._home))

    # Testdata path.
    self._testdata_dir = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), 'testdata')

    # Copy data.
    chicago_taxi_pipeline_dir = os.path.join(
        os.path.dirname(
            os.path.dirname(
                os.path.dirname(os.path.dirname(os.path.abspath(__file__))))),
        'examples', 'chicago_taxi_pipeline', '')
    data_dir = os.path.join(chicago_taxi_pipeline_dir, 'data', 'simple')
    content = fileio.listdir(data_dir)
    assert content, 'content in {} is empty'.format(data_dir)
    target_data_dir = os.path.join(self._home, 'taxi', 'data', 'simple')
    io_utils.copy_dir(data_dir, target_data_dir)
    assert fileio.isdir(target_data_dir)
    content = fileio.listdir(target_data_dir)
    assert content, 'content in {} is {}'.format(target_data_dir, content)
    io_utils.copy_file(
        os.path.join(chicago_taxi_pipeline_dir, 'taxi_utils.py'),
        os.path.join(self._home, 'taxi', 'taxi_utils.py'))

    # Initialize CLI runner.
    self.runner = click_testing.CliRunner()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2642')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_airflow_e2e_test.py: 39-92
</a>
<div class="mid" id="frag2642" style="display:none"><pre>
  def setUp(self):
    super().setUp()

    # List of packages installed.
    self._pip_list = pip_utils.get_package_names()

    # Check if Apache Airflow is installed before running E2E tests.
    if labels.AIRFLOW_PACKAGE_NAME not in self._pip_list:
      sys.exit('Apache Airflow not installed.')

    # Change the encoding for Click since Python 3 is configured to use ASCII as
    # encoding for the environment.
    if codecs.lookup(locale.getpreferredencoding()).name == 'ascii':
      os.environ['LANG'] = 'en_US.utf-8'

    # Setup airflow_home in a temp directory
    self._airflow_home = os.path.join(self.tmp_dir, 'airflow')
    self.enter_context(
        test_case_utils.override_env_var('AIRFLOW_HOME', self._airflow_home))
    self.enter_context(
        test_case_utils.override_env_var('HOME', self._airflow_home))

    absl.logging.info('Using %s as AIRFLOW_HOME and HOME in this e2e test',
                      self._airflow_home)

    # Testdata path.
    self._testdata_dir = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), 'testdata')

    self._pipeline_name = 'chicago_taxi_simple'
    self._pipeline_path = os.path.join(self._testdata_dir,
                                       'test_pipeline_airflow_1.py')

    # Copy data.
    chicago_taxi_pipeline_dir = os.path.join(
        os.path.dirname(
            os.path.dirname(
                os.path.dirname(os.path.dirname(os.path.abspath(__file__))))),
        'examples', 'chicago_taxi_pipeline')
    data_dir = os.path.join(chicago_taxi_pipeline_dir, 'data', 'simple')
    content = fileio.listdir(data_dir)
    assert content, 'content in {} is empty'.format(data_dir)
    target_data_dir = os.path.join(self._airflow_home, 'taxi', 'data', 'simple')
    io_utils.copy_dir(data_dir, target_data_dir)
    assert fileio.isdir(target_data_dir)
    content = fileio.listdir(target_data_dir)
    assert content, 'content in {} is {}'.format(target_data_dir, content)
    io_utils.copy_file(
        os.path.join(chicago_taxi_pipeline_dir, 'taxi_utils.py'),
        os.path.join(self._airflow_home, 'taxi', 'taxi_utils.py'))

    # Initialize CLI runner.
    self.runner = click_testing.CliRunner()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2608')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_beam_e2e_test.py: 31-71
</a>
<div class="mid" id="frag2608" style="display:none"><pre>
  def setUp(self):
    super().setUp()

    # Change the encoding for Click since Python 3 is configured to use ASCII as
    # encoding for the environment.
    if codecs.lookup(locale.getpreferredencoding()).name == 'ascii':
      os.environ['LANG'] = 'en_US.utf-8'

    # Setup beam_home in a temp directory
    self._home = self.tmp_dir
    self._beam_home = os.path.join(self._home, 'beam')
    self.enter_context(
        test_case_utils.override_env_var('BEAM_HOME', self._beam_home))
    self.enter_context(
        test_case_utils.override_env_var('HOME', self._home))

    # Testdata path.
    self._testdata_dir = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), 'testdata')

    # Copy data.
    chicago_taxi_pipeline_dir = os.path.join(
        os.path.dirname(
            os.path.dirname(
                os.path.dirname(os.path.dirname(os.path.abspath(__file__))))),
        'examples', 'chicago_taxi_pipeline', '')
    data_dir = os.path.join(chicago_taxi_pipeline_dir, 'data', 'simple')
    content = fileio.listdir(data_dir)
    assert content, 'content in {} is empty'.format(data_dir)
    target_data_dir = os.path.join(self._home, 'taxi', 'data', 'simple')
    io_utils.copy_dir(data_dir, target_data_dir)
    assert fileio.isdir(target_data_dir)
    content = fileio.listdir(target_data_dir)
    assert content, 'content in {} is {}'.format(target_data_dir, content)
    io_utils.copy_file(
        os.path.join(chicago_taxi_pipeline_dir, 'taxi_utils.py'),
        os.path.join(self._home, 'taxi', 'taxi_utils.py'))

    # Initialize CLI runner.
    self.runner = click_testing.CliRunner()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 113:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2595')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_local_e2e_test.py: 73-89
</a>
<div class="mid" id="frag2595" style="display:none"><pre>
  def _valid_create_and_check(self, pipeline_path, pipeline_name):
    handler_pipeline_path = os.path.join(self._local_home, pipeline_name)

    # Create a pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'create', '--engine', 'local', '--pipeline_path',
        pipeline_path
    ])
    logging.info('[CLI] %s', result.output)
    self.assertIn('CLI', result.output)
    self.assertIn('Creating pipeline', result.output)
    self.assertTrue(
        fileio.exists(
            os.path.join(handler_pipeline_path, 'pipeline_args.json')))
    self.assertIn('Pipeline "{}" created successfully.'.format(pipeline_name),
                  result.output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2609')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_beam_e2e_test.py: 72-87
</a>
<div class="mid" id="frag2609" style="display:none"><pre>
  def _valid_create_and_check(self, pipeline_path, pipeline_name):
    handler_pipeline_path = os.path.join(self._beam_home, pipeline_name)

    # Create a pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'create', '--engine', 'beam', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Creating pipeline', result.output)
    self.assertTrue(
        fileio.exists(
            os.path.join(handler_pipeline_path, 'pipeline_args.json')))
    self.assertIn('Pipeline "{}" created successfully.'.format(pipeline_name),
                  result.output)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 114:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2596')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_local_e2e_test.py: 90-105
</a>
<div class="mid" id="frag2596" style="display:none"><pre>
  def testPipelineCreate(self):
    # Create a pipeline.
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_local_1.py')
    pipeline_name = 'chicago_taxi_local'
    self._valid_create_and_check(pipeline_path, pipeline_name)

    # Test pipeline create when pipeline already exists.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'create', '--engine', 'local', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Creating pipeline', result.output)
    self.assertTrue('Pipeline "{}" already exists.'.format(pipeline_name),
                    result.output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2610')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_beam_e2e_test.py: 88-103
</a>
<div class="mid" id="frag2610" style="display:none"><pre>
  def testPipelineCreate(self):
    # Create a pipeline.
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_beam_1.py')
    pipeline_name = 'chicago_taxi_beam'
    self._valid_create_and_check(pipeline_path, pipeline_name)

    # Test pipeline create when pipeline already exists.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'create', '--engine', 'beam', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Creating pipeline', result.output)
    self.assertTrue('Pipeline "{}" already exists.'.format(pipeline_name),
                    result.output)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 115:</b> &nbsp; 4 fragments, nominal size 24 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2597')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_local_e2e_test.py: 106-137
</a>
<div class="mid" id="frag2597" style="display:none"><pre>
  def testPipelineUpdate(self):
    pipeline_name = 'chicago_taxi_local'
    handler_pipeline_path = os.path.join(self._local_home, pipeline_name)
    pipeline_path_1 = os.path.join(self._testdata_dir,
                                   'test_pipeline_local_1.py')
    # Try pipeline update when pipeline does not exist.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'update', '--engine', 'local', '--pipeline_path',
        pipeline_path_1
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Updating pipeline', result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(pipeline_name),
                  result.output)
    self.assertFalse(fileio.exists(handler_pipeline_path))

    # Now update an existing pipeline.
    self._valid_create_and_check(pipeline_path_1, pipeline_name)
    pipeline_path_2 = os.path.join(self._testdata_dir,
                                   'test_pipeline_local_2.py')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'update', '--engine', 'local', '--pipeline_path',
        pipeline_path_2
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Updating pipeline', result.output)
    self.assertIn('Pipeline "{}" updated successfully.'.format(pipeline_name),
                  result.output)
    self.assertTrue(
        fileio.exists(
            os.path.join(handler_pipeline_path, 'pipeline_args.json')))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2599')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_local_e2e_test.py: 171-200
</a>
<div class="mid" id="frag2599" style="display:none"><pre>
  def testPipelineDelete(self):
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_local_1.py')
    pipeline_name = 'chicago_taxi_local'
    handler_pipeline_path = os.path.join(self._local_home, pipeline_name)

    # Try deleting a non existent pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'delete', '--engine', 'local', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Deleting pipeline', result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(pipeline_name),
                  result.output)
    self.assertFalse(fileio.exists(handler_pipeline_path))

    # Create a pipeline.
    self._valid_create_and_check(pipeline_path, pipeline_name)

    # Now delete the pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'delete', '--engine', 'local', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Deleting pipeline', result.output)
    self.assertFalse(fileio.exists(handler_pipeline_path))
    self.assertIn('Pipeline "{}" deleted successfully.'.format(pipeline_name),
                  result.output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2611')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_beam_e2e_test.py: 104-135
</a>
<div class="mid" id="frag2611" style="display:none"><pre>
  def testPipelineUpdate(self):
    pipeline_name = 'chicago_taxi_beam'
    handler_pipeline_path = os.path.join(self._beam_home, pipeline_name)
    pipeline_path_1 = os.path.join(self._testdata_dir,
                                   'test_pipeline_beam_1.py')
    # Try pipeline update when pipeline does not exist.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'update', '--engine', 'beam', '--pipeline_path',
        pipeline_path_1
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Updating pipeline', result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(pipeline_name),
                  result.output)
    self.assertFalse(fileio.exists(handler_pipeline_path))

    # Now update an existing pipeline.
    self._valid_create_and_check(pipeline_path_1, pipeline_name)
    pipeline_path_2 = os.path.join(self._testdata_dir,
                                   'test_pipeline_beam_2.py')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'update', '--engine', 'beam', '--pipeline_path',
        pipeline_path_2
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Updating pipeline', result.output)
    self.assertIn('Pipeline "{}" updated successfully.'.format(pipeline_name),
                  result.output)
    self.assertTrue(
        fileio.exists(
            os.path.join(handler_pipeline_path, 'pipeline_args.json')))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2613')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_beam_e2e_test.py: 169-198
</a>
<div class="mid" id="frag2613" style="display:none"><pre>
  def testPipelineDelete(self):
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_beam_1.py')
    pipeline_name = 'chicago_taxi_beam'
    handler_pipeline_path = os.path.join(self._beam_home, pipeline_name)

    # Try deleting a non existent pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'delete', '--engine', 'beam', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Deleting pipeline', result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(pipeline_name),
                  result.output)
    self.assertFalse(fileio.exists(handler_pipeline_path))

    # Create a pipeline.
    self._valid_create_and_check(pipeline_path, pipeline_name)

    # Now delete the pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'delete', '--engine', 'beam', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Deleting pipeline', result.output)
    self.assertFalse(fileio.exists(handler_pipeline_path))
    self.assertIn('Pipeline "{}" deleted successfully.'.format(pipeline_name),
                  result.output)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 116:</b> &nbsp; 3 fragments, nominal size 23 lines, similarity 79%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2598')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_local_e2e_test.py: 138-170
</a>
<div class="mid" id="frag2598" style="display:none"><pre>
  def testPipelineCompile(self):
    # Invalid DSL path
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_flink.py')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'local', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Invalid pipeline path: {}'.format(pipeline_path),
                  result.output)

    # Wrong Runner.
    pipeline_path = os.path.join(self.tmp_dir, 'empty_file.py')
    io_utils.write_string_file(pipeline_path, '')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'local', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Cannot find LocalDagRunner.run()', result.output)

    # Successful compilation.
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_local_2.py')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'local', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Pipeline compiled successfully', result.output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2612')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_beam_e2e_test.py: 136-168
</a>
<div class="mid" id="frag2612" style="display:none"><pre>
  def testPipelineCompile(self):
    # Invalid DSL path
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_flink.py')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'beam', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Invalid pipeline path: {}'.format(pipeline_path),
                  result.output)

    # Wrong Runner.
    pipeline_path = os.path.join(self.tmp_dir, 'empty_file.py')
    io_utils.write_string_file(pipeline_path, '')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'beam', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Cannot find BeamDagRunner.run()', result.output)

    # Successful compilation.
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_beam_2.py')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'beam', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Pipeline compiled successfully', result.output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2652')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_airflow_e2e_test.py: 196-224
</a>
<div class="mid" id="frag2652" style="display:none"><pre>
  def testPipelineCompile(self):
    # Invalid DSL path
    pipeline_path = os.path.join(self._testdata_dir, 'non_existing.py')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'airflow', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Invalid pipeline path: {}'.format(pipeline_path),
                  result.output)

    # Wrong Runner.
    pipeline_path = os.path.join(self.tmp_dir, 'empty_file.py')
    io_utils.write_string_file(pipeline_path, '')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'airflow', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Cannot find AirflowDagRunner.run()', result.output)

    # Successful compilation.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'airflow', '--pipeline_path',
        self._pipeline_path
    ])
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Pipeline compiled successfully', result.output)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 117:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2600')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_local_e2e_test.py: 201-228
</a>
<div class="mid" id="frag2600" style="display:none"><pre>
  def testPipelineList(self):

    # Try listing pipelines when there are none.
    result = self.runner.invoke(cli_group,
                                ['pipeline', 'list', '--engine', 'local'])
    self.assertIn('CLI', result.output)
    self.assertIn('Listing all pipelines', result.output)
    self.assertIn('No pipelines to display.', result.output)

    # Create pipelines.
    pipeline_name_1 = 'chicago_taxi_local'
    pipeline_path_1 = os.path.join(self._testdata_dir,
                                   'test_pipeline_local_1.py')
    self._valid_create_and_check(pipeline_path_1, pipeline_name_1)

    pipeline_name_2 = 'chicago_taxi_local_v2'
    pipeline_path_2 = os.path.join(self._testdata_dir,
                                   'test_pipeline_local_3.py')
    self._valid_create_and_check(pipeline_path_2, pipeline_name_2)

    # List pipelines.
    result = self.runner.invoke(cli_group,
                                ['pipeline', 'list', '--engine', 'local'])
    self.assertIn('CLI', result.output)
    self.assertIn('Listing all pipelines', result.output)
    self.assertIn(pipeline_name_1, result.output)
    self.assertIn(pipeline_name_2, result.output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2614')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_beam_e2e_test.py: 199-226
</a>
<div class="mid" id="frag2614" style="display:none"><pre>
  def testPipelineList(self):

    # Try listing pipelines when there are none.
    result = self.runner.invoke(cli_group,
                                ['pipeline', 'list', '--engine', 'beam'])
    self.assertIn('CLI', result.output)
    self.assertIn('Listing all pipelines', result.output)
    self.assertIn('No pipelines to display.', result.output)

    # Create pipelines.
    pipeline_name_1 = 'chicago_taxi_beam'
    pipeline_path_1 = os.path.join(self._testdata_dir,
                                   'test_pipeline_beam_1.py')
    self._valid_create_and_check(pipeline_path_1, pipeline_name_1)

    pipeline_name_2 = 'chicago_taxi_beam_v2'
    pipeline_path_2 = os.path.join(self._testdata_dir,
                                   'test_pipeline_beam_3.py')
    self._valid_create_and_check(pipeline_path_2, pipeline_name_2)

    # List pipelines.
    result = self.runner.invoke(cli_group,
                                ['pipeline', 'list', '--engine', 'beam'])
    self.assertIn('CLI', result.output)
    self.assertIn('Listing all pipelines', result.output)
    self.assertIn(pipeline_name_1, result.output)
    self.assertIn(pipeline_name_2, result.output)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 118:</b> &nbsp; 2 fragments, nominal size 43 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2601')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_local_e2e_test.py: 229-292
</a>
<div class="mid" id="frag2601" style="display:none"><pre>
  def testPipelineSchema(self):
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_local_2.py')
    pipeline_name = 'chicago_taxi_local'

    # Try getting schema without creating pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'schema', '--engine', 'local', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Getting latest schema.', result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(pipeline_name),
                  result.output)

    # Create a pipeline.
    self._valid_create_and_check(pipeline_path, pipeline_name)

    # Try getting schema without creating a pipeline run.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'schema', '--engine', 'local', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Getting latest schema.', result.output)
    self.assertIn(
        'Create a run before inferring schema. If pipeline is already running, then wait for it to successfully finish.',
        result.output)

    # Run pipeline.
    self._valid_run_and_check(pipeline_name)

    # Try inferring schema without SchemaGen component.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'schema', '--engine', 'local', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Getting latest schema.', result.output)
    self.assertIn(
        'Either SchemaGen component does not exist or pipeline is still running. If pipeline is running, then wait for it to successfully finish.',
        result.output)

    # Create a pipeline.
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_local_3.py')
    pipeline_name = 'chicago_taxi_local_v2'
    self._valid_create_and_check(pipeline_path, pipeline_name)

    # Run pipeline
    self._valid_run_and_check(pipeline_name)

    # Infer Schema when pipeline runs for the first time.
    schema_path = os.path.join(os.getcwd(), 'schema.pbtxt')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'schema', '--engine', 'local', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Getting latest schema.', result.output)
    self.assertTrue(fileio.exists(schema_path))
    self.assertIn('Path to schema: {}'.format(schema_path), result.output)
    self.assertIn(
        '*********SCHEMA FOR {}**********'.format(pipeline_name.upper()),
        result.output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2615')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_beam_e2e_test.py: 227-290
</a>
<div class="mid" id="frag2615" style="display:none"><pre>
  def testPipelineSchema(self):
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_beam_2.py')
    pipeline_name = 'chicago_taxi_beam'

    # Try getting schema without creating pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'schema', '--engine', 'beam', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Getting latest schema.', result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(pipeline_name),
                  result.output)

    # Create a pipeline.
    self._valid_create_and_check(pipeline_path, pipeline_name)

    # Try getting schema without creating a pipeline run.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'schema', '--engine', 'beam', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Getting latest schema.', result.output)
    self.assertIn(
        'Create a run before inferring schema. If pipeline is already running, then wait for it to successfully finish.',
        result.output)

    # Run pipeline.
    self._valid_run_and_check(pipeline_name)

    # Try inferring schema without SchemaGen component.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'schema', '--engine', 'beam', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Getting latest schema.', result.output)
    self.assertIn(
        'Either SchemaGen component does not exist or pipeline is still running. If pipeline is running, then wait for it to successfully finish.',
        result.output)

    # Create a pipeline.
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_beam_3.py')
    pipeline_name = 'chicago_taxi_beam_v2'
    self._valid_create_and_check(pipeline_path, pipeline_name)

    # Run pipeline
    self._valid_run_and_check(pipeline_name)

    # Infer Schema when pipeline runs for the first time.
    schema_path = os.path.join(os.getcwd(), 'schema.pbtxt')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'schema', '--engine', 'beam', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Getting latest schema.', result.output)
    self.assertTrue(fileio.exists(schema_path))
    self.assertIn('Path to schema: {}'.format(schema_path), result.output)
    self.assertIn(
        '*********SCHEMA FOR {}**********'.format(pipeline_name.upper()),
        result.output)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 119:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2603')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_local_e2e_test.py: 303-324
</a>
<div class="mid" id="frag2603" style="display:none"><pre>
  def testRunCreate(self):
    # Create a pipeline first.
    pipeline_name_1 = 'chicago_taxi_local'
    pipeline_path_1 = os.path.join(self._testdata_dir,
                                   'test_pipeline_local_2.py')
    self._valid_create_and_check(pipeline_path_1, pipeline_name_1)

    # Now run a different pipeline
    pipeline_name_2 = 'chicago_taxi_local_v2'
    result = self.runner.invoke(cli_group, [
        'run', 'create', '--engine', 'local', '--pipeline_name', pipeline_name_2
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Creating a run for pipeline: {}'.format(pipeline_name_2),
                  result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(pipeline_name_2),
                  result.output)

    # Now run the pipeline
    self._valid_run_and_check(pipeline_name_1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2617')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_beam_e2e_test.py: 301-322
</a>
<div class="mid" id="frag2617" style="display:none"><pre>
  def testRunCreate(self):
    # Create a pipeline first.
    pipeline_name_1 = 'chicago_taxi_beam'
    pipeline_path_1 = os.path.join(self._testdata_dir,
                                   'test_pipeline_beam_2.py')
    self._valid_create_and_check(pipeline_path_1, pipeline_name_1)

    # Now run a different pipeline
    pipeline_name_2 = 'chicago_taxi_beam_v2'
    result = self.runner.invoke(cli_group, [
        'run', 'create', '--engine', 'beam', '--pipeline_name', pipeline_name_2
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Creating a run for pipeline: {}'.format(pipeline_name_2),
                  result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(pipeline_name_2),
                  result.output)

    # Now run the pipeline
    self._valid_run_and_check(pipeline_name_1)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 120:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2631')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_kubeflow_e2e_test.py: 201-222
</a>
<div class="mid" id="frag2631" style="display:none"><pre>
  def testPipelineUpdate(self):
    # Try pipeline update when pipeline does not exist.
    with self.assertRaises(subprocess.CalledProcessError) as cm:
      test_utils.run_cli([
          'pipeline', 'update', '--engine', 'kubeflow', '--pipeline_path',
          self._pipeline_path, '--endpoint', self._endpoint
      ])
    self.assertIn('Cannot find pipeline "{}".'.format(self._pipeline_name),
                  cm.exception.output)

    # Now update an existing pipeline.
    self._valid_create_and_check(self._pipeline_path, self._pipeline_name)

    result = test_utils.run_cli([
        'pipeline', 'update', '--engine', 'kubeflow', '--pipeline_path',
        self._pipeline_path, '--endpoint', self._endpoint
    ])
    self.assertIn('Updating pipeline', result)
    self.assertIn(
        'Pipeline "{}" updated successfully.'.format(self._pipeline_name),
        result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2637')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_kubeflow_e2e_test.py: 313-337
</a>
<div class="mid" id="frag2637" style="display:none"><pre>
  def testRunCreate(self):
    with self.assertRaises(subprocess.CalledProcessError) as cm:
      test_utils.run_cli([
          'run', 'create', '--engine', 'kubeflow', '--pipeline_name',
          self._pipeline_name, '--endpoint', self._endpoint
      ])
    self.assertIn('Cannot find pipeline "{}".'.format(self._pipeline_name),
                  cm.exception.output)

    # Now create a pipeline.
    self._valid_create_and_check(self._pipeline_path, self._pipeline_name)

    # Run pipeline.
    result = test_utils.run_cli([
        'run', 'create', '--engine', 'kubeflow', '--pipeline_name',
        self._pipeline_name, '--endpoint', self._endpoint
    ])

    self.assertIn('Creating a run for pipeline: {}'.format(self._pipeline_name),
                  result)
    self.assertNotIn(
        'Pipeline "{}" does not exist.'.format(self._pipeline_name), result)
    self.assertIn('Run created for pipeline: {}'.format(self._pipeline_name),
                  result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2633')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_kubeflow_e2e_test.py: 252-273
</a>
<div class="mid" id="frag2633" style="display:none"><pre>
  def testPipelineDelete(self):
    # Try deleting a non existent pipeline.
    with self.assertRaises(subprocess.CalledProcessError) as cm:
      test_utils.run_cli([
          'pipeline', 'delete', '--engine', 'kubeflow', '--pipeline_name',
          self._pipeline_name, '--endpoint', self._endpoint
      ])
    self.assertIn('Cannot find pipeline "{}".'.format(self._pipeline_name),
                  cm.exception.output)

    # Create a pipeline.
    self._valid_create_and_check(self._pipeline_path, self._pipeline_name)

    # Now delete the pipeline.
    result = test_utils.run_cli([
        'pipeline', 'delete', '--engine', 'kubeflow', '--pipeline_name',
        self._pipeline_name, '--endpoint', self._endpoint
    ])
    self.assertIn('Deleting pipeline', result)
    self.assertIn(
        'Pipeline {} deleted successfully.'.format(self._pipeline_name), result)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 121:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2651')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_airflow_e2e_test.py: 172-195
</a>
<div class="mid" id="frag2651" style="display:none"><pre>
  def testPipelineUpdate(self):
    # Try pipeline update when pipeline does not exist.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'update', '--engine', 'airflow', '--pipeline_path',
        self._pipeline_path
    ])
    self.assertIn('Updating pipeline', result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(self._pipeline_name),
                  result.output)
    self.assertFalse(self._does_pipeline_args_file_exist(self._pipeline_name))

    # Now update an existing pipeline.
    self._valid_create_and_check(self._pipeline_path, self._pipeline_name)

    result = self.runner.invoke(cli_group, [
        'pipeline', 'update', '--engine', 'airflow', '--pipeline_path',
        self._pipeline_path
    ])
    self.assertIn('Updating pipeline', result.output)
    self.assertIn(
        'Pipeline "{}" updated successfully.'.format(self._pipeline_name),
        result.output)
    self.assertTrue(self._does_pipeline_args_file_exist(self._pipeline_name))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2653')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/e2e/cli_airflow_e2e_test.py: 225-249
</a>
<div class="mid" id="frag2653" style="display:none"><pre>
  def testPipelineDelete(self):
    # Try deleting a non existent pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'delete', '--engine', 'airflow', '--pipeline_name',
        self._pipeline_name
    ])
    self.assertIn('Deleting pipeline', result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(self._pipeline_name),
                  result.output)
    self.assertFalse(self._does_pipeline_args_file_exist(self._pipeline_name))

    # Create a pipeline.
    self._valid_create_and_check(self._pipeline_path, self._pipeline_name)

    # Now delete the pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'delete', '--engine', 'airflow', '--pipeline_name',
        self._pipeline_name
    ])
    self.assertIn('Deleting pipeline', result.output)
    self.assertFalse(self._does_pipeline_args_file_exist(self._pipeline_name))
    self.assertIn(
        'Pipeline "{}" deleted successfully.'.format(self._pipeline_name),
        result.output)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 122:</b> &nbsp; 9 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2679')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/testdata/test_pipeline_local_2.py: 36-53
</a>
<div class="mid" id="frag2679" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     metadata_path: str) -&gt; pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      additional_pipeline_args={},
  )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2683')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/testdata/test_pipeline_beam_1.py: 37-60
</a>
<div class="mid" id="frag2683" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     metadata_path: str) -&gt; pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  infer_schema = SchemaGen(statistics=statistics_gen.outputs['statistics'])

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen, statistics_gen, infer_schema],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      additional_pipeline_args={},
  )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2685')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/testdata/test_pipeline_kubeflow_v2_2.py: 32-47
</a>
<div class="mid" id="frag2685" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str,
                     data_root: str) -&gt; pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen],
      enable_cache=True,
      additional_pipeline_args={},
  )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2691')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/testdata/test_pipeline_local_1.py: 39-63
</a>
<div class="mid" id="frag2691" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     metadata_path: str) -&gt; pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  infer_schema = SchemaGen(statistics=statistics_gen.outputs['statistics'])

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen, statistics_gen, infer_schema],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      additional_pipeline_args={},
  )

# We need to guard this in this conditional because this file is loaded multiple
# times in a single test run of local_handler_test.py.
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2690')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/testdata/test_pipeline_beam_2.py: 36-53
</a>
<div class="mid" id="frag2690" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     metadata_path: str) -&gt; pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      additional_pipeline_args={},
  )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2688')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/testdata/test_pipeline_kubeflow_v2_1.py: 34-55
</a>
<div class="mid" id="frag2688" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str,
                     data_root: str) -&gt; pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  infer_schema = SchemaGen(statistics=statistics_gen.outputs['statistics'])

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen, statistics_gen, infer_schema],
      enable_cache=True,
      additional_pipeline_args={},
  )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3035')" href="javascript:;">
tfx-1.6.0/tfx/examples/custom_components/hello_world/example/taxi_pipeline_hello.py: 43-65
</a>
<div class="mid" id="frag3035" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     metadata_path: str) -&gt; pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""
  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  hello = component.HelloComponent(
      input_data=example_gen.outputs['examples'], name=u'HelloWorld')

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=hello.outputs['output_data'])

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen, hello, statistics_gen],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path))


# To run this pipeline from the python CLI:
#   $python taxi_pipeline_hello.py
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2684')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/testdata/test_pipeline_local_3.py: 51-79
</a>
<div class="mid" id="frag2684" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     metadata_path: str) -&gt; pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  infer_schema = SchemaGen(statistics=statistics_gen.outputs['statistics'])

  # Performs anomaly detection based on statistics and data schema.
  validate_stats = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=infer_schema.outputs['schema'])

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen, statistics_gen, infer_schema, validate_stats],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      additional_pipeline_args={},
  )


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2687')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/testdata/test_pipeline_beam_3.py: 51-79
</a>
<div class="mid" id="frag2687" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     metadata_path: str) -&gt; pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  infer_schema = SchemaGen(statistics=statistics_gen.outputs['statistics'])

  # Performs anomaly detection based on statistics and data schema.
  validate_stats = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=infer_schema.outputs['schema'])

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen, statistics_gen, infer_schema, validate_stats],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      additional_pipeline_args={},
  )


</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 123:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2704')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/commands/pipeline_test.py: 49-62
</a>
<div class="mid" id="frag2704" style="display:none"><pre>
  def testPipelineUpdate(self):
    result = self.runner.invoke(pipeline_group, [
        'update', '--pipeline_path', 'chicago.py', '--engine', 'kubeflow',
        '--iap_client_id', 'fake_id', '--namespace', 'kubeflow', '--endpoint',
        'endpoint_url'
    ])
    self.assertIn('Updating pipeline', result.output)
    result = self.runner.invoke(pipeline_group, [
        'update', '--pipeline-path', 'chicago.py', '--engine', 'kubeflow',
        '--iap-client-id', 'fake_id', '--namespace', 'kubeflow', '--endpoint',
        'endpoint_url'
    ])
    self.assertIn('Updating pipeline', result.output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2731')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/commands/run_test.py: 155-171
</a>
<div class="mid" id="frag2731" style="display:none"><pre>
  def testRunDelete(self):
    result = self.runner.invoke(run_group, [
        'delete', '--run_id', 'kubeflow_run_id', '--engine', 'kubeflow',
        '--iap_client_id', 'fake_id', '--namespace', 'kubeflow', '--endpoint',
        'endpoint_url'
    ])
    self.assertIn('Deleting run', result.output)
    self.assertSucceeded(result)
    result = self.runner.invoke(run_group, [
        'delete', '--run-id', 'kubeflow_run_id', '--engine', 'kubeflow',
        '--iap-client-id', 'fake_id', '--namespace', 'kubeflow', '--endpoint',
        'endpoint_url'
    ])
    self.assertIn('Deleting run', result.output)
    self.assertSucceeded(result)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 124:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2727')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/commands/run_test.py: 101-112
</a>
<div class="mid" id="frag2727" style="display:none"><pre>
  def testRunList(self):
    result = self.runner.invoke(
        run_group,
        ['list', '--pipeline_name', 'chicago', '--engine', 'airflow'])
    self.assertIn('Listing all runs of pipeline', result.output)
    self.assertSucceeded(result)
    result = self.runner.invoke(
        run_group,
        ['list', '--pipeline-name', 'chicago', '--engine', 'airflow'])
    self.assertIn('Listing all runs of pipeline', result.output)
    self.assertSucceeded(result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2730')" href="javascript:;">
tfx-1.6.0/tfx/tools/cli/commands/run_test.py: 143-154
</a>
<div class="mid" id="frag2730" style="display:none"><pre>
  def testRunTerminate(self):
    result = self.runner.invoke(
        run_group,
        ['terminate', '--run_id', 'airflow_run_id', '--engine', 'airflow'])
    self.assertIn('Terminating run.', result.output)
    self.assertSucceeded(result)
    result = self.runner.invoke(
        run_group,
        ['terminate', '--run-id', 'airflow_run_id', '--engine', 'airflow'])
    self.assertIn('Terminating run.', result.output)
    self.assertSucceeded(result)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 125:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2754')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/taxi/kubeflow_runner.py: 50-97
</a>
<div class="mid" id="frag2754" style="display:none"><pre>
def run():
  """Define a kubeflow pipeline."""

  # Metadata config. The defaults works work with the installation of
  # KF Pipelines using Kubeflow. If installing KF Pipelines using the
  # lightweight deployment option, you may need to override the defaults.
  # If you use Kubeflow, metadata will be written to MySQL database inside
  # Kubeflow cluster.
  metadata_config = tfx.orchestration.experimental.get_default_kubeflow_metadata_config(
  )

  runner_config = tfx.orchestration.experimental.KubeflowDagRunnerConfig(
      kubeflow_metadata_config=metadata_config,
      tfx_image=configs.PIPELINE_IMAGE)
  pod_labels = {
      'add-pod-env': 'true',
      tfx.orchestration.experimental.LABEL_KFP_SDK_ENV: 'tfx-template'
  }
  tfx.orchestration.experimental.KubeflowDagRunner(
      config=runner_config, pod_labels_to_attach=pod_labels
  ).run(
      pipeline.create_pipeline(
          pipeline_name=configs.PIPELINE_NAME,
          pipeline_root=PIPELINE_ROOT,
          data_path=DATA_PATH,
          # TODO(step 7): (Optional) Uncomment below to use BigQueryExampleGen.
          # query=configs.BIG_QUERY_QUERY,
          # TODO(step 5): (Optional) Set the path of the customized schema.
          # schema_path=generated_schema_path,
          preprocessing_fn=configs.PREPROCESSING_FN,
          run_fn=configs.RUN_FN,
          train_args=tfx.proto.TrainArgs(num_steps=configs.TRAIN_NUM_STEPS),
          eval_args=tfx.proto.EvalArgs(num_steps=configs.EVAL_NUM_STEPS),
          eval_accuracy_threshold=configs.EVAL_ACCURACY_THRESHOLD,
          serving_model_dir=SERVING_MODEL_DIR,
          # TODO(step 7): (Optional) Uncomment below to use provide GCP related
          #               config for BigQuery with Beam DirectRunner.
          # beam_pipeline_args=configs
          # .BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS,
          # TODO(step 8): (Optional) Uncomment below to use Dataflow.
          # beam_pipeline_args=configs.DATAFLOW_BEAM_PIPELINE_ARGS,
          # TODO(step 9): (Optional) Uncomment below to use Cloud AI Platform.
          # ai_platform_training_args=configs.GCP_AI_PLATFORM_TRAINING_ARGS,
          # TODO(step 9): (Optional) Uncomment below to use Cloud AI Platform.
          # ai_platform_serving_args=configs.GCP_AI_PLATFORM_SERVING_ARGS,
      ))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2797')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/penguin/kubeflow_runner.py: 50-90
</a>
<div class="mid" id="frag2797" style="display:none"><pre>
def run():
  """Define a kubeflow pipeline."""

  # Metadata config. The defaults works work with the installation of
  # KF Pipelines using Kubeflow. If installing KF Pipelines using the
  # lightweight deployment option, you may need to override the defaults.
  # If you use Kubeflow, metadata will be written to MySQL database inside
  # Kubeflow cluster.
  metadata_config = tfx.orchestration.experimental.get_default_kubeflow_metadata_config(
  )

  runner_config = tfx.orchestration.experimental.KubeflowDagRunnerConfig(
      kubeflow_metadata_config=metadata_config,
      tfx_image=configs.PIPELINE_IMAGE)
  pod_labels = {
      'add-pod-env': 'true',
      tfx.orchestration.experimental.LABEL_KFP_SDK_ENV: 'tfx-template'
  }
  tfx.orchestration.experimental.KubeflowDagRunner(
      config=runner_config, pod_labels_to_attach=pod_labels
  ).run(
      pipeline.create_pipeline(
          pipeline_name=configs.PIPELINE_NAME,
          pipeline_root=PIPELINE_ROOT,
          data_path=DATA_PATH,
          # NOTE: Use `query` instead of `data_path` to use BigQueryExampleGen.
          # query=configs.BIG_QUERY_QUERY,
          # NOTE: Set the path of the customized schema if any.
          # schema_path=generated_schema_path,
          preprocessing_fn=configs.PREPROCESSING_FN,
          run_fn=configs.RUN_FN,
          train_args=tfx.proto.TrainArgs(num_steps=configs.TRAIN_NUM_STEPS),
          eval_args=tfx.proto.EvalArgs(num_steps=configs.EVAL_NUM_STEPS),
          eval_accuracy_threshold=configs.EVAL_ACCURACY_THRESHOLD,
          serving_model_dir=SERVING_MODEL_DIR,
          # NOTE: Provide GCP configs to use BigQuery with Beam DirectRunner.
          # beam_pipeline_args=configs.
          # BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS,
      ))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 126:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2773')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/taxi/local_runner.py: 52-77
</a>
<div class="mid" id="frag2773" style="display:none"><pre>
def run():
  """Define a local pipeline."""

  tfx.orchestration.LocalDagRunner().run(
      pipeline.create_pipeline(
          pipeline_name=configs.PIPELINE_NAME,
          pipeline_root=PIPELINE_ROOT,
          data_path=DATA_PATH,
          # TODO(step 7): (Optional) Uncomment here to use BigQueryExampleGen.
          # query=configs.BIG_QUERY_QUERY,
          # TODO(step 5): (Optional) Set the path of the customized schema.
          # schema_path=generated_schema_path,
          preprocessing_fn=configs.PREPROCESSING_FN,
          run_fn=configs.RUN_FN,
          train_args=tfx.proto.TrainArgs(num_steps=configs.TRAIN_NUM_STEPS),
          eval_args=tfx.proto.EvalArgs(num_steps=configs.EVAL_NUM_STEPS),
          eval_accuracy_threshold=configs.EVAL_ACCURACY_THRESHOLD,
          serving_model_dir=SERVING_MODEL_DIR,
          # TODO(step 7): (Optional) Uncomment here to use provide GCP related
          #               config for BigQuery with Beam DirectRunner.
          # beam_pipeline_args=configs.
          # BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS,
          metadata_connection_config=tfx.orchestration.metadata
          .sqlite_metadata_connection_config(METADATA_PATH)))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2812')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/penguin/local_runner.py: 53-77
</a>
<div class="mid" id="frag2812" style="display:none"><pre>
def run():
  """Define a pipeline."""

  tfx.orchestration.LocalDagRunner().run(
      pipeline.create_pipeline(
          pipeline_name=configs.PIPELINE_NAME,
          pipeline_root=PIPELINE_ROOT,
          data_path=DATA_PATH,
          # NOTE: Use `query` instead of `data_path` to use BigQueryExampleGen.
          # query=configs.BIG_QUERY_QUERY,
          # NOTE: Set the path of the customized schema if any.
          # schema_path=generated_schema_path,
          preprocessing_fn=configs.PREPROCESSING_FN,
          run_fn=configs.RUN_FN,
          train_args=tfx.proto.TrainArgs(num_steps=configs.TRAIN_NUM_STEPS),
          eval_args=tfx.proto.EvalArgs(num_steps=configs.EVAL_NUM_STEPS),
          eval_accuracy_threshold=configs.EVAL_ACCURACY_THRESHOLD,
          serving_model_dir=SERVING_MODEL_DIR,
          # NOTE: Provide GCP configs to use BigQuery with Beam DirectRunner.
          # beam_pipeline_args=configs.
          # BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS,
          metadata_connection_config=tfx.orchestration.metadata
          .sqlite_metadata_connection_config(METADATA_PATH)))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 127:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2793')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/test_utils.py: 152-164
</a>
<div class="mid" id="frag2793" style="display:none"><pre>
  def _create_pipeline(self):
    result = self._runCli([
        'pipeline',
        'create',
        '--engine',
        'local',
        '--pipeline_path',
        'local_runner.py',
    ])
    self.assertIn(
        'Pipeline "{}" created successfully.'.format(self._pipeline_name),
        result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2794')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/test_utils.py: 165-177
</a>
<div class="mid" id="frag2794" style="display:none"><pre>
  def _update_pipeline(self):
    result = self._runCli([
        'pipeline',
        'update',
        '--engine',
        'local',
        '--pipeline_path',
        'local_runner.py',
    ])
    self.assertIn(
        'Pipeline "{}" updated successfully.'.format(self._pipeline_name),
        result)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 128:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2798')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/penguin/models/model.py: 35-83
</a>
<div class="mid" id="frag2798" style="display:none"><pre>
def _get_tf_examples_serving_signature(model, schema, tf_transform_output):
  """Returns a serving signature that accepts `tensorflow.Example`."""

  if tf_transform_output is None:  # Transform component is not used.

    @tf.function(input_signature=[
        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')
    ])
    def serve_tf_examples_fn(serialized_tf_example):
      """Returns the output to be used in the serving signature."""
      raw_feature_spec = schema_utils.schema_as_feature_spec(
          schema).feature_spec
      # Remove label feature since these will not be present at serving time.
      raw_feature_spec.pop(features.LABEL_KEY)
      raw_features = tf.io.parse_example(serialized_tf_example,
                                         raw_feature_spec)
      logging.info('serve_features = %s', raw_features)

      outputs = model(raw_features)
      # TODO(b/154085620): Convert the predicted labels from the model using a
      # reverse-lookup (opposite of transform.py).
      return {'outputs': outputs}

  else:  # Transform component exists.
    # We need to track the layers in the model in order to save it.
    # TODO(b/162357359): Revise once the bug is resolved.
    model.tft_layer_inference = tf_transform_output.transform_features_layer()

    @tf.function(input_signature=[
        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')
    ])
    def serve_tf_examples_fn(serialized_tf_example):
      """Returns the output to be used in the serving signature."""
      raw_feature_spec = tf_transform_output.raw_feature_spec()
      # Remove label feature since these will not be present at serving time.
      raw_feature_spec.pop(features.LABEL_KEY)
      raw_features = tf.io.parse_example(serialized_tf_example,
                                         raw_feature_spec)
      transformed_features = model.tft_layer_inference(raw_features)
      logging.info('serve_transformed_features = %s', transformed_features)

      outputs = model(transformed_features)
      # TODO(b/154085620): Convert the predicted labels from the model using a
      # reverse-lookup (opposite of transform.py).
      return {'outputs': outputs}

  return serve_tf_examples_fn


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2801')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/penguin/models/model.py: 84-118
</a>
<div class="mid" id="frag2801" style="display:none"><pre>
def _get_transform_features_signature(model, schema, tf_transform_output):
  """Returns a serving signature that applies tf.Transform to features."""

  if tf_transform_output is None:  # Transform component is not used.
    @tf.function(input_signature=[
        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')
    ])
    def transform_features_fn(serialized_tf_example):
      """Returns the transformed_features to be fed as input to evaluator."""
      raw_feature_spec = schema_utils.schema_as_feature_spec(
          schema).feature_spec
      raw_features = tf.io.parse_example(serialized_tf_example,
                                         raw_feature_spec)
      logging.info('eval_features = %s', raw_features)
      return raw_features
  else:  # Transform component exists.
    # We need to track the layers in the model in order to save it.
    # TODO(b/162357359): Revise once the bug is resolved.
    model.tft_layer_eval = tf_transform_output.transform_features_layer()

    @tf.function(input_signature=[
        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')
    ])
    def transform_features_fn(serialized_tf_example):
      """Returns the transformed_features to be fed as input to evaluator."""
      raw_feature_spec = tf_transform_output.raw_feature_spec()
      raw_features = tf.io.parse_example(serialized_tf_example,
                                         raw_feature_spec)
      transformed_features = model.tft_layer_eval(raw_features)
      logging.info('eval_transformed_features = %s', transformed_features)
      return transformed_features

  return transform_features_fn


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 129:</b> &nbsp; 6 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2834')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/container_based_test_case.py: 181-195
</a>
<div class="mid" id="frag2834" style="display:none"><pre>
  def _create_pipeline(self):
    self._runCli([
        'pipeline',
        'create',
        '--engine',
        'kubeflow',
        '--pipeline_path',
        'kubeflow_runner.py',
        '--endpoint',
        self._endpoint,
        '--build-image',
        '--build-base-image',
        self._base_container_image,
    ])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2842')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/container_based_test_case.py: 277-289
</a>
<div class="mid" id="frag2842" style="display:none"><pre>
  def _create_pipeline(self):
    self._runCli([
        'pipeline',
        'create',
        '--engine',
        'vertex',
        '--pipeline-path',
        'kubeflow_v2_runner.py',
        '--build-image',
        '--build-base-image',
        self._base_container_image,
    ])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2843')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/container_based_test_case.py: 290-300
</a>
<div class="mid" id="frag2843" style="display:none"><pre>
  def _update_pipeline(self):
    self._runCli([
        'pipeline',
        'update',
        '--engine',
        'vertex',
        '--pipeline_path',
        'kubeflow_v2_runner.py',
        '--build-image',
    ])

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2836')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/container_based_test_case.py: 206-218
</a>
<div class="mid" id="frag2836" style="display:none"><pre>
  def _update_pipeline(self):
    self._runCli([
        'pipeline',
        'update',
        '--engine',
        'kubeflow',
        '--pipeline_path',
        'kubeflow_runner.py',
        '--endpoint',
        self._endpoint,
        '--build-image',
    ])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2844')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/container_based_test_case.py: 301-315
</a>
<div class="mid" id="frag2844" style="display:none"><pre>
  def _run_pipeline(self):
    result = self._runCli([
        'run',
        'create',
        '--engine',
        'vertex',
        '--pipeline_name',
        self._pipeline_name,
        '--project',
        self._GCP_PROJECT_ID,
        '--region',
        self._GCP_REGION,
    ])
    run_id = self._parse_run_id(result)
    self._wait_until_completed(run_id)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2837')" href="javascript:;">
tfx-1.6.0/tfx/experimental/templates/container_based_test_case.py: 219-234
</a>
<div class="mid" id="frag2837" style="display:none"><pre>
  def _run_pipeline(self):
    result = self._runCli([
        'run',
        'create',
        '--engine',
        'kubeflow',
        '--pipeline_name',
        self._pipeline_name,
        '--endpoint',
        self._endpoint,
    ])
    run_id = self._parse_run_id(result)
    self._wait_until_completed(run_id)
    kubeflow_test_utils.print_failure_log_for_run(self._endpoint, run_id,
                                                  self._namespace)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 130:</b> &nbsp; 2 fragments, nominal size 38 lines, similarity 89%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2851')" href="javascript:;">
tfx-1.6.0/tfx/experimental/pipeline_testing/examples/chicago_taxi_pipeline/taxi_pipeline_regression_e2e_test.py: 34-82
</a>
<div class="mid" id="frag2851" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    self._pipeline_name = 'beam_stub_test'
    # This example assumes that the taxi data and taxi utility function are
    # stored in tfx/examples/chicago_taxi_pipeline. Feel free to customize this
    # as needed.
    taxi_root = os.path.dirname(taxi_pipeline_local.__file__)
    self._data_root = os.path.join(taxi_root, 'data', 'simple')
    self._module_file = os.path.join(taxi_root, 'taxi_utils.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    # Metadata path for recording successful pipeline run.
    self._recorded_mlmd_path = os.path.join(self._test_dir, 'tfx', 'record',
                                            'metadata.db')
    # Metadata path for stub pipeline runs.
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')
    self._recorded_output_dir = os.path.join(self._test_dir, 'testdata')

    # Runs the pipeline and record to self._recorded_output_dir
    record_taxi_pipeline = taxi_pipeline_local._create_pipeline(  # pylint:disable=protected-access
        pipeline_name=self._pipeline_name,
        data_root=self._data_root,
        module_file=self._module_file,
        serving_model_dir=self._serving_model_dir,
        pipeline_root=self._pipeline_root,
        metadata_path=self._recorded_mlmd_path,
        beam_pipeline_args=[])

    BeamDagRunner().run(record_taxi_pipeline)

    pipeline_recorder_utils.record_pipeline(
        output_dir=self._recorded_output_dir,
        metadata_db_uri=self._recorded_mlmd_path,
        pipeline_name=self._pipeline_name)

    self.taxi_pipeline = taxi_pipeline_local._create_pipeline(  # pylint:disable=protected-access
        pipeline_name=self._pipeline_name,
        data_root=self._data_root,
        module_file=self._module_file,
        serving_model_dir=self._serving_model_dir,
        pipeline_root=self._pipeline_root,
        metadata_path=self._metadata_path,
        beam_pipeline_args=[])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2861')" href="javascript:;">
tfx-1.6.0/tfx/experimental/pipeline_testing/examples/imdb_pipeline/imdb_stub_pipeline_regression_e2e_test.py: 34-81
</a>
<div class="mid" id="frag2861" style="display:none"><pre>
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    self._pipeline_name = 'imdb_stub_test'
    # This example assumes that the imdb data and imdb utility function are
    # stored in tfx/examples/imdb. Feel free to customize this as needed.
    imdb_root = os.path.dirname(imdb_pipeline_native_keras.__file__)
    self._data_root = os.path.join(imdb_root, 'data')
    self._module_file = os.path.join(imdb_root, 'imdb_utils_native_keras.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'pipelines',
                                       self._pipeline_name)
    # Metadata path for recording successful pipeline run.
    self._recorded_mlmd_path = os.path.join(self._test_dir, 'record',
                                            'metadata.db')
    # Metadata path for stub pipeline
    self._metadata_path = os.path.join(self._test_dir, 'metadata',
                                       self._pipeline_name, 'metadata.db')
    self._recorded_output_dir = os.path.join(self._test_dir, 'testdata')

    record_imdb_pipeline = imdb_pipeline_native_keras._create_pipeline(  # pylint:disable=protected-access
        pipeline_name=self._pipeline_name,
        data_root=self._data_root,
        module_file=self._module_file,
        serving_model_dir=self._serving_model_dir,
        pipeline_root=self._pipeline_root,
        metadata_path=self._recorded_mlmd_path,
        beam_pipeline_args=[])

    BeamDagRunner().run(record_imdb_pipeline)

    pipeline_recorder_utils.record_pipeline(
        output_dir=self._recorded_output_dir,
        metadata_db_uri=self._recorded_mlmd_path,
        pipeline_name=self._pipeline_name)

    # Run pipeline with stub executors.
    self.imdb_pipeline = imdb_pipeline_native_keras._create_pipeline(  # pylint:disable=protected-access
        pipeline_name=self._pipeline_name,
        data_root=self._data_root,
        module_file=self._module_file,
        serving_model_dir=self._serving_model_dir,
        pipeline_root=self._pipeline_root,
        metadata_path=self._metadata_path,
        beam_pipeline_args=[])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 131:</b> &nbsp; 2 fragments, nominal size 57 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2860')" href="javascript:;">
tfx-1.6.0/tfx/experimental/pipeline_testing/examples/chicago_taxi_pipeline/taxi_pipeline_regression_e2e_test.py: 117-200
</a>
<div class="mid" id="frag2860" style="display:none"><pre>
  def testStubbedTaxiPipelineBeam(self):
    pipeline_ir = compiler.Compiler().compile(self.taxi_pipeline)

    logging.info('Replacing with test_data_dir:%s', self._recorded_output_dir)
    pipeline_mock.replace_executor_with_stub(pipeline_ir,
                                             self._recorded_output_dir, [])

    BeamDagRunner().run_with_ir(pipeline_ir)

    self.assertTrue(fileio.exists(self._metadata_path))

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)

    # Verify that recorded files are successfully copied to the output uris.
    with metadata.Metadata(metadata_config) as m:
      artifacts = m.store.get_artifacts()
      artifact_count = len(artifacts)
      executions = m.store.get_executions()
      execution_count = len(executions)
      # Artifact count is greater by 7 due to extra artifacts produced by
      # Evaluator(blessing and evaluation), Trainer(model and model_run) and
      # Transform(example, graph, cache, pre_transform_statistics,
      # pre_transform_schema, post_transform_statistics, post_transform_schema,
      # post_transform_anomalies) minus Resolver which doesn't generate
      # new artifact.
      self.assertEqual(artifact_count, execution_count + 7)
      self.assertLen(self.taxi_pipeline.components, execution_count)

      for execution in executions:
        component_id = pipeline_recorder_utils.get_component_id_from_execution(
            m, execution)
        if component_id.startswith('Resolver'):
          continue
        eid = [execution.id]
        events = m.store.get_events_by_execution_ids(eid)
        output_events = [
            x for x in events if x.type == metadata_store_pb2.Event.OUTPUT
        ]
        for event in output_events:
          steps = event.path.steps
          self.assertTrue(steps[0].HasField('key'))
          name = steps[0].key
          artifacts = m.store.get_artifacts_by_id([event.artifact_id])
          for idx, artifact in enumerate(artifacts):
            self.assertDirectoryEqual(
                artifact.uri,
                os.path.join(self._recorded_output_dir, component_id, name,
                             str(idx)))

    # Calls verifier for pipeline output artifacts, excluding the resolver node.
    BeamDagRunner().run(self.taxi_pipeline)
    pipeline_outputs = executor_verifier_utils.get_pipeline_outputs(
        self.taxi_pipeline.metadata_connection_config, self._pipeline_name)

    verifier_map = {
        'model': self._verify_model,
        'model_run': self._verify_model,
        'examples': self._verify_examples,
        'schema': self._verify_schema,
        'anomalies': self._verify_anomalies,
        'evaluation': self._verify_evaluation,
        # A subdirectory of updated_analyzer_cache has changing name.
        'updated_analyzer_cache': self._veryify_root_dir,
    }

    # List of components to verify. Resolver is ignored because it
    # doesn't have an executor.
    verify_component_ids = [
        component.id
        for component in self.taxi_pipeline.components
        if not component.id.startswith('Resolver')
    ]

    for component_id in verify_component_ids:
      logging.info('Verifying %s', component_id)
      for key, artifact_dict in pipeline_outputs[component_id].items():
        for idx, artifact in artifact_dict.items():
          recorded_uri = os.path.join(self._recorded_output_dir, component_id,
                                      key, str(idx))
          verifier_map.get(key, self._verify_file_path)(artifact.uri,
                                                        recorded_uri)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2870')" href="javascript:;">
tfx-1.6.0/tfx/experimental/pipeline_testing/examples/imdb_pipeline/imdb_stub_pipeline_regression_e2e_test.py: 116-186
</a>
<div class="mid" id="frag2870" style="display:none"><pre>
  def testStubbedImdbPipelineBeam(self):
    pipeline_ir = compiler.Compiler().compile(self.imdb_pipeline)

    pipeline_mock.replace_executor_with_stub(pipeline_ir,
                                             self._recorded_output_dir, [])

    BeamDagRunner().run_with_ir(pipeline_ir)

    self.assertTrue(fileio.exists(self._metadata_path))

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)

    # Verify that recorded files are successfully copied to the output uris.
    with metadata.Metadata(metadata_config) as m:
      for execution in m.store.get_executions():
        component_id = pipeline_recorder_utils.get_component_id_from_execution(
            m, execution)
        if component_id.startswith('Resolver'):
          continue
        eid = [execution.id]
        events = m.store.get_events_by_execution_ids(eid)
        output_events = [
            x for x in events if x.type == metadata_store_pb2.Event.OUTPUT
        ]
        for event in output_events:
          steps = event.path.steps
          assert steps[0].HasField('key')
          name = steps[0].key
          artifacts = m.store.get_artifacts_by_id([event.artifact_id])
          for idx, artifact in enumerate(artifacts):
            self.assertDirectoryEqual(
                artifact.uri,
                os.path.join(self._recorded_output_dir, component_id, name,
                             str(idx)))

    # Calls verifier for pipeline output artifacts, excluding the resolver node.
    BeamDagRunner().run(self.imdb_pipeline)
    pipeline_outputs = executor_verifier_utils.get_pipeline_outputs(
        self.imdb_pipeline.metadata_connection_config,
        self._pipeline_name)

    verifier_map = {
        'model': self._verify_model,
        'model_run': self._verify_model,
        'examples': self._verify_examples,
        'schema': self._verify_schema,
        'anomalies': self._verify_anomalies,
        'evaluation': self._verify_evaluation,
        # A subdirectory of updated_analyzer_cache has changing name.
        'updated_analyzer_cache': self._veryify_root_dir,
    }

    # List of components to verify. Resolver is ignored because it
    # doesn't have an executor.
    verify_component_ids = [
        component.id
        for component in self.imdb_pipeline.components
        if not component.id.startswith('Resolver')
    ]

    for component_id in verify_component_ids:
      for key, artifact_dict in pipeline_outputs[component_id].items():
        for idx, artifact in artifact_dict.items():
          logging.info('Verifying %s', component_id)
          recorded_uri = os.path.join(self._recorded_output_dir, component_id,
                                      key, str(idx))
          verifier_map.get(key, self._verify_file_path)(artifact.uri,
                                                        recorded_uri)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 132:</b> &nbsp; 6 fragments, nominal size 21 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2885')" href="javascript:;">
tfx-1.6.0/tfx/examples/bert/cola/bert_cola_pipeline_e2e_test.py: 66-91
</a>
<div class="mid" id="frag2885" style="display:none"><pre>
  def testColaPipelineNativeKeras(self):
    pipeline = bert_cola_pipeline._create_pipeline(
        pipeline_name=self._pipeline_name,
        data_root=self._data_root,
        module_file=self._module_file,
        serving_model_dir=self._serving_model_dir,
        pipeline_root=self._pipeline_root,
        metadata_path=self._metadata_path,
        beam_pipeline_args=[])

    LocalDagRunner().run(pipeline)

    self.assertTrue(fileio.exists(self._serving_model_dir))
    self.assertTrue(fileio.exists(self._metadata_path))
    expected_execution_count = 9  # 8 components + 1 resolver
    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(expected_execution_count, execution_count)

    self.assertPipelineExecution()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3017')" href="javascript:;">
tfx-1.6.0/tfx/examples/tfjs_next_page_prediction/tfjs_next_page_prediction_e2e_test.py: 73-100
</a>
<div class="mid" id="frag3017" style="display:none"><pre>
  def testTFJSPagePredictionPipeline(self):
    if not tf.executing_eagerly():
      self.skipTest('The test requires TF2.')
    pipeline = tfjs_next_page_prediction_pipeline._create_pipeline(
        pipeline_name=self._pipeline_name,
        data_root=self._data_root,
        module_file=self._module_file,
        serving_model_dir=self._serving_model_dir,
        pipeline_root=self._pipeline_root,
        metadata_path=self._metadata_path,
        beam_pipeline_args=[])

    LocalDagRunner().run(pipeline)

    self.assertTrue(fileio.exists(self._serving_model_dir))
    self.assertTrue(fileio.exists(self._metadata_path))
    expected_execution_count = 9  # 8 components + 1 resolver
    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(expected_execution_count, execution_count)

    self.assertPipelineExecution()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2925')" href="javascript:;">
tfx-1.6.0/tfx/examples/cifar10/cifar10_pipeline_native_keras_e2e_test.py: 75-101
</a>
<div class="mid" id="frag2925" style="display:none"><pre>
  def testCIFAR10PipelineNativeKeras(self):
    pipeline = cifar10_pipeline_native_keras._create_pipeline(
        pipeline_name=self._pipeline_name,
        data_root=self._data_root,
        module_file=self._module_file,
        serving_model_dir_lite=self._serving_model_dir_lite,
        pipeline_root=self._pipeline_root,
        metadata_path=self._metadata_path,
        labels_path=self._labels_path,
        beam_pipeline_args=[])

    LocalDagRunner().run(pipeline)

    self.assertTrue(fileio.exists(self._serving_model_dir_lite))
    self.assertTrue(fileio.exists(self._metadata_path))
    expected_execution_count = 9  # 8 components + 1 resolver
    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(expected_execution_count, execution_count)

    self.assertPipelineExecution()


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2905')" href="javascript:;">
tfx-1.6.0/tfx/examples/bert/mrpc/bert_mrpc_pipeline_e2e_test.py: 66-91
</a>
<div class="mid" id="frag2905" style="display:none"><pre>
  def testMrpcPipelineNativeKeras(self):
    pipeline = bert_mrpc_pipeline._create_pipeline(
        pipeline_name=self._pipeline_name,
        data_root=self._data_root,
        module_file=self._module_file,
        serving_model_dir=self._serving_model_dir,
        pipeline_root=self._pipeline_root,
        metadata_path=self._metadata_path,
        beam_pipeline_args=[])

    LocalDagRunner().run(pipeline)

    self.assertTrue(fileio.exists(self._serving_model_dir))
    self.assertTrue(fileio.exists(self._metadata_path))
    expected_execution_count = 9  # 8 components + 1 resolver
    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(expected_execution_count, execution_count)

    self.assertPipelineExecution()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3088')" href="javascript:;">
tfx-1.6.0/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_local_e2e_test.py: 73-96
</a>
<div class="mid" id="frag3088" style="display:none"><pre>
  def testTaxiPipelineBeam(self):
    LocalDagRunner().run(
        taxi_pipeline_local._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            serving_model_dir=self._serving_model_dir,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            beam_pipeline_args=[]))

    self.assertTrue(fileio.exists(self._serving_model_dir))
    self.assertTrue(fileio.exists(self._metadata_path))
    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(10, execution_count)

    self.assertPipelineExecution()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2910')" href="javascript:;">
tfx-1.6.0/tfx/examples/ranking/ranking_pipeline_e2e_test.py: 51-72
</a>
<div class="mid" id="frag2910" style="display:none"><pre>
  def testPipeline(self):
    BeamDagRunner().run(
        ranking_pipeline._create_pipeline(
            pipeline_name=self._pipeline_name,
            pipeline_root=self._tfx_root,
            data_root=self._data_root,
            module_file=self._module_file,
            serving_model_dir=self._serving_model_dir,
            metadata_path=self._metadata_path,
            beam_pipeline_args=['--direct_num_workers=1']))
    self.assertTrue(tf.io.gfile.exists(self._serving_model_dir))
    self.assertTrue(tf.io.gfile.exists(self._metadata_path))

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(9, execution_count)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 133:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2889')" href="javascript:;">
tfx-1.6.0/tfx/examples/bert/cola/bert_cola_utils.py: 78-157
</a>
<div class="mid" id="frag2889" style="display:none"><pre>
def stats_options_updater_fn(
    stats_type: stats_options_util.StatsType,
    stats_options: tfdv.StatsOptions) -&gt; tfdv.StatsOptions:
  """Update transform stats.

  This function is called by the Transform component before it computes
  pre-transform or post-transform statistics. It takes as input a stats_type,
  which indicates whether this call is intended for pre-transform or
  post-transform statistics. It also takes as argument the StatsOptions that
  are to be (optionally) modified before being passed onto TDFV.

  Args:
    stats_type: The type of statistics that are to be computed (pre-transform or
      post-transform).
    stats_options: The configuration to pass to TFDV for computing the desired
      statistics.

  Returns:
    An updated StatsOptions object.
  """
  if stats_type == stats_options_util.StatsType.POST_TRANSFORM:
    for f in stats_options.schema.feature:
      if f.name == _INPUT_WORD_IDS:
        # Here we extend the schema for the input_word_ids feature to enable
        # NLP statistics to be computed. We pass the vocabulary (_BERT_VOCAB)
        # that was used in tokenizing this feature, key tokens of interest
        # (e.g. "[CLS]",  "[PAD]", "[SEP]", "[UNK]") and key thresholds to
        # validate. For more information on the field descriptions, see here:
        # https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/schema.proto
        text_format.Parse(
            """
            vocabulary: "{vocab}"
            coverage: {{
              min_coverage: 1.0
              min_avg_token_length: 3.0
              excluded_string_tokens: ["[CLS]", "[PAD]", "[SEP]"]
              oov_string_tokens: ["[UNK]"]
             }}
             token_constraints {{
               string_value: "[CLS]"
               min_per_sequence: 1
               max_per_sequence: 1
               min_fraction_of_sequences: 1
               max_fraction_of_sequences: 1
             }}
             token_constraints {{
               string_value: "[PAD]"
               min_per_sequence: 0
               max_per_sequence: {max_pad_per_seq}
               min_fraction_of_sequences: 0
               max_fraction_of_sequences: 1
             }}
             token_constraints {{
               string_value: "[SEP]"
               min_per_sequence: 1
               max_per_sequence: 1
               min_fraction_of_sequences: 1
               max_fraction_of_sequences: 1
             }}
             token_constraints {{
               string_value: "[UNK]"
               min_per_sequence: 0
               max_per_sequence: {max_unk_per_seq}
               min_fraction_of_sequences: 0
               max_fraction_of_sequences: 1
             }}
             sequence_length_constraints {{
               excluded_string_value: ["[PAD]"]
               min_sequence_length: 3
               max_sequence_length: {max_seq_len}
             }}
            """.format(
                vocab=_BERT_VOCAB,
                max_pad_per_seq=_MAX_LEN - 3,  # [CLS], [SEP], Token
                max_unk_per_seq=_MAX_LEN - 2,  # [CLS], [SEP]
                max_seq_len=_MAX_LEN),
            f.natural_language_domain)
  return stats_options


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2897')" href="javascript:;">
tfx-1.6.0/tfx/examples/bert/mrpc/bert_mrpc_utils.py: 81-156
</a>
<div class="mid" id="frag2897" style="display:none"><pre>
def stats_options_updater_fn(
    stats_type: stats_options_util.StatsType,
    stats_options: tfdv.StatsOptions) -&gt; tfdv.StatsOptions:
  """Update transform stats.

  This function is called by the Transform component before it computes
  pre-transform or post-transform statistics. It takes as input a stats_type,
  which indicates whether this call is intended for pre-transform or
  post-transform statistics. It also takes as argument the StatsOptions that
  are to be (optionally) modified before being passed onto TDFV.

  Args:
    stats_type: The type of statistics that are to be computed (pre-transform or
      post-transform).
    stats_options: The configuration to pass to TFDV for computing the desired
      statistics.

  Returns:
    An updated StatsOptions object.
  """
  if stats_type == stats_options_util.StatsType.POST_TRANSFORM:
    for f in stats_options.schema.feature:
      if f.name == _INPUT_WORD_IDS:
        # Here we extend the schema for the input_word_ids feature to enable
        # NLP statistics to be computed. We pass the vocabulary (_BERT_VOCAB)
        # that was used in tokenizing this feature, key tokens of interest
        # (e.g. "[CLS]",  "[PAD]", "[SEP]", "[UNK]") and key thresholds to
        # validate. For more information on the field descriptions, see here:
        # https://github.com/tensorflow/metadata/blob/master/
        # tensorflow_metadata/proto/v0/schema.proto
        text_format.Parse(
            """
            vocabulary: "{vocab}"
            coverage: {{
              min_coverage: 1.0
              min_avg_token_length: 3.0
              excluded_string_tokens: ["[CLS]", "[PAD]", "[SEP]"]
              oov_string_tokens: ["[UNK]"]
             }}
             token_constraints {{
               string_value: "[CLS]"
               min_per_sequence: 1
               max_per_sequence: 1
               min_fraction_of_sequences: 1
               max_fraction_of_sequences: 1
             }}
             token_constraints {{
               string_value: "[PAD]"
               min_per_sequence: 0
               max_per_sequence: {max_pad_per_seq}
               min_fraction_of_sequences: 0
               max_fraction_of_sequences: 1
             }}
             token_constraints {{
               string_value: "[SEP]"
               min_per_sequence: 2
               max_per_sequence: 2
               min_fraction_of_sequences: 1
               max_fraction_of_sequences: 1
             }}
             token_constraints {{
               string_value: "[UNK]"
               min_per_sequence: 0
               max_per_sequence: {max_unk_per_seq}
               min_fraction_of_sequences: 0
               max_fraction_of_sequences: 1
             }}
            """.format(
                vocab=_BERT_VOCAB,
                max_pad_per_seq=_MAX_LEN - 3,  # [CLS], 2x[SEP], Token
                max_unk_per_seq=_MAX_LEN - 4  # [CLS], 2x[SEP]
            ),
            f.natural_language_domain)
  return stats_options


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 134:</b> &nbsp; 4 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2890')" href="javascript:;">
tfx-1.6.0/tfx/examples/bert/cola/bert_cola_utils.py: 158-184
</a>
<div class="mid" id="frag2890" style="display:none"><pre>
def _input_fn(file_pattern: List[str],
              data_accessor: tfx.components.DataAccessor,
              tf_transform_output: tft.TFTransformOutput,
              batch_size: int = 200) -&gt; tf.data.Dataset:
  """Generates features and label for tuning/training.

  Args:
    file_pattern: List of paths or patterns of materialized transformed input
      tfrecord files.
    data_accessor: DataAccessor for converting input to RecordBatch.
    tf_transform_output: A TFTransformOutput.
    batch_size: representing the number of consecutive elements of returned
      dataset to combine in a single batch

  Returns:
    A dataset that contains (features, indices) tuple where features is a
      dictionary of Tensors, and indices is a single Tensor of label indices.
  """
  dataset = data_accessor.tf_dataset_factory(
      file_pattern,
      tfxio.TensorFlowDatasetOptions(
          batch_size=batch_size, label_key=_LABEL_KEY),
      tf_transform_output.transformed_metadata.schema)
  dataset = dataset.repeat()
  return dataset.prefetch(tf.data.AUTOTUNE)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3009')" href="javascript:;">
tfx-1.6.0/tfx/examples/tfjs_next_page_prediction/tfjs_next_page_prediction_util.py: 73-98
</a>
<div class="mid" id="frag3009" style="display:none"><pre>
def _input_fn(file_pattern: List[str],
              data_accessor: tfx.components.DataAccessor,
              tf_transform_output: tft.TFTransformOutput,
              batch_size: int = 200) -&gt; tf.data.Dataset:
  """Generates features and label for tuning/training.

  Args:
    file_pattern: List of paths or patterns of input tfrecord files.
    data_accessor: DataAccessor for converting input to RecordBatch.
    tf_transform_output: A TFTransformOutput.
    batch_size: representing the number of consecutive elements of returned
      dataset to combine in a single batch.

  Returns:
    A dataset that contains (features, indices) tuple where features is a
      dictionary of Tensors, and indices is a single Tensor of label indices.
  """
  dataset = data_accessor.tf_dataset_factory(
      file_pattern,
      tfxio.TensorFlowDatasetOptions(
          batch_size=batch_size, label_key=_LABEL_KEY),
      tf_transform_output.transformed_metadata.schema)

  return dataset.repeat()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2898')" href="javascript:;">
tfx-1.6.0/tfx/examples/bert/mrpc/bert_mrpc_utils.py: 157-183
</a>
<div class="mid" id="frag2898" style="display:none"><pre>
def _input_fn(file_pattern: List[str],
              data_accessor: tfx.components.DataAccessor,
              tf_transform_output: tft.TFTransformOutput,
              batch_size: int = 200) -&gt; tf.data.Dataset:
  """Generates features and label for tuning/training.

  Args:
    file_pattern: List of paths or patterns of input tfrecord files.
    data_accessor: DataAccessor for converting input to RecordBatch.
    tf_transform_output: A TFTransformOutput.
    batch_size: representing the number of consecutive elements of returned
      dataset to combine in a single batch

  Returns:
    A dataset that contains (features, indices) tuple where features is a
      dictionary of Tensors, and indices is a single Tensor of label indices.
  """
  dataset = data_accessor.tf_dataset_factory(
      file_pattern,
      tfxio.TensorFlowDatasetOptions(
          batch_size=batch_size, label_key=_LABEL_KEY),
      tf_transform_output.transformed_metadata.schema)
  dataset = dataset.repeat()

  return dataset.prefetch(tf.data.AUTOTUNE)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3002')" href="javascript:;">
tfx-1.6.0/tfx/examples/imdb/imdb_utils_native_keras.py: 99-124
</a>
<div class="mid" id="frag3002" style="display:none"><pre>
def _input_fn(file_pattern: List[str],
              data_accessor: DataAccessor,
              tf_transform_output: tft.TFTransformOutput,
              batch_size: int = 200) -&gt; tf.data.Dataset:
  """Generates features and label for tuning/training.

  Args:
    file_pattern: List of paths or patterns of input tfrecord files.
    data_accessor: DataAccessor for converting input to RecordBatch.
    tf_transform_output: A TFTransformOutput.
    batch_size: representing the number of consecutive elements of returned
      dataset to combine in a single batch.

  Returns:
    A dataset that contains (features, indices) tuple where features is a
      dictionary of Tensors, and indices is a single Tensor of label indices.
  """
  dataset = data_accessor.tf_dataset_factory(
      file_pattern,
      dataset_options.TensorFlowDatasetOptions(
          batch_size=batch_size, label_key=_transformed_name(_LABEL_KEY)),
      tf_transform_output.transformed_metadata.schema)

  return dataset.repeat()


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 135:</b> &nbsp; 4 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2891')" href="javascript:;">
tfx-1.6.0/tfx/examples/bert/cola/bert_cola_utils.py: 185-204
</a>
<div class="mid" id="frag2891" style="display:none"><pre>
def _get_serve_tf_examples_fn(model, tf_transform_output):
  """Returns a function that parses a serialized tf.Example."""

  model.tft_layer = tf_transform_output.transform_features_layer()

  @tf.function
  def serve_tf_examples_fn(serialized_tf_examples):
    """Returns the output to be used in the serving signature."""
    feature_spec = tf_transform_output.raw_feature_spec()
    feature_spec.pop(_LABEL_KEY)
    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)

    transformed_features = model.tft_layer(parsed_features)

    return model(transformed_features)

  return serve_tf_examples_fn


# TFX Trainer will call this function.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2933')" href="javascript:;">
tfx-1.6.0/tfx/examples/mnist/mnist_utils_native_keras.py: 27-44
</a>
<div class="mid" id="frag2933" style="display:none"><pre>
def _get_serve_tf_examples_fn(model, tf_transform_output):
  """Returns a function that parses a serialized tf.Example."""

  model.tft_layer = tf_transform_output.transform_features_layer()

  @tf.function
  def serve_tf_examples_fn(serialized_tf_examples):
    """Returns the output to be used in the serving signature."""
    feature_spec = tf_transform_output.raw_feature_spec()
    feature_spec.pop(base.LABEL_KEY)
    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
    transformed_features = model.tft_layer(parsed_features)
    return model(transformed_features)

  return serve_tf_examples_fn


# TFX Transform will call this function.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3004')" href="javascript:;">
tfx-1.6.0/tfx/examples/imdb/imdb_utils_native_keras.py: 155-171
</a>
<div class="mid" id="frag3004" style="display:none"><pre>
def _get_serve_tf_examples_fn(model, tf_transform_output):
  """Returns a function that parses a serialized tf.Example."""
  model.tft_layer = tf_transform_output.transform_features_layer()

  @tf.function
  def serve_tf_examples_fn(serialized_tf_examples):
    """Returns the output to be used in the serving signature."""
    feature_spec = tf_transform_output.raw_feature_spec()
    feature_spec.pop(_LABEL_KEY)
    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
    transformed_features = model.tft_layer(parsed_features)
    return model(transformed_features)

  return serve_tf_examples_fn


# TFX Trainer will call this function.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2899')" href="javascript:;">
tfx-1.6.0/tfx/examples/bert/mrpc/bert_mrpc_utils.py: 184-205
</a>
<div class="mid" id="frag2899" style="display:none"><pre>
def _get_serve_tf_examples_fn(model, tf_transform_output):
  """Returns a function that parses a serialized tf.Example."""

  model.tft_layer = tf_transform_output.transform_features_layer()

  @tf.function
  def serve_tf_examples_fn(serialized_tf_examples):
    """Returns the output to be used in the serving signature."""
    feature_spec = tf_transform_output.raw_feature_spec()
    feature_spec.pop(_LABEL_KEY)
    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)

    transformed_features = model.tft_layer(parsed_features)

    return model(transformed_features)

  return serve_tf_examples_fn


# TFX Trainer will call this function.


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 136:</b> &nbsp; 3 fragments, nominal size 31 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2893')" href="javascript:;">
tfx-1.6.0/tfx/examples/bert/cola/bert_cola_utils.py: 205-246
</a>
<div class="mid" id="frag2893" style="display:none"><pre>
def run_fn(fn_args: tfx.components.FnArgs):
  """Train the model based on given args.

  Args:
    fn_args: Holds args used to train the model as name/value pairs.
  """
  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

  train_dataset = _input_fn(
      fn_args.train_files,
      fn_args.data_accessor,
      tf_transform_output,
      batch_size=_TRAIN_BATCH_SIZE)

  eval_dataset = _input_fn(
      fn_args.eval_files,
      fn_args.data_accessor,
      tf_transform_output,
      batch_size=_EVAL_BATCH_SIZE)

  mirrored_strategy = tf.distribute.MirroredStrategy()
  with mirrored_strategy.scope():
    bert_layer = hub.KerasLayer(_BERT_LINK, trainable=True)
    model = build_and_compile_bert_classifier(bert_layer, _MAX_LEN, 2)

  model.fit(
      train_dataset,
      epochs=_EPOCHS,
      steps_per_epoch=fn_args.train_steps,
      validation_data=eval_dataset,
      validation_steps=fn_args.eval_steps)

  signatures = {
      'serving_default':
          _get_serve_tf_examples_fn(model,
                                    tf_transform_output).get_concrete_function(
                                        tf.TensorSpec(
                                            shape=[None],
                                            dtype=tf.string,
                                            name='examples')),
  }
  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3006')" href="javascript:;">
tfx-1.6.0/tfx/examples/imdb/imdb_utils_native_keras.py: 172-217
</a>
<div class="mid" id="frag3006" style="display:none"><pre>
def run_fn(fn_args: FnArgs):
  """Train the model based on given args.

  Args:
    fn_args: Holds args used to train the model as name/value pairs.
  """
  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

  train_dataset = _input_fn(
      fn_args.train_files,
      fn_args.data_accessor,
      tf_transform_output,
      batch_size=_TRAIN_BATCH_SIZE)

  eval_dataset = _input_fn(
      fn_args.eval_files,
      fn_args.data_accessor,
      tf_transform_output,
      batch_size=_EVAL_BATCH_SIZE)

  mirrored_strategy = tf.distribute.MirroredStrategy()
  with mirrored_strategy.scope():
    model = _build_keras_model()

  # In distributed training, it is common to use num_steps instead of num_epochs
  # to control training.
  # Reference: https://stackoverflow.com/questions/45989971/
  # /distributed-training-with-tf-estimator-resulting-in-more-training-steps

  model.fit(
      train_dataset,
      steps_per_epoch=fn_args.train_steps,
      validation_data=eval_dataset,
      validation_steps=fn_args.eval_steps)

  signatures = {
      'serving_default':
          _get_serve_tf_examples_fn(model,
                                    tf_transform_output).get_concrete_function(
                                        tf.TensorSpec(
                                            shape=[None],
                                            dtype=tf.string,
                                            name='examples')),
  }

  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2901')" href="javascript:;">
tfx-1.6.0/tfx/examples/bert/mrpc/bert_mrpc_utils.py: 206-247
</a>
<div class="mid" id="frag2901" style="display:none"><pre>
def run_fn(fn_args: tfx.components.FnArgs):
  """Train the model based on given args.

  Args:
    fn_args: Holds args used to train the model as name/value pairs.
  """
  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

  train_dataset = _input_fn(
      fn_args.train_files,
      fn_args.data_accessor,
      tf_transform_output,
      batch_size=_TRAIN_BATCH_SIZE)

  eval_dataset = _input_fn(
      fn_args.eval_files,
      fn_args.data_accessor,
      tf_transform_output,
      batch_size=_EVAL_BATCH_SIZE)

  mirrored_strategy = tf.distribute.MirroredStrategy()
  with mirrored_strategy.scope():
    bert_layer = hub.KerasLayer(_BERT_LINK, trainable=True)
    model = build_and_compile_bert_classifier(bert_layer, _MAX_LEN, 2, 2e-5)

  model.fit(
      train_dataset,
      epochs=_EPOCHS,
      steps_per_epoch=fn_args.train_steps,
      validation_data=eval_dataset,
      validation_steps=fn_args.eval_steps)

  signatures = {
      'serving_default':
          _get_serve_tf_examples_fn(model,
                                    tf_transform_output).get_concrete_function(
                                        tf.TensorSpec(
                                            shape=[None],
                                            dtype=tf.string,
                                            name='examples')),
  }
  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 137:</b> &nbsp; 5 fragments, nominal size 15 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2923')" href="javascript:;">
tfx-1.6.0/tfx/examples/cifar10/cifar10_pipeline_native_keras_e2e_test.py: 46-64
</a>
<div class="mid" id="frag2923" style="display:none"><pre>
  def assertExecutedOnce(self, component: str) -&gt; None:
    """Check the component is executed exactly once."""
    component_path = os.path.join(self._pipeline_root, component)
    self.assertTrue(fileio.exists(component_path))
    outputs = fileio.listdir(component_path)

    self.assertIn('.system', outputs)
    outputs.remove('.system')
    system_paths = [
        os.path.join('.system', path)
        for path in fileio.listdir(os.path.join(component_path, '.system'))
    ]
    self.assertNotEmpty(system_paths)
    self.assertIn('.system/executor_execution', system_paths)
    outputs.extend(system_paths)
    for output in outputs:
      execution = fileio.listdir(os.path.join(component_path, output))
      self.assertLen(execution, 1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3015')" href="javascript:;">
tfx-1.6.0/tfx/examples/tfjs_next_page_prediction/tfjs_next_page_prediction_e2e_test.py: 44-62
</a>
<div class="mid" id="frag3015" style="display:none"><pre>
  def assertExecutedOnce(self, component: str) -&gt; None:
    """Check the component is executed exactly once."""
    component_path = os.path.join(self._pipeline_root, component)
    self.assertTrue(fileio.exists(component_path))
    outputs = fileio.listdir(component_path)

    self.assertIn('.system', outputs)
    outputs.remove('.system')
    system_paths = [
        os.path.join('.system', path)
        for path in fileio.listdir(os.path.join(component_path, '.system'))
    ]
    self.assertNotEmpty(system_paths)
    self.assertIn('.system/executor_execution', system_paths)
    outputs.extend(system_paths)
    for output in outputs:
      execution = fileio.listdir(os.path.join(component_path, output))
      self.assertLen(execution, 1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3086')" href="javascript:;">
tfx-1.6.0/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_local_e2e_test.py: 43-62
</a>
<div class="mid" id="frag3086" style="display:none"><pre>
  def assertExecutedOnce(self, component: str) -&gt; None:
    """Check the component is executed exactly once."""
    component_path = os.path.join(self._pipeline_root, component)
    self.assertTrue(fileio.exists(component_path))
    outputs = fileio.listdir(component_path)

    self.assertIn('.system', outputs)
    outputs.remove('.system')
    system_paths = [
        os.path.join('.system', path)
        for path in fileio.listdir(os.path.join(component_path, '.system'))
    ]
    self.assertNotEmpty(system_paths)
    self.assertIn('.system/executor_execution', system_paths)
    outputs.extend(system_paths)
    self.assertNotEmpty(outputs)
    for output in outputs:
      execution = fileio.listdir(os.path.join(component_path, output))
      self.assertLen(execution, 1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3070')" href="javascript:;">
tfx-1.6.0/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_native_keras_e2e_test.py: 49-67
</a>
<div class="mid" id="frag3070" style="display:none"><pre>
  def assertExecutedOnce(self, component: str) -&gt; None:
    """Check the component is executed exactly once."""
    component_path = os.path.join(self._pipeline_root, component)
    self.assertTrue(fileio.exists(component_path))
    outputs = fileio.listdir(component_path)
    self.assertIn('.system', outputs)
    outputs.remove('.system')
    system_paths = [
        os.path.join('.system', path)
        for path in fileio.listdir(os.path.join(component_path, '.system'))
    ]
    self.assertNotEmpty(system_paths)
    self.assertIn('.system/executor_execution', system_paths)
    outputs.extend(system_paths)
    self.assertNotEmpty(outputs)
    for output in outputs:
      execution = fileio.listdir(os.path.join(component_path, output))
      self.assertLen(execution, 1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2938')" href="javascript:;">
tfx-1.6.0/tfx/examples/mnist/mnist_pipeline_native_keras_e2e_test.py: 51-69
</a>
<div class="mid" id="frag2938" style="display:none"><pre>
  def assertExecutedOnce(self, component: str) -&gt; None:
    """Check the component is executed exactly once."""
    component_path = os.path.join(self._pipeline_root, component)
    self.assertTrue(fileio.exists(component_path))
    outputs = fileio.listdir(component_path)
    self.assertIn('.system', outputs)
    outputs.remove('.system')
    system_paths = [
        os.path.join('.system', path)
        for path in fileio.listdir(os.path.join(component_path, '.system'))
    ]
    self.assertNotEmpty(system_paths)
    self.assertIn('.system/executor_execution', system_paths)
    outputs.extend(system_paths)
    self.assertNotEmpty(outputs)
    for output in outputs:
      execution = fileio.listdir(os.path.join(component_path, output))
      self.assertLen(execution, 1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 138:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2939')" href="javascript:;">
tfx-1.6.0/tfx/examples/mnist/mnist_pipeline_native_keras_e2e_test.py: 70-82
</a>
<div class="mid" id="frag2939" style="display:none"><pre>
  def assertPipelineExecution(self) -&gt; None:
    self.assertExecutedOnce('ImportExampleGen')
    self.assertExecutedOnce('Evaluator.mnist')
    self.assertExecutedOnce('Evaluator.mnist_lite')
    self.assertExecutedOnce('ExampleValidator')
    self.assertExecutedOnce('Pusher.mnist')
    self.assertExecutedOnce('Pusher.mnist_lite')
    self.assertExecutedOnce('SchemaGen')
    self.assertExecutedOnce('StatisticsGen')
    self.assertExecutedOnce('Trainer.mnist')
    self.assertExecutedOnce('Trainer.mnist_lite')
    self.assertExecutedOnce('Transform')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3124')" href="javascript:;">
tfx-1.6.0/tfx/examples/penguin/penguin_pipeline_local_infraval_e2e_test.py: 94-104
</a>
<div class="mid" id="frag3124" style="display:none"><pre>
  def _assertPipelineExecution(self):
    self._assertExecutedOnce('CsvExampleGen')
    self._assertExecutedOnce('Evaluator')
    self._assertExecutedOnce('ExampleValidator')
    self._assertExecutedOnce('ImportSchemaGen')
    self._assertExecutedOnce('InfraValidator')
    self._assertExecutedOnce('Pusher')
    self._assertExecutedOnce('StatisticsGen')
    self._assertExecutedOnce('Trainer')
    self._assertExecutedOnce('Transform')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 139:</b> &nbsp; 3 fragments, nominal size 45 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2940')" href="javascript:;">
tfx-1.6.0/tfx/examples/mnist/mnist_pipeline_native_keras_e2e_test.py: 83-131
</a>
<div class="mid" id="frag2940" style="display:none"><pre>
  def testMNISTPipelineNativeKeras(self):
    if not tf.executing_eagerly():
      self.skipTest('The test requires TF2.')
    BeamDagRunner().run(
        mnist_pipeline_native_keras._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            module_file_lite=self._module_file_lite,
            serving_model_dir=self._serving_model_dir,
            serving_model_dir_lite=self._serving_model_dir_lite,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            beam_pipeline_args=[]))

    self.assertTrue(fileio.exists(self._serving_model_dir))
    self.assertTrue(fileio.exists(self._serving_model_dir_lite))
    self.assertTrue(fileio.exists(self._metadata_path))
    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    expected_execution_count = 11
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(execution_count, expected_execution_count)

    self.assertPipelineExecution()

    # Runs pipeline the second time.
    BeamDagRunner().run(
        mnist_pipeline_native_keras._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            module_file_lite=self._module_file_lite,
            serving_model_dir=self._serving_model_dir,
            serving_model_dir_lite=self._serving_model_dir_lite,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            beam_pipeline_args=[]))

    # Asserts cache execution.
    with metadata.Metadata(metadata_config) as m:
      # Artifact count is unchanged.
      self.assertLen(m.store.get_artifacts(), artifact_count)
      self.assertLen(m.store.get_executions(), expected_execution_count * 2)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3072')" href="javascript:;">
tfx-1.6.0/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_native_keras_e2e_test.py: 78-138
</a>
<div class="mid" id="frag3072" style="display:none"><pre>
  def testTaxiPipelineNativeKeras(self):
    BeamDagRunner().run(
        taxi_pipeline_native_keras._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            serving_model_dir=self._serving_model_dir,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            beam_pipeline_args=[]))

    self.assertTrue(fileio.exists(self._serving_model_dir))
    self.assertTrue(fileio.exists(self._metadata_path))
    expected_execution_count = 9  # 8 components + 1 resolver
    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(expected_execution_count, execution_count)

    self.assertPipelineExecution()

    # Runs pipeline the second time.
    BeamDagRunner().run(
        taxi_pipeline_native_keras._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            serving_model_dir=self._serving_model_dir,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            beam_pipeline_args=[]))

    # All executions but Evaluator and Pusher are cached.
    # Note that Resolver will always execute.
    with metadata.Metadata(metadata_config) as m:
      # Artifact count is increased by 3 caused by Evaluator and Pusher.
      self.assertLen(m.store.get_artifacts(), artifact_count + 3)
      artifact_count = len(m.store.get_artifacts())
      self.assertLen(m.store.get_executions(), expected_execution_count * 2)

    # Runs pipeline the third time.
    BeamDagRunner().run(
        taxi_pipeline_native_keras._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            serving_model_dir=self._serving_model_dir,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            beam_pipeline_args=[]))

    # Asserts cache execution.
    with metadata.Metadata(metadata_config) as m:
      # Artifact count is unchanged.
      self.assertLen(m.store.get_artifacts(), artifact_count)
      self.assertLen(m.store.get_executions(), expected_execution_count * 3)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3125')" href="javascript:;">
tfx-1.6.0/tfx/examples/penguin/penguin_pipeline_local_infraval_e2e_test.py: 108-177
</a>
<div class="mid" id="frag3125" style="display:none"><pre>
  def testPenguinPipelineLocal(self, make_warmup):
    LocalDagRunner().run(
        penguin_pipeline_local_infraval._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            accuracy_threshold=0.1,
            serving_model_dir=self._serving_model_dir,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            user_provided_schema_path=self._schema_path,
            beam_pipeline_args=[],
            make_warmup=make_warmup))

    self.assertTrue(fileio.exists(self._serving_model_dir))
    self.assertTrue(fileio.exists(self._metadata_path))
    expected_execution_count = 10  # 9 components + 1 resolver
    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(expected_execution_count, execution_count)

    self._assertPipelineExecution()
    self._assertInfraValidatorPassed()

    # Runs pipeline the second time.
    LocalDagRunner().run(
        penguin_pipeline_local_infraval._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            accuracy_threshold=0.1,
            serving_model_dir=self._serving_model_dir,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            user_provided_schema_path=self._schema_path,
            beam_pipeline_args=[],
            make_warmup=make_warmup))

    # All executions but Evaluator and Pusher are cached.
    with metadata.Metadata(metadata_config) as m:
      # Artifact count is increased by 3 caused by Evaluator and Pusher.
      self.assertLen(m.store.get_artifacts(), artifact_count + 3)
      artifact_count = len(m.store.get_artifacts())
      self.assertLen(m.store.get_executions(), expected_execution_count * 2)

    # Runs pipeline the third time.
    LocalDagRunner().run(
        penguin_pipeline_local_infraval._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            accuracy_threshold=0.1,
            serving_model_dir=self._serving_model_dir,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            user_provided_schema_path=self._schema_path,
            beam_pipeline_args=[],
            make_warmup=make_warmup))

    # Asserts cache execution.
    with metadata.Metadata(metadata_config) as m:
      # Artifact count is unchanged.
      self.assertLen(m.store.get_artifacts(), artifact_count)
      self.assertLen(m.store.get_executions(), expected_execution_count * 3)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 140:</b> &nbsp; 4 fragments, nominal size 46 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2945')" href="javascript:;">
tfx-1.6.0/tfx/examples/bigquery_ml/taxi_pipeline_kubeflow_gcp_bqml.py: 155-227
</a>
<div class="mid" id="frag2945" style="display:none"><pre>
def _create_pipeline(
    pipeline_name: str, pipeline_root: str, query: str, module_file: str,
    beam_pipeline_args: List[str], ai_platform_training_args: Dict[str, str],
    bigquery_serving_args: Dict[str, str]) -&gt; pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX and Kubeflow Pipelines."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = big_query_example_gen_component.BigQueryExampleGen(query=query)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that implements a model.
  # to train a model on Google Cloud AI Platform.
  trainer = Trainer(
      custom_executor_spec=executor_spec.ExecutorClassSpec(
          ai_platform_trainer_executor.Executor),
      module_file=module_file,
      transformed_examples=transform.outputs['transformed_examples'],
      schema=schema_gen.outputs['schema'],
      transform_graph=transform.outputs['transform_graph'],
      train_args=trainer_pb2.TrainArgs(num_steps=10000),
      eval_args=trainer_pb2.EvalArgs(num_steps=5000),
      custom_config={'ai_platform_training_args': ai_platform_training_args})

  # Uses TFMA to compute a evaluation statistics over features of a model.
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[
          evaluator_pb2.SingleSlicingSpec(
              column_for_slicing=['trip_start_hour'])
      ]))

  # Performs quality validation of a candidate model (compared to a baseline).
  model_validator = ModelValidator(
      examples=example_gen.outputs['examples'], model=trainer.outputs['model'])

  # Checks whether the model passed the validation steps and pushes the model
  # to  Google Cloud BigQuery ML if check passed.
  pusher = Pusher(
      custom_executor_spec=executor_spec.ExecutorClassSpec(
          bigquery_pusher_executor.Executor),
      model=trainer.outputs['model'],
      model_blessing=model_validator.outputs['blessing'],
      custom_config={'bigquery_serving_args': bigquery_serving_args})

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen, statistics_gen, schema_gen, example_validator, transform,
          trainer, evaluator, model_validator, pusher
      ],
      beam_pipeline_args=beam_pipeline_args,
  )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3034')" href="javascript:;">
tfx-1.6.0/tfx/examples/custom_components/presto_example_gen/example/taxi_pipeline_presto.py: 64-134
</a>
<div class="mid" id="frag3034" style="display:none"><pre>
def _create_pipeline(pipeline_name: str, pipeline_root: str, module_file: str,
                     presto_config: presto_config_pb2.PrestoConnConfig,
                     query: str, serving_model_dir: str,
                     metadata_path: str) -&gt; pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""
  # Brings data into the pipeline or otherwise joins/converts training data
  example_gen = PrestoExampleGen(presto_config, query=query)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that implements a model.
  trainer = Trainer(
      module_file=module_file,
      transformed_examples=transform.outputs['transformed_examples'],
      schema=schema_gen.outputs['schema'],
      transform_graph=transform.outputs['transform_graph'],
      train_args=trainer_pb2.TrainArgs(num_steps=10000),
      eval_args=trainer_pb2.EvalArgs(num_steps=5000))

  # Uses TFMA to compute a evaluation statistics over features of a model.
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[
          evaluator_pb2.SingleSlicingSpec(
              column_for_slicing=['trip_start_hour'])
      ]))

  # Performs quality validation of a candidate model (compared to a baseline).
  model_validator = ModelValidator(
      examples=example_gen.outputs['examples'], model=trainer.outputs['model'])

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=model_validator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen, statistics_gen, schema_gen, example_validator, transform,
          trainer, evaluator, model_validator, pusher
      ],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
  )


# To run this pipeline from the python CLI:
#   $python taxi_pipeline_presto.py
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3058')" href="javascript:;">
tfx-1.6.0/tfx/examples/custom_components/slack/example/taxi_pipeline_slack_kubeflow.py: 81-161
</a>
<div class="mid" id="frag3058" style="display:none"><pre>
def _create_pipeline():
  """Implements the chicago taxi pipeline with TFX."""
  examples = csv_input(_data_root)

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input=examples)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      preprocessing_fn=_taxi_transformer_func)

  # Uses user-provided Python function that implements a model.
  trainer = Trainer(
      trainer_fn=_taxi_trainer_func,
      examples=transform.outputs['transformed_examples'],
      schema=schema_gen.outputs['schema'],
      transform_graph=transform.outputs['transform_graph'],
      train_args=trainer_pb2.TrainArgs(num_steps=10000),
      eval_args=trainer_pb2.EvalArgs(num_steps=5000))

  # Uses TFMA to compute a evaluation statistics over features of a model.
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[
          evaluator_pb2.SingleSlicingSpec(
              column_for_slicing=['trip_start_hour'])
      ]))

  # Performs quality validation of a candidate model (compared to a baseline).
  model_validator = ModelValidator(
      examples=example_gen.outputs['examples'], model=trainer.outputs['model'])

  # This custom component serves as a bridge between pipeline and human model
  # reviewers to enable review-and-push workflow in model development cycle. It
  # utilizes Slack API to send message to user-defined Slack channel with model
  # URI info and wait for go / no-go decision from the same Slack channel:
  #   * To approve the model, users need to reply the thread sent out by the bot
  #     started by SlackComponent with 'lgtm' or 'approve'.
  #   * To reject the model, users need to reply the thread sent out by the bot
  #     started by SlackComponent with 'decline' or 'reject'.
  slack_validator = SlackComponent(
      model=trainer.outputs['model'],
      model_blessing=model_validator.outputs['blessing'],
      slack_token=_slack_token,
      slack_channel_id=_slack_channel_id,
      timeout_sec=3600,
  )
  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=slack_validator.outputs['slack_blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=_serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=_pipeline_name,
      pipeline_root=_pipeline_root,
      components=[
          example_gen, statistics_gen, schema_gen, example_validator, transform,
          trainer, evaluator, model_validator, slack_validator, pusher
      ],
      enable_cache=True,
  )


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3047')" href="javascript:;">
tfx-1.6.0/tfx/examples/custom_components/slack/example/taxi_pipeline_slack.py: 74-155
</a>
<div class="mid" id="frag3047" style="display:none"><pre>
def _create_pipeline():
  """Implements the chicago taxi pipeline with TFX."""
  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=_data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=_taxi_module_file)

  # Uses user-provided Python function that implements a model.
  trainer = Trainer(
      module_file=_taxi_module_file,
      examples=transform.outputs['transformed_examples'],
      schema=schema_gen.outputs['schema'],
      transform_graph=transform.outputs['transform_graph'],
      train_args=trainer_pb2.TrainArgs(num_steps=10000),
      eval_args=trainer_pb2.EvalArgs(num_steps=5000))

  # Uses TFMA to compute a evaluation statistics over features of a model.
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[
          evaluator_pb2.SingleSlicingSpec(
              column_for_slicing=['trip_start_hour'])
      ]))

  # Performs quality validation of a candidate model (compared to a baseline).
  model_validator = ModelValidator(
      examples=example_gen.outputs['examples'], model=trainer.outputs['model'])

  # This custom component serves as a bridge between pipeline and human model
  # reviewers to enable review-and-push workflow in model development cycle. It
  # utilizes Slack API to send message to user-defined Slack channel with model
  # URI info and wait for go / no-go decision from the same Slack channel:
  #   * To approve the model, users need to reply the thread sent out by the bot
  #     started by SlackComponent with 'lgtm' or 'approve'.
  #   * To reject the model, users need to reply the thread sent out by the bot
  #     started by SlackComponent with 'decline' or 'reject'.
  slack_validator = SlackComponent(
      model=trainer.outputs['model'],
      model_blessing=model_validator.outputs['blessing'],
      slack_token=_slack_token,
      slack_channel_id=_slack_channel_id,
      timeout_sec=3600,
  )

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=slack_validator.outputs['slack_blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=_serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=_pipeline_name,
      pipeline_root=_pipeline_root,
      components=[
          example_gen, statistics_gen, schema_gen, example_validator, transform,
          trainer, evaluator, model_validator, slack_validator, pusher
      ],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          _metadata_db_root),
  )


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 141:</b> &nbsp; 2 fragments, nominal size 45 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2958')" href="javascript:;">
tfx-1.6.0/tfx/examples/bigquery_ml/taxi_utils_bqml_test.py: 50-111
</a>
<div class="mid" id="frag2958" style="display:none"><pre>
  def testPreprocessingFn(self):
    schema_file = os.path.join(self._testdata_path, 'schema_gen/schema.pbtxt')
    schema = io_utils.parse_pbtxt_file(schema_file, schema_pb2.Schema())
    feature_spec = taxi_utils_bqml._get_raw_feature_spec(schema)
    working_dir = self.get_temp_dir()
    transform_output_path = os.path.join(working_dir, 'transform_output')
    transformed_examples_path = os.path.join(
        working_dir, 'transformed_examples')

    # Run very simplified version of executor logic.
    # TODO(kestert): Replace with tft_unit.assertAnalyzeAndTransformResults.
    # Generate legacy `DatasetMetadata` object.  Future version of Transform
    # will accept the `Schema` proto directly.
    legacy_metadata = dataset_metadata.DatasetMetadata(
        schema_utils.schema_from_feature_spec(feature_spec))
    tfxio = tf_example_record.TFExampleRecord(
        file_pattern=os.path.join(self._testdata_path,
                                  'csv_example_gen/train/*'),
        telemetry_descriptors=['Tests'],
        schema=legacy_metadata.schema)
    with beam.Pipeline() as p:
      with tft_beam.Context(temp_dir=os.path.join(working_dir, 'tmp')):
        examples = p | 'ReadTrainData' &gt;&gt; tfxio.BeamSource()
        (transformed_examples, transformed_metadata), transform_fn = (
            (examples, tfxio.TensorAdapterConfig())
            | 'AnalyzeAndTransform' &gt;&gt; tft_beam.AnalyzeAndTransformDataset(
                taxi_utils_bqml.preprocessing_fn))

        # WriteTransformFn writes transform_fn and metadata to subdirectories
        # tensorflow_transform.SAVED_MODEL_DIR and
        # tensorflow_transform.TRANSFORMED_METADATA_DIR respectively.
        # pylint: disable=expression-not-assigned
        (transform_fn
         | 'WriteTransformFn' &gt;&gt; tft_beam.WriteTransformFn(
             transform_output_path))

        encoder = tft.coders.ExampleProtoCoder(transformed_metadata.schema)
        (transformed_examples
         | 'EncodeTrainData' &gt;&gt; beam.Map(encoder.encode)
         | 'WriteTrainData' &gt;&gt; beam.io.WriteToTFRecord(
             os.path.join(transformed_examples_path,
                          'train/transformed_examples.gz'),
             coder=beam.coders.BytesCoder()))
        # pylint: enable=expression-not-assigned

    # Verify the output matches golden output.
    # NOTE: we don't verify that transformed examples match golden output.
    expected_transformed_schema = io_utils.parse_pbtxt_file(
        os.path.join(
            self._testdata_path,
            'transform/transform_output/transformed_metadata/schema.pbtxt'),
        schema_pb2.Schema())
    transformed_schema = io_utils.parse_pbtxt_file(
        os.path.join(transform_output_path,
                     'transformed_metadata/schema.pbtxt'),
        schema_pb2.Schema())
    # Clear annotations so we only have to test main schema.
    for feature in transformed_schema.feature:
      feature.ClearField('annotation')
    transformed_schema.ClearField('annotation')
    self.assertEqual(transformed_schema, expected_transformed_schema)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3091')" href="javascript:;">
tfx-1.6.0/tfx/examples/chicago_taxi_pipeline/taxi_utils_test.py: 52-112
</a>
<div class="mid" id="frag3091" style="display:none"><pre>
  def testPreprocessingFn(self):
    schema_file = os.path.join(self._testdata_path, 'schema_gen/schema.pbtxt')
    schema = io_utils.parse_pbtxt_file(schema_file, schema_pb2.Schema())
    feature_spec = taxi_utils._get_raw_feature_spec(schema)
    working_dir = self.get_temp_dir()
    transform_graph_path = os.path.join(working_dir, 'transform_graph')
    transformed_examples_path = os.path.join(
        working_dir, 'transformed_examples')

    # Run very simplified version of executor logic.
    # TODO(kestert): Replace with tft_unit.assertAnalyzeAndTransformResults.
    # Generate legacy `DatasetMetadata` object.  Future version of Transform
    # will accept the `Schema` proto directly.
    legacy_metadata = dataset_metadata.DatasetMetadata(
        schema_utils.schema_from_feature_spec(feature_spec))
    tfxio = tf_example_record.TFExampleRecord(
        file_pattern=os.path.join(self._testdata_path,
                                  'csv_example_gen/Split-train/*'),
        telemetry_descriptors=['Tests'],
        schema=legacy_metadata.schema)
    with beam.Pipeline() as p:
      with tft_beam.Context(temp_dir=os.path.join(working_dir, 'tmp')):
        examples = p | 'ReadTrainData' &gt;&gt; tfxio.BeamSource()
        (transformed_examples, transformed_metadata), transform_fn = (
            (examples, tfxio.TensorAdapterConfig())
            | 'AnalyzeAndTransform' &gt;&gt; tft_beam.AnalyzeAndTransformDataset(
                taxi_utils.preprocessing_fn))

        # WriteTransformFn writes transform_fn and metadata to subdirectories
        # tensorflow_transform.SAVED_MODEL_DIR and
        # tensorflow_transform.TRANSFORMED_METADATA_DIR respectively.
        # pylint: disable=expression-not-assigned
        (transform_fn
         |
         'WriteTransformFn' &gt;&gt; tft_beam.WriteTransformFn(transform_graph_path))

        encoder = tft.coders.ExampleProtoCoder(transformed_metadata.schema)
        (transformed_examples
         | 'EncodeTrainData' &gt;&gt; beam.Map(encoder.encode)
         | 'WriteTrainData' &gt;&gt; beam.io.WriteToTFRecord(
             os.path.join(transformed_examples_path,
                          'Split-train/transformed_examples.gz'),
             coder=beam.coders.BytesCoder()))
        # pylint: enable=expression-not-assigned

    # Verify the output matches golden output.
    # NOTE: we don't verify that transformed examples match golden output.
    expected_transformed_schema = io_utils.parse_pbtxt_file(
        os.path.join(
            self._testdata_path,
            'transform/transform_graph/transformed_metadata/schema.pbtxt'),
        schema_pb2.Schema())
    transformed_schema = io_utils.parse_pbtxt_file(
        os.path.join(transform_graph_path, 'transformed_metadata/schema.pbtxt'),
        schema_pb2.Schema())
    # Clear annotations so we only have to test main schema.
    transformed_schema.ClearField('annotation')
    for feature in transformed_schema.feature:
      feature.ClearField('annotation')
    self.assertEqual(transformed_schema, expected_transformed_schema)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 142:</b> &nbsp; 2 fragments, nominal size 49 lines, similarity 74%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2959')" href="javascript:;">
tfx-1.6.0/tfx/examples/bigquery_ml/taxi_utils_bqml_test.py: 112-169
</a>
<div class="mid" id="frag2959" style="display:none"><pre>
  def testTrainerFn(self):
    temp_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    schema_file = os.path.join(self._testdata_path, 'schema_gen/schema.pbtxt')
    trainer_fn_args = trainer_executor.TrainerFnArgs(
        train_files=os.path.join(self._testdata_path,
                                 'transform/transformed_examples/train/*.gz'),
        transform_output=os.path.join(self._testdata_path,
                                      'transform/transform_output/'),
        serving_model_dir=os.path.join(temp_dir, 'serving_model_dir'),
        eval_files=os.path.join(self._testdata_path,
                                'transform/transformed_examples/eval/*.gz'),
        schema_file=schema_file,
        train_steps=1,
        eval_steps=1,
        base_model=os.path.join(self._testdata_path,
                                'trainer/current/serving_model_dir'),
        data_accessor=DataAccessor(
            tf_dataset_factory=tfxio_utils.get_tf_dataset_factory_from_artifact(
                [standard_artifacts.Examples()], []),
            record_batch_factory=None,
            data_view_decode_fn=None))
    schema = io_utils.parse_pbtxt_file(schema_file, schema_pb2.Schema())
    training_spec = taxi_utils_bqml.trainer_fn(trainer_fn_args, schema)

    estimator = training_spec['estimator']
    train_spec = training_spec['train_spec']
    eval_spec = training_spec['eval_spec']
    eval_input_receiver_fn = training_spec['eval_input_receiver_fn']

    self.assertIsInstance(estimator, tf.estimator.Estimator)
    self.assertIsInstance(train_spec, tf.estimator.TrainSpec)
    self.assertIsInstance(eval_spec, tf.estimator.EvalSpec)
    self.assertIsInstance(eval_input_receiver_fn, types.FunctionType)

    # Train for one step, then eval for one step.
    eval_result, exports = tf.estimator.train_and_evaluate(
        estimator, train_spec, eval_spec)
    self.assertGreater(eval_result['loss'], 0.0)
    self.assertEqual(len(exports), 1)
    self.assertGreaterEqual(len(fileio.listdir(exports[0])), 1)

    # Export the eval saved model.
    eval_savedmodel_path = tfma.export.export_eval_savedmodel(
        estimator=estimator,
        export_dir_base=path_utils.eval_model_dir(temp_dir),
        eval_input_receiver_fn=eval_input_receiver_fn)
    self.assertGreaterEqual(len(fileio.listdir(eval_savedmodel_path)), 1)

    # Test exported serving graph.
    with tf.compat.v1.Session() as sess:
      metagraph_def = tf.compat.v1.saved_model.loader.load(
          sess, [tf.saved_model.SERVING], exports[0])
      self.assertIsInstance(metagraph_def, tf.compat.v1.MetaGraphDef)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3092')" href="javascript:;">
tfx-1.6.0/tfx/examples/chicago_taxi_pipeline/taxi_utils_test.py: 113-176
</a>
<div class="mid" id="frag3092" style="display:none"><pre>
  def testTrainerFn(self):
    temp_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    schema_file = os.path.join(self._testdata_path, 'schema_gen/schema.pbtxt')
    data_accessor = DataAccessor(
        tf_dataset_factory=tfxio_utils.get_tf_dataset_factory_from_artifact(
            [standard_artifacts.Examples()], []),
        record_batch_factory=None,
        data_view_decode_fn=None)
    trainer_fn_args = trainer_executor.TrainerFnArgs(
        train_files=os.path.join(
            self._testdata_path,
            'transform/transformed_examples/Split-train/*.gz'),
        transform_output=os.path.join(self._testdata_path,
                                      'transform/transform_graph'),
        serving_model_dir=os.path.join(temp_dir, 'serving_model_dir'),
        eval_files=os.path.join(
            self._testdata_path,
            'transform/transformed_examples/Split-eval/*.gz'),
        schema_file=schema_file,
        train_steps=1,
        eval_steps=1,
        base_model=None,
        data_accessor=data_accessor)
    schema = io_utils.parse_pbtxt_file(schema_file, schema_pb2.Schema())
    training_spec = taxi_utils.trainer_fn(trainer_fn_args, schema)

    estimator = training_spec['estimator']
    train_spec = training_spec['train_spec']
    eval_spec = training_spec['eval_spec']
    eval_input_receiver_fn = training_spec['eval_input_receiver_fn']

    self.assertIsInstance(estimator,
                          tf.estimator.DNNLinearCombinedClassifier)
    self.assertIsInstance(train_spec, tf.estimator.TrainSpec)
    self.assertIsInstance(eval_spec, tf.estimator.EvalSpec)
    self.assertIsInstance(eval_input_receiver_fn, types.FunctionType)

    # Test keep_max_checkpoint in RunConfig
    self.assertGreater(estimator._config.keep_checkpoint_max, 1)

    # Train for one step, then eval for one step.
    eval_result, exports = tf.estimator.train_and_evaluate(
        estimator, train_spec, eval_spec)
    self.assertGreater(eval_result['loss'], 0.0)
    self.assertEqual(len(exports), 1)
    self.assertGreaterEqual(len(fileio.listdir(exports[0])), 1)

    # Export the eval saved model.
    eval_savedmodel_path = tfma.export.export_eval_savedmodel(
        estimator=estimator,
        export_dir_base=path_utils.eval_model_dir(temp_dir),
        eval_input_receiver_fn=eval_input_receiver_fn)
    self.assertGreaterEqual(len(fileio.listdir(eval_savedmodel_path)), 1)

    # Test exported serving graph.
    with tf.compat.v1.Session() as sess:
      metagraph_def = tf.compat.v1.saved_model.loader.load(
          sess, [tf.saved_model.SERVING], exports[0])
      self.assertIsInstance(metagraph_def, tf.compat.v1.MetaGraphDef)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 143:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2979')" href="javascript:;">
tfx-1.6.0/tfx/examples/airflow_workshop/notebooks/utils.py: 347-384
</a>
<div class="mid" id="frag2979" style="display:none"><pre>
  def get_source_artifact_of_type(self, artifact_id, source_type_name):
    """Returns the source artifact of `source_type_name` for `artifact_id`.

    This method recursively traverses the events and associated executions that
    led to generating `artifact_id` to find an artifact of type
    `source_type_name` that was an input for these events.

    Args:
      artifact_id: A `int` indicating the id of an artifact.
      source_type_name: A `str` indicating the type of an artifact that is
          a direct or indirect input for generating `artifact_id`.

    Returns:
      A `metadata_store_pb2.Artifact` of type `source_type_name` that is a
      direct/indirect input for generating `artifact_id` or `None` if no such
      artifact exists.
    """
    a_events = self.metadata_store.get_events_by_artifact_ids([artifact_id])
    for a_event in a_events:
      if _is_input_event(a_event):
        continue
      [execution] = self.metadata_store.get_executions_by_id(
          [a_event.execution_id])
      e_events = self.metadata_store.get_events_by_execution_ids([execution.id])
      for e_event in e_events:
        if _is_output_event(e_event):
          continue
        [artifact] = self.metadata_store.get_artifacts_by_id(
            [e_event.artifact_id])
        [artifact_type] = self.metadata_store.get_artifact_types_by_id(
            [artifact.type_id])
        if artifact_type.name == source_type_name:
          return artifact
        input_artifact = self.get_source_artifact_of_type(
            artifact.id, source_type_name)
        if input_artifact:
          return input_artifact

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2980')" href="javascript:;">
tfx-1.6.0/tfx/examples/airflow_workshop/notebooks/utils.py: 385-423
</a>
<div class="mid" id="frag2980" style="display:none"><pre>
  def get_dest_artifact_of_type(self, artifact_id, dest_type_name):
    """Returns the destination artifact of `dest_type_name` from `artifact_id`.

    This method recursively traverses the events and associated executions that
    consumed `artifact_id` to find an artifact of type `dest_type_name` that was
    an output for these events.

    Args:
      artifact_id: A `int` indicating the id of an artifact.
      dest_type_name: A `str` indicating the type of an artifact that is
          a output of an event that directly/indirectly consumed `artifact_id`.

    Returns:
      A `metadata_store_pb2.Artifact` of type `dest_type_name` that is a
      direct/indirect output from `artifact_id` or `None` if no such artifact
      exists.
    """
    a_events = self.metadata_store.get_events_by_artifact_ids([artifact_id])
    for a_event in a_events:
      if _is_output_event(a_event):
        continue
      [execution] = self.metadata_store.get_executions_by_id(
          [a_event.execution_id])
      e_events = self.metadata_store.get_events_by_execution_ids(
          [execution.id])
      for e_event in e_events:
        if _is_input_event(e_event):
          continue
        [artifact] = self.metadata_store.get_artifacts_by_id(
            [e_event.artifact_id])
        [artifact_type] = self.metadata_store.get_artifact_types_by_id(
            [artifact.type_id])
        if artifact_type.name == dest_type_name:
          return artifact
        dest_artifact = self.get_dest_artifact_of_type(
            artifact.id, dest_type_name)
        if dest_artifact:
          return dest_artifact

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 144:</b> &nbsp; 2 fragments, nominal size 33 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3106')" href="javascript:;">
tfx-1.6.0/tfx/examples/penguin/penguin_utils_keras.py: 130-175
</a>
<div class="mid" id="frag3106" style="display:none"><pre>
def run_fn(fn_args: tfx.components.FnArgs):
  """Train the model based on given args.

  Args:
    fn_args: Holds args used to train the model as name/value pairs.
  """
  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

  train_dataset = base.input_fn(
      fn_args.train_files,
      fn_args.data_accessor,
      tf_transform_output,
      base.TRAIN_BATCH_SIZE)

  eval_dataset = base.input_fn(
      fn_args.eval_files,
      fn_args.data_accessor,
      tf_transform_output,
      base.EVAL_BATCH_SIZE)

  if fn_args.hyperparameters:
    hparams = keras_tuner.HyperParameters.from_config(fn_args.hyperparameters)
  else:
    # This is a shown case when hyperparameters is decided and Tuner is removed
    # from the pipeline. User can also inline the hyperparameters directly in
    # _build_keras_model.
    hparams = _get_hyperparameters()
  absl.logging.info('HyperParameters for training: %s' % hparams.get_config())

  mirrored_strategy = tf.distribute.MirroredStrategy()
  with mirrored_strategy.scope():
    model = _make_keras_model(hparams)

  # Write logs to path
  tensorboard_callback = tf.keras.callbacks.TensorBoard(
      log_dir=fn_args.model_run_dir, update_freq='batch')

  model.fit(
      train_dataset,
      steps_per_epoch=fn_args.train_steps,
      validation_data=eval_dataset,
      validation_steps=fn_args.eval_steps,
      callbacks=[tensorboard_callback])

  signatures = base.make_serving_signatures(model, tf_transform_output)
  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3117')" href="javascript:;">
tfx-1.6.0/tfx/examples/penguin/penguin_utils_cloud_tuner.py: 274-338
</a>
<div class="mid" id="frag3117" style="display:none"><pre>
def run_fn(fn_args: tfx.components.FnArgs):
  """Train the model based on given args.

  Args:
    fn_args: Holds args as name/value pairs. See
      https://www.tensorflow.org/tfx/api_docs/python/tfx/components/trainer/fn_args_utils/FnArgs.
      - train_files: List of file paths containing training tf.Example data.
      - eval_files: List of file paths containing eval tf.Example data.
      - data_accessor: Contains factories that can create tf.data.Datasets or
        other means to access the train/eval data. They provide a uniform way of
        accessing data, regardless of how the data is stored on disk.
      - train_steps: number of train steps.
      - eval_steps: number of eval steps.
      - transform_output: A uri to a path containing statistics and metadata
        from TFTransform component. produced by TFT. Will be None if not
        specified.
      - model_run_dir: A single uri for the output directory of model training
        related files.
      - hyperparameters: An optional keras_tuner.HyperParameters config.
  """
  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

  train_dataset = _input_fn(
      fn_args.train_files,
      fn_args.data_accessor,
      tf_transform_output,
      batch_size=_TRAIN_BATCH_SIZE)

  eval_dataset = _input_fn(
      fn_args.eval_files,
      fn_args.data_accessor,
      tf_transform_output,
      batch_size=_EVAL_BATCH_SIZE)

  if fn_args.hyperparameters:
    hparams = keras_tuner.HyperParameters.from_config(fn_args.hyperparameters)
  else:
    # This is a shown case when hyperparameters is decided and Tuner is removed
    # from the pipeline. User can also inline the hyperparameters directly in
    # _build_keras_model.
    hparams = _get_hyperparameters()
  logging.info('HyperParameters for training: %s', hparams.get_config())

  mirrored_strategy = tf.distribute.MirroredStrategy()
  with mirrored_strategy.scope():
    model = _build_keras_model(hparams)

  # Write logs to path
  tensorboard_callback = tf.keras.callbacks.TensorBoard(
      log_dir=fn_args.model_run_dir, update_freq='batch')

  model.fit(
      train_dataset,
      steps_per_epoch=fn_args.train_steps,
      validation_data=eval_dataset,
      validation_steps=fn_args.eval_steps,
      callbacks=[tensorboard_callback])

  signatures = {
      'serving_default':
          _get_tf_examples_serving_signature(model, tf_transform_output),
      'transform_features':
          _get_transform_features_signature(model, tf_transform_output),
  }
  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 145:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3108')" href="javascript:;">
tfx-1.6.0/tfx/examples/penguin/penguin_utils_cloud_tuner.py: 64-90
</a>
<div class="mid" id="frag3108" style="display:none"><pre>
def _get_tf_examples_serving_signature(model, tf_transform_output):
  """Returns a serving signature that accepts `tensorflow.Example`."""

  # We need to track the layers in the model in order to save it.
  # TODO(b/162357359): Revise once the bug is resolved.
  model.tft_layer_inference = tf_transform_output.transform_features_layer()

  @tf.function(input_signature=[
      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')
  ])
  def serve_tf_examples_fn(serialized_tf_example):
    """Returns the output to be used in the serving signature."""
    raw_feature_spec = tf_transform_output.raw_feature_spec()
    # Remove label feature since these will not be present at serving time.
    raw_feature_spec.pop(_LABEL_KEY)
    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)
    transformed_features = model.tft_layer_inference(raw_features)
    logging.info('serve_transformed_features = %s', transformed_features)

    outputs = model(transformed_features)
    # TODO(b/154085620): Convert the predicted labels from the model using a
    # reverse-lookup (opposite of transform.py).
    return {'outputs': outputs}

  return serve_tf_examples_fn


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3110')" href="javascript:;">
tfx-1.6.0/tfx/examples/penguin/penguin_utils_cloud_tuner.py: 91-111
</a>
<div class="mid" id="frag3110" style="display:none"><pre>
def _get_transform_features_signature(model, tf_transform_output):
  """Returns a serving signature that applies tf.Transform to features."""

  # We need to track the layers in the model in order to save it.
  # TODO(b/162357359): Revise once the bug is resolved.
  model.tft_layer_eval = tf_transform_output.transform_features_layer()

  @tf.function(input_signature=[
      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')
  ])
  def transform_features_fn(serialized_tf_example):
    """Returns the transformed_features to be fed as input to evaluator."""
    raw_feature_spec = tf_transform_output.raw_feature_spec()
    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)
    transformed_features = model.tft_layer_eval(raw_features)
    logging.info('eval_transformed_features = %s', transformed_features)
    return transformed_features

  return transform_features_fn


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 146:</b> &nbsp; 2 fragments, nominal size 76 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3135')" href="javascript:;">
tfx-1.6.0/tfx/examples/penguin/experimental/penguin_pipeline_sklearn_gcp.py: 112-223
</a>
<div class="mid" id="frag3135" style="display:none"><pre>
def _create_pipeline(
    pipeline_name: str,
    pipeline_root: str,
    data_root: str,
    trainer_module_file: str,
    evaluator_module_file: str,
    ai_platform_training_args: Optional[Dict[str, str]],
    ai_platform_serving_args: Optional[Dict[str, str]],
    beam_pipeline_args: List[str],
) -&gt; tfx.dsl.Pipeline:
  """Implements the Penguin pipeline with TFX."""
  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = tfx.components.CsvExampleGen(
      input_base=os.path.join(data_root, 'labelled'))

  # Computes statistics over data for visualization and example validation.
  statistics_gen = tfx.components.StatisticsGen(
      examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = tfx.components.SchemaGen(
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = tfx.components.ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # TODO(humichael): Handle applying transformation component in Milestone 3.

  # Uses user-provided Python function that trains a model.
  # Num_steps is not provided during evaluation because the scikit-learn model
  # loads and evaluates the entire test set at once.
  trainer = tfx.extensions.google_cloud_ai_platform.Trainer(
      module_file=trainer_module_file,
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      train_args=tfx.proto.TrainArgs(num_steps=2000),
      eval_args=tfx.proto.EvalArgs(),
      custom_config={
          tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY:
          ai_platform_training_args,
      })

  # Get the latest blessed model for model validation.
  model_resolver = tfx.dsl.Resolver(
      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,
      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),
      model_blessing=tfx.dsl.Channel(
          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(
              'latest_blessed_model_resolver')

  # Uses TFMA to compute evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(label_key='species')],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='Accuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          lower_bound={'value': 0.6}),
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-10})))
          ])
      ])

  evaluator = tfx.components.Evaluator(
      module_file=evaluator_module_file,
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  pusher = tfx.extensions.google_cloud_ai_platform.Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      custom_config={
          tfx.extensions.google_cloud_ai_platform.experimental
          .PUSHER_SERVING_ARGS_KEY: ai_platform_serving_args,
      })

  return tfx.dsl.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen,
          statistics_gen,
          schema_gen,
          example_validator,
          trainer,
          model_resolver,
          evaluator,
          pusher,
      ],
      enable_cache=True,
      beam_pipeline_args=beam_pipeline_args,
  )


# To run this pipeline from the python CLI:
# $ tfx pipeline create \
#   --engine kubeflow \
#   --pipeline-path penguin_pipeline_sklearn_gcp.py \
#   --endpoint my-gcp-endpoint.pipelines.googleusercontent.com
# See TFX CLI guide for creating TFX pipelines:
# https://github.com/tensorflow/tfx/blob/master/docs/guide/cli.md#create
# For endpoint, see guide on connecting to hosted AI Platform Pipelines:
# https://cloud.google.com/ai-platform/pipelines/docs/connecting-with-sdk
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3148')" href="javascript:;">
tfx-1.6.0/tfx/examples/penguin/experimental/penguin_pipeline_sklearn_local.py: 66-166
</a>
<div class="mid" id="frag3148" style="display:none"><pre>
def _create_pipeline(
    pipeline_name: str,
    pipeline_root: str,
    data_root: str,
    trainer_module_file: str,
    evaluator_module_file: str,
    serving_model_dir: str,
    metadata_path: str,
    beam_pipeline_args: List[str],
) -&gt; tfx.dsl.Pipeline:
  """Implements the Penguin pipeline with TFX."""
  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = tfx.components.CsvExampleGen(
      input_base=os.path.join(data_root, 'labelled'))

  # Computes statistics over data for visualization and example validation.
  statistics_gen = tfx.components.StatisticsGen(
      examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = tfx.components.SchemaGen(
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = tfx.components.ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # TODO(humichael): Handle applying transformation component in Milestone 3.

  # Uses user-provided Python function that trains a model.
  # Num_steps is not provided during evaluation because the scikit-learn model
  # loads and evaluates the entire test set at once.
  trainer = tfx.components.Trainer(
      module_file=trainer_module_file,
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      train_args=tfx.proto.TrainArgs(num_steps=2000),
      eval_args=tfx.proto.EvalArgs())

  # Get the latest blessed model for model validation.
  model_resolver = tfx.dsl.Resolver(
      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,
      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),
      model_blessing=tfx.dsl.Channel(
          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(
              'latest_blessed_model_resolver')

  # Uses TFMA to compute evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(label_key='species')],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='Accuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          lower_bound={'value': 0.6}),
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-10})))
          ])
      ])
  evaluator = tfx.components.Evaluator(
      module_file=evaluator_module_file,
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  pusher = tfx.components.Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=tfx.proto.PushDestination(
          filesystem=tfx.proto.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  return tfx.dsl.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen,
          statistics_gen,
          schema_gen,
          example_validator,
          trainer,
          model_resolver,
          evaluator,
          pusher,
      ],
      enable_cache=True,
      metadata_connection_config=tfx.orchestration.metadata
      .sqlite_metadata_connection_config(metadata_path),
      beam_pipeline_args=beam_pipeline_args,
  )


# To run this pipeline from the python CLI:
#   $python penguin_pipeline_sklearn_local.py
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

<clones>
<systeminfo processor="nicad6" system="chainer-7.8.1" granularity="functions-blind" threshold="0%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="10744" npairs="165"/>
<runinfo ncompares="194846" cputime="99525"/>
<classinfo nclasses="118"/>

<class classid="1" nclones="2" nlines="15" similarity="100">
<source file="systems/chainer-7.8.1/chainer/functions/array/scatter_add.py" startline="13" endline="30" pcid="1223">
    def __init__(self, slices):
        if isinstance(slices, list):
            if all([isinstance(s, int) for s in slices]):
                slices = slices,
            slices = tuple(slices)
        elif not isinstance(slices, tuple):
            slices = slices,

        if chainer.is_debug():
            n_ellipses = 0
            for s in slices:
                if s is Ellipsis:
                    n_ellipses += 1
            if n_ellipses > 1:
                raise ValueError('Only one Ellipsis is allowed')

        self.slices = slices

</source>
<source file="systems/chainer-7.8.1/chainer/functions/array/get_item.py" startline="19" endline="36" pcid="1316">

    def __init__(self, slices):
        if isinstance(slices, list):
            if all([isinstance(s, int) for s in slices]):
                slices = slices,
            slices = tuple(slices)
        elif not isinstance(slices, tuple):
            slices = slices,

        if chainer.is_debug():
            n_ellipses = 0
            for s in slices:
                if s is Ellipsis:
                    n_ellipses += 1
            if n_ellipses > 1:
                raise ValueError('Only one Ellipsis is allowed')

        self.slices = slices
</source>
</class>

<class classid="2" nclones="2" nlines="19" similarity="100">
<source file="systems/chainer-7.8.1/chainer/functions/pooling/roi_average_pooling_2d.py" startline="53" endline="73" pcid="1364">
    def __init__(self, outsize, spatial_scale):
        outh, outw = _pair(outsize)
        if not (isinstance(outh, numbers.Integral) and outh > 0):
            raise TypeError(
                'outsize[0] must be positive integer: {}, {}'
                .format(type(outh), outh))
        if not (isinstance(outw, numbers.Integral) and outw > 0):
            raise TypeError(
                'outsize[1] must be positive integer: {}, {}'
                .format(type(outw), outw))
        if isinstance(spatial_scale, numbers.Integral):
            spatial_scale = float(spatial_scale)
        if not (isinstance(spatial_scale, numbers.Real) and
                spatial_scale > 0):
            raise TypeError(
                'spatial_scale must be a positive float number: {}, {}'
                .format(type(spatial_scale), spatial_scale))

        self.outh, self.outw = outh, outw
        self.spatial_scale = spatial_scale

</source>
<source file="systems/chainer-7.8.1/chainer/functions/pooling/roi_max_pooling_2d.py" startline="54" endline="73" pcid="1400">
    def __init__(self, outsize, spatial_scale):
        outh, outw = _pair(outsize)
        if not (isinstance(outh, numbers.Integral) and outh > 0):
            raise TypeError(
                'outsize[0] must be positive integer: {}, {}'
                .format(type(outh), outh))
        if not (isinstance(outw, numbers.Integral) and outw > 0):
            raise TypeError(
                'outsize[1] must be positive integer: {}, {}'
                .format(type(outw), outw))
        if isinstance(spatial_scale, numbers.Integral):
            spatial_scale = float(spatial_scale)
        if not (isinstance(spatial_scale, numbers.Real) and
                spatial_scale > 0):
            raise TypeError(
                'spatial_scale must be a positive float number: {}, {}'
                .format(type(spatial_scale), spatial_scale))
        self.outh, self.outw = outh, outw
        self.spatial_scale = spatial_scale

</source>
</class>

<class classid="3" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.8.1/chainer/functions/pooling/roi_average_pooling_2d.py" startline="74" endline="88" pcid="1365">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)

        x_type, roi_type, roi_index_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim == 4,
            x_type.dtype == roi_type.dtype,
            roi_type.ndim == 2,
            roi_type.shape[1] == 4,
            roi_index_type.dtype == numpy.int32,
            roi_index_type.ndim == 1,
            roi_type.shape[0] == roi_index_type.shape[0],
        )

</source>
<source file="systems/chainer-7.8.1/chainer/functions/pooling/roi_max_pooling_2d.py" startline="74" endline="88" pcid="1401">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)

        x_type, roi_type, roi_index_type = in_types
        type_check.expect(
            x_type.dtype.kind == 'f',
            x_type.ndim == 4,
            x_type.dtype == roi_type.dtype,
            roi_type.ndim == 2,
            roi_type.shape[1] == 4,
            roi_index_type.dtype == numpy.int32,
            roi_index_type.ndim == 1,
            roi_type.shape[0] == roi_index_type.shape[0],
        )

</source>
</class>

<class classid="4" nclones="2" nlines="26" similarity="100">
<source file="systems/chainer-7.8.1/chainer/functions/pooling/roi_max_align_2d.py" startline="42" endline="69" pcid="1392">
    """ROI max align over a set of 2d planes."""

    def __init__(self, outsize, spatial_scale, sampling_ratio=None):
        outh, outw = _pair(outsize)
        if not (isinstance(outh, numbers.Integral) and outh > 0):
            raise TypeError(
                'outsize[0] must be positive integer: {}, {}'
                .format(type(outh), outh))
        if not (isinstance(outw, numbers.Integral) and outw > 0):
            raise TypeError(
                'outsize[1] must be positive integer: {}, {}'
                .format(type(outw), outw))
        if isinstance(spatial_scale, numbers.Integral):
            spatial_scale = float(spatial_scale)
        if not (isinstance(spatial_scale, numbers.Real) and
                spatial_scale > 0):
            raise TypeError(
                'spatial_scale must be a positive float number: {}, {}'
                .format(type(spatial_scale), spatial_scale))
        sampling_ratio = _pair(sampling_ratio)
        if not all((isinstance(s, numbers.Integral) and s >= 1) or
                   s is None for s in sampling_ratio):
            raise TypeError(
                'sampling_ratio must be integer >= 1 or a pair of it: {}'
                .format(sampling_ratio))

        self.outh, self.outw = outh, outw
        self.spatial_scale = spatial_scale
</source>
<source file="systems/chainer-7.8.1/chainer/functions/pooling/roi_average_align_2d.py" startline="105" endline="132" pcid="1502">
    def __init__(self, outsize, spatial_scale, sampling_ratio=None):
        outh, outw = _pair(outsize)
        if not (isinstance(outh, numbers.Integral) and outh > 0):
            raise TypeError(
                'outsize[0] must be positive integer: {}, {}'
                .format(type(outh), outh))
        if not (isinstance(outw, numbers.Integral) and outw > 0):
            raise TypeError(
                'outsize[1] must be positive integer: {}, {}'
                .format(type(outw), outw))
        if isinstance(spatial_scale, numbers.Integral):
            spatial_scale = float(spatial_scale)
        if not (isinstance(spatial_scale, numbers.Real) and
                spatial_scale > 0):
            raise TypeError(
                'spatial_scale must be a positive float number: {}, {}'
                .format(type(spatial_scale), spatial_scale))
        sampling_ratio = _pair(sampling_ratio)
        if not all((isinstance(s, numbers.Integral) and s >= 1) or
                   s is None for s in sampling_ratio):
            raise TypeError(
                'sampling_ratio must be integer >= 1 or a pair of it: {}'
                .format(sampling_ratio))

        self.outh, self.outw = outh, outw
        self.spatial_scale = spatial_scale
        self.sampling_ratio = sampling_ratio

</source>
</class>

<class classid="5" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.8.1/chainer/functions/pooling/roi_max_align_2d.py" startline="70" endline="84" pcid="1393">
        self.sampling_ratio = sampling_ratio

    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)

        x_type, roi_type, roi_index_type = in_types
        type_check.expect(
            x_type.dtype == numpy.float32,
            x_type.ndim == 4,
            roi_type.dtype == numpy.float32,
            roi_type.ndim == 2,
            roi_type.shape[1] == 4,
            roi_index_type.dtype == numpy.int32,
            roi_index_type.ndim == 1,
            roi_type.shape[0] == roi_index_type.shape[0],
</source>
<source file="systems/chainer-7.8.1/chainer/functions/pooling/roi_average_align_2d.py" startline="133" endline="147" pcid="1503">
    def check_type_forward(self, in_types):
        type_check.expect(in_types.size() == 3)

        x_type, roi_type, roi_index_type = in_types
        type_check.expect(
            x_type.dtype == numpy.float32,
            x_type.ndim == 4,
            roi_type.dtype == numpy.float32,
            roi_type.ndim == 2,
            roi_type.shape[1] == 4,
            roi_index_type.dtype == numpy.int32,
            roi_index_type.ndim == 1,
            roi_type.shape[0] == roi_index_type.shape[0],
        )

</source>
</class>

<class classid="6" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.8.1/chainer/functions/pooling/pooling_nd_kernel.py" startline="32" endline="45" pcid="1542">
    def _generate(self, ndim):
        self.ndim = ndim
        self.ds = conv_nd_kernel.vars('d', ndim)
        self.outs = conv_nd_kernel.vars('out', ndim)
        self.ks = conv_nd_kernel.vars('k', ndim)
        self.ss = conv_nd_kernel.vars('s', ndim)
        self.ps = conv_nd_kernel.vars('p', ndim)

        in_params = self._in_params()
        out_params = self._out_params()
        operation = self._operation()
        name = '{}_pool_{}d_fwd'.format(self.name(), self.ndim)
        return in_params, out_params, operation, name

</source>
<source file="systems/chainer-7.8.1/chainer/functions/pooling/pooling_nd_kernel.py" startline="181" endline="194" pcid="1562">
    def _generate(self, ndim):
        self.ndim = ndim
        self.ds = conv_nd_kernel.vars('d', ndim)
        self.outs = conv_nd_kernel.vars('out', ndim)
        self.ks = conv_nd_kernel.vars('k', ndim)
        self.ss = conv_nd_kernel.vars('s', ndim)
        self.ps = conv_nd_kernel.vars('p', ndim)

        in_params = self._in_params()
        out_params = self._out_params()
        operation = self._operation()
        name = '{}_pool_{}d_bwd'.format(self.name(), self.ndim)
        return in_params, out_params, operation, name

</source>
</class>

<class classid="7" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.8.1/chainer/functions/pooling/pooling_nd_kernel.py" startline="46" endline="61" pcid="1543">
    def _in_params(self):
        # 2D: raw T in, int32 d_0, int32 d_1, int32 out_0, int32 out_1,
        #     int32 k_0, int32 k_1, int32 s_0, int32 s_1, int32 p_0,
        #     int32 p_1, ...
        def aux(x):
            return 'int32 {}'.format(x)
        in_params = self.in_params()
        if type(in_params) is tuple:
            raws = in_params[0]
            in_params = in_params[1]
        else:
            raws = []
        vars = self.ds + self.outs + self.ks + self.ss + self.ps
        return ', '.join(
            ['raw T in'] + raws + conv_nd_kernel.map_(aux, vars) + in_params)

</source>
<source file="systems/chainer-7.8.1/chainer/functions/pooling/pooling_nd_kernel.py" startline="195" endline="210" pcid="1563">
    def _in_params(self):
        # 2D: raw T gy, int32 d_0, int32 d_1, int32 out_0, int32 out_1,
        #     int32 k_0, int32 k_1, int32 s_0, int32 s_1, int32 p_0,
        #     int32 p_1, ...
        def aux(x):
            return 'int32 {}'.format(x)
        in_params = self.in_params()
        if type(in_params) is tuple:
            raws = in_params[0]
            in_params = in_params[1]
        else:
            raws = []
        vars = self.ds + self.outs + self.ks + self.ss + self.ps
        return ', '.join(
            ['raw T gy'] + raws + conv_nd_kernel.map_(aux, vars) + in_params)

</source>
</class>

<class classid="8" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.8.1/chainer/functions/math/erf.py" startline="26" endline="40" pcid="1577">
    def forward_cpu(self, x):
        global _erf_cpu
        if _erf_cpu is None:
            try:
                from scipy import special
                _erf_cpu = special.erf
            except ImportError:
                warnings.warn(
                    'SciPy is not available. Forward computation of erf in CPU'
                    ' can be slow without SciPy.',
                    chainer.warnings.PerformanceWarning)
                _erf_cpu = numpy.vectorize(math.erf)
        self.retain_inputs((0,))
        return utils.force_array(_erf_cpu(x[0]), dtype=x[0].dtype),

</source>
<source file="systems/chainer-7.8.1/chainer/functions/math/erfc.py" startline="26" endline="40" pcid="2009">
    def forward_cpu(self, x):
        global _erfc_cpu
        if _erfc_cpu is None:
            try:
                from scipy import special
                _erfc_cpu = special.erfc
            except ImportError:
                warnings.warn(
                    'SciPy is not available. Forward computation of erfc in'
                    ' CPU can be slow without SciPy.',
                    chainer.warnings.PerformanceWarning)
                _erfc_cpu = numpy.vectorize(math.erfc)
        self.retain_inputs((0,))
        return utils.force_array(_erfc_cpu(x[0]), dtype=x[0].dtype),

</source>
</class>

<class classid="9" nclones="3" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/chainer/functions/math/log_ndtr.py" startline="23" endline="35" pcid="1583">
    def forward_cpu(self, x):
        global _log_ndtr_cpu
        if _log_ndtr_cpu is None:
            try:
                from scipy import special
                _log_ndtr_cpu = special.log_ndtr
            except ImportError:
                raise ImportError('SciPy is not available. Forward computation'
                                  ' of log_ndtr can not be done.')

        self.retain_inputs((0,))
        return utils.force_array(_log_ndtr_cpu(x[0]), dtype=x[0].dtype),

</source>
<source file="systems/chainer-7.8.1/chainer/functions/math/lgamma.py" startline="20" endline="31" pcid="1632">
    def forward_cpu(self, x):
        global _lgamma_cpu
        if _lgamma_cpu is None:
            try:
                from scipy import special
                _lgamma_cpu = special.gammaln
            except ImportError:
                raise ImportError('SciPy is not available. Forward computation'
                                  ' of lgamma can not be done.')
        self.retain_inputs((0,))
        return utils.force_array(_lgamma_cpu(x[0]), dtype=x[0].dtype),

</source>
<source file="systems/chainer-7.8.1/chainer/functions/math/digamma.py" startline="22" endline="33" pcid="1759">
    def forward_cpu(self, x):
        global _digamma_cpu
        if _digamma_cpu is None:
            try:
                from scipy import special
                _digamma_cpu = special.digamma
            except ImportError:
                raise ImportError('SciPy is not available. Forward computation'
                                  ' of digamma can not be done.')
        self.retain_inputs((0,))
        return utils.force_array(_digamma_cpu(x[0]), dtype=x[0].dtype),

</source>
</class>

<class classid="10" nclones="5" nlines="13" similarity="100">
<source file="systems/chainer-7.8.1/chainer/functions/math/average.py" startline="31" endline="48" pcid="1637">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x',))
        type_check.expect(in_types[0].dtype.kind == 'f')

        if self.axis is not None:
            for axis in self.axis:
                if axis >= 0:
                    type_check.expect(
                        axis < in_types[0].ndim,
                    )
                else:
                    type_check.expect(
                        -axis - 1 < in_types[0].ndim,
                    )

    # TODO(kataoka): override `forward_chainerx` if `chainerx.mean` does not
    # overflow for large float16 inputs

</source>
<source file="systems/chainer-7.8.1/chainer/functions/math/sum.py" startline="33" endline="47" pcid="1896">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x',))
        type_check.expect(in_types[0].dtype.kind == 'f')

        if self.axis is not None:
            for axis in self.axis:
                if axis >= 0:
                    type_check.expect(
                        axis < in_types[0].ndim,
                    )
                else:
                    type_check.expect(
                        -axis - 1 < in_types[0].ndim,
                    )

</source>
<source file="systems/chainer-7.8.1/chainer/functions/math/logsumexp.py" startline="27" endline="41" pcid="1711">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x',))
        type_check.expect(in_types[0].dtype.kind == 'f')

        if self.axis is not None:
            for axis in self.axis:
                if axis >= 0:
                    type_check.expect(
                        axis < in_types[0].ndim,
                    )
                else:
                    type_check.expect(
                        -axis - 1 < in_types[0].ndim,
                    )

</source>
<source file="systems/chainer-7.8.1/chainer/functions/math/prod.py" startline="31" endline="45" pcid="1735">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x',))
        type_check.expect(in_types[0].dtype.kind == 'f')

        if self.axis is not None:
            for axis in self.axis:
                if axis >= 0:
                    type_check.expect(
                        axis < in_types[0].ndim,
                    )
                else:
                    type_check.expect(
                        -axis - 1 < in_types[0].ndim,
                    )

</source>
<source file="systems/chainer-7.8.1/chainer/functions/math/minmax.py" startline="33" endline="47" pcid="1926">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x',))
        type_check.expect(in_types[0].dtype.kind == 'f')

        if self.axis is not None:
            for axis in self.axis:
                if axis >= 0:
                    type_check.expect(
                        axis < in_types[0].ndim,
                    )
                else:
                    type_check.expect(
                        -axis - 1 < in_types[0].ndim,
                    )

</source>
</class>

<class classid="11" nclones="2" nlines="14" similarity="100">
<source file="systems/chainer-7.8.1/chainer/functions/math/prod.py" startline="15" endline="30" pcid="1734">
    def __init__(self, axis=None, keepdims=False):
        if axis is None:
            self.axis = None
        elif isinstance(axis, six.integer_types):
            self.axis = (axis,)
        elif isinstance(axis, tuple) and all(
                isinstance(a, six.integer_types) for a in axis):
            if len(set(axis)) != len(axis):
                raise ValueError('duplicate value in axis: ({})'.format(
                    ', '.join(map(str, axis))))
            self.axis = axis
        else:
            raise TypeError('None, int or tuple of int are required')

        self.keepdims = keepdims

</source>
<source file="systems/chainer-7.8.1/chainer/functions/math/sum.py" startline="17" endline="32" pcid="1895">
    def __init__(self, axis=None, keepdims=False):
        if axis is None:
            self.axis = None
        elif isinstance(axis, six.integer_types):
            self.axis = (axis,)
        elif isinstance(axis, tuple) and all(
                isinstance(a, six.integer_types) for a in axis):
            if len(set(axis)) != len(axis):
                raise ValueError('duplicate value in axis: ({})'.format(
                    ', '.join(map(str, axis))))
            self.axis = axis
        else:
            raise TypeError('None, int or tuple of int are required')

        self.keepdims = keepdims

</source>
</class>

<class classid="12" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.8.1/chainer/functions/evaluation/classification_summary.py" startline="26" endline="43" pcid="2376">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x', 't'))
        x_type, t_type = in_types

        type_check.expect(
            x_type.dtype.kind == 'f',
            t_type.dtype.kind == 'i'
        )

        t_ndim = type_check.eval(t_type.ndim)
        type_check.expect(
            x_type.ndim >= t_type.ndim,
            x_type.shape[0] == t_type.shape[0],
            x_type.shape[2: t_ndim + 1] == t_type.shape[1:]
        )
        for i in six.moves.range(t_ndim + 1, type_check.eval(x_type.ndim)):
            type_check.expect(x_type.shape[i] == 1)

</source>
<source file="systems/chainer-7.8.1/chainer/functions/evaluation/accuracy.py" startline="15" endline="32" pcid="2388">
    def check_type_forward(self, in_types):
        type_check._argname(in_types, ('x', 't'))
        x_type, t_type = in_types

        type_check.expect(
            x_type.dtype.kind == 'f',
            t_type.dtype.kind == 'i'
        )

        t_ndim = type_check.eval(t_type.ndim)
        type_check.expect(
            x_type.ndim >= t_type.ndim,
            x_type.shape[0] == t_type.shape[0],
            x_type.shape[2: t_ndim + 1] == t_type.shape[1:]
        )
        for i in six.moves.range(t_ndim + 1, type_check.eval(x_type.ndim)):
            type_check.expect(x_type.shape[i] == 1)

</source>
</class>

<class classid="13" nclones="3" nlines="22" similarity="100">
<source file="systems/chainer-7.8.1/chainer/functions/rnn/n_step_gru.py" startline="21" endline="48" pcid="2425">
def _extract_apply_in_data(inputs):
    if not inputs:
        return False, ()

    if chainerx.is_available():
        has_chainerx_array = False

        # Unwrap arrays
        arrays = []
        for x in inputs:
            if isinstance(x, variable.Variable):
                if x._has_chainerx_array:
                    arrays.append(x._data[0])
                    has_chainerx_array = True
                else:
                    arrays.append(x.array)
            else:  # x is ndarray
                arrays.append(x)
                if not has_chainerx_array:
                    if isinstance(x, chainerx.ndarray):
                        has_chainerx_array = True
        return has_chainerx_array, tuple(arrays)
    else:
        return False, tuple([
            x.array if isinstance(x, variable.Variable) else x
            for x in inputs])


</source>
<source file="systems/chainer-7.8.1/chainer/functions/rnn/n_step_lstm.py" startline="20" endline="47" pcid="2443">
def _extract_apply_in_data(inputs):
    if not inputs:
        return False, ()

    if chainerx.is_available():
        has_chainerx_array = False

        # Unwrap arrays
        arrays = []
        for x in inputs:
            if isinstance(x, variable.Variable):
                if x._has_chainerx_array:
                    arrays.append(x._data[0])
                    has_chainerx_array = True
                else:
                    arrays.append(x.array)
            else:  # x is ndarray
                arrays.append(x)
                if not has_chainerx_array:
                    if isinstance(x, chainerx.ndarray):
                        has_chainerx_array = True
        return has_chainerx_array, tuple(arrays)
    else:
        return False, tuple([
            x.array if isinstance(x, variable.Variable) else x
            for x in inputs])


</source>
<source file="systems/chainer-7.8.1/chainer/functions/rnn/n_step_rnn.py" startline="63" endline="90" pcid="2453">
def _extract_apply_in_data(inputs):
    if not inputs:
        return False, ()

    if chainerx.is_available():
        has_chainerx_array = False

        # Unwrap arrays
        arrays = []
        for x in inputs:
            if isinstance(x, variable.Variable):
                if x._has_chainerx_array:
                    arrays.append(x._data[0])
                    has_chainerx_array = True
                else:
                    arrays.append(x.array)
            else:  # x is ndarray
                arrays.append(x)
                if not has_chainerx_array:
                    if isinstance(x, chainerx.ndarray):
                        has_chainerx_array = True
        return has_chainerx_array, tuple(arrays)
    else:
        return False, tuple([
            x.array if isinstance(x, variable.Variable) else x
            for x in inputs])


</source>
</class>

<class classid="14" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.8.1/chainer/functions/rnn/n_step_gru.py" startline="49" endline="65" pcid="2426">
def _combine_inputs(hx, ws, bs, xs, num_layers, directions):
    combined = []
    combined.append(hx)
    for x in xs:
        combined.append(x)

    for n in range(num_layers):
        for direction in range(directions):
            idx = directions * n + direction

            for i in range(6):
                combined.append(ws[idx][i])
            for i in range(6):
                combined.append(bs[idx][i])
    return combined


</source>
<source file="systems/chainer-7.8.1/chainer/functions/rnn/n_step_rnn.py" startline="91" endline="107" pcid="2454">
def _combine_inputs(hx, ws, bs, xs, num_layers, directions):
    combined = []
    combined.append(hx)
    for x in xs:
        combined.append(x)

    for n in range(num_layers):
        for direction in range(directions):
            idx = directions * n + direction

            for i in range(2):
                combined.append(ws[idx][i])
            for i in range(2):
                combined.append(bs[idx][i])
    return combined


</source>
</class>

<class classid="15" nclones="2" nlines="15" similarity="100">
<source file="systems/chainer-7.8.1/chainer/functions/rnn/n_step_gru.py" startline="66" endline="82" pcid="2427">
def _seperate_inputs(combined, num_layers, seq_length, directions):
    hx = combined[0]
    xs = combined[1: 1 + seq_length]
    ws = []
    bs = []
    index = 1 + seq_length
    for n in range(num_layers):
        ws.append(combined[index: index + 6])
        bs.append(combined[index + 6: index + 12])
        index += 12
        if directions == 2:
            ws.append(combined[index: index + 6])
            bs.append(combined[index + 6: index + 12])
            index += 12
    return hx, ws, bs, xs


</source>
<source file="systems/chainer-7.8.1/chainer/functions/rnn/n_step_rnn.py" startline="108" endline="124" pcid="2455">
def _seperate_inputs(combined, num_layers, seq_length, directions):
    hx = combined[0]
    xs = combined[1: 1 + seq_length]
    ws = []
    bs = []
    index = 1 + seq_length
    for n in range(num_layers):
        ws.append(combined[index: index + 2])
        bs.append(combined[index + 2: index + 4])
        index += 4
        if directions == 2:
            ws.append(combined[index: index + 2])
            bs.append(combined[index + 2: index + 4])
            index += 4
    return hx, ws, bs, xs


</source>
</class>

<class classid="16" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.8.1/chainer/utils/conv_nd_kernel.py" startline="98" endline="112" pcid="2519">
    def _compile_out_x(self, ndim, outs):
        # 2D: int out_x0 = i / (out_1) % out_0;
        #     int out_x1 = i % out_1;
        def aux(out_x, xs):
            head = xs[0]
            tail = xs[1:]
            if tail:
                return 'int {} = i / ({}) % {};'.format(
                    out_x, mulexp(tail), head)
            else:
                return 'int {} = i % {};'.format(out_x, head)
        out_xs = vars('out_x', ndim)
        out_x_decls = map_(aux, out_xs, succ_sublists(outs))
        return out_x_decls, out_xs

</source>
<source file="systems/chainer-7.8.1/chainer/utils/conv_nd_kernel.py" startline="199" endline="213" pcid="2530">
    def _compile_x(self, ndim, ds):
        # 2D: int x_0 = i / (d_1) % d_0;
        #     int x_1 = i % d_1;
        def aux(x, ds):
            head = ds[0]
            tail = ds[1:]
            if tail:
                return 'int {} = i / ({}) % {};'.format(
                    x, mulexp(tail), head)
            else:
                return 'int {} = i % {};'.format(x, head)
        xs = vars('x', ndim)
        x_decls = map_(aux, xs, succ_sublists(ds))
        return x_decls, xs

</source>
</class>

<class classid="17" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/chainer/datasets/kuzushiji_mnist.py" startline="30" endline="79" pcid="2582">
def get_kuzushiji_mnist(withlabel=True, ndim=1, scale=1., dtype=None,
                        label_dtype=numpy.int32, rgb_format=False):
    """Gets the Kuzushiji-MNIST dataset.

    `Kuzushiji-MNIST (KMNIST) <http://codh.rois.ac.jp/kmnist/>`_ is a set of
    hand-written Japanese characters represented by grey-scale 28x28 images.
    In the original images, each pixel is represented by one-byte unsigned
    integer. This function scales the pixels to floating point values in the
    interval ``[0, scale]``.

    This function returns the training set and the test set of the official
    KMNIST dataset. If ``withlabel`` is ``True``, each dataset consists of
    tuples of images and labels, otherwise it only consists of images.

    Args:
        withlabel (bool): If ``True``, it returns datasets with labels. In this
            case, each example is a tuple of an image and a label. Otherwise,
            the datasets only contain images.
        ndim (int): Number of dimensions of each image. The shape of each image
            is determined depending on ``ndim`` as follows:

            - ``ndim == 1``: the shape is ``(784,)``
            - ``ndim == 2``: the shape is ``(28, 28)``
            - ``ndim == 3``: the shape is ``(1, 28, 28)``

        scale (float): Pixel value scale. If it is 1 (default), pixels are
            scaled to the interval ``[0, 1]``.
        dtype: Data type of resulting image arrays. ``chainer.config.dtype`` is
            used by default (see :ref:`configuration`).
        label_dtype: Data type of the labels.
        rgb_format (bool): if ``ndim == 3`` and ``rgb_format`` is ``True``, the
            image will be converted to rgb format by duplicating the channels
            so the image shape is (3, 28, 28). Default is ``False``.

    Returns:
        A tuple of two datasets. If ``withlabel`` is ``True``, both datasets
        are :class:`~chainer.datasets.TupleDataset` instances. Otherwise, both
        datasets are arrays of images.

    """
    dtype = chainer.get_dtype(dtype)
    train_raw = _retrieve_kuzushiji_mnist_training()
    train = preprocess_mnist(train_raw, withlabel, ndim, scale, dtype,
                             label_dtype, rgb_format)
    test_raw = _retrieve_kuzushiji_mnist_test()
    test = preprocess_mnist(test_raw, withlabel, ndim, scale, dtype,
                            label_dtype, rgb_format)
    return train, test


</source>
<source file="systems/chainer-7.8.1/chainer/datasets/mnist.py" startline="11" endline="59" pcid="2636">
def get_mnist(withlabel=True, ndim=1, scale=1., dtype=None,
              label_dtype=numpy.int32, rgb_format=False):
    """Gets the MNIST dataset.

    `MNIST <http://yann.lecun.com/exdb/mnist/>`_ is a set of hand-written
    digits represented by grey-scale 28x28 images. In the original images, each
    pixel is represented by one-byte unsigned integer. This function
    scales the pixels to floating point values in the interval ``[0, scale]``.

    This function returns the training set and the test set of the official
    MNIST dataset. If ``withlabel`` is ``True``, each dataset consists of
    tuples of images and labels, otherwise it only consists of images.

    Args:
        withlabel (bool): If ``True``, it returns datasets with labels. In this
            case, each example is a tuple of an image and a label. Otherwise,
            the datasets only contain images.
        ndim (int): Number of dimensions of each image. The shape of each image
            is determined depending on ``ndim`` as follows:

            - ``ndim == 1``: the shape is ``(784,)``
            - ``ndim == 2``: the shape is ``(28, 28)``
            - ``ndim == 3``: the shape is ``(1, 28, 28)``

        scale (float): Pixel value scale. If it is 1 (default), pixels are
            scaled to the interval ``[0, 1]``.
        dtype: Data type of resulting image arrays. ``chainer.config.dtype`` is
            used by default (see :ref:`configuration`).
        label_dtype: Data type of the labels.
        rgb_format (bool): if ``ndim == 3`` and ``rgb_format`` is ``True``, the
            image will be converted to rgb format by duplicating the channels
            so the image shape is (3, 28, 28). Default is ``False``.

    Returns:
        A tuple of two datasets. If ``withlabel`` is ``True``, both datasets
        are :class:`~chainer.datasets.TupleDataset` instances. Otherwise, both
        datasets are arrays of images.

    """
    dtype = chainer.get_dtype(dtype)
    train_raw = _retrieve_mnist_training()
    train = preprocess_mnist(train_raw, withlabel, ndim, scale, dtype,
                             label_dtype, rgb_format)
    test_raw = _retrieve_mnist_test()
    test = preprocess_mnist(test_raw, withlabel, ndim, scale, dtype,
                            label_dtype, rgb_format)
    return train, test


</source>
</class>

<class classid="18" nclones="2" nlines="11" similarity="100">
<source file="systems/chainer-7.8.1/chainer/optimizers/adam.py" startline="539" endline="551" pcid="2741">
    def __init__(self,
                 alpha=_default_hyperparam.alpha,
                 beta1=_default_hyperparam.beta1,
                 beta2=_default_hyperparam.beta2,
                 final_lr=_default_hyperparam.final_lr,
                 gamma=_default_hyperparam.gamma,
                 eps=_default_hyperparam.eps,
                 eta=_default_hyperparam.eta):
        super(AdaBound, self).__init__(
            alpha=alpha, beta1=beta1, beta2=beta2, eps=eps, eta=eta,
            amsgrad=False, adabound=True, final_lr=final_lr, gamma=gamma)


</source>
<source file="systems/chainer-7.8.1/chainer/optimizers/adam.py" startline="571" endline="581" pcid="2742">
    def __init__(self,
                 alpha=_default_hyperparam.alpha,
                 beta1=_default_hyperparam.beta1,
                 beta2=_default_hyperparam.beta2,
                 final_lr=_default_hyperparam.final_lr,
                 gamma=_default_hyperparam.gamma,
                 eps=_default_hyperparam.eps,
                 eta=_default_hyperparam.eta):
        super(AMSBound, self).__init__(
            alpha=alpha, beta1=beta1, beta2=beta2, eps=eps, eta=eta,
            amsgrad=True, adabound=True, final_lr=final_lr, gamma=gamma)
</source>
</class>

<class classid="19" nclones="2" nlines="11" similarity="100">
<source file="systems/chainer-7.8.1/chainer/links/caffe/caffe_function.py" startline="581" endline="593" pcid="3394">
def _get_height(blob):
    if blob.height > 0:
        return blob.height
    elif len(blob.shape.dim) == 2:
        return blob.shape.dim[0]
    elif len(blob.shape.dim) == 4:
        return blob.shape.dim[2]
    else:
        raise RuntimeError(
            '{}-dimensional array is not supported'.format(
                len(blob.shape.dim)))


</source>
<source file="systems/chainer-7.8.1/chainer/links/caffe/caffe_function.py" startline="594" endline="609" pcid="3395">
def _get_width(blob):
    if blob.width > 0:
        return blob.width
    elif len(blob.shape.dim) == 2:
        return blob.shape.dim[1]
    elif len(blob.shape.dim) == 4:
        return blob.shape.dim[3]
    else:
        raise RuntimeError(
            '{}-dimensional array is not supported'.format(
                len(blob.shape.dim)))


# Internal class
# __call__ must return Variable or tuple

</source>
</class>

<class classid="20" nclones="3" nlines="18" similarity="100">
<source file="systems/chainer-7.8.1/chainerx_cc/examples/imagenet_py/image_dataset.py" startline="18" endline="45" pcid="3503">
    def get_example(self, i):
        # It reads the i-th image/label pair and return a preprocessed image.
        # It applies following preprocesses:
        #     - Cropping (random or center rectangular)
        #     - Random flip
        #     - Scaling to [0, 1] value
        crop_size = self.crop_size

        image, label = self.base[i]
        _, h, w = image.shape

        if self.random:
            # Randomly crop a region and flip the image
            top = random.randint(0, h - crop_size - 1)
            left = random.randint(0, w - crop_size - 1)
            if random.randint(0, 1):
                image = image[:, :, ::-1]
        else:
            # Crop the center
            top = (h - crop_size) // 2
            left = (w - crop_size) // 2
        bottom = top + crop_size
        right = left + crop_size

        image = image[:, top:bottom, left:right]
        image -= self.mean[:, top:bottom, left:right]
        image *= (1.0 / 255.0)  # Scale to [0, 1]
        return image, label
</source>
<source file="systems/chainer-7.8.1/examples/chainermn/imagenet/train_imagenet.py" startline="49" endline="80" pcid="10448">
    def get_example(self, i):
        # It reads the i-th image/label pair and return a preprocessed image.
        # It applies following preprocesses:
        #     - Cropping (random or center rectangular)
        #     - Random flip
        #     - Scaling to [0, 1] value
        crop_size = self.crop_size

        image, label = self.base[i]
        _, h, w = image.shape

        if self.random:
            # Randomly crop a region and flip the image
            top = random.randint(0, h - crop_size - 1)
            left = random.randint(0, w - crop_size - 1)
            if random.randint(0, 1):
                image = image[:, :, ::-1]
        else:
            # Crop the center
            top = (h - crop_size) // 2
            left = (w - crop_size) // 2
        bottom = top + crop_size
        right = left + crop_size

        image = image[:, top:bottom, left:right]
        image -= self.mean[:, top:bottom, left:right]
        image *= (1.0 / 255.0)  # Scale to [0, 1]
        return image, label


# chainermn.create_multi_node_evaluator can be also used with user customized
# evaluator classes that inherit chainer.training.extensions.Evaluator.
</source>
<source file="systems/chainer-7.8.1/examples/imagenet/dataset_util.py" startline="19" endline="46" pcid="10588">
    def get_example(self, i):
        # It reads the i-th image/label pair and return a preprocessed image.
        # It applies following preprocesses:
        #     - Cropping (random or center rectangular)
        #     - Random flip
        #     - Scaling to [0, 1] value
        crop_size = self.crop_size

        image, label = self.base[i]
        _, h, w = image.shape

        if self.random:
            # Randomly crop a region and flip the image
            top = random.randint(0, h - crop_size - 1)
            left = random.randint(0, w - crop_size - 1)
            if random.randint(0, 1):
                image = image[:, :, ::-1]
        else:
            # Crop the center
            top = (h - crop_size) // 2
            left = (w - crop_size) // 2
        bottom = top + crop_size
        right = left + crop_size

        image = image[:, top:bottom, left:right]
        image -= self.mean[:, top:bottom, left:right]
        image *= (1.0 / 255.0)  # Scale to [0, 1]
        return image, label
</source>
</class>

<class classid="21" nclones="2" nlines="15" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/caffe_tests/test_caffe_function.py" startline="166" endline="184" pcid="3696">
    def test_convolution(self):
        self.init_func()
        self.assertEqual(len(self.func.layers), 1)
        f = self.func.l1
        self.assertIsInstance(f, links.Convolution2D)
        for i in range(3):  # 3 == group
            in_slice = slice(i * 4, (i + 1) * 4)  # 4 == channels
            out_slice = slice(i * 2, (i + 1) * 2)  # 2 == num / group
            w = f.W.data[out_slice, in_slice]
            numpy.testing.assert_array_equal(
                w.flatten(), range(i * 32, (i + 1) * 32))

        numpy.testing.assert_array_equal(
            f.b.data, range(6))

        self.call(['x'], ['y'])
        self.mock.assert_called_once_with(self.inputs[0])


</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/caffe_tests/test_caffe_function.py" startline="219" endline="237" pcid="3697">
    def test_deconvolution(self):
        self.init_func()
        self.assertEqual(len(self.func.layers), 1)
        f = self.func.l1
        self.assertIsInstance(f, links.Deconvolution2D)
        for i in range(3):  # 3 == group
            in_slice = slice(i * 4, (i + 1) * 4)  # 4 == channels
            out_slice = slice(i * 2, (i + 1) * 2)  # 2 == num / group
            w = f.W.data[out_slice, in_slice]
            numpy.testing.assert_array_equal(
                w.flatten(), range(i * 32, (i + 1) * 32))

        numpy.testing.assert_array_equal(
            f.b.data, range(12))

        self.call(['x'], ['y'])
        self.mock.assert_called_once_with(self.inputs[0])


</source>
</class>

<class classid="22" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/rnn_tests/test_link_peephole.py" startline="171" endline="184" pcid="3782">
    def check_to_cpu(self, c, h):
        self.link.c = c
        self.link.h = h
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/connection_tests/test_zoneoutlstm.py" startline="174" endline="187" pcid="4098">

    def check_to_cpu(self, c, h):
        self.link.c = c
        self.link.h = h
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
</source>
</class>

<class classid="23" nclones="2" nlines="23" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/rnn_tests/test_link_peephole.py" startline="194" endline="217" pcid="3785">
    def check_to_cpu_to_gpu(self, c, h):
        self.link.c = c
        self.link.h = h
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/connection_tests/test_zoneoutlstm.py" startline="197" endline="220" pcid="4101">

    def check_to_cpu_to_gpu(self, c, h):
        self.link.c = c
        self.link.h = h
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_cpu()
        self.assertIs(self.link.xp, numpy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        self.assertIs(self.link.xp, cuda.cupy)
        self.assertIsInstance(self.link.c.data, self.link.xp.ndarray)
        self.assertIsInstance(self.link.h.data, self.link.xp.ndarray)
</source>
</class>

<class classid="24" nclones="2" nlines="14" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/normalization_tests/test_decorrelated_batch_normalization.py" startline="43" endline="59" pcid="3944">
def _calc_projection_1group(x, mean, eps):
    spatial_ndim = len(x.shape[2:])
    spatial_axis = tuple(range(2, 2 + spatial_ndim))
    b, C = x.shape[:2]
    m = b
    for i in spatial_axis:
        m *= x.shape[i]

    x_hat = x.transpose((1, 0) + spatial_axis).reshape(C, -1)
    mean = x_hat.mean(axis=1)
    x_hat = x_hat - mean[:, None]
    cov = x_hat.dot(x_hat.T) / m + eps * numpy.eye(C, dtype=x.dtype)
    eigvals, eigvectors = numpy.linalg.eigh(cov)
    projection = eigvectors.dot(numpy.diag(eigvals ** -0.5)).dot(eigvectors.T)
    return projection


</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/normalization_tests/test_decorrelated_batch_normalization.py" startline="37" endline="53" pcid="6729">
def _calc_projection_1group(x, mean, eps):
    spatial_ndim = len(x.shape[2:])
    spatial_axis = tuple(range(2, 2 + spatial_ndim))
    b, C = x.shape[:2]
    m = b
    for i in spatial_axis:
        m *= x.shape[i]

    x_hat = x.transpose((1, 0) + spatial_axis).reshape(C, -1)
    mean = x_hat.mean(axis=1)
    x_hat = x_hat - mean[:, None]
    cov = x_hat.dot(x_hat.T) / m + eps * numpy.eye(C, dtype=x.dtype)
    eigvals, eigvectors = numpy.linalg.eigh(cov)
    projection = eigvectors.dot(numpy.diag(eigvals ** -0.5)).dot(eigvectors.T)
    return projection


</source>
</class>

<class classid="25" nclones="2" nlines="22" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/normalization_tests/test_decorrelated_batch_normalization.py" startline="121" endline="156" pcid="3949">
    def generate_inputs(self):
        dtype = self.dtype
        ndim = self.ndim
        shape = (5, self.n_channels) + (2,) * ndim
        m = 5 * 2 ** ndim

        # NOTE(kataoka): The current implementation uses linalg.eigh. Small
        # eigenvalues of the correlation matrix, which can be as small as
        # eps=2e-5, cannot be computed with good *relative* accuracy, but
        # the eigenvalues are used later as `eigvals ** -0.5`. Require the
        # following is sufficiently large:
        # min(eigvals[:k]) == min(singular_vals ** 2 / m + eps)
        min_singular_value = 0.1
        # NOTE(kataoka): Decorrelated batch normalization should be free from
        # "stochastic axis swapping". Requiring a gap between singular values
        # just hides mistakes in implementations.
        min_singular_value_gap = 0.001
        g = self.groups
        zca_shape = g, self.n_channels // g, m
        x = numpy.random.uniform(-1, 1, zca_shape)
        mean = x.mean(axis=2, keepdims=True)
        a = x - mean
        u, s, vh = numpy.linalg.svd(a, full_matrices=False)
        # Decrement the latter dim because of the constraint `sum(_) == 0`
        k = min(zca_shape[1], zca_shape[2] - 1)
        s[:, :k] += (
            min_singular_value
            + min_singular_value_gap * numpy.arange(k)
        )[::-1]
        a = numpy.einsum('bij,bj,bjk->bik', u, s, vh)
        x = a + mean

        x = x.reshape((self.n_channels, shape[0]) + shape[2:]).swapaxes(0, 1)
        x = x.astype(dtype)
        return x,

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/normalization_tests/test_decorrelated_batch_normalization.py" startline="101" endline="136" pcid="6732">
    def generate_inputs(self):
        dtype = self.dtype
        ndim = self.ndim
        shape = (5, self.n_channels) + (2,) * ndim
        m = 5 * 2 ** ndim

        # NOTE(kataoka): The current implementation uses linalg.eigh. Small
        # eigenvalues of the correlation matrix, which can be as small as
        # eps=2e-5, cannot be computed with good *relative* accuracy, but
        # the eigenvalues are used later as `eigvals ** -0.5`. Require the
        # following is sufficiently large:
        # min(eigvals[:k]) == min(singular_vals ** 2 / m + eps)
        min_singular_value = 0.1
        # NOTE(kataoka): Decorrelated batch normalization should be free from
        # "stochastic axis swapping". Requiring a gap between singular values
        # just hides mistakes in implementations.
        min_singular_value_gap = 0.001
        g = self.groups
        zca_shape = g, self.n_channels // g, m
        x = numpy.random.uniform(-1, 1, zca_shape)
        mean = x.mean(axis=2, keepdims=True)
        a = x - mean
        u, s, vh = numpy.linalg.svd(a, full_matrices=False)
        # Decrement the latter dim because of the constraint `sum(_) == 0`
        k = min(zca_shape[1], zca_shape[2] - 1)
        s[:, :k] += (
            min_singular_value
            + min_singular_value_gap * numpy.arange(k)
        )[::-1]
        a = numpy.einsum('bij,bj,bjk->bik', u, s, vh)
        x = a + mean

        x = x.reshape((self.n_channels, shape[0]) + shape[2:]).swapaxes(0, 1)
        x = x.astype(dtype)
        return x,

</source>
</class>

<class classid="26" nclones="2" nlines="21" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/model_tests/test_vision.py" startline="74" endline="96" pcid="4006">
    def test_prepare(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)
        x3 = numpy.random.uniform(0, 255, (160, 120, 3)).astype(self.dtype)
        x4 = numpy.random.uniform(0, 255, (1, 160, 120)).astype(self.dtype)
        x5 = numpy.random.uniform(0, 255, (3, 160, 120)).astype(numpy.uint8)

        y1 = resnet.prepare(x1)
        assert y1.shape == (3, 224, 224)
        assert y1.dtype == self.dtype
        y2 = resnet.prepare(x2)
        assert y2.shape == (3, 224, 224)
        assert y2.dtype == self.dtype
        y3 = resnet.prepare(x3, size=None)
        assert y3.shape == (3, 160, 120)
        assert y3.dtype == self.dtype
        y4 = resnet.prepare(x4)
        assert y4.shape == (3, 224, 224)
        assert y4.dtype == self.dtype
        y5 = resnet.prepare(x5, size=None)
        assert y5.shape == (3, 160, 120)
        assert y5.dtype == self.dtype

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/model_tests/test_vision.py" startline="216" endline="238" pcid="4022">
    def test_prepare(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)
        x3 = numpy.random.uniform(0, 255, (160, 120, 3)).astype(self.dtype)
        x4 = numpy.random.uniform(0, 255, (1, 160, 120)).astype(self.dtype)
        x5 = numpy.random.uniform(0, 255, (3, 160, 120)).astype(numpy.uint8)

        y1 = vgg.prepare(x1)
        assert y1.shape == (3, 224, 224)
        assert y1.dtype == self.dtype
        y2 = vgg.prepare(x2)
        assert y2.shape == (3, 224, 224)
        assert y2.dtype == self.dtype
        y3 = vgg.prepare(x3, size=None)
        assert y3.shape == (3, 160, 120)
        assert y3.dtype == self.dtype
        y4 = vgg.prepare(x4)
        assert y4.shape == (3, 224, 224)
        assert y4.dtype == self.dtype
        y5 = vgg.prepare(x5, size=None)
        assert y5.shape == (3, 160, 120)
        assert y5.dtype == self.dtype

</source>
</class>

<class classid="27" nclones="2" nlines="17" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/model_tests/test_vision.py" startline="239" endline="257" pcid="4023">
    def check_extract(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)
        result = self.link.extract([x1, x2], layers=['pool3', 'fc7'])
        assert len(result) == 2
        y1 = cuda.to_cpu(result['pool3'].data)
        assert y1.shape == (2, 256, 28, 28)
        assert y1.dtype == self.dtype
        y2 = cuda.to_cpu(result['fc7'].data)
        assert y2.shape == (2, 4096)
        assert y2.dtype == self.dtype

        x3 = numpy.random.uniform(0, 255, (80, 60)).astype(numpy.uint8)
        result = self.link.extract([x3], layers=['pool1'], size=None)
        assert len(result) == 1
        y3 = cuda.to_cpu(result['pool1'].data)
        assert y3.shape == (1, 64, 40, 30)
        assert y3.dtype == self.dtype

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/model_tests/test_vision.py" startline="383" endline="402" pcid="4041">
    def check_extract(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)

        result = self.link.extract([x1, x2], layers=['pool5', 'loss3_fc'])
        assert len(result) == 2
        y1 = cuda.to_cpu(result['pool5'].data)
        assert y1.shape == (2, 1024, 1, 1)
        assert y1.dtype == self.dtype
        y2 = cuda.to_cpu(result['loss3_fc'].data)
        assert y2.shape == (2, 1000)
        assert y2.dtype == self.dtype

        x3 = numpy.random.uniform(0, 255, (80, 60)).astype(numpy.uint8)
        result = self.link.extract([x3], layers=['pool1'], size=None)
        assert len(result) == 1
        y3 = cuda.to_cpu(result['pool1'].data)
        assert y3.shape == (1, 64, 20, 15)
        assert y3.dtype == self.dtype

</source>
</class>

<class classid="28" nclones="2" nlines="11" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/model_tests/test_vision.py" startline="267" endline="278" pcid="4026">
    def check_predict(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)
        result = self.link.predict([x1, x2], oversample=False)
        y = cuda.to_cpu(result.data)
        assert y.shape == (2, 1000)
        assert y.dtype == self.dtype
        result = self.link.predict([x1, x2], oversample=True)
        y = cuda.to_cpu(result.data)
        assert y.shape == (2, 1000)
        assert y.dtype == self.dtype

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/model_tests/test_vision.py" startline="414" endline="426" pcid="4044">
    def check_predict(self):
        x1 = numpy.random.uniform(0, 255, (320, 240, 3)).astype(numpy.uint8)
        x2 = numpy.random.uniform(0, 255, (320, 240)).astype(numpy.uint8)

        result = self.link.predict([x1, x2], oversample=False)
        y = cuda.to_cpu(result.data)
        assert y.shape == (2, 1000)
        assert y.dtype == self.dtype
        result = self.link.predict([x1, x2], oversample=True)
        y = cuda.to_cpu(result.data)
        assert y.shape == (2, 1000)
        assert y.dtype == self.dtype

</source>
</class>

<class classid="29" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/connection_tests/test_scale.py" startline="100" endline="111" pcid="4199">
    def test_backward_gpu(self):
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        x = cuda.to_gpu(self.x)
        if self.learn_W:
            W = None
        else:
            W = cuda.to_gpu(self.W)
        gy = cuda.to_gpu(self.gy)
        self.check_backward(x, W, gy)


</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/connection_tests/test_bias.py" startline="86" endline="97" pcid="4323">
    def test_backward_gpu(self):
        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        x = cuda.to_gpu(self.x)
        if self.learn_b:
            b = None
        else:
            b = cuda.to_gpu(self.b)
        gy = cuda.to_gpu(self.gy)
        self.check_backward(x, b, gy)


</source>
</class>

<class classid="30" nclones="3" nlines="17" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/connection_tests/test_convolution_2d.py" startline="94" endline="115" pcid="4210">
    def test_pickling(self, backend_config):
        x_data, = self.generate_inputs()

        link = self.create_link(self.generate_params())
        link.to_device(backend_config.device)

        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)

        y = link(x)
        y_data1 = y.data
        del x, y
        pickled = pickle.dumps(link, -1)
        del link
        link = pickle.loads(pickled)
        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)
        y = link(x)
        y_data2 = y.data

        testing.assert_allclose(y_data1, y_data2, atol=0, rtol=0)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/connection_tests/test_convolution_nd.py" startline="106" endline="127" pcid="4272">
    def test_pickling(self, backend_config):
        x_data, = self.generate_inputs()

        link = self.create_link(self.generate_params())
        link.to_device(backend_config.device)

        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)

        y = link(x)
        y_data1 = y.data
        del x, y
        pickled = pickle.dumps(link, -1)
        del link
        link = pickle.loads(pickled)
        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)
        y = link(x)
        y_data2 = y.data

        testing.assert_allclose(y_data1, y_data2, atol=0, rtol=0)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/connection_tests/test_convolution_2d.py" startline="213" endline="235" pcid="4220">
    def test_pickling(self, backend_config):
        x_data, = self.generate_inputs()

        link = self.create_link(self.generate_params())
        link.to_device(backend_config.device)

        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)

        y = link(x)
        y_data1 = y.data
        del x, y
        pickled = pickle.dumps(link, -1)
        del link
        link = pickle.loads(pickled)
        x = chainer.Variable(x_data)
        x.to_device(backend_config.device)
        y = link(x)
        y_data2 = y.data

        testing.assert_allclose(y_data1, y_data2, atol=0, rtol=0)


</source>
</class>

<class classid="31" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/connection_tests/test_dilated_convolution_2d.py" startline="45" endline="57" pcid="4230">
    def check_forward_consistency(self):
        x_cpu = chainer.Variable(self.x)
        y_cpu = self.link(x_cpu)
        self.assertEqual(y_cpu.data.dtype, numpy.float32)

        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        y_gpu = self.link(x_gpu)
        self.assertEqual(y_gpu.data.dtype, numpy.float32)

        testing.assert_allclose(y_cpu.data, y_gpu.data.get())

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/connection_tests/test_dilated_convolution_2d.py" startline="147" endline="159" pcid="4243">
    def check_forward_consistency(self):
        x_cpu = chainer.Variable(self.x)
        y_cpu = self.link(x_cpu)
        self.assertEqual(y_cpu.data.dtype, numpy.float32)

        with testing.assert_warns(DeprecationWarning):
            self.link.to_gpu()
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        y_gpu = self.link(x_gpu)
        self.assertEqual(y_gpu.data.dtype, numpy.float32)

        testing.assert_allclose(y_cpu.data, y_gpu.data.get())

</source>
</class>

<class classid="32" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/connection_tests/test_dilated_convolution_2d.py" startline="88" endline="104" pcid="4237">
    def check_pickling(self, x_data):
        x = chainer.Variable(x_data)
        y = self.link(x)
        y_data1 = y.data

        del x, y

        pickled = pickle.dumps(self.link, -1)
        del self.link
        self.link = pickle.loads(pickled)

        x = chainer.Variable(x_data)
        y = self.link(x)
        y_data2 = y.data

        testing.assert_allclose(y_data1, y_data2, atol=0, rtol=0)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/links_tests/connection_tests/test_dilated_convolution_2d.py" startline="190" endline="206" pcid="4250">
    def check_pickling(self, x_data):
        x = chainer.Variable(x_data)
        y = self.link(x)
        y_data1 = y.data

        del x, y

        pickled = pickle.dumps(self.link, -1)
        del self.link
        self.link = pickle.loads(pickled)

        x = chainer.Variable(x_data)
        y = self.link(x)
        y_data2 = y.data

        testing.assert_allclose(y_data1, y_data2, atol=0, rtol=0)

</source>
</class>

<class classid="33" nclones="2" nlines="15" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/test_link.py" startline="244" endline="259" pcid="5398">

    def test_copy_with_share_mode(self):
        link = self.link.copy(mode='share')
        self.assertIsInstance(link._params, set)
        self.assertIsInstance(link._persistent, set)
        self.assertTrue(hasattr(link, 'x'))
        self.assertTrue(hasattr(link, 'y'))
        self.assertTrue(hasattr(link, 'u'))
        self.assertTrue(hasattr(link, 'p'))
        self.assertIsNot(link.x, self.link.x)
        self.assertIs(link.x.array, self.link.x.array)
        self.assertIsNot(link.y, self.link.y)
        self.assertIs(link.y.array, self.link.y.array)
        self.assertIsNone(link.u.array)
        self.assertIs(link.p, self.link.p)
        self.assertIs(link.name, None)
</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/test_link.py" startline="260" endline="275" pcid="5399">

    def test_copy_with_copy_mode(self):
        link = self.link.copy(mode='copy')
        self.assertIsInstance(link._params, set)
        self.assertIsInstance(link._persistent, set)
        self.assertTrue(hasattr(link, 'x'))
        self.assertTrue(hasattr(link, 'y'))
        self.assertTrue(hasattr(link, 'u'))
        self.assertTrue(hasattr(link, 'p'))
        self.assertIsNot(link.x, self.link.x)
        self.assertIsNot(link.x.array, self.link.x.array)
        self.assertIsNot(link.y, self.link.y)
        self.assertIsNot(link.y.array, self.link.y.array)
        self.assertIsNone(link.u.array)
        self.assertIsNot(link.p, self.link.p)
        self.assertIsNot(link.name, None)
</source>
</class>

<class classid="34" nclones="2" nlines="14" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/test_link.py" startline="815" endline="836" pcid="5439">

    def test_serialize(self, backend_config):
        call_record = []

        def serializer(key, value):
            call_record.append((key, value))
            return value

        l = chainer.Link()
        with l.init_scope():
            l.x = chainer.Parameter()  # uninitialized
        l.to_device(backend_config.device)

        l.serialize(serializer)

        # Link is kept uninitialized
        self.assertIsNone(l.x.array)

        # Check inputs to the serializer
        self.assertEqual(len(call_record), 1)
        self.assertEqual(call_record[0][0], 'x')
        self.assertIs(call_record[0][1], None)
</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/test_link.py" startline="837" endline="860" pcid="5441">

    def test_deserialize(self, backend_config):
        # Deserializes uninitialized parameters into uninitialied ones.
        call_record = []

        def serializer(key, value):
            call_record.append((key, value))
            return None  # to be uninitialized

        l = chainer.Link()
        with l.init_scope():
            l.x = chainer.Parameter()  # uninitialized
        l.to_device(backend_config.device)

        l.serialize(serializer)

        # Link is kept uninitialized
        self.assertIsNone(l.x.array)

        # Check inputs to the serializer
        self.assertEqual(len(call_record), 1)
        self.assertEqual(call_record[0][0], 'x')
        self.assertIs(call_record[0][1], None)

</source>
</class>

<class classid="35" nclones="2" nlines="15" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/test_link.py" startline="2272" endline="2292" pcid="5549">

    def setUp(self):
        self.link = chainer.Link()
        shape = (2, 2)
        dtype = numpy.float32
        y_array = numpy.random.rand(*shape).astype(dtype)
        pa_array = numpy.random.rand(*shape).astype(dtype)
        ps_scalar = 2.4

        with self.link.init_scope():
            # Initialized parameter
            self.link.y = chainer.Parameter(y_array)
            # Uninitialized parameter
            self.link.v = chainer.Parameter()
            # Persistent ndarray
            self.link.add_persistent('pa', pa_array)
            # Persistent scalar
            self.link.add_persistent('ps', ps_scalar)
        self.y_array = y_array
        self.pa_array = pa_array
        self.ps_scalar = ps_scalar
</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/test_link.py" startline="2426" endline="2446" pcid="5557">

    def setUp(self):
        self.link = chainer.Link()
        shape = (2, 2)
        dtype = numpy.float32
        y_array = numpy.random.rand(*shape).astype(dtype)
        pa_array = numpy.random.rand(*shape).astype(dtype)
        ps_scalar = 2.4

        with self.link.init_scope():
            # Initialized parameter
            self.link.y = chainer.Parameter(y_array)
            # Uninitialized parameter
            self.link.v = chainer.Parameter()
            # Persistent ndarray
            self.link.add_persistent('pa', pa_array)
            # Persistent scalar
            self.link.add_persistent('ps', ps_scalar)
        self.y_array = y_array
        self.pa_array = pa_array
        self.ps_scalar = ps_scalar
</source>
</class>

<class classid="36" nclones="2" nlines="11" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/test_function.py" startline="114" endline="127" pcid="5731">
    def check_check_type_forward(self):
        self.assertEqual(self.f.check_type_forward.call_count, 1)
        ts = self.f.check_type_forward.call_args[0][0]
        self.assertIsInstance(ts, type_check.LightTypeInfoTuple)
        self.assertEqual(len(ts), 2)

        t1 = ts[0]
        assert t1.shape == self.x_shape
        assert t1.dtype == numpy.float32

        t2 = ts[1]
        assert t2.shape == self.x_shape
        assert t2.dtype == numpy.int32

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/test_function_node.py" startline="116" endline="129" pcid="8752">
    def check_check_type_forward(self):
        self.assertEqual(self.f.check_type_forward.call_count, 1)
        ts = self.f.check_type_forward.call_args[0][0]
        self.assertIsInstance(ts, type_check.LightTypeInfoTuple)
        self.assertEqual(len(ts), 2)

        t1 = ts[0]
        assert t1.shape == self.x_shape
        assert t1.dtype == numpy.float32

        t2 = ts[1]
        assert t2.shape == self.x_shape
        assert t2.dtype == numpy.int32

</source>
</class>

<class classid="37" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/test_function.py" startline="439" endline="452" pcid="5766">
    def test_force_backprop_mode(self):
        with chainer.no_backprop_mode():
            with chainer.force_backprop_mode():
                y = self.x + 1
        self.assertTrue(y.creator_node is not None)

        y = self.x + 1
        self.assertTrue(y.creator_node is not None)

        with chainer.force_backprop_mode():
            y = self.x + 1
        self.assertTrue(y.creator_node is not None)


</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/test_function_node.py" startline="680" endline="692" pcid="8817">
    def test_force_backprop_mode(self):
        with chainer.no_backprop_mode():
            with chainer.force_backprop_mode():
                y = self.x + 1
        self.assertTrue(y.creator_node is not None)

        y = self.x + 1
        self.assertTrue(y.creator_node is not None)

        with chainer.force_backprop_mode():
            y = self.x + 1
        self.assertTrue(y.creator_node is not None)

</source>
</class>

<class classid="38" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/dataset_tests/tabular_tests/test_asmode.py" startline="15" endline="26" pcid="5804">
    def test_astuple(self):
        dataset = dummy_dataset.DummyDataset(mode=self.mode, convert=True)
        view = dataset.astuple()
        self.assertIsInstance(view, chainer.dataset.TabularDataset)
        self.assertEqual(len(view), len(dataset))
        self.assertEqual(view.keys, dataset.keys)
        self.assertEqual(view.mode, tuple)
        self.assertEqual(
            view.get_examples(None, None), dataset.get_examples(None, None))
        self.assertEqual(view.convert(view.fetch()), 'converted')


</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/dataset_tests/tabular_tests/test_asmode.py" startline="34" endline="45" pcid="5805">
    def test_asdict(self):
        dataset = dummy_dataset.DummyDataset(mode=self.mode, convert=True)
        view = dataset.asdict()
        self.assertIsInstance(view, chainer.dataset.TabularDataset)
        self.assertEqual(len(view), len(dataset))
        self.assertEqual(view.keys, dataset.keys)
        self.assertEqual(view.mode, dict)
        self.assertEqual(
            view.get_examples(None, None), dataset.get_examples(None, None))
        self.assertEqual(view.convert(view.fetch()), 'converted')


</source>
</class>

<class classid="39" nclones="2" nlines="17" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/backends_tests/test_cuda.py" startline="262" endline="281" pcid="6127">
    def test_numpy_scalar(self):
        dtype = self.dtype
        if dtype is numpy.bool_:
            x = dtype(True)
        elif issubclass(dtype, numpy.complex_):
            x = dtype(3.2 - 2.4j)
        elif issubclass(dtype, numpy.integer):
            x = dtype(3)
        elif issubclass(dtype, numpy.floating):
            x = dtype(3.2)
        else:
            assert False

        y = cuda.to_cpu(x)
        assert isinstance(y, numpy.ndarray)
        assert y.shape == ()
        assert y.dtype == dtype
        assert y == x


</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/backends_tests/test_cuda.py" startline="451" endline="470" pcid="6146">
    def test_numpy_scalar(self):
        dtype = self.dtype
        if dtype is numpy.bool_:
            x = dtype(True)
        elif issubclass(dtype, numpy.complex_):
            x = dtype(3.2 - 2.4j)
        elif issubclass(dtype, numpy.integer):
            x = dtype(3)
        elif issubclass(dtype, numpy.floating):
            x = dtype(3.2)
        else:
            assert False

        y = cuda.to_gpu(x)
        assert isinstance(y, cuda.ndarray)
        assert y.shape == ()
        assert y.dtype == dtype
        assert y == x


</source>
</class>

<class classid="40" nclones="2" nlines="15" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/distributions_tests/test_cauchy.py" startline="43" endline="59" pcid="6257">
    def check_mean(self, is_gpu):
        with testing.assert_warns(RuntimeWarning):
            if is_gpu:
                mean1 = self.gpu_dist.mean.data
            else:
                mean1 = self.cpu_dist.mean.data

        if self.scipy_onebyone:
            mean2 = []
            for one_params in self.scipy_onebyone_params_iter():
                mean2.append(self.scipy_dist.mean(**one_params))
            mean2 = numpy.vstack(mean2).reshape(
                self.shape + self.cpu_dist.event_shape)
        else:
            mean2 = self.scipy_dist.mean(**self.scipy_params)
        array.assert_allclose(mean1, mean2)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/distributions_tests/test_cauchy.py" startline="88" endline="104" pcid="6263">
    def check_variance(self, is_gpu):
        with testing.assert_warns(RuntimeWarning):
            if is_gpu:
                variance1 = self.gpu_dist.variance.data
            else:
                variance1 = self.cpu_dist.variance.data

        if self.scipy_onebyone:
            variance2 = []
            for one_params in self.scipy_onebyone_params_iter():
                variance2.append(self.scipy_dist.var(**one_params))
            variance2 = numpy.vstack(variance2).reshape(
                self.shape + self.cpu_dist.event_shape)
        else:
            variance2 = self.scipy_dist.var(**self.scipy_params)
        array.assert_allclose(variance1, variance2)

</source>
</class>

<class classid="41" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/distributions_tests/test_cauchy.py" startline="67" endline="80" pcid="6260">
    def check_sample(self, is_gpu):
        if is_gpu:
            smp1 = self.gpu_dist.sample(
                sample_shape=(100000,)+self.sample_shape).data
            smp1 = cuda.to_cpu(smp1)
        else:
            smp1 = self.cpu_dist.sample(
                sample_shape=(100000,)+self.sample_shape).data
        smp2 = self.scipy_dist.rvs(
            size=(100000,)+self.sample_shape+self.shape, **self.scipy_params)
        testing.assert_allclose(numpy.median(smp1, axis=0),
                                numpy.median(smp2, axis=0),
                                atol=3e-2, rtol=3e-2)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/distributions_tests/test_pareto.py" startline="43" endline="56" pcid="6268">
    def check_sample(self, is_gpu):
        if is_gpu:
            smp1 = self.gpu_dist.sample(
                sample_shape=(100000,)+self.sample_shape).data
            smp1 = cuda.to_cpu(smp1)
        else:
            smp1 = self.cpu_dist.sample(
                sample_shape=(100000,)+self.sample_shape).data
        smp2 = self.scipy_dist.rvs(
            size=(100000,)+self.sample_shape+self.shape, **self.scipy_params)
        testing.assert_allclose(numpy.median(smp1, axis=0),
                                numpy.median(smp2, axis=0),
                                atol=3e-2, rtol=3e-2)

</source>
</class>

<class classid="42" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/training_tests/extensions_tests/test_polynomial_shift.py" startline="28" endline="40" pcid="6401">
    def setUp(self):
        self.optimizer = mock.MagicMock()
        self.extension = extensions.PolynomialShift(
            'x', self.rate, self.max_count, self.init, self.target,
            self.optimizer)

        self.interval = 4
        self.expect = [e for e in self.expect for _ in range(self.interval)]
        self.trigger = util.get_trigger((self.interval, 'iteration'))

        self.trainer = testing.get_trainer_with_mock_updater(self.trigger)
        self.trainer.updater.get_optimizer.return_value = self.optimizer

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/training_tests/extensions_tests/test_inverse_shift.py" startline="26" endline="38" pcid="6424">
    def setUp(self):
        self.optimizer = mock.MagicMock()
        self.extension = extensions.InverseShift(
            'x', self.gamma, self.power, self.init, self.target,
            self.optimizer)

        self.interval = 4
        self.expect = [e for e in self.expect for _ in range(self.interval)]
        self.trigger = util.get_trigger((self.interval, 'iteration'))

        self.trainer = testing.get_trainer_with_mock_updater(self.trigger)
        self.trainer.updater.get_optimizer.return_value = self.optimizer

</source>
</class>

<class classid="43" nclones="5" nlines="11" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/training_tests/extensions_tests/test_polynomial_shift.py" startline="41" endline="54" pcid="6402">
    def _run_trainer(self, extension, expect, optimizer=None):
        if optimizer is None:
            optimizer = self.optimizer
        extension.initialize(self.trainer)

        actual = []
        for _ in expect:
            self.trainer.updater.update()
            actual.append(optimizer.x)
            if self.trigger(self.trainer):
                extension(self.trainer)

        self.assertEqual(actual, expect)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/training_tests/extensions_tests/test_step_shift.py" startline="38" endline="51" pcid="6469">
    def _run_trainer(self, extension, expect, optimizer=None):
        if optimizer is None:
            optimizer = self.optimizer
        extension.initialize(self.trainer)

        actual = []
        for _ in expect:
            self.trainer.updater.update()
            actual.append(optimizer.x)
            if self.trigger(self.trainer):
                extension(self.trainer)

        self.assertEqual(actual, expect)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/training_tests/extensions_tests/test_exponential_shift.py" startline="32" endline="45" pcid="6414">
    def _run_trainer(self, extension, expect, optimizer=None):
        if optimizer is None:
            optimizer = self.optimizer
        extension.initialize(self.trainer)

        actual = []
        for _ in expect:
            self.trainer.updater.update()
            actual.append(optimizer.x)
            if self.trigger(self.trainer):
                extension(self.trainer)

        self.assertEqual(actual, expect)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/training_tests/extensions_tests/test_linear_shift.py" startline="27" endline="40" pcid="6439">
    def _run_trainer(self, extension, expect, optimizer=None):
        if optimizer is None:
            optimizer = self.optimizer
        extension.initialize(self.trainer)

        actual = []
        for _ in expect:
            self.trainer.updater.update()
            actual.append(optimizer.x)
            if self.trigger(self.trainer):
                extension(self.trainer)

        self.assertEqual(actual, expect)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/training_tests/extensions_tests/test_inverse_shift.py" startline="39" endline="52" pcid="6425">
    def _run_trainer(self, extension, expect, optimizer=None):
        if optimizer is None:
            optimizer = self.optimizer
        extension.initialize(self.trainer)

        actual = []
        for _ in expect:
            self.trainer.updater.update()
            actual.append(optimizer.x)
            if self.trigger(self.trainer):
                extension(self.trainer)

        self.assertEqual(actual, expect)

</source>
</class>

<class classid="44" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/training_tests/extensions_tests/test_inverse_shift.py" startline="73" endline="89" pcid="6429">
    def test_resume(self):
        new_optimizer = mock.Mock()
        new_extension = extensions.InverseShift(
            'x', self.gamma, self.power, self.init, self.target, new_optimizer)

        self.trainer.extend(self.extension)
        self.trainer.run()

        new_trainer = testing.get_trainer_with_mock_updater((3, 'iteration'))
        new_trainer.extend(new_extension)
        testing.save_and_load_npz(self.trainer, new_trainer)

        new_extension.initialize(new_trainer)
        self.assertEqual(new_optimizer.x, self.optimizer.x)
        self.assertIsInstance(new_optimizer.x, float)


</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/training_tests/extensions_tests/test_step_shift.py" startline="71" endline="87" pcid="6473">
    def test_resume(self):
        new_optimizer = mock.Mock()
        new_extension = extensions.StepShift(
            'x', self.gamma, self.step, self.init, self.target, new_optimizer)

        self.trainer.extend(self.extension)
        self.trainer.run()

        new_trainer = testing.get_trainer_with_mock_updater((5, 'iteration'))
        new_trainer.extend(new_extension)
        testing.save_and_load_npz(self.trainer, new_trainer)

        new_extension.initialize(new_trainer)
        self.assertEqual(new_optimizer.x, self.optimizer.x)
        self.assertIsInstance(new_optimizer.x, float)


</source>
</class>

<class classid="45" nclones="2" nlines="11" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/training_tests/triggers_tests/test_early_stopping_trigger.py" startline="53" endline="67" pcid="6556">
    def test_early_stopping_trigger_with_max_epoch(self):
        key = 'main/loss'
        trigger = triggers.EarlyStoppingTrigger(monitor=key, patience=3,
                                                check_trigger=(1, 'epoch'),
                                                max_trigger=(3, 'epoch'))
        trigger = util.get_trigger(trigger)

        accuracies = [100, 80, 30]
        accuracies = numpy.asarray([
            chainer.Variable(numpy.asarray(acc, dtype=numpy.float32))
            for acc in accuracies])

        expected = [False, False, True]
        _test_trigger(self, trigger, key, accuracies, expected)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/training_tests/triggers_tests/test_early_stopping_trigger.py" startline="68" endline="83" pcid="6557">
    def test_early_stopping_trigger_with_max_iteration(self):
        key = 'main/loss'
        trigger = triggers.EarlyStoppingTrigger(monitor=key, patience=3,
                                                check_trigger=(1, 'epoch'),
                                                max_trigger=(3, 'iteration'))
        trigger = util.get_trigger(trigger)

        accuracies = [100, 80, 30]
        accuracies = numpy.asarray([
            chainer.Variable(numpy.asarray(acc, dtype=numpy.float32))
            for acc in accuracies])

        expected = [False, False, True]
        _test_trigger(self, trigger, key, accuracies, expected)


</source>
</class>

<class classid="46" nclones="2" nlines="15" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/rnn_tests/test_function_lstm.py" startline="77" endline="94" pcid="6567">
    def generate_grad_outputs(self, outputs_template):
        grad_out = []
        c = outputs_template[0]
        h = outputs_template[1]

        c_shape = c.shape
        h_shape = h.shape
        if self.grad_outputs[0] is True:
            grad_out.append(_shaped_random(c_shape, c.dtype))
        else:
            grad_out.append(None)

        if self.grad_outputs[1] is True:
            grad_out.append(_shaped_random(h_shape, h.dtype))
        else:
            grad_out.append(None)
        return tuple(grad_out)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/rnn_tests/test_function_lstm.py" startline="95" endline="113" pcid="6568">
    def generate_grad_grad_inputs(self, inputs_template):
        grad_grad_in = []
        c = inputs_template[0]
        x = inputs_template[1]

        c_shape = c.shape
        x_shape = x.shape
        if self.grad_grad_inputs[0] is True:
            grad_grad_in.append(_shaped_random(c_shape, c.dtype))
        else:
            grad_grad_in.append(None)

        if self.grad_grad_inputs[1] is True:
            grad_grad_in.append(_shaped_random(x_shape, x.dtype))
        else:
            grad_grad_in.append(None)
        return tuple(grad_grad_in)


</source>
</class>

<class classid="47" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_lstm.py" startline="141" endline="153" pcid="6581">

    def process_inputs(self, inputs):
        h = inputs[0]
        c = inputs[1]
        xs = inputs[2: 2 + len(self.batches)]
        ws = []
        bs = []
        index = 2 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 8])
            bs.append(inputs[index + 8: index + 16])
            index += 16
        return h, c, ws, bs, xs
</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="82" endline="94" pcid="9658">
    def process_input(self, inputs):
        h = inputs[0]
        c = inputs[1]
        xs = inputs[2:2 + len(self.batches)]
        ws = []
        bs = []
        index = 2 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 8])
            bs.append(inputs[index + 8: index + 16])
            index += 16
        return h, c, ws, bs, xs

</source>
</class>

<class classid="48" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_lstm.py" startline="154" endline="167" pcid="6582">

    def forward(self, inputs, device):
        h, c, ws, bs, xs = self.process_inputs(inputs)
        if h.array.dtype == numpy.float64:
            with chainer.using_config('use_cudnn', 'never'):
                out = F.n_step_lstm(self.n_layers, 0.0, h, c, ws, bs, xs)
        else:
            out = F.n_step_lstm(self.n_layers, 0.0, h, c, ws, bs, xs)
        rets = []
        rets.append(out[0])
        rets.append(out[1])
        for i in range(len(out[2])):
            rets.append(out[2][i])
        return tuple(rets)
</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_lstm.py" startline="294" endline="308" pcid="6588">

    def forward(self, inputs, device):
        h, c, ws, bs, xs = self.process_inputs(inputs)
        if h.array.dtype == numpy.float64:
            with chainer.using_config('use_cudnn', 'never'):
                out = F.n_step_bilstm(self.n_layers, 0.0, h, c, ws, bs, xs)
        else:
            out = F.n_step_bilstm(self.n_layers, 0.0, h, c, ws, bs, xs)

        rets = []
        rets.append(out[0])
        rets.append(out[1])
        for i in range(len(out[2])):
            rets.append(out[2][i])
        return tuple(rets)
</source>
</class>

<class classid="49" nclones="2" nlines="14" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_lstm.py" startline="279" endline="293" pcid="6587">

    def process_inputs(self, inputs):
        h = inputs[0]
        c = inputs[1]
        xs = inputs[2:2 + len(self.batches)]
        ws = []
        bs = []
        index = 2 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 8])
            bs.append(inputs[index + 8: index + 16])
            ws.append(inputs[index + 16: index + 24])
            bs.append(inputs[index + 24: index + 32])
            index += 32
        return h, c, ws, bs, xs
</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="190" endline="204" pcid="9664">
    def process_input(self, inputs):
        h = inputs[0]
        c = inputs[1]
        xs = inputs[2:2 + len(self.batches)]
        ws = []
        bs = []
        index = 2 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 8])
            bs.append(inputs[index + 8: index + 16])
            ws.append(inputs[index + 16: index + 24])
            bs.append(inputs[index + 24: index + 32])
            index += 32
        return h, c, ws, bs, xs

</source>
</class>

<class classid="50" nclones="2" nlines="19" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py" startline="62" endline="85" pcid="6596">
    def generate_inputs(self):
        h_shape = (self.n_layers, self.batches[0], self.hidden_size)
        dtype = numpy.float32

        h = array(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            return in_size if i == 0 and j < 3 else out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])
        for n in range(self.n_layers):
            for i in range(6):
                inputs.append(array((out_size, w_in(n, i)), dtype))
            for i in range(6):
                inputs.append(array((out_size,), dtype))
        return tuple(inputs)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_rnn.py" startline="75" endline="98" pcid="6628">
    def generate_inputs(self):
        h_shape = (self.n_layers, self.batches[0], self.hidden_size)
        dtype = self.dtype

        h = array(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            return in_size if i == 0 and j < 1 else out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])
        for n in range(self.n_layers):
            for i in range(2):
                inputs.append(array((out_size, w_in(n, i)), dtype))
            for i in range(2):
                inputs.append(array((out_size,), dtype))
        return tuple(inputs)

</source>
</class>

<class classid="51" nclones="4" nlines="11" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py" startline="86" endline="99" pcid="6598">
    def process_inputs(self, inputs):
        h = inputs[0]

        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 6])
            bs.append(inputs[index + 6: index + 12])
            index += 12

        return h, ws, bs, xs

</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="294" endline="305" pcid="9670">
    def process_input(self, inputs):
        h = inputs[0]
        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 6])
            bs.append(inputs[index + 6: index + 12])
            index += 12
        return h, ws, bs, xs

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_rnn.py" startline="99" endline="112" pcid="6630">
    def process_inputs(self, inputs):
        h = inputs[0]

        xs = inputs[1: 1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 2])
            bs.append(inputs[index + 2: index + 4])
            index += 4

        return h, ws, bs, xs

</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="506" endline="517" pcid="9682">
    def process_input(self, inputs):
        h = inputs[0]
        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 2])
            bs.append(inputs[index + 2: index + 4])
            index += 4
        return h, ws, bs, xs

</source>
</class>

<class classid="52" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py" startline="100" endline="113" pcid="6599">
    def forward(self, inputs, device):
        h, ws, bs, xs = self.process_inputs(inputs)
        if h.array.dtype == numpy.float64:
            with chainer.using_config('use_cudnn', 'never'):
                out = F.n_step_gru(self.n_layers, 0.0, h, ws, bs, xs)
        else:
            out = F.n_step_gru(self.n_layers, 0.0, h, ws, bs, xs)

        rets = []
        rets.append(out[0])
        for i in range(len(out[1])):
            rets.append(out[1][i])
        return tuple(rets)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py" startline="233" endline="245" pcid="6605">
    def forward(self, inputs, device):
        h, ws, bs, xs = self.process_inputs(inputs)
        if h.array.dtype == numpy.float64:
            with chainer.using_config('use_cudnn', 'never'):
                out = F.n_step_bigru(self.n_layers, 0.0, h, ws, bs, xs)
        else:
            out = F.n_step_bigru(self.n_layers, 0.0, h, ws, bs, xs)
        rets = []
        rets.append(out[0])
        for i in range(len(out[1])):
            rets.append(out[1][i])
        return tuple(rets)

</source>
</class>

<class classid="53" nclones="2" nlines="25" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py" startline="188" endline="218" pcid="6602">
    def generate_inputs(self):
        h_shape = (self.n_layers * 2, self.batches[0], self.hidden_size)
        dtype = numpy.float32

        h = array(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            if i == 0 and j < 3:
                return in_size
            elif i > 0 and j < 3:
                return out_size * 2
            else:
                return out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for direction in (0, 1):
                for i in range(6):
                    inputs.append(array((out_size, w_in(n, i)), dtype))
                for i in range(6):
                    inputs.append(array((out_size,), dtype))
        return tuple(inputs)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_rnn.py" startline="200" endline="230" pcid="6634">
    def generate_inputs(self):
        h_shape = (self.n_layers * 2, self.batches[0], self.hidden_size)
        dtype = self.dtype

        h = array(h_shape, dtype)
        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            if i == 0 and j < 1:
                return in_size
            elif i > 0 and j < 1:
                return out_size * 2
            else:
                return out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for direction in (0, 1):
                for i in range(2):
                    inputs.append(array((out_size, w_in(n, i)), dtype))
                for i in range(2):
                    inputs.append(array((out_size,), dtype))
        return tuple(inputs)

</source>
</class>

<class classid="54" nclones="4" nlines="13" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_gru.py" startline="219" endline="232" pcid="6604">
    def process_inputs(self, inputs):
        h = inputs[0]
        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 6])
            bs.append(inputs[index + 6: index + 12])
            ws.append(inputs[index + 12: index + 18])
            bs.append(inputs[index + 18: index + 24])
            index += 24
        return h, ws, bs, xs

</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="397" endline="410" pcid="9676">
    def process_input(self, inputs):
        h = inputs[0]
        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 6])
            bs.append(inputs[index + 6: index + 12])
            ws.append(inputs[index + 12: index + 18])
            bs.append(inputs[index + 18: index + 24])
            index += 24
        return h, ws, bs, xs

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/rnn_tests/test_function_n_step_rnn.py" startline="231" endline="244" pcid="6636">
    def process_inputs(self, inputs):
        h = inputs[0]
        xs = inputs[1: 1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 2])
            bs.append(inputs[index + 2: index + 4])
            ws.append(inputs[index + 4: index + 6])
            bs.append(inputs[index + 6: index + 8])
            index += 8
        return h, ws, bs, xs

</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="625" endline="638" pcid="9688">
    def process_input(self, inputs):
        h = inputs[0]
        xs = inputs[1:1 + len(self.batches)]
        ws = []
        bs = []
        index = 1 + len(self.batches)
        for n in range(self.n_layers):
            ws.append(inputs[index: index + 2])
            bs.append(inputs[index + 2: index + 4])
            ws.append(inputs[index + 4: index + 6])
            bs.append(inputs[index + 6: index + 8])
            index += 8
        return h, ws, bs, xs

</source>
</class>

<class classid="55" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/math_tests/test_minimum.py" startline="43" endline="55" pcid="7382">
    def setUp(self):
        if self.dtype == numpy.float16:
            eps = 1e-2
            self.check_forward_options.update({'atol': 1e-4, 'rtol': 1e-3})
            self.check_backward_options.update({
                'atol': 1e-2, 'rtol': 1e-2})
            self.check_double_backward_options.update({
                'atol': 1e-2, 'rtol': 1e-2})
        else:
            eps = 1e-3
        self.check_backward_options['eps'] = eps
        self.check_double_backward_options['eps'] = eps

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/math_tests/test_maximum.py" startline="43" endline="55" pcid="7417">
    def setUp(self):
        if self.dtype == numpy.float16:
            eps = 1e-2
            self.check_forward_options.update({'atol': 1e-4, 'rtol': 1e-3})
            self.check_backward_options.update({
                'atol': 1e-2, 'rtol': 1e-2})
            self.check_double_backward_options.update({
                'atol': 1e-2, 'rtol': 1e-2})
        else:
            eps = 1e-3
        self.check_backward_options['eps'] = eps
        self.check_double_backward_options['eps'] = eps

</source>
</class>

<class classid="56" nclones="2" nlines="11" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/math_tests/test_det.py" startline="263" endline="275" pcid="7456">
    def test_answer_gpu_cpu(self):
        x = cuda.to_gpu(self.x)
        y = F.det(chainer.Variable(x))
        gpu = cuda.to_cpu(y.data)
        if self.dtype == numpy.float16:
            cpu = numpy.linalg.det(
                self.x.astype(numpy.float32)).astype(numpy.float16)
            testing.assert_allclose(gpu, cpu, atol=5e-3, rtol=5e-3)
        else:
            cpu = numpy.linalg.det(self.x)
            testing.assert_allclose(gpu, cpu)


</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/math_tests/test_det.py" startline="287" endline="299" pcid="7458">
    def test_answer_gpu_cpu(self):
        x = cuda.to_gpu(self.x)
        y = F.batch_det(chainer.Variable(x))
        gpu = cuda.to_cpu(y.data)
        if self.dtype == numpy.float16:
            cpu = numpy.linalg.det(
                self.x.astype(numpy.float32)).astype(numpy.float16)
            testing.assert_allclose(gpu, cpu, atol=5e-3, rtol=5e-3)
        else:
            cpu = numpy.linalg.det(self.x)
            testing.assert_allclose(gpu, cpu)


</source>
</class>

<class classid="57" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/math_tests/test_inv.py" startline="52" endline="65" pcid="7480">
    def setUp(self):
        if self.dtype == numpy.float16:
            self.check_forward_dtype = numpy.float32
            self.check_forward_options.update({'atol': 1e-3, 'rtol': 1e-3})
            self.check_backward_options.update({'atol': 1e-3, 'rtol': 1e-3})
            self.check_double_backward_options.update({
                'atol': 5e-3, 'rtol': 5e-3})
        else:
            self.check_forward_dtype = self.dtype
            self.check_forward_options.update({'atol': 1e-4, 'rtol': 1e-4})
            self.check_backward_options.update({'atol': 5e-4, 'rtol': 5e-4})
            self.check_double_backward_options.update({
                'atol': 5e-4, 'rtol': 5e-4})

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/math_tests/test_inv.py" startline="112" endline="125" pcid="7485">
    def setUp(self):
        if self.dtype == numpy.float16:
            self.check_forward_dtype = numpy.float32
            self.check_forward_options.update({'atol': 1e-3, 'rtol': 1e-3})
            self.check_backward_options.update({'atol': 2e-3, 'rtol': 2e-3})
            self.check_double_backward_options.update({
                'atol': 5e-3, 'rtol': 5e-3})
        else:
            self.check_forward_dtype = self.dtype
            self.check_forward_options.update({'atol': 1e-4, 'rtol': 1e-4})
            self.check_backward_options.update({'atol': 5e-4, 'rtol': 5e-4})
            self.check_double_backward_options.update({
                'atol': 1e-3, 'rtol': 1e-3})

</source>
</class>

<class classid="58" nclones="2" nlines="35" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/array_tests/test_space_2_depth.py" startline="18" endline="54" pcid="7868">
    def setUp(self):
        self.depth = numpy.arange(96).reshape(2, 8, 3, 2).astype(self.dtype)
        self.space = numpy.array([[[[0.,  12.,   1.,  13.],
                                    [24.,  36.,  25.,  37.],
                                    [2.,  14.,   3.,  15.],
                                    [26.,  38.,  27.,  39.],
                                    [4.,  16.,   5.,  17.],
                                    [28.,  40.,  29.,  41.]],
                                   [[6.,  18.,   7.,  19.],
                                    [30.,  42.,  31.,  43.],
                                    [8.,  20.,   9.,  21.],
                                    [32.,  44.,  33.,  45.],
                                    [10.,  22.,  11.,  23.],
                                    [34.,  46.,  35.,  47.]]],
                                  [[[48.,  60.,  49.,  61.],
                                    [72.,  84.,  73.,  85.],
                                    [50.,  62.,  51.,  63.],
                                    [74.,  86.,  75.,  87.],
                                    [52.,  64.,  53.,  65.],
                                    [76.,  88.,  77.,  89.]],
                                   [[54.,  66.,  55.,  67.],
                                    [78.,  90.,  79.,  91.],
                                    [56.,  68.,  57.,  69.],
                                    [80.,  92.,  81.,  93.],
                                    [58.,  70.,  59.,  71.],
                                    [82.,  94.,  83.,  95.]]]]
                                 ).astype(self.dtype)
        self.x = numpy.random.randn(2, 2, 6, 4).astype(self.dtype)
        self.gy = numpy.random.randn(2, 8, 3, 2).astype(self.dtype)
        self.ggx = numpy.random.randn(2, 2, 6, 4).astype(self.dtype)
        self.r = 2
        self.check_backward_options = {}
        self.check_double_backward_options = {}
        if self.dtype == numpy.float16:
            self.check_backward_options = {'atol': 5e-4, 'rtol': 5e-3}
            self.check_double_backward_options = {'atol': 5e-3, 'rtol': 5e-2}

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/array_tests/test_depth_2_space.py" startline="18" endline="54" pcid="7909">
    def setUp(self):
        self.depth = numpy.arange(96).reshape(2, 8, 3, 2).astype(self.dtype)
        self.space = numpy.array([[[[0.,  12.,   1.,  13.],
                                    [24.,  36.,  25.,  37.],
                                    [2.,  14.,   3.,  15.],
                                    [26.,  38.,  27.,  39.],
                                    [4.,  16.,   5.,  17.],
                                    [28.,  40.,  29.,  41.]],
                                   [[6.,  18.,   7.,  19.],
                                    [30.,  42.,  31.,  43.],
                                    [8.,  20.,   9.,  21.],
                                    [32.,  44.,  33.,  45.],
                                    [10.,  22.,  11.,  23.],
                                    [34.,  46.,  35.,  47.]]],
                                  [[[48.,  60.,  49.,  61.],
                                    [72.,  84.,  73.,  85.],
                                    [50.,  62.,  51.,  63.],
                                    [74.,  86.,  75.,  87.],
                                    [52.,  64.,  53.,  65.],
                                    [76.,  88.,  77.,  89.]],
                                   [[54.,  66.,  55.,  67.],
                                    [78.,  90.,  79.,  91.],
                                    [56.,  68.,  57.,  69.],
                                    [80.,  92.,  81.,  93.],
                                    [58.,  70.,  59.,  71.],
                                    [82.,  94.,  83.,  95.]]]]
                                 ).astype(self.dtype)
        self.x = numpy.random.randn(2, 8, 3, 2).astype(self.dtype)
        self.gy = numpy.random.randn(2, 2, 6, 4).astype(self.dtype)
        self.ggx = numpy.random.randn(2, 8, 3, 2).astype(self.dtype)
        self.r = 2
        self.check_backward_options = {}
        self.check_double_backward_options = {}
        if self.dtype == numpy.float16:
            self.check_backward_options = {'atol': 5e-4, 'rtol': 5e-3}
            self.check_double_backward_options = {'atol': 5e-3, 'rtol': 5e-2}

</source>
</class>

<class classid="59" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py" startline="99" endline="109" pcid="8332">
    def generate_inputs(self):
        W = numpy.random.normal(
            0, self.W_scale, self.W_shape).astype(self.W_dtype)
        x = numpy.random.uniform(-1, 1, self.x_shape).astype(self.x_dtype)
        if self.nobias:
            return x, W
        else:
            b = numpy.random.uniform(
                -1, 1, self.out_channels).astype(self.x_dtype)
            return x, W, b

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py" startline="118" endline="128" pcid="8371">
    def generate_inputs(self):
        W = numpy.random.normal(
            0, self.W_scale, self.W_shape).astype(self.W_dtype)
        x = numpy.random.uniform(-1, 1, self.x_shape).astype(self.x_dtype)
        if self.nobias:
            return x, W
        else:
            b = numpy.random.uniform(
                -1, 1, self.out_channels).astype(self.x_dtype)
            return x, W, b

</source>
</class>

<class classid="60" nclones="2" nlines="11" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py" startline="110" endline="127" pcid="8333">
    def forward_expected(self, inputs):
        """
        Current forward_expected implementation depends on
        F.convolution_nd itself and thus it's only capable
        of checking consistency between backends, not absolute
        correctness of computations
        """
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        y_expected = F.convolution_nd(
            x, W, b, stride=self.stride, pad=self.pad,
            cover_all=self.cover_all, dilate=self.dilate,
            groups=self.groups)
        return y_expected.array,

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py" startline="129" endline="146" pcid="8372">
    def forward_expected(self, inputs):
        """
        Current forward_expected implementation depends on
        F.deconvolution_nd itself and thus it's only capable
        of checking consistency between backends, not absolute
        correctness of computations
        """
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        y_expected = F.deconvolution_nd(
            x, W, b, stride=self.stride, pad=self.pad,
            outsize=self.outsize, dilate=self.dilate,
            groups=self.groups)
        return y_expected.array,

</source>
</class>

<class classid="61" nclones="2" nlines="11" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py" startline="128" endline="139" pcid="8334">
    def forward(self, inputs, device):
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        y = F.convolution_nd(
            x, W, b, stride=self.stride, pad=self.pad,
            cover_all=self.cover_all, dilate=self.dilate,
            groups=self.groups)
        return y,

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py" startline="147" endline="158" pcid="8373">
    def forward(self, inputs, device):
        if self.nobias:
            x, W = inputs
            b = None
        else:
            x, W, b = inputs
        y = F.deconvolution_nd(
            x, W, b, stride=self.stride, pad=self.pad,
            outsize=self.outsize, dilate=self.dilate,
            groups=self.groups)
        return y,

</source>
</class>

<class classid="62" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py" startline="223" endline="234" pcid="8341">
    def setUp(self):
        N = 2
        in_channels = 3
        out_channels = 2
        dtype = numpy.float32

        x_shape = (N, in_channels, 3, 3, 3)
        self.x_data = numpy.random.uniform(-1, 1, x_shape).astype(dtype)
        W_shape = (out_channels, in_channels, 1, 1, 1)
        self.W_data = numpy.random.uniform(-1, 1, W_shape).astype(dtype)
        self.b_data = numpy.random.uniform(-1, 1, out_channels).astype(dtype)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py" startline="243" endline="254" pcid="8380">
    def setUp(self):
        N = 2
        in_channels = 3
        out_channels = 2
        dtype = numpy.float32

        x_shape = (N, in_channels, 3, 3, 3)
        self.x_data = numpy.random.uniform(-1, 1, x_shape).astype(dtype)
        W_shape = (in_channels, out_channels, 1, 1, 1)
        self.W_data = numpy.random.uniform(-1, 1, W_shape).astype(dtype)
        self.b_data = numpy.random.uniform(-1, 1, out_channels).astype(dtype)

</source>
</class>

<class classid="63" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/connection_tests/test_convolution_nd.py" startline="290" endline="302" pcid="8347">
    def _get_data(self, ndim):
        in_channels = 3
        out_channels = 2
        dtype = numpy.float32

        x_shape = (2, in_channels) + (3,) * ndim
        x = numpy.random.uniform(-1, 1, x_shape).astype(dtype)
        W_shape = (out_channels, in_channels) + (1,) * ndim
        W = numpy.random.uniform(-1, 1, W_shape).astype(dtype)
        b = numpy.random.uniform(-1, 1, out_channels).astype(dtype)

        return x, W, b

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/connection_tests/test_deconvolution_nd.py" startline="349" endline="361" pcid="8389">
    def _get_data(self, ndim):
        in_channels = 3
        out_channels = 2
        dtype = numpy.float32

        x_shape = (2, in_channels) + (3,) * ndim
        x = numpy.random.uniform(-1, 1, x_shape).astype(dtype)
        W_shape = (in_channels, out_channels) + (1,) * ndim
        W = numpy.random.uniform(-1, 1, W_shape).astype(dtype)
        b = numpy.random.uniform(-1, 1, out_channels).astype(dtype)

        return x, W, b

</source>
</class>

<class classid="64" nclones="2" nlines="19" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/connection_tests/test_deformable_convolution_2d_sampler.py" startline="28" endline="52" pcid="8352">
    def setUp(self):
        in_channels = 3
        out_channels = 2
        batch_size = 2
        h = 9
        w = 9

        kh, kw, sy, sx, ph, pw = self.params

        self.stride = (sy, sx)
        self.pad = (ph, pw)

        self.W = numpy.random.normal(
            size=(out_channels, in_channels, kh, kw)).astype(numpy.float32)
        self.b = numpy.random.uniform(
            size=(out_channels,)).astype(numpy.float32)

        self.x = numpy.random.uniform(
            size=(batch_size, in_channels, h, w)).astype(numpy.float32)

        out_h = utils.conv.get_conv_outsize(h, kh, sy, ph)
        out_w = utils.conv.get_conv_outsize(w, kw, sx, pw)
        self.offset = numpy.zeros(
            (batch_size, 2 * kh * kw, out_h, out_w), dtype=numpy.float32)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/connection_tests/test_deformable_convolution_2d_sampler.py" startline="91" endline="115" pcid="8356">
    def setUp(self):
        in_channels = 3
        out_channels = 2
        batch_size = 2
        h = 9
        w = 9

        kh, kw, sy, sx, ph, pw = self.params

        self.stride = (sy, sx)
        self.pad = (ph, pw)

        self.W = numpy.random.normal(
            size=(out_channels, in_channels, kh, kw)).astype(numpy.float32)
        self.b = numpy.random.uniform(
            size=(out_channels,)).astype(numpy.float32)

        self.x = numpy.random.uniform(
            size=(batch_size, in_channels, h, w)).astype(numpy.float32)

        out_h = utils.conv.get_conv_outsize(h, kh, sy, ph)
        out_w = utils.conv.get_conv_outsize(w, kw, sx, pw)
        self.offset = numpy.zeros(
            (batch_size, 2 * kh * kw, out_h, out_w), dtype=numpy.float32)

</source>
</class>

<class classid="65" nclones="2" nlines="21" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_pooling_2d.py" startline="30" endline="52" pcid="8479">
    def setUp(self):
        N = 3
        n_channels = 3
        self.x = pooling_nd_helper.shuffled_linspace(
            (N, n_channels, 12, 8), self.dtype)
        self.rois = numpy.array([
            [1, 1, 7, 7],
            [2, 6, 12, 8],
            [1, 3, 11, 6],
            [3, 3, 4, 4]
        ], dtype=self.dtype)
        self.roi_indices = numpy.array([0, 2, 1, 0], dtype=numpy.int32)
        n_rois = self.rois.shape[0]
        outsize = _pair(self.outsize)
        self.gy = numpy.random.uniform(
            -1, 1, (n_rois, n_channels,
                    outsize[0], outsize[1])).astype(self.dtype)
        if self.dtype == numpy.float16:
            self.check_backward_options = {
                'dtype': numpy.float64, 'atol': 1e-2, 'rtol': 1e-2}
        else:
            self.check_backward_options = {'atol': 1e-3, 'rtol': 1e-2}

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_pooling_2d.py" startline="30" endline="52" pcid="8502">
    def setUp(self):
        N = 3
        n_channels = 3
        self.x = pooling_nd_helper.shuffled_linspace(
            (N, n_channels, 12, 8), self.dtype)
        self.rois = numpy.array([
            [1, 1, 7, 7],
            [2, 6, 12, 8],
            [1, 3, 11, 6],
            [3, 3, 4, 4]
        ], dtype=self.dtype)
        self.roi_indices = numpy.array([0, 2, 1, 0], dtype=numpy.int32)
        n_rois = self.rois.shape[0]
        outsize = _pair(self.outsize)
        self.gy = numpy.random.uniform(
            -1, 1, (n_rois, n_channels,
                    outsize[0], outsize[1])).astype(self.dtype)
        if self.dtype == numpy.float16:
            self.check_backward_options = {
                'dtype': numpy.float64, 'atol': 1e-2, 'rtol': 1e-2}
        else:
            self.check_backward_options = {'atol': 1e-3, 'rtol': 1e-2}

</source>
</class>

<class classid="66" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_pooling_2d.py" startline="53" endline="64" pcid="8480">
    def check_forward(self, x_data, roi_data, roi_index_data):
        x = chainer.Variable(x_data)
        rois = chainer.Variable(roi_data)
        roi_indices = chainer.Variable(roi_index_data)
        y = functions.roi_max_pooling_2d(
            x, rois, roi_indices, outsize=self.outsize,
            spatial_scale=self.spatial_scale)
        self.assertEqual(y.data.dtype, self.dtype)
        y_data = cuda.to_cpu(y.data)

        self.assertEqual(self.gy.shape, y_data.shape)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_pooling_2d.py" startline="53" endline="64" pcid="8503">
    def check_forward(self, x_data, roi_data, roi_index_data):
        x = chainer.Variable(x_data)
        rois = chainer.Variable(roi_data)
        roi_indices = chainer.Variable(roi_index_data)
        y = functions.roi_average_pooling_2d(
            x, rois, roi_indices, outsize=self.outsize,
            spatial_scale=self.spatial_scale)
        self.assertEqual(y.data.dtype, self.dtype)
        y_data = cuda.to_cpu(y.data)

        self.assertEqual(self.gy.shape, y_data.shape)

</source>
</class>

<class classid="67" nclones="2" nlines="14" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_pooling_2d.py" startline="75" endline="92" pcid="8483">
    def test_forward_cpu_gpu_equal(self):
        # cpu
        x_cpu = chainer.Variable(self.x)
        rois_cpu = chainer.Variable(self.rois)
        roi_indices_cpu = chainer.Variable(self.roi_indices)
        y_cpu = functions.roi_max_pooling_2d(
            x_cpu, rois_cpu, roi_indices_cpu, outsize=self.outsize,
            spatial_scale=self.spatial_scale)

        # gpu
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        rois_gpu = chainer.Variable(cuda.to_gpu(self.rois))
        roi_indices_gpu = chainer.Variable(cuda.to_gpu(self.roi_indices))
        y_gpu = functions.roi_max_pooling_2d(
            x_gpu, rois_gpu, roi_indices_gpu, outsize=self.outsize,
            spatial_scale=self.spatial_scale)
        testing.assert_allclose(y_cpu.data, cuda.to_cpu(y_gpu.data))

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_pooling_2d.py" startline="75" endline="92" pcid="8506">
    def test_forward_cpu_gpu_equal(self):
        # cpu
        x_cpu = chainer.Variable(self.x)
        rois_cpu = chainer.Variable(self.rois)
        roi_indices_cpu = chainer.Variable(self.roi_indices)
        y_cpu = functions.roi_average_pooling_2d(
            x_cpu, rois_cpu, roi_indices_cpu, outsize=self.outsize,
            spatial_scale=self.spatial_scale)

        # gpu
        x_gpu = chainer.Variable(cuda.to_gpu(self.x))
        rois_gpu = chainer.Variable(cuda.to_gpu(self.rois))
        roi_indices_gpu = chainer.Variable(cuda.to_gpu(self.roi_indices))
        y_gpu = functions.roi_average_pooling_2d(
            x_gpu, rois_gpu, roi_indices_gpu, outsize=self.outsize,
            spatial_scale=self.spatial_scale)
        testing.assert_allclose(y_cpu.data, cuda.to_cpu(y_gpu.data))

</source>
</class>

<class classid="68" nclones="2" nlines="20" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/pooling_tests/test_max_pooling_2d.py" startline="140" endline="162" pcid="8498">
    def _check(self, x):
        out, indices = functions.max_pooling_2d(
            x, 2, cover_all=False, return_indices=True)
        assert isinstance(out, chainer.Variable)
        assert isinstance(out.array, type(x))
        assert isinstance(indices, type(x))
        assert indices.shape == out.array.shape

        # Calculate expected indices.
        expect = numpy.zeros(indices.shape, dtype=indices.dtype)
        for i in six.moves.range(2):
            for c in six.moves.range(3):
                xx = x[i, c]
                expect[i, c] = numpy.array([
                    [xx[0:2, 0:2].ravel().argmax(),
                     xx[0:2, 2:4].ravel().argmax()],
                    [xx[2:4, 0:2].ravel().argmax(),
                     xx[2:4, 2:4].ravel().argmax()],
                ])
        if out.xp is cuda.cupy:
            expect = cuda.to_gpu(expect)
        assert (expect == indices).all()

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/pooling_tests/test_max_pooling_nd.py" startline="187" endline="209" pcid="8597">
    def _check(self, x):
        out, indices = functions.max_pooling_nd(
            x, 2, cover_all=False, return_indices=True)
        assert isinstance(out, chainer.Variable)
        assert isinstance(out.array, type(x))
        assert isinstance(indices, type(x))
        assert indices.shape == out.array.shape

        # Calculate expected indices.
        expect = numpy.zeros(indices.shape, dtype=indices.dtype)
        for i in six.moves.range(2):
            for c in six.moves.range(3):
                xx = x[i, c]
                expect[i, c] = numpy.array([
                    [xx[0:2, 0:2].ravel().argmax(),
                     xx[0:2, 2:4].ravel().argmax()],
                    [xx[2:4, 0:2].ravel().argmax(),
                     xx[2:4, 2:4].ravel().argmax()],
                ])
        if out.xp is cuda.cupy:
            expect = cuda.to_gpu(expect)
        assert (expect == indices).all()

</source>
</class>

<class classid="69" nclones="2" nlines="19" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_align_2d.py" startline="36" endline="55" pcid="8534">
    def setUp(self):
        N = 3
        n_channels = 3
        self.x = pooling_nd_helper.shuffled_linspace(
            (N, n_channels, 12, 8), numpy.float32)
        self.rois = numpy.array([
            [1, 1, 6, 6],
            [2, 6, 11, 7],
            [1, 3, 10, 5],
            [3, 3, 3, 3],
            [1.1, 2.2, 3.3, 4.4],
        ], dtype=numpy.float32)
        self.roi_indices = numpy.array([0, 2, 1, 0, 2], dtype=numpy.int32)
        n_rois = self.rois.shape[0]
        outsize = _pair(self.outsize)
        self.gy = numpy.random.uniform(
            -1, 1, (n_rois, n_channels,
                    outsize[0], outsize[1])).astype(numpy.float32)
        self.check_backward_options = {'atol': 5e-4, 'rtol': 5e-3}

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_align_2d.py" startline="36" endline="55" pcid="8544">
    def setUp(self):
        N = 3
        n_channels = 3
        self.x = pooling_nd_helper.shuffled_linspace(
            (N, n_channels, 12, 8), numpy.float32)
        self.rois = numpy.array([
            [1, 1, 6, 6],
            [6, 2, 7, 11],
            [3, 1, 5, 10],
            [3, 3, 3, 3],
            [1.1, 2.2, 3.3, 4.4],
        ], dtype=numpy.float32)
        self.roi_indices = numpy.array([0, 2, 1, 0, 2], dtype=numpy.int32)
        n_rois = self.rois.shape[0]
        outsize = _pair(self.outsize)
        self.gy = numpy.random.uniform(
            -1, 1, (n_rois, n_channels,
                    outsize[0], outsize[1])).astype(numpy.float32)
        self.check_backward_options = {'atol': 5e-4, 'rtol': 5e-3}

</source>
</class>

<class classid="70" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/pooling_tests/test_roi_average_align_2d.py" startline="56" endline="69" pcid="8535">
    def check_forward(self, x_data, roi_data, roi_index_data):
        x = chainer.Variable(x_data)
        rois = chainer.Variable(roi_data)
        roi_indices = chainer.Variable(roi_index_data)
        y = functions.roi_average_align_2d(
            x, rois, roi_indices, outsize=self.outsize,
            spatial_scale=self.spatial_scale,
            sampling_ratio=self.sampling_ratio,
        )
        self.assertEqual(y.data.dtype, numpy.float32)
        y_data = cuda.to_cpu(y.data)

        self.assertEqual(self.gy.shape, y_data.shape)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/functions_tests/pooling_tests/test_roi_max_align_2d.py" startline="56" endline="69" pcid="8545">
    def check_forward(self, x_data, roi_data, roi_index_data):
        x = chainer.Variable(x_data)
        rois = chainer.Variable(roi_data)
        roi_indices = chainer.Variable(roi_index_data)
        y = functions.roi_max_align_2d(
            x, rois, roi_indices, outsize=self.outsize,
            spatial_scale=self.spatial_scale,
            sampling_ratio=self.sampling_ratio,
        )
        self.assertEqual(y.data.dtype, numpy.float32)
        y_data = cuda.to_cpu(y.data)

        self.assertEqual(self.gy.shape, y_data.shape)

</source>
</class>

<class classid="71" nclones="2" nlines="29" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="39" endline="68" pcid="8932">
    def test_iterator_repeat(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="24" endline="53" pcid="8978">
    def test_iterator_repeat(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

</source>
</class>

<class classid="72" nclones="2" nlines="29" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="72" endline="102" pcid="8933">
    def test_iterator_list_type(self):
        dataset = [[i, numpy.zeros((10,)) + i] for i in range(6)]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, list)
                    self.assertIsInstance(x[1], numpy.ndarray)
                    batches[x[0]] = x[1]

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                numpy.testing.assert_allclose(dataset[k][1], v)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="54" endline="84" pcid="8979">
    def test_iterator_list_type(self):
        dataset = [[i, numpy.zeros((10,)) + i] for i in range(6)]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, list)
                    self.assertIsInstance(x[1], numpy.ndarray)
                    batches[x[0]] = x[1]

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                numpy.testing.assert_allclose(dataset[k][1], v)

</source>
</class>

<class classid="73" nclones="2" nlines="29" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="106" endline="136" pcid="8934">
    def test_iterator_tuple_type(self):
        dataset = [(i, numpy.zeros((10,)) + i) for i in range(6)]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, tuple)
                    self.assertIsInstance(x[1], numpy.ndarray)
                    batches[x[0]] = x[1]

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                numpy.testing.assert_allclose(dataset[k][1], v)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="85" endline="115" pcid="8980">
    def test_iterator_tuple_type(self):
        dataset = [(i, numpy.zeros((10,)) + i) for i in range(6)]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, tuple)
                    self.assertIsInstance(x[1], numpy.ndarray)
                    batches[x[0]] = x[1]

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                numpy.testing.assert_allclose(dataset[k][1], v)

</source>
</class>

<class classid="74" nclones="2" nlines="32" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="140" endline="173" pcid="8935">
    def test_iterator_dict_type(self):
        dataset = [{i: numpy.zeros((10,)) + i} for i in range(6)]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, dict)
                    k = tuple(x)[0]
                    v = x[k]
                    self.assertIsInstance(v, numpy.ndarray)
                    batches[k] = v

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                x = dataset[k][tuple(dataset[k])[0]]
                numpy.testing.assert_allclose(x, v)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="116" endline="149" pcid="8981">
    def test_iterator_dict_type(self):
        dataset = [{i: numpy.zeros((10,)) + i} for i in range(6)]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batches = {}
            for j in range(3):
                batch = it.next()
                self.assertEqual(len(batch), 2)
                if j != 2:
                    self.assertFalse(it.is_new_epoch)
                else:
                    self.assertTrue(it.is_new_epoch)
                self.assertAlmostEqual(
                    it.epoch_detail, (3 * i + j + 1) * 2 / 6)
                self.assertAlmostEqual(
                    it.previous_epoch_detail, (3 * i + j) * 2 / 6)
                for x in batch:
                    self.assertIsInstance(x, dict)
                    k = tuple(x)[0]
                    v = x[k]
                    self.assertIsInstance(v, numpy.ndarray)
                    batches[k] = v

            self.assertEqual(len(batches), len(dataset))
            for k, v in six.iteritems(batches):
                x = dataset[k][tuple(dataset[k])[0]]
                numpy.testing.assert_allclose(x, v)

</source>
</class>

<class classid="75" nclones="2" nlines="18" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="200" endline="220" pcid="8938">
    def test_iterator_not_repeat_not_even(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultiprocessIterator(
            dataset, 2, repeat=False, **self.options)

        self.assertAlmostEqual(it.epoch_detail, 0 / 5)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 2 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 5)
        batch2 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 4 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 5)
        batch3 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 5 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 5)
        self.assertRaises(StopIteration, it.next)

        self.assertEqual(len(batch3), 1)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="167" endline="187" pcid="8984">
    def test_iterator_not_repeat_not_even(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultithreadIterator(
            dataset, 2, repeat=False, **self.options)

        self.assertAlmostEqual(it.epoch_detail, 0 / 5)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 2 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 5)
        batch2 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 4 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 5)
        batch3 = it.next()
        self.assertAlmostEqual(it.epoch_detail, 5 / 5)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 5)
        self.assertRaises(StopIteration, it.next)

        self.assertEqual(len(batch3), 1)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)

</source>
</class>

<class classid="76" nclones="2" nlines="14" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="243" endline="258" pcid="8941">
    def test_copy_not_repeat(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultiprocessIterator(
            dataset, 2, repeat=False, **self.options)
        copy_it = copy.copy(it)
        batches = sum([it.next() for _ in range(3)], [])
        self.assertEqual(sorted(batches), dataset)
        for _ in range(2):
            self.assertRaises(StopIteration, it.next)
        it = None

        batches = sum([copy_it.next() for _ in range(3)], [])
        self.assertEqual(sorted(batches), dataset)
        for _ in range(2):
            self.assertRaises(StopIteration, copy_it.next)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="201" endline="216" pcid="8987">
    def test_copy_not_repeat(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultithreadIterator(
            dataset, 2, repeat=False, **self.options)
        copy_it = copy.copy(it)
        batches = sum([it.next() for _ in range(3)], [])
        self.assertEqual(sorted(batches), dataset)
        for _ in range(2):
            self.assertRaises(StopIteration, it.next)
        it = None

        batches = sum([copy_it.next() for _ in range(3)], [])
        self.assertEqual(sorted(batches), dataset)
        for _ in range(2):
            self.assertRaises(StopIteration, copy_it.next)

</source>
</class>

<class classid="77" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="262" endline="273" pcid="8942">
    def test_reset(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultiprocessIterator(
            dataset, 2, repeat=False, **self.options)

        for trial in range(4):
            batches = sum([it.next() for _ in range(3)], [])
            self.assertEqual(sorted(batches), dataset)
            for _ in range(2):
                self.assertRaises(StopIteration, it.next)
            it.reset()

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="217" endline="228" pcid="8988">
    def test_reset(self):
        dataset = [1, 2, 3, 4, 5]
        it = iterators.MultithreadIterator(
            dataset, 2, repeat=False, **self.options)

        for trial in range(4):
            batches = sum([it.next() for _ in range(3)], [])
            self.assertEqual(sorted(batches), dataset)
            for _ in range(2):
                self.assertRaises(StopIteration, it.next)
            it.reset()

</source>
</class>

<class classid="78" nclones="2" nlines="32" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="430" endline="466" pcid="8951">
    def test_iterator_serialize(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 / 6)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertEqual(len(batch1), 2)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 6)
        batch2 = it.next()
        self.assertEqual(len(batch2), 2)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        target = dict()
        it.serialize(serializers.DictionarySerializer(target))

        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        it.serialize(serializers.NpzDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        batch3 = it.next()
        self.assertEqual(len(batch3), 2)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
        self.assertAlmostEqual(it.epoch_detail, 6 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 6)

</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="265" endline="302" pcid="8993">
    def test_iterator_serialize(self):
        dataset = [1, 2, 3, 4, 5, 6]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 / 6)
        self.assertIsNone(it.previous_epoch_detail)
        batch1 = it.next()
        self.assertEqual(len(batch1), 2)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 0 / 6)
        batch2 = it.next()
        self.assertEqual(len(batch2), 2)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        target = dict()
        it.serialize(serializers.DictionarySerializer(target))

        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        it.serialize(serializers.NpzDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 4 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 2 / 6)

        batch3 = it.next()
        self.assertEqual(len(batch3), 2)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(sorted(batch1 + batch2 + batch3), dataset)
        self.assertAlmostEqual(it.epoch_detail, 6 / 6)
        self.assertAlmostEqual(it.previous_epoch_detail, 4 / 6)


</source>
</class>

<class classid="79" nclones="3" nlines="30" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multiprocess_iterator.py" startline="523" endline="555" pcid="8955">
    def test_iterator_repeat(self):
        dataset = [1, 2, 3]
        it = iterators.MultiprocessIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

            self.assertEqual(
                sorted(batch1 + batch2 + batch3), [1, 1, 2, 2, 3, 3])


</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_serial_iterator.py" startline="306" endline="338" pcid="9018">
    def test_iterator_repeat(self):
        dataset = [1, 2, 3]
        it = iterators.SerialIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

            self.assertEqual(
                sorted(batch1 + batch2 + batch3), [1, 1, 2, 2, 3, 3])


</source>
<source file="systems/chainer-7.8.1/tests/chainer_tests/iterators_tests/test_multithread_iterator.py" startline="310" endline="342" pcid="8996">
    def test_iterator_repeat(self):
        dataset = [1, 2, 3]
        it = iterators.MultithreadIterator(dataset, 2, **self.options)
        for i in range(3):
            self.assertEqual(it.epoch, i)
            self.assertAlmostEqual(it.epoch_detail, i + 0 / 6)
            if i == 0:
                self.assertIsNone(it.previous_epoch_detail)
            else:
                self.assertAlmostEqual(it.previous_epoch_detail, i - 2 / 6)
            batch1 = it.next()
            self.assertEqual(len(batch1), 2)
            self.assertIsInstance(batch1, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 2 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 0 / 6)
            batch2 = it.next()
            self.assertEqual(len(batch2), 2)
            self.assertIsInstance(batch2, list)
            self.assertFalse(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 4 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 2 / 6)
            batch3 = it.next()
            self.assertEqual(len(batch3), 2)
            self.assertIsInstance(batch3, list)
            self.assertTrue(it.is_new_epoch)
            self.assertAlmostEqual(it.epoch_detail, i + 6 / 6)
            self.assertAlmostEqual(it.previous_epoch_detail, i + 4 / 6)

            self.assertEqual(
                sorted(batch1 + batch2 + batch3), [1, 1, 2, 2, 3, 3])


</source>
</class>

<class classid="80" nclones="2" nlines="17" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainermn_tests/iterators_tests/test_synchronized_iterator.py" startline="20" endline="41" pcid="9249">
    def test_sync(self):
        # test the case when datasize is a multiple of batchsize
        iterator = chainermn.iterators.create_synchronized_iterator(
            chainer.iterators.SerialIterator(
                self.dataset, batch_size=4, shuffle=True),
            self.communicator)

        for e in range(3):
            self.assertEqual(e, iterator.epoch)

            while True:
                batch = np.array(iterator.next(), dtype=np.float32)
                if self.communicator.rank == 0:
                    for rank_from in range(1, self.communicator.size):
                        _batch = self.communicator.recv(rank_from, tag=0)
                        chainer.testing.assert_allclose(batch, _batch)
                else:
                    self.communicator.send(batch, dest=0, tag=0)

                if iterator.is_new_epoch:
                    break

</source>
<source file="systems/chainer-7.8.1/tests/chainermn_tests/iterators_tests/test_synchronized_iterator.py" startline="42" endline="63" pcid="9250">
    def test_sync_frag(self):
        # test the case when datasize is not a multiple of batchsize
        iterator = chainermn.iterators.create_synchronized_iterator(
            chainer.iterators.SerialIterator(
                self.dataset, batch_size=7, shuffle=True),
            self.communicator)

        for e in range(3):
            self.assertEqual(e, iterator.epoch)

            while True:
                batch = np.array(iterator.next(), dtype=np.float32)
                if self.communicator.rank == 0:
                    for rank_from in range(1, self.communicator.size):
                        _batch = self.communicator.recv(rank_from, tag=0)
                        chainer.testing.assert_allclose(batch, _batch)
                else:
                    self.communicator.send(batch, dest=0, tag=0)

                if iterator.is_new_epoch:
                    break

</source>
</class>

<class classid="81" nclones="2" nlines="17" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainermn_tests/iterators_tests/test_synchronized_iterator.py" startline="64" endline="82" pcid="9251">
    def test_sync_no_repeat(self):
        iterator = chainermn.iterators.create_synchronized_iterator(
            chainer.iterators.SerialIterator(
                self.dataset, batch_size=4, shuffle=True, repeat=False),
            self.communicator)

        for e in range(3):
            try:
                while True:
                    batch = np.array(iterator.next(), dtype=np.float32)
                    if self.communicator.rank == 0:
                        for rank_from in range(1, self.communicator.size):
                            _batch = self.communicator.recv(rank_from, tag=0)
                            chainer.testing.assert_allclose(batch, _batch)
                    else:
                        self.communicator.send(batch, dest=0, tag=0)
            except StopIteration:
                iterator.reset()

</source>
<source file="systems/chainer-7.8.1/tests/chainermn_tests/iterators_tests/test_synchronized_iterator.py" startline="83" endline="100" pcid="9252">
    def test_sync_no_repeat_frag(self):
        iterator = chainermn.iterators.create_synchronized_iterator(
            chainer.iterators.SerialIterator(
                self.dataset, batch_size=7, shuffle=True, repeat=False),
            self.communicator)

        for e in range(3):
            try:
                while True:
                    batch = np.array(iterator.next(), dtype=np.float32)
                    if self.communicator.rank == 0:
                        for rank_from in range(1, self.communicator.size):
                            _batch = self.communicator.recv(rank_from, tag=0)
                            chainer.testing.assert_allclose(batch, _batch)
                    else:
                        self.communicator.send(batch, dest=0, tag=0)
            except StopIteration:
                iterator.reset()
</source>
</class>

<class classid="82" nclones="2" nlines="16" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainermn_tests/iterators_tests/test_multi_node_iterator.py" startline="72" endline="90" pcid="9260">

    def test_mn_iterator(self):
        # Datasize is a multiple of batchsize.
        bs = 4
        iterator = chainermn.iterators.create_multi_node_iterator(
            self.iterator_class(
                self.dataset, batch_size=bs, shuffle=True),
            self.communicator)

        for e in range(3):
            for i in range(100):
                batch = iterator.next()
                if self.communicator.rank == 0:
                    for rank_from in range(1, self.communicator.size):
                        _batch = self.communicator.mpi_comm.recv(
                            source=rank_from)
                        self.assertEqual(batch, _batch)
                else:
                    self.communicator.mpi_comm.ssend(batch, dest=0)
</source>
<source file="systems/chainer-7.8.1/tests/chainermn_tests/iterators_tests/test_multi_node_iterator.py" startline="91" endline="109" pcid="9261">

    def test_mn_iterator_frag(self):
        # Batasize is not a multiple of batchsize.
        bs = 7
        iterator = chainermn.iterators.create_multi_node_iterator(
            self.iterator_class(
                self.dataset, batch_size=bs, shuffle=True),
            self.communicator)

        for e in range(3):
            for i in range(100):
                batch = iterator.next()
                if self.communicator.rank == 0:
                    for rank_from in range(1, self.communicator.size):
                        _batch = self.communicator.mpi_comm.recv(
                            source=rank_from)
                        self.assertEqual(batch, _batch)
                else:
                    self.communicator.mpi_comm.ssend(batch, dest=0)
</source>
</class>

<class classid="83" nclones="2" nlines="38" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainermn_tests/iterators_tests/test_iterator_compatibility.py" startline="72" endline="117" pcid="9275">

    def test_multi_node_iterator_compatibility(self):
        iters = (
            lambda: chainermn.iterators.create_multi_node_iterator(
                self.iterator_class(
                    self.dataset, batch_size=self.bs),
                self.communicator),
            lambda: self.iterator_class(
                self.dataset, batch_size=self.bs),
        )

        bs_n_ratio = 1. * self.bs / self.N

        it_before, it_after = iters

        it = it_before()

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 * bs_n_ratio)
        batch1 = it.next()
        self.assertEqual(len(batch1), self.bs)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 1 * bs_n_ratio)
        batch2 = it.next()
        self.assertEqual(len(batch2), self.bs)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 * bs_n_ratio)

        target = dict()
        it.serialize(DummySerializer(target))

        it = it_after()
        it.serialize(DummyDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 * bs_n_ratio)

        batch3 = it.next()
        self.assertEqual(len(batch3), self.bs)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(
            sorted(batch1 + batch2 + batch3),
            self.dataset.tolist())
        self.assertAlmostEqual(it.epoch_detail, 3 * bs_n_ratio)
</source>
<source file="systems/chainer-7.8.1/tests/chainermn_tests/iterators_tests/test_iterator_compatibility.py" startline="118" endline="168" pcid="9276">

    def test_synchronized_iterator_compatibility(self):
        """
        Do not use `chainer.testing.parameterize` to share the code with
        `test_multi_node_iterator_compatibility` because pytest cannot
        guarantee the execution order of tests produced by `parameterize`,
        which causes unexpected behaviors with MPI programs.
        """
        iters = (
            lambda: chainermn.iterators.create_synchronized_iterator(
                self.iterator_class(
                    self.dataset, batch_size=self.bs),
                self.communicator),
            lambda: self.iterator_class(
                self.dataset, batch_size=self.bs),
        )

        bs_n_ratio = 1. * self.bs / self.N

        it_before, it_after = iters

        it = it_before()

        self.assertEqual(it.epoch, 0)
        self.assertAlmostEqual(it.epoch_detail, 0 * bs_n_ratio)
        batch1 = it.next()
        self.assertEqual(len(batch1), self.bs)
        self.assertIsInstance(batch1, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 1 * bs_n_ratio)
        batch2 = it.next()
        self.assertEqual(len(batch2), self.bs)
        self.assertIsInstance(batch2, list)
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 * bs_n_ratio)

        target = dict()
        it.serialize(DummySerializer(target))

        it = it_after()
        it.serialize(DummyDeserializer(target))
        self.assertFalse(it.is_new_epoch)
        self.assertAlmostEqual(it.epoch_detail, 2 * bs_n_ratio)

        batch3 = it.next()
        self.assertEqual(len(batch3), self.bs)
        self.assertIsInstance(batch3, list)
        self.assertTrue(it.is_new_epoch)
        self.assertEqual(
            sorted(batch1 + batch2 + batch3),
            self.dataset.tolist())
</source>
</class>

<class classid="84" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="324" endline="335" pcid="9426">
def test_asarray_from_chainerx_array(dtype):
    obj = array_utils.create_dummy_ndarray(chainerx, (2, 3), 'int32')
    a = chainerx.asarray(obj, dtype=dtype)
    if a.dtype == obj.dtype:
        assert a is obj
    else:
        assert a is not obj
    e = chainerx.array(obj, dtype=dtype, copy=False)
    chainerx.testing.assert_array_equal_ex(e, a)
    assert e.device is a.device


</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="440" endline="451" pcid="9435">
def test_asanyarray_from_chainerx_array(dtype):
    obj = array_utils.create_dummy_ndarray(chainerx, (2, 3), 'int32')
    a = chainerx.asanyarray(obj, dtype=dtype)
    if a.dtype == obj.dtype:
        assert a is obj
    else:
        assert a is not obj
    e = chainerx.array(obj, dtype=dtype, copy=False)
    chainerx.testing.assert_array_equal_ex(e, a)
    assert e.device is a.device


</source>
</class>

<class classid="85" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="512" endline="523" pcid="9441">
def test_zeros(xp, shape_as_sequence_or_int, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name
    if dtype_spec is Unspecified:
        out = xp.zeros(shape_as_sequence_or_int)
    else:
        out = xp.zeros(shape_as_sequence_or_int, dtype_spec)
    if dtype_spec in (None, Unspecified):
        out = dtype_utils.cast_if_numpy_array(xp, out, 'float32')
    return out


</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="554" endline="565" pcid="9445">
def test_ones(xp, shape_as_sequence_or_int, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name
    if dtype_spec is Unspecified:
        out = xp.ones(shape_as_sequence_or_int)
    else:
        out = xp.ones(shape_as_sequence_or_int, dtype_spec)
    if dtype_spec in (None, Unspecified):
        out = dtype_utils.cast_if_numpy_array(xp, out, 'float32')
    return out


</source>
</class>

<class classid="86" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="852" endline="864" pcid="9469">
def test_eye_with_default(xp, N, M, k, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name

    if M is None and k is None:
        return xp.eye(N, dtype=dtype_spec)
    elif M is None:
        return xp.eye(N, k=k, dtype=dtype_spec)
    elif k is None:
        return xp.eye(N, M=M, dtype=dtype_spec)
    assert False


</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_creation.py" startline="1213" endline="1225" pcid="9497">
def test_tri_with_default(xp, N, M, k, dtype_spec, device):
    if xp is numpy and isinstance(dtype_spec, chainerx.dtype):
        dtype_spec = dtype_spec.name

    if M is None and k is None:
        return xp.tri(N, dtype=dtype_spec)
    elif M is None:
        return xp.tri(N, k=k, dtype=dtype_spec)
    elif k is None:
        return xp.tri(N, M=M, dtype=dtype_spec)
    assert False


</source>
</class>

<class classid="87" nclones="2" nlines="15" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_connection.py" startline="136" endline="152" pcid="9552">
    def generate_inputs(self):
        x_shape = self.x_shape
        w_shape = self.w_shape
        b_shape = self.b_shape
        if len(self.in_dtypes) == 3:
            x_dtype, w_dtype, b_dtype = self.in_dtypes
        else:
            (x_dtype, w_dtype), b_dtype = self.in_dtypes, None
        x = array_utils.uniform(x_shape, x_dtype)
        w = array_utils.uniform(w_shape, w_dtype)

        if b_shape is None:
            return x, w
        else:
            b = array_utils.uniform(b_shape, b_dtype)
            return x, w, b

</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_connection.py" startline="323" endline="339" pcid="9559">
    def generate_inputs(self):
        x_shape = self.x_shape
        w_shape = self.w_shape
        b_shape = self.b_shape
        if len(self.in_dtypes) == 3:
            x_dtype, w_dtype, b_dtype = self.in_dtypes
        else:
            (x_dtype, w_dtype), b_dtype = self.in_dtypes, None
        x = array_utils.uniform(x_shape, x_dtype)
        w = array_utils.uniform(w_shape, w_dtype)

        if b_shape is None:
            return x, w
        else:
            b = array_utils.uniform(b_shape, b_dtype)
            return x, w, b

</source>
</class>

<class classid="88" nclones="4" nlines="19" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="34" endline="53" pcid="9655">
    def setup(self):
        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-3, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':
                self.skip_backward_test = True
            self.skip_double_backward_test = True

</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="135" endline="155" pcid="9661">
    def setup(self):

        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-3, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':
                self.skip_backward_test = True
            self.skip_double_backward_test = True

</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="344" endline="364" pcid="9673">
    def setup(self):

        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-2, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':
                self.skip_backward_test = True
            self.skip_double_backward_test = True

</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="246" endline="266" pcid="9667">
    def setup(self):
        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-3, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':

                self.skip_backward_test = True
            self.skip_double_backward_test = True

</source>
</class>

<class classid="89" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="105" endline="117" pcid="9660">
    def forward_chainer(self, inputs):
        h, c, ws, bs, xs = self.process_input(inputs)
        out = chainer.functions.n_step_lstm(
            self.n_layers, 0.0, h, c, ws, bs, xs)
        rets = []
        rets.append(out[0])
        rets.append(out[1])
        for i in range(len(out[2])):
            rets.append(out[2][i])

        return tuple(rets)


</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="215" endline="227" pcid="9666">
    def forward_chainer(self, inputs):
        h, c, ws, bs, xs = self.process_input(inputs)
        out = chainer.functions.n_step_bilstm(
            self.n_layers, 0.0, h, c, ws, bs, xs)
        rets = []
        rets.append(out[0])
        rets.append(out[1])
        for i in range(len(out[2])):
            rets.append(out[2][i])

        return tuple(rets)


</source>
</class>

<class classid="90" nclones="2" nlines="20" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="267" endline="293" pcid="9668">
    def generate_inputs(self):
        h_shape = (self.n_layers, self.batches[0], self.hidden_size)
        dtype = self.in_dtypes[0]

        h = array_utils.uniform(h_shape, dtype)

        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array_utils.uniform((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            return in_size if i == 0 and j < 3 else out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for i in range(6):
                inputs.append(array_utils.uniform(
                    (out_size, w_in(n, i)), dtype))
            for i in range(6):
                inputs.append(array_utils.uniform((out_size,), dtype))
        return tuple(inputs)

</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="479" endline="505" pcid="9680">
    def generate_inputs(self):
        h_shape = (self.n_layers, self.batches[0], self.hidden_size)
        dtype = self.in_dtypes[0]

        h = array_utils.uniform(h_shape, dtype)

        in_size = self.input_size
        out_size = self.hidden_size
        xs = [array_utils.uniform((self.batches[b], in_size), dtype)
              for b in range(len(self.batches))]

        def w_in(i, j):
            return in_size if i == 0 and j < 1 else out_size

        inputs = []
        inputs.append(h)
        for i in range(len(self.batches)):
            inputs.append(xs[i])

        for n in range(self.n_layers):
            for i in range(2):
                inputs.append(array_utils.uniform(
                    (out_size, w_in(n, i)), dtype))
            for i in range(2):
                inputs.append(array_utils.uniform((out_size,), dtype))
        return tuple(inputs)

</source>
</class>

<class classid="91" nclones="2" nlines="20" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="458" endline="478" pcid="9679">
    def setup(self):
        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-2, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':
                self.skip_forward_test = True
                self.skip_backward_test = True
            self.skip_double_backward_test = True

</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_rnn.py" startline="564" endline="585" pcid="9685">
    def setup(self):

        self.check_forward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_backward_options.update({
            'rtol': 1e-2, 'atol': 1e-2})
        self.check_double_backward_options.update({
            'rtol': 5e-2, 'atol': 5e-2})
        if self.in_dtypes[0] == 'float16':
            self.check_forward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
            self.check_double_backward_options.update({
                'rtol': 1e-1, 'atol': 1e-1})
        device = chainerx.get_default_device()
        if device.backend.name == 'cuda':
            if self.in_dtypes[0] != 'float32':
                self.skip_forward_test = True
                self.skip_backward_test = True
            self.skip_double_backward_test = True

</source>
</class>

<class classid="92" nclones="2" nlines="11" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py" startline="1469" endline="1480" pcid="9735">
    def generate_inputs(self):
        dtype1, dtype2 = self.in_dtypes
        shape1, shape2 = self.in_shapes
        low1 = -5 if numpy.dtype(dtype1).kind != 'u' else 2
        low2 = -5 if numpy.dtype(dtype2).kind != 'u' else 2
        high = 5
        a = array_utils.uniform(shape1, dtype1, low=low1, high=high)
        b = array_utils.uniform(shape2, dtype2, low=low2, high=high)
        a[numpy.logical_and(-0.5 < a, a < 0.5)] = 1
        b[numpy.logical_and(-0.5 < b, b < 0.5)] = 1
        return a, b

</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_arithmetic.py" startline="1543" endline="1554" pcid="9739">
    def generate_inputs(self):
        dtype1, dtype2 = self.in_dtypes
        shape1, shape2 = self.in_shapes
        low1 = -5 if numpy.dtype(dtype1).kind != 'u' else 2
        low2 = -5 if numpy.dtype(dtype2).kind != 'u' else 2
        high = 5
        a = array_utils.uniform(shape1, dtype1, low=low1, high=high)
        b = array_utils.uniform(shape2, dtype2, low=low2, high=high)
        a[numpy.logical_and(-0.5 < a, a < 0.5)] = 1
        b[numpy.logical_and(-0.5 < b, b < 0.5)] = 1
        return a, b

</source>
</class>

<class classid="93" nclones="2" nlines="17" similarity="100">
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_activation.py" startline="230" endline="249" pcid="9797">
    def setup(self):
        in_dtype, = self.in_dtypes
        if isinstance(self.alpha_range, tuple):
            l, u = self.alpha_range
            self.alpha = random.uniform(l, u)
        elif self.alpha_range is Unspecified:
            self.alpha = 1.0
        else:
            self.alpha = self.alpha_range

        if numpy.dtype(in_dtype).kind != 'f':
            self.skip_backward_test = True
            self.skip_double_backward_test = True

        if in_dtype == 'float16':
            self.check_forward_options.update({'rtol': 1e-3, 'atol': 1e-3})
            self.check_backward_options.update({'rtol': 2e-3, 'atol': 2e-3})
            self.check_double_backward_options.update(
                {'rtol': 1e-2, 'atol': 1e-2})

</source>
<source file="systems/chainer-7.8.1/tests/chainerx_tests/unit_tests/routines_tests/test_activation.py" startline="367" endline="386" pcid="9802">
    def setup(self):
        in_dtype, = self.in_dtypes
        if isinstance(self.beta_range, tuple):
            l, u = self.beta_range
            self.beta = random.uniform(l, u)
        elif self.beta_range is Unspecified:
            self.beta = 1.0
        else:
            self.beta = self.beta_range

        if numpy.dtype(in_dtype).kind != 'f':
            self.skip_backward_test = True
            self.skip_double_backward_test = True

        if in_dtype == 'float16':
            self.check_forward_options.update({'rtol': 2e-3, 'atol': 2e-3})
            self.check_backward_options.update({'rtol': 2e-3, 'atol': 2e-3})
            self.check_double_backward_options.update(
                {'rtol': 1e-2, 'atol': 1e-2})

</source>
</class>

<class classid="94" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/onnx_chainer/functions/math.py" startline="18" endline="30" pcid="9904">
def convert_AddConstant(
        func, opset_version, input_names, output_names, context):
    value_name = context.add_const(
        np.array(func.value, dtype=func.inputs[0].dtype), 'value')
    input_names.append(value_name)

    if opset_version == 1:
        return onnx_helper.make_node(
            'Add', input_names, output_names, consumed_inputs=[1, 1]),
    elif opset_version == 6 or opset_version == 7:
        return onnx_helper.make_node('Add', input_names, output_names),


</source>
<source file="systems/chainer-7.8.1/onnx_chainer/functions/math.py" startline="64" endline="76" pcid="9908">
def convert_MulConstant(
        func, opset_version, input_names, output_names, context):
    value_name = context.add_const(
        np.array(func.value, dtype=func.inputs[0].dtype), 'value')
    input_names.append(value_name)

    if opset_version == 1:
        return onnx_helper.make_node(
            'Mul', input_names, output_names, consumed_inputs=[1, 1]),
    elif opset_version == 6 or opset_version == 7:
        return onnx_helper.make_node('Mul', input_names, output_names),


</source>
</class>

<class classid="95" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/onnx_chainer/functions/math.py" startline="41" endline="53" pcid="9906">
def convert_SubFromConstant(
        func, opset_version, input_names, output_names, context):
    value_name = context.add_const(
        np.array(func.value, dtype=func.inputs[0].dtype), 'value')
    input_names[:0] = [value_name]

    if opset_version == 1:
        return onnx_helper.make_node(
            'Sub', input_names, output_names, consumed_inputs=[1, 1]),
    elif opset_version == 6 or opset_version == 7:
        return onnx_helper.make_node('Sub', input_names, output_names),


</source>
<source file="systems/chainer-7.8.1/onnx_chainer/functions/math.py" startline="96" endline="108" pcid="9911">
def convert_DivFromConstant(
        func, opset_version, input_names, output_names, context):
    value_name = context.add_const(
        np.array(func.value, dtype=func.inputs[0].dtype), 'value')
    input_names[:0] = [value_name]

    if opset_version == 1:
        return onnx_helper.make_node(
            'Div', input_names, output_names, consumed_inputs=[1, 1]),
    elif opset_version == 6 or opset_version == 7:
        return onnx_helper.make_node('Div', input_names, output_names),


</source>
</class>

<class classid="96" nclones="2" nlines="14" similarity="100">
<source file="systems/chainer-7.8.1/chainermn/communicators/_memory_utility.py" startline="109" endline="123" pcid="10169">
    def from_device(self, src, size, offset=0, stream=None):
        dst = self.memory + offset
        xp = chainer.backend.get_array_module(src)
        if xp == cp:
            src_data = src.data
        elif xp == chx:
            src_data = _get_memory_pointer_from_chainerx(src)
        else:
            raise ValueError(
                '{} is from an unsupported array module'.format(type(src)))
        if stream is None:
            dst.copy_from_device(src_data, size)
        else:
            dst.copy_from_device_async(src_data, size, stream)

</source>
<source file="systems/chainer-7.8.1/chainermn/communicators/_memory_utility.py" startline="124" endline="138" pcid="10170">
    def to_device(self, dst, size, offset=0, stream=None):
        src = self.memory + offset
        xp = chainer.backend.get_array_module(dst)
        if xp == cp:
            dst_data = dst.data
        elif xp == chx:
            dst_data = _get_memory_pointer_from_chainerx(dst)
        else:
            raise ValueError(
                '{} is from an unsupported array module'.format(type(dst)))
        if stream is None:
            dst_data.copy_from_device(src, size)
        else:
            dst_data.copy_from_device_async(src, size, stream)

</source>
</class>

<class classid="97" nclones="2" nlines="16" similarity="100">
<source file="systems/chainer-7.8.1/chainermn/communicators/_memory_utility.py" startline="253" endline="270" pcid="10181">
def _batched_pack_params(params_data, buffer, dtype, stream=None):
    n_params = params_data.n_params
    n_elems = params_data.n_elems
    params_dptr = params_data.dptr
    params_dtype = params_data.dtype
    params_size_csum = params_data.size_csum
    buf_dtype = _communication_utility._get_nccl_type_id(dtype)
    n_threads = 128
    n_blocks = (n_elems + n_threads - 1) // n_threads
    if stream is None:
        stream = cp.cuda.get_current_stream()
    with stream:
        _cupy_batched_pack_params()(
            (n_blocks, ), (n_threads, ),
            (buffer.memory.ptr, buf_dtype, n_elems,
             params_dptr, params_dtype, params_size_csum, n_params))


</source>
<source file="systems/chainer-7.8.1/chainermn/communicators/_memory_utility.py" startline="271" endline="288" pcid="10182">
def _batched_unpack_params(params_data, buffer, dtype, stream=None):
    n_params = params_data.n_params
    n_elems = params_data.n_elems
    params_dptr = params_data.dptr
    params_dtype = params_data.dtype
    params_size_csum = params_data.size_csum
    buf_dtype = _communication_utility._get_nccl_type_id(dtype)
    n_threads = 128
    n_blocks = (n_elems + n_threads - 1) // n_threads
    if stream is None:
        stream = cp.cuda.get_current_stream()
    with stream:
        _cupy_batched_unpack_params()(
            (n_blocks, ), (n_threads, ),
            (buffer.memory.ptr, buf_dtype, n_elems,
             params_dptr, params_dtype, params_size_csum, n_params))


</source>
</class>

<class classid="98" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.8.1/examples/ptb/train_ptb.py" startline="82" endline="105" pcid="10327">
    def __next__(self):
        # This iterator returns a list representing a mini-batch. Each item
        # indicates a different position in the original sequence. Each item is
        # represented by a pair of two word IDs. The first word is at the
        # "current" position, while the second word at the next position.
        # At each iteration, the iteration count is incremented, which pushes
        # forward the "current" position.
        length = len(self.dataset)
        if not self.repeat and self.iteration * self.batch_size >= length:
            # If not self.repeat, this iterator stops at the end of the first
            # epoch (i.e., when all words are visited once).
            raise StopIteration
        cur_words = self.get_words()
        self._previous_epoch_detail = self.epoch_detail
        self.iteration += 1
        next_words = self.get_words()

        epoch = self.iteration * self.batch_size // length
        self.is_new_epoch = self.epoch < epoch
        if self.is_new_epoch:
            self.epoch = epoch

        return list(zip(cur_words, next_words))

</source>
<source file="systems/chainer-7.8.1/examples/static_graph_optimizations/ptb/train_ptb_custom_loop.py" startline="120" endline="143" pcid="10503">
    def __next__(self):
        # This iterator returns a list representing a mini-batch. Each item
        # indicates a different position in the original sequence. Each item is
        # represented by a pair of two word IDs. The first word is at the
        # "current" position, while the second word at the next position.
        # At each iteration, the iteration count is incremented, which pushes
        # forward the "current" position.
        length = len(self.dataset)
        if not self.repeat and self.iteration * self.batch_size >= length:
            # If not self.repeat, this iterator stops at the end of the first
            # epoch (i.e., when all words are visited once).
            raise StopIteration
        cur_words = self.get_words()
        self._previous_epoch_detail = self.epoch_detail
        self.iteration += 1
        next_words = self.get_words()

        epoch = self.iteration * self.batch_size // length
        self.is_new_epoch = self.epoch < epoch
        if self.is_new_epoch:
            self.epoch = epoch

        return list(zip(cur_words, next_words))

</source>
</class>

<class classid="99" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.8.1/examples/ptb/train_ptb.py" startline="122" endline="139" pcid="10331">
    def serialize(self, serializer):
        # It is important to serialize the state to be recovered on resume.
        self.iteration = serializer('iteration', self.iteration)
        self.epoch = serializer('epoch', self.epoch)
        try:
            self._previous_epoch_detail = serializer(
                'previous_epoch_detail', self._previous_epoch_detail)
        except KeyError:
            # guess previous_epoch_detail for older version
            self._previous_epoch_detail = self.epoch + \
                (self.current_position - self.batch_size) / len(self.dataset)
            if self.epoch_detail > 0:
                self._previous_epoch_detail = max(
                    self._previous_epoch_detail, 0.)
            else:
                self._previous_epoch_detail = -1.


</source>
<source file="systems/chainer-7.8.1/examples/static_graph_optimizations/ptb/train_ptb_custom_loop.py" startline="160" endline="176" pcid="10507">
    def serialize(self, serializer):
        # It is important to serialize the state to be recovered on resume.
        self.iteration = serializer('iteration', self.iteration)
        self.epoch = serializer('epoch', self.epoch)
        try:
            self._previous_epoch_detail = serializer(
                'previous_epoch_detail', self._previous_epoch_detail)
        except KeyError:
            # guess previous_epoch_detail for older version
            self._previous_epoch_detail = self.epoch + \
                (self.current_position - self.batch_size) / len(self.dataset)
            if self.epoch_detail > 0:
                self._previous_epoch_detail = max(
                    self._previous_epoch_detail, 0.)
            else:
                self._previous_epoch_detail = -1.

</source>
</class>

<class classid="100" nclones="2" nlines="19" similarity="100">
<source file="systems/chainer-7.8.1/examples/cifar/models/VGG.py" startline="60" endline="79" pcid="10372">
    def __init__(self, class_labels=10):
        super(VGG, self).__init__()
        with self.init_scope():
            self.block1_1 = Block(64, 3)
            self.block1_2 = Block(64, 3)
            self.block2_1 = Block(128, 3)
            self.block2_2 = Block(128, 3)
            self.block3_1 = Block(256, 3)
            self.block3_2 = Block(256, 3)
            self.block3_3 = Block(256, 3)
            self.block4_1 = Block(512, 3)
            self.block4_2 = Block(512, 3)
            self.block4_3 = Block(512, 3)
            self.block5_1 = Block(512, 3)
            self.block5_2 = Block(512, 3)
            self.block5_3 = Block(512, 3)
            self.fc1 = L.Linear(None, 512, nobias=True)
            self.bn_fc1 = L.BatchNormalization(512)
            self.fc2 = L.Linear(None, class_labels, nobias=True)

</source>
<source file="systems/chainer-7.8.1/examples/static_graph_optimizations/cifar/models/VGG.py" startline="61" endline="80" pcid="10523">
    def __init__(self, class_labels=10):
        super(VGG, self).__init__()
        with self.init_scope():
            self.block1_1 = Block(64, 3)
            self.block1_2 = Block(64, 3)
            self.block2_1 = Block(128, 3)
            self.block2_2 = Block(128, 3)
            self.block3_1 = Block(256, 3)
            self.block3_2 = Block(256, 3)
            self.block3_3 = Block(256, 3)
            self.block4_1 = Block(512, 3)
            self.block4_2 = Block(512, 3)
            self.block4_3 = Block(512, 3)
            self.block5_1 = Block(512, 3)
            self.block5_2 = Block(512, 3)
            self.block5_3 = Block(512, 3)
            self.fc1 = L.Linear(None, 512, nobias=True)
            self.bn_fc1 = L.BatchNormalization(512)
            self.fc2 = L.Linear(None, class_labels, nobias=True)

</source>
</class>

<class classid="101" nclones="3" nlines="33" similarity="100">
<source file="systems/chainer-7.8.1/examples/cifar/models/VGG.py" startline="80" endline="122" pcid="10373">
    def forward(self, x):
        # 64 channel blocks:
        h = self.block1_1(x)
        h = F.dropout(h, ratio=0.3)
        h = self.block1_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 128 channel blocks:
        h = self.block2_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block2_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 256 channel blocks:
        h = self.block3_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block4_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block5_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        h = F.dropout(h, ratio=0.5)
        h = self.fc1(h)
        h = self.bn_fc1(h)
        h = F.relu(h)
        h = F.dropout(h, ratio=0.5)
        return self.fc2(h)
</source>
<source file="systems/chainer-7.8.1/examples/chainermn/cifar/models/VGG.py" startline="93" endline="135" pcid="10434">
    def forward(self, x):
        # 64 channel blocks:
        h = self.block1_1(x)
        h = F.dropout(h, ratio=0.3)
        h = self.block1_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 128 channel blocks:
        h = self.block2_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block2_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 256 channel blocks:
        h = self.block3_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block4_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block5_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        h = F.dropout(h, ratio=0.5)
        h = self.fc1(h)
        h = self.bn_fc1(h)
        h = F.relu(h)
        h = F.dropout(h, ratio=0.5)
        return self.fc2(h)
</source>
<source file="systems/chainer-7.8.1/examples/static_graph_optimizations/cifar/models/VGG.py" startline="82" endline="124" pcid="10524">
    def __call__(self, x):
        # 64 channel blocks:
        h = self.block1_1(x)
        h = F.dropout(h, ratio=0.3)
        h = self.block1_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 128 channel blocks:
        h = self.block2_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block2_2(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 256 channel blocks:
        h = self.block3_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block3_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block4_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block4_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        # 512 channel blocks:
        h = self.block5_1(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_2(h)
        h = F.dropout(h, ratio=0.4)
        h = self.block5_3(h)
        h = F.max_pooling_2d(h, ksize=2, stride=2)

        h = F.dropout(h, ratio=0.5)
        h = self.fc1(h)
        h = self.bn_fc1(h)
        h = F.relu(h)
        h = F.dropout(h, ratio=0.5)
        return self.fc2(h)
</source>
</class>

<class classid="102" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.8.1/examples/chainermn/imagenet/models/nin.py" startline="13" endline="26" pcid="10451">
    def __init__(self):
        super(NIN, self).__init__()
        conv_init = I.HeNormal()  # MSRA scaling

        with self.init_scope():
            self.mlpconv1 = L.MLPConvolution2D(
                None, (96, 96, 96), 11, stride=4, conv_init=conv_init)
            self.mlpconv2 = L.MLPConvolution2D(
                None, (256, 256, 256), 5, pad=2, conv_init=conv_init)
            self.mlpconv3 = L.MLPConvolution2D(
                None, (384, 384, 384), 3, pad=1, conv_init=conv_init)
            self.mlpconv4 = L.MLPConvolution2D(
                None, (1024, 1024, 1000), 3, pad=1, conv_init=conv_init)

</source>
<source file="systems/chainer-7.8.1/examples/imagenet/nin.py" startline="13" endline="26" pcid="10564">
    def __init__(self):
        super(NIN, self).__init__()
        conv_init = I.HeNormal()  # MSRA scaling

        with self.init_scope():
            self.mlpconv1 = L.MLPConvolution2D(
                None, (96, 96, 96), 11, stride=4, conv_init=conv_init)
            self.mlpconv2 = L.MLPConvolution2D(
                None, (256, 256, 256), 5, pad=2, conv_init=conv_init)
            self.mlpconv3 = L.MLPConvolution2D(
                None, (384, 384, 384), 3, pad=1, conv_init=conv_init)
            self.mlpconv4 = L.MLPConvolution2D(
                None, (1024, 1024, 1000), 3, pad=1, conv_init=conv_init)

</source>
</class>

<class classid="103" nclones="2" nlines="22" similarity="100">
<source file="systems/chainer-7.8.1/examples/chainermn/imagenet/models/googlenet.py" startline="10" endline="34" pcid="10453">
    def __init__(self):
        super(GoogLeNet, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(None, 64, 7, stride=2, pad=3)
            self.conv2_reduce = L.Convolution2D(None, 64, 1)
            self.conv2 = L.Convolution2D(None, 192, 3, stride=1, pad=1)
            self.inc3a = L.Inception(None, 64, 96, 128, 16, 32, 32)
            self.inc3b = L.Inception(None, 128, 128, 192, 32, 96, 64)
            self.inc4a = L.Inception(None, 192, 96, 208, 16, 48, 64)
            self.inc4b = L.Inception(None, 160, 112, 224, 24, 64, 64)
            self.inc4c = L.Inception(None, 128, 128, 256, 24, 64, 64)
            self.inc4d = L.Inception(None, 112, 144, 288, 32, 64, 64)
            self.inc4e = L.Inception(None, 256, 160, 320, 32, 128, 128)
            self.inc5a = L.Inception(None, 256, 160, 320, 32, 128, 128)
            self.inc5b = L.Inception(None, 384, 192, 384, 48, 128, 128)
            self.loss3_fc = L.Linear(None, 1000)

            self.loss1_conv = L.Convolution2D(None, 128, 1)
            self.loss1_fc1 = L.Linear(None, 1024)
            self.loss1_fc2 = L.Linear(None, 1000)

            self.loss2_conv = L.Convolution2D(None, 128, 1)
            self.loss2_fc1 = L.Linear(None, 1024)
            self.loss2_fc2 = L.Linear(None, 1000)

</source>
<source file="systems/chainer-7.8.1/examples/imagenet/googlenet.py" startline="10" endline="34" pcid="10584">
    def __init__(self):
        super(GoogLeNet, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(None,  64, 7, stride=2, pad=3)
            self.conv2_reduce = L.Convolution2D(None,  64, 1)
            self.conv2 = L.Convolution2D(None, 192, 3, stride=1, pad=1)
            self.inc3a = L.Inception(None,  64,  96, 128, 16,  32,  32)
            self.inc3b = L.Inception(None, 128, 128, 192, 32,  96,  64)
            self.inc4a = L.Inception(None, 192,  96, 208, 16,  48,  64)
            self.inc4b = L.Inception(None, 160, 112, 224, 24,  64,  64)
            self.inc4c = L.Inception(None, 128, 128, 256, 24,  64,  64)
            self.inc4d = L.Inception(None, 112, 144, 288, 32,  64,  64)
            self.inc4e = L.Inception(None, 256, 160, 320, 32, 128, 128)
            self.inc5a = L.Inception(None, 256, 160, 320, 32, 128, 128)
            self.inc5b = L.Inception(None, 384, 192, 384, 48, 128, 128)
            self.loss3_fc = L.Linear(None, 1000)

            self.loss1_conv = L.Convolution2D(None, 128, 1)
            self.loss1_fc1 = L.Linear(None, 1024)
            self.loss1_fc2 = L.Linear(None, 1000)

            self.loss2_conv = L.Convolution2D(None, 128, 1)
            self.loss2_fc1 = L.Linear(None, 1024)
            self.loss2_fc2 = L.Linear(None, 1000)

</source>
</class>

<class classid="104" nclones="2" nlines="41" similarity="100">
<source file="systems/chainer-7.8.1/examples/chainermn/imagenet/models/googlenet.py" startline="35" endline="84" pcid="10454">
    def __call__(self, x, t):
        h = F.relu(self.conv1(x))
        h = F.local_response_normalization(
            F.max_pooling_2d(h, 3, stride=2), n=5)
        h = F.relu(self.conv2_reduce(h))
        h = F.relu(self.conv2(h))
        h = F.max_pooling_2d(
            F.local_response_normalization(h, n=5), 3, stride=2)

        h = self.inc3a(h)
        h = self.inc3b(h)
        h = F.max_pooling_2d(h, 3, stride=2)
        h = self.inc4a(h)

        l = F.average_pooling_2d(h, 5, stride=3)
        l = F.relu(self.loss1_conv(l))
        l = F.relu(self.loss1_fc1(l))
        l = self.loss1_fc2(l)
        loss1 = F.softmax_cross_entropy(l, t)

        h = self.inc4b(h)
        h = self.inc4c(h)
        h = self.inc4d(h)

        l = F.average_pooling_2d(h, 5, stride=3)
        l = F.relu(self.loss2_conv(l))
        l = F.relu(self.loss2_fc1(l))
        l = self.loss2_fc2(l)
        loss2 = F.softmax_cross_entropy(l, t)

        h = self.inc4e(h)
        h = F.max_pooling_2d(h, 3, stride=2)
        h = self.inc5a(h)
        h = self.inc5b(h)

        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.loss3_fc(F.dropout(h, 0.4))
        loss3 = F.softmax_cross_entropy(h, t)

        loss = 0.3 * (loss1 + loss2) + loss3
        accuracy = F.accuracy(h, t)

        chainer.report({
            'loss': loss,
            'loss1': loss1,
            'loss2': loss2,
            'loss3': loss3,
            'accuracy': accuracy
        }, self)
        return loss
</source>
<source file="systems/chainer-7.8.1/examples/imagenet/googlenet.py" startline="35" endline="84" pcid="10585">
    def forward(self, x, t):
        h = F.relu(self.conv1(x))
        h = F.local_response_normalization(
            F.max_pooling_2d(h, 3, stride=2), n=5)
        h = F.relu(self.conv2_reduce(h))
        h = F.relu(self.conv2(h))
        h = F.max_pooling_2d(
            F.local_response_normalization(h, n=5), 3, stride=2)

        h = self.inc3a(h)
        h = self.inc3b(h)
        h = F.max_pooling_2d(h, 3, stride=2)
        h = self.inc4a(h)

        l = F.average_pooling_2d(h, 5, stride=3)
        l = F.relu(self.loss1_conv(l))
        l = F.relu(self.loss1_fc1(l))
        l = self.loss1_fc2(l)
        loss1 = F.softmax_cross_entropy(l, t)

        h = self.inc4b(h)
        h = self.inc4c(h)
        h = self.inc4d(h)

        l = F.average_pooling_2d(h, 5, stride=3)
        l = F.relu(self.loss2_conv(l))
        l = F.relu(self.loss2_fc1(l))
        l = self.loss2_fc2(l)
        loss2 = F.softmax_cross_entropy(l, t)

        h = self.inc4e(h)
        h = F.max_pooling_2d(h, 3, stride=2)
        h = self.inc5a(h)
        h = self.inc5b(h)

        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.loss3_fc(F.dropout(h, 0.4))
        loss3 = F.softmax_cross_entropy(h, t)

        loss = 0.3 * (loss1 + loss2) + loss3
        accuracy = F.accuracy(h, t)

        chainer.report({
            'loss': loss,
            'loss1': loss1,
            'loss2': loss2,
            'loss3': loss3,
            'accuracy': accuracy
        }, self)
        return loss
</source>
</class>

<class classid="105" nclones="2" nlines="11" similarity="100">
<source file="systems/chainer-7.8.1/examples/chainermn/imagenet/models/alex.py" startline="12" endline="23" pcid="10455">
    def __init__(self):
        super(Alex, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(None, 96, 11, stride=4)
            self.conv2 = L.Convolution2D(None, 256, 5, pad=2)
            self.conv3 = L.Convolution2D(None, 384, 3, pad=1)
            self.conv4 = L.Convolution2D(None, 384, 3, pad=1)
            self.conv5 = L.Convolution2D(None, 256, 3, pad=1)
            self.fc6 = L.Linear(None, 4096)
            self.fc7 = L.Linear(None, 4096)
            self.fc8 = L.Linear(None, 1000)

</source>
<source file="systems/chainer-7.8.1/examples/imagenet/alex.py" startline="12" endline="23" pcid="10589">
    def __init__(self):
        super(Alex, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(None,  96, 11, stride=4)
            self.conv2 = L.Convolution2D(None, 256,  5, pad=2)
            self.conv3 = L.Convolution2D(None, 384,  3, pad=1)
            self.conv4 = L.Convolution2D(None, 384,  3, pad=1)
            self.conv5 = L.Convolution2D(None, 256,  3, pad=1)
            self.fc6 = L.Linear(None, 4096)
            self.fc7 = L.Linear(None, 4096)
            self.fc8 = L.Linear(None, 1000)

</source>
</class>

<class classid="106" nclones="2" nlines="14" similarity="100">
<source file="systems/chainer-7.8.1/examples/chainermn/imagenet/models/alex.py" startline="24" endline="38" pcid="10456">
    def __call__(self, x, t):
        h = F.max_pooling_2d(F.local_response_normalization(
            F.relu(self.conv1(x))), 3, stride=2)
        h = F.max_pooling_2d(F.local_response_normalization(
            F.relu(self.conv2(h))), 3, stride=2)
        h = F.relu(self.conv3(h))
        h = F.relu(self.conv4(h))
        h = F.max_pooling_2d(F.relu(self.conv5(h)), 3, stride=2)
        h = F.dropout(F.relu(self.fc6(h)))
        h = F.dropout(F.relu(self.fc7(h)))
        h = self.fc8(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss
</source>
<source file="systems/chainer-7.8.1/examples/imagenet/alex.py" startline="24" endline="38" pcid="10590">
    def forward(self, x, t):
        h = F.max_pooling_2d(F.local_response_normalization(
            F.relu(self.conv1(x))), 3, stride=2)
        h = F.max_pooling_2d(F.local_response_normalization(
            F.relu(self.conv2(h))), 3, stride=2)
        h = F.relu(self.conv3(h))
        h = F.relu(self.conv4(h))
        h = F.max_pooling_2d(F.relu(self.conv5(h)), 3, stride=2)
        h = F.dropout(F.relu(self.fc6(h)))
        h = F.dropout(F.relu(self.fc7(h)))
        h = self.fc8(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss
</source>
</class>

<class classid="107" nclones="2" nlines="11" similarity="100">
<source file="systems/chainer-7.8.1/examples/chainermn/imagenet/models/resnet50.py" startline="84" endline="95" pcid="10463">
    def __init__(self):
        super(ResNet50, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(
                3, 64, 7, 2, 3, initialW=initializers.HeNormal())
            self.bn1 = L.BatchNormalization(64)
            self.res2 = Block(3, 64, 64, 256, 1)
            self.res3 = Block(4, 256, 128, 512)
            self.res4 = Block(6, 512, 256, 1024)
            self.res5 = Block(3, 1024, 512, 2048)
            self.fc = L.Linear(2048, 1000)

</source>
<source file="systems/chainer-7.8.1/examples/imagenet/resnet50.py" startline="86" endline="97" pcid="10597">
    def __init__(self):
        super(ResNet50, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(
                3, 64, 7, 2, 3, initialW=initializers.HeNormal())
            self.bn1 = L.BatchNormalization(64)
            self.res2 = Block(3, 64, 64, 256, 1)
            self.res3 = Block(4, 256, 128, 512)
            self.res4 = Block(6, 512, 256, 1024)
            self.res5 = Block(3, 1024, 512, 2048)
            self.fc = L.Linear(2048, 1000)

</source>
</class>

<class classid="108" nclones="3" nlines="12" similarity="100">
<source file="systems/chainer-7.8.1/examples/chainermn/imagenet/models/resnet50.py" startline="96" endline="108" pcid="10464">
    def __call__(self, x, t):
        h = self.bn1(self.conv1(x))
        h = F.max_pooling_2d(F.relu(h), 3, stride=2)
        h = self.res2(h)
        h = self.res3(h)
        h = self.res4(h)
        h = self.res5(h)
        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.fc(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss
</source>
<source file="systems/chainer-7.8.1/examples/imagenet/resnet50.py" startline="98" endline="112" pcid="10598">
    def forward(self, x, t):
        h = self.bn1(self.conv1(x))
        h = F.max_pooling_2d(F.relu(h), 3, stride=2)
        h = self.res2(h)
        h = self.res3(h)
        h = self.res4(h)
        h = self.res5(h)
        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.fc(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss


</source>
<source file="systems/chainer-7.8.1/examples/imagenet/resnext50.py" startline="95" endline="107" pcid="10583">
    def forward(self, x, t):
        h = self.bn1(self.conv1(x))
        h = F.max_pooling_2d(F.relu(h), 3, stride=2)
        h = self.res2(h)
        h = self.res3(h)
        h = self.res4(h)
        h = self.res5(h)
        h = F.average_pooling_2d(h, 7, stride=1)
        h = self.fc(h)

        loss = F.softmax_cross_entropy(h, t)
        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)
        return loss
</source>
</class>

<class classid="109" nclones="2" nlines="39" similarity="100">
<source file="systems/chainer-7.8.1/examples/chainermn/imagenet/models/googlenetbn.py" startline="12" endline="53" pcid="10465">
    def __init__(self):
        super(GoogLeNetBN, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(
                None, 64, 7, stride=2, pad=3, nobias=True)
            self.norm1 = L.BatchNormalization(64)
            self.conv2 = L.Convolution2D(None, 192, 3, pad=1, nobias=True)
            self.norm2 = L.BatchNormalization(192)
            self.inc3a = L.InceptionBN(
                None, 64, 64, 64, 64, 96, 'avg', 32)
            self.inc3b = L.InceptionBN(
                None, 64, 64, 96, 64, 96, 'avg', 64)
            self.inc3c = L.InceptionBN(
                None, 0, 128, 160, 64, 96, 'max', stride=2)
            self.inc4a = L.InceptionBN(
                None, 224, 64, 96, 96, 128, 'avg', 128)
            self.inc4b = L.InceptionBN(
                None, 192, 96, 128, 96, 128, 'avg', 128)
            self.inc4c = L.InceptionBN(
                None, 128, 128, 160, 128, 160, 'avg', 128)
            self.inc4d = L.InceptionBN(
                None, 64, 128, 192, 160, 192, 'avg', 128)
            self.inc4e = L.InceptionBN(
                None, 0, 128, 192, 192, 256, 'max', stride=2)
            self.inc5a = L.InceptionBN(
                None, 352, 192, 320, 160, 224, 'avg', 128)
            self.inc5b = L.InceptionBN(
                None, 352, 192, 320, 192, 224, 'max', 128)
            self.out = L.Linear(None, 1000)

            self.conva = L.Convolution2D(None, 128, 1, nobias=True)
            self.norma = L.BatchNormalization(128)
            self.lina = L.Linear(None, 1024, nobias=True)
            self.norma2 = L.BatchNormalization(1024)
            self.outa = L.Linear(None, 1000)

            self.convb = L.Convolution2D(None, 128, 1, nobias=True)
            self.normb = L.BatchNormalization(128)
            self.linb = L.Linear(None, 1024, nobias=True)
            self.normb2 = L.BatchNormalization(1024)
            self.outb = L.Linear(None, 1000)

</source>
<source file="systems/chainer-7.8.1/examples/imagenet/googlenetbn.py" startline="12" endline="53" pcid="10601">
    def __init__(self):
        super(GoogLeNetBN, self).__init__()
        with self.init_scope():
            self.conv1 = L.Convolution2D(
                None, 64, 7, stride=2, pad=3, nobias=True)
            self.norm1 = L.BatchNormalization(64)
            self.conv2 = L.Convolution2D(None, 192, 3, pad=1, nobias=True)
            self.norm2 = L.BatchNormalization(192)
            self.inc3a = L.InceptionBN(
                None, 64, 64, 64, 64, 96, 'avg', 32)
            self.inc3b = L.InceptionBN(
                None, 64, 64, 96, 64, 96, 'avg', 64)
            self.inc3c = L.InceptionBN(
                None, 0, 128, 160, 64, 96, 'max', stride=2)
            self.inc4a = L.InceptionBN(
                None, 224, 64, 96, 96, 128, 'avg', 128)
            self.inc4b = L.InceptionBN(
                None, 192, 96, 128, 96, 128, 'avg', 128)
            self.inc4c = L.InceptionBN(
                None, 160, 128, 160, 128, 160, 'avg', 128)
            self.inc4d = L.InceptionBN(
                None, 96, 128, 192, 160, 192, 'avg', 128)
            self.inc4e = L.InceptionBN(
                None, 0, 128, 192, 192, 256, 'max', stride=2)
            self.inc5a = L.InceptionBN(
                None, 352, 192, 320, 160, 224, 'avg', 128)
            self.inc5b = L.InceptionBN(
                None, 352, 192, 320, 192, 224, 'max', 128)
            self.out = L.Linear(None, 1000)

            self.conva = L.Convolution2D(None, 128, 1, nobias=True)
            self.norma = L.BatchNormalization(128)
            self.lina = L.Linear(None, 1024, nobias=True)
            self.norma2 = L.BatchNormalization(1024)
            self.outa = L.Linear(None, 1000)

            self.convb = L.Convolution2D(None, 128, 1, nobias=True)
            self.normb = L.BatchNormalization(128)
            self.linb = L.Linear(None, 1024, nobias=True)
            self.normb2 = L.BatchNormalization(1024)
            self.outb = L.Linear(None, 1000)

</source>
</class>

<class classid="110" nclones="2" nlines="37" similarity="100">
<source file="systems/chainer-7.8.1/examples/chainermn/imagenet/models/googlenetbn.py" startline="54" endline="97" pcid="10466">
    def __call__(self, x, t):
        h = F.max_pooling_2d(
            F.relu(self.norm1(self.conv1(x))), 3, stride=2, pad=1)
        h = F.max_pooling_2d(
            F.relu(self.norm2(self.conv2(h))), 3, stride=2, pad=1)

        h = self.inc3a(h)
        h = self.inc3b(h)
        h = self.inc3c(h)
        h = self.inc4a(h)

        a = F.average_pooling_2d(h, 5, stride=3)
        a = F.relu(self.norma(self.conva(a)))
        a = F.relu(self.norma2(self.lina(a)))
        a = self.outa(a)
        loss1 = F.softmax_cross_entropy(a, t)

        h = self.inc4b(h)
        h = self.inc4c(h)
        h = self.inc4d(h)

        b = F.average_pooling_2d(h, 5, stride=3)
        b = F.relu(self.normb(self.convb(b)))
        b = F.relu(self.normb2(self.linb(b)))
        b = self.outb(b)
        loss2 = F.softmax_cross_entropy(b, t)

        h = self.inc4e(h)
        h = self.inc5a(h)
        h = F.average_pooling_2d(self.inc5b(h), 7)
        h = self.out(h)
        loss3 = F.softmax_cross_entropy(h, t)

        loss = 0.3 * (loss1 + loss2) + loss3
        accuracy = F.accuracy(h, t)

        chainer.report({
            'loss': loss,
            'loss1': loss1,
            'loss2': loss2,
            'loss3': loss3,
            'accuracy': accuracy,
        }, self)
        return loss
</source>
<source file="systems/chainer-7.8.1/examples/imagenet/googlenetbn.py" startline="54" endline="97" pcid="10602">
    def forward(self, x, t):
        h = F.max_pooling_2d(
            F.relu(self.norm1(self.conv1(x))),  3, stride=2, pad=1)
        h = F.max_pooling_2d(
            F.relu(self.norm2(self.conv2(h))), 3, stride=2, pad=1)

        h = self.inc3a(h)
        h = self.inc3b(h)
        h = self.inc3c(h)
        h = self.inc4a(h)

        a = F.average_pooling_2d(h, 5, stride=3)
        a = F.relu(self.norma(self.conva(a)))
        a = F.relu(self.norma2(self.lina(a)))
        a = self.outa(a)
        loss1 = F.softmax_cross_entropy(a, t)

        h = self.inc4b(h)
        h = self.inc4c(h)
        h = self.inc4d(h)

        b = F.average_pooling_2d(h, 5, stride=3)
        b = F.relu(self.normb(self.convb(b)))
        b = F.relu(self.normb2(self.linb(b)))
        b = self.outb(b)
        loss2 = F.softmax_cross_entropy(b, t)

        h = self.inc4e(h)
        h = self.inc5a(h)
        h = F.average_pooling_2d(self.inc5b(h), 7)
        h = self.out(h)
        loss3 = F.softmax_cross_entropy(h, t)

        loss = 0.3 * (loss1 + loss2) + loss3
        accuracy = F.accuracy(h, t)

        chainer.report({
            'loss': loss,
            'loss1': loss1,
            'loss2': loss2,
            'loss3': loss3,
            'accuracy': accuracy,
        }, self)
        return loss
</source>
</class>

<class classid="111" nclones="2" nlines="10" similarity="100">
<source file="systems/chainer-7.8.1/examples/chainermn/imagenet/compute_mean.py" startline="10" endline="21" pcid="10467">
def compute_mean(dataset):
    print('compute mean image')
    sum_image = 0
    N = len(dataset)
    for i, (image, _) in enumerate(dataset):
        sum_image += image
        sys.stderr.write('{} / {}\r'.format(i, N))
        sys.stderr.flush()
    sys.stderr.write('\n')
    return sum_image / N


</source>
<source file="systems/chainer-7.8.1/examples/imagenet/compute_mean.py" startline="10" endline="21" pcid="10604">
def compute_mean(dataset):
    print('compute mean image')
    sum_image = 0
    N = len(dataset)
    for i, (image, _) in enumerate(dataset):
        sum_image += image
        sys.stderr.write('{} / {}\r'.format(i, N))
        sys.stderr.flush()
    sys.stderr.write('\n')
    return sum_image / N


</source>
</class>

<class classid="112" nclones="2" nlines="12" similarity="100">
<source file="systems/chainer-7.8.1/examples/chainermn/imagenet/compute_mean.py" startline="22" endline="36" pcid="10468">
def main():
    parser = argparse.ArgumentParser(description='Compute images mean array')
    parser.add_argument('dataset',
                        help='Path to training image-label list file')
    parser.add_argument('--root', '-R', default='.',
                        help='Root directory path of image files')
    parser.add_argument('--output', '-o', default='mean.npy',
                        help='path to output mean array')
    args = parser.parse_args()

    dataset = chainer.datasets.LabeledImageDataset(args.dataset, args.root)
    mean = compute_mean(dataset)
    np.save(args.output, mean)


</source>
<source file="systems/chainer-7.8.1/examples/imagenet/compute_mean.py" startline="22" endline="36" pcid="10605">
def main():
    parser = argparse.ArgumentParser(description='Compute images mean array')
    parser.add_argument('dataset',
                        help='Path to training image-label list file')
    parser.add_argument('--root', '-R', default='.',
                        help='Root directory path of image files')
    parser.add_argument('--output', '-o', default='mean.npy',
                        help='path to output mean array')
    args = parser.parse_args()

    dataset = chainer.datasets.LabeledImageDataset(args.dataset, args.root)
    mean = compute_mean(dataset)
    np.save(args.output, mean)


</source>
</class>

<class classid="113" nclones="2" nlines="17" similarity="100">
<source file="systems/chainer-7.8.1/examples/chainermn/dcgan/net.py" startline="23" endline="41" pcid="10472">
    def __init__(self, n_hidden, bottom_width=4, ch=512, wscale=0.02):
        super(Generator, self).__init__()
        self.n_hidden = n_hidden
        self.ch = ch
        self.bottom_width = bottom_width

        with self.init_scope():
            w = chainer.initializers.Normal(wscale)
            self.l0 = L.Linear(self.n_hidden, bottom_width * bottom_width * ch,
                               initialW=w)
            self.dc1 = L.Deconvolution2D(ch, ch // 2, 4, 2, 1, initialW=w)
            self.dc2 = L.Deconvolution2D(ch // 2, ch // 4, 4, 2, 1, initialW=w)
            self.dc3 = L.Deconvolution2D(ch // 4, ch // 8, 4, 2, 1, initialW=w)
            self.dc4 = L.Deconvolution2D(ch // 8, 3, 3, 1, 1, initialW=w)
            self.bn0 = L.BatchNormalization(bottom_width * bottom_width * ch)
            self.bn1 = L.BatchNormalization(ch // 2)
            self.bn2 = L.BatchNormalization(ch // 4)
            self.bn3 = L.BatchNormalization(ch // 8)

</source>
<source file="systems/chainer-7.8.1/examples/dcgan/net.py" startline="27" endline="45" pcid="10618">
    def __init__(self, n_hidden, bottom_width=4, ch=512, wscale=0.02):
        super(Generator, self).__init__()
        self.n_hidden = n_hidden
        self.ch = ch
        self.bottom_width = bottom_width

        with self.init_scope():
            w = chainer.initializers.Normal(wscale)
            self.l0 = L.Linear(self.n_hidden, bottom_width * bottom_width * ch,
                               initialW=w)
            self.dc1 = L.Deconvolution2D(ch, ch // 2, 4, 2, 1, initialW=w)
            self.dc2 = L.Deconvolution2D(ch // 2, ch // 4, 4, 2, 1, initialW=w)
            self.dc3 = L.Deconvolution2D(ch // 4, ch // 8, 4, 2, 1, initialW=w)
            self.dc4 = L.Deconvolution2D(ch // 8, 3, 3, 1, 1, initialW=w)
            self.bn0 = L.BatchNormalization(bottom_width * bottom_width * ch)
            self.bn1 = L.BatchNormalization(ch // 2)
            self.bn2 = L.BatchNormalization(ch // 4)
            self.bn3 = L.BatchNormalization(ch // 8)

</source>
</class>

<class classid="114" nclones="2" nlines="18" similarity="100">
<source file="systems/chainer-7.8.1/examples/chainermn/dcgan/net.py" startline="57" endline="75" pcid="10475">

    def __init__(self, bottom_width=4, ch=512, wscale=0.02):
        w = chainer.initializers.Normal(wscale)
        super(Discriminator, self).__init__()
        with self.init_scope():
            self.c0_0 = L.Convolution2D(3, ch // 8, 3, 1, 1, initialW=w)
            self.c0_1 = L.Convolution2D(ch // 8, ch // 4, 4, 2, 1, initialW=w)
            self.c1_0 = L.Convolution2D(ch // 4, ch // 4, 3, 1, 1, initialW=w)
            self.c1_1 = L.Convolution2D(ch // 4, ch // 2, 4, 2, 1, initialW=w)
            self.c2_0 = L.Convolution2D(ch // 2, ch // 2, 3, 1, 1, initialW=w)
            self.c2_1 = L.Convolution2D(ch // 2, ch // 1, 4, 2, 1, initialW=w)
            self.c3_0 = L.Convolution2D(ch // 1, ch // 1, 3, 1, 1, initialW=w)
            self.l4 = L.Linear(bottom_width * bottom_width * ch, 1, initialW=w)
            self.bn0_1 = L.BatchNormalization(ch // 4, use_gamma=False)
            self.bn1_0 = L.BatchNormalization(ch // 4, use_gamma=False)
            self.bn1_1 = L.BatchNormalization(ch // 2, use_gamma=False)
            self.bn2_0 = L.BatchNormalization(ch // 2, use_gamma=False)
            self.bn2_1 = L.BatchNormalization(ch // 1, use_gamma=False)
            self.bn3_0 = L.BatchNormalization(ch // 1, use_gamma=False)
</source>
<source file="systems/chainer-7.8.1/examples/dcgan/net.py" startline="62" endline="80" pcid="10621">

    def __init__(self, bottom_width=4, ch=512, wscale=0.02):
        w = chainer.initializers.Normal(wscale)
        super(Discriminator, self).__init__()
        with self.init_scope():
            self.c0_0 = L.Convolution2D(3, ch // 8, 3, 1, 1, initialW=w)
            self.c0_1 = L.Convolution2D(ch // 8, ch // 4, 4, 2, 1, initialW=w)
            self.c1_0 = L.Convolution2D(ch // 4, ch // 4, 3, 1, 1, initialW=w)
            self.c1_1 = L.Convolution2D(ch // 4, ch // 2, 4, 2, 1, initialW=w)
            self.c2_0 = L.Convolution2D(ch // 2, ch // 2, 3, 1, 1, initialW=w)
            self.c2_1 = L.Convolution2D(ch // 2, ch // 1, 4, 2, 1, initialW=w)
            self.c3_0 = L.Convolution2D(ch // 1, ch // 1, 3, 1, 1, initialW=w)
            self.l4 = L.Linear(bottom_width * bottom_width * ch, 1, initialW=w)
            self.bn0_1 = L.BatchNormalization(ch // 4, use_gamma=False)
            self.bn1_0 = L.BatchNormalization(ch // 4, use_gamma=False)
            self.bn1_1 = L.BatchNormalization(ch // 2, use_gamma=False)
            self.bn2_0 = L.BatchNormalization(ch // 2, use_gamma=False)
            self.bn2_1 = L.BatchNormalization(ch // 1, use_gamma=False)
            self.bn3_0 = L.BatchNormalization(ch // 1, use_gamma=False)
</source>
</class>

<class classid="115" nclones="2" nlines="14" similarity="100">
<source file="systems/chainer-7.8.1/examples/imagenet/resnext50.py" startline="41" endline="56" pcid="10578">
    def __init__(self, in_size, ch, groups=1):
        super(BottleNeckB, self).__init__()
        initialW = initializers.HeNormal()

        with self.init_scope():
            self.conv1 = L.Convolution2D(
                in_size, ch, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn1 = L.BatchNormalization(ch)
            self.conv2 = L.Convolution2D(
                ch, ch, 3, 1, 1, initialW=initialW, nobias=True,
                groups=groups)
            self.bn2 = L.BatchNormalization(ch)
            self.conv3 = L.Convolution2D(
                ch, in_size, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn3 = L.BatchNormalization(in_size)

</source>
<source file="systems/chainer-7.8.1/examples/imagenet/resnet50.py" startline="44" endline="59" pcid="10593">
    def __init__(self, in_size, ch, groups=1):
        super(BottleNeckB, self).__init__()
        initialW = initializers.HeNormal()

        with self.init_scope():
            self.conv1 = L.Convolution2D(
                in_size, ch, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn1 = L.BatchNormalization(ch)
            self.conv2 = L.Convolution2D(
                ch, ch, 3, 1, 1, initialW=initialW, nobias=True,
                groups=groups)
            self.bn2 = L.BatchNormalization(ch)
            self.conv3 = L.Convolution2D(
                ch, in_size, 1, 1, 0, initialW=initialW, nobias=True)
            self.bn3 = L.BatchNormalization(in_size)

</source>
</class>

<class classid="116" nclones="2" nlines="70" similarity="100">
<source file="systems/chainer-7.8.1/chainerx/_docs/routines.py" startline="22" endline="508" pcid="10716">
def _docs_creation():
    _docs.set_doc(
        chainerx.empty,
        """empty(shape, dtype, device=None)
Returns an array without initializing the elements.

Args:
    shape (tuple of ints): Shape of the array.
    dtype: Data type of the array.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    :class:`~chainerx.ndarray`: New array with elements not initialized.

.. seealso:: :func:`numpy.empty`
""")

    _docs.set_doc(
        chainerx.empty_like,
        """empty_like(a, device=None)
Returns a new array with same shape and dtype of a given array.

Args:
    a (~chainerx.ndarray): Prototype array.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    :class:`~chainerx.ndarray`: New array with same shape and dtype as ``a`` \
with elements not initialized.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the prototype array.

.. seealso:: :func:`numpy.empty_like`
""")

    _docs.set_doc(
        chainerx.eye,
        """eye(N, M=None, k=0, dtype=float64, device=None)
Returns a 2-D array with ones on the diagonals and zeros elsewhere.

Args:
    N (int): Number of rows.
    M (int): Number of columns. M == N by default.
    k (int): Index of the diagonal. Zero indicates the main diagonal,
        a positive index an upper diagonal, and a negative index a lower
        diagonal.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: A 2-D array with given diagonals filled with ones and
    zeros elsewhere.

.. seealso:: :func:`numpy.eye`
""")

    _docs.set_doc(
        chainerx.tri,
        """tri(N, M=None, k=0, dtype=float32, device=None)
Returns a 2-D array with ones at and below the given diagonal
and zeros elsewhere.

Args:
    N (int): Number of rows.
    M (int): Number of columns. M == N by default.
    k (int): Index of the diagonal. Zero indicates the main diagonal,
        a positive index an upper diagonal, and a negative index a lower
        diagonal.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: A 2-D array with given diagonals filled ones at and
    below the given diagonal and zeros elsewhere.

.. seealso:: :func:`numpy.tri`
""")

    _docs.set_doc(
        chainerx.tril,
        """tril(m, k=0)
Lower triangle of an array.

Returns a copy of an array with elements above the k-th diagonal zeroed.

Args:
    m (~chainerx.ndarray): Input array.
    k (int): Index of the diagonal. Zero indicates the main diagonal,
        a positive index an upper diagonal, and a negative index a lower
        diagonal.

Returns:
    ~chainerx.ndarray: Lower triangle of ``m``.

.. seealso:: :func:`numpy.tril`
""")

    _docs.set_doc(
        chainerx.triu,
        """triu(m, k=0)
Upper triangle of an array.

Returns a copy of an array with elements below the k-th diagonal zeroed.

Args:
    m (~chainerx.ndarray): Input array.
    k (int): Index of the diagonal. Zero indicates the main diagonal,
        a positive index an upper diagonal, and a negative index a lower
        diagonal.

Returns:
    ~chainerx.ndarray: Upper triangle of ``m``.

.. seealso:: :func:`numpy.triu`
""")

    _docs.set_doc(
        chainerx.identity,
        """identity(n, dtype=None, device=None)
Returns a 2-D identity array.

It is equivalent to ``eye(n, n, dtype)``.

Args:
    n (int): Number of rows and columns.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: A 2-D identity array.

.. seealso:: :func:`numpy.identity`
""")

    _docs.set_doc(
        chainerx.ones,
        """ones(shape, dtype, device=None)
Returns a new array of given shape and dtype, filled with ones.

Args:
    shape (tuple of ints): Shape of the array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: New array.

.. seealso:: :func:`numpy.ones`
""")

    _docs.set_doc(
        chainerx.ones_like,
        """ones_like(a, device=None)
Returns an array of ones with same shape and dtype as a given array.

Args:
    a (~chainerx.ndarray): Prototype array.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: New array.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the prototype array.

.. seealso:: :func:`numpy.ones_like`
""")

    _docs.set_doc(
        chainerx.zeros,
        """zeros(shape, dtype, device=None)
Returns a new array of given shape and dtype, filled with zeros.

Args:
    shape (tuple of ints): Shape of the array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: New array.

.. seealso:: :func:`numpy.zeros`
""")

    _docs.set_doc(
        chainerx.zeros_like,
        """zeros_like(a, device=None)
Returns an array of zeros with same shape and dtype as a given array.

Args:
    a (~chainerx.ndarray): Prototype array.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: New array.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the prototype array.

.. seealso:: :func:`numpy.zeros_like`
""")

    _docs.set_doc(
        chainerx.full,
        """full(shape, fill_value, dtype, device=None)
Returns a new array of given shape and dtype, filled with a given value.

Args:
    shape (tuple of ints): Shape of the array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: New array.

.. seealso:: :func:`numpy.full`
""")

    _docs.set_doc(
        chainerx.full_like,
        """full_like(a, fill_value, dtype=None, device=None)
Returns a full array with same shape and dtype as a given array.

Args:
    a (~chainerx.ndarray): Prototype array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: New array.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the prototype array.

.. seealso:: :func:`numpy.full_like`
""")

    _docs.set_doc(
        chainerx.array,
        """array(object, dtype=None, copy=True, device=None)
Creates an array.

Args:
    object: A :class:`~chainerx.ndarray` object or any other object that can be
        passed to :func:`numpy.array`.
    dtype: Data type. If omitted, it's inferred from the input.
    copy (bool): If ``True``, the object is always copied. Otherwise, a copy
        will only be made if it is needed to satisfy any of the other
        requirements (dtype, device, etc.).
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: New array.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the input array.

.. seealso:: :func:`numpy.array`
""")

    _docs.set_doc(
        chainerx.asarray,
        """asarray(a, dtype=None, device=None)
Converts an object to an array.

Args:
    a: The source object.
    dtype: Data type. If omitted, it's inferred from the input.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: Array interpretation of ``a``. If ``a`` is already an \
ndarray on the given device with matching dtype, no copy is performed.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the input array.

.. seealso:: :func:`numpy.asarray`
""")

    _docs.set_doc(
        chainerx.ascontiguousarray,
        """ascontiguousarray(a, dtype=None, device=None)
Returns a C-contiguous array.

Args:
    a (~chainerx.ndarray): Source array.
    dtype: Data type.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: C-contiguous array. A copy will be made only if needed.

Warning:
    If ``device`` argument is omitted, the new array is created on the default
    device, not the device of the input array.

.. seealso:: :func:`numpy.ascontiguousarray`
""")

    _docs.set_doc(
        chainerx.copy,
        """copy(a)
Creates a copy of a given array.

Args:
    a (~chainerx.ndarray): Source array.

Returns:
    ~chainerx.ndarray: A copy array on the same device as ``a``.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.copy`
""")

    _docs.set_doc(
        chainerx.frombuffer,
        """frombuffer(buffer, dtype=float, count=-1, offset=0, device=None)
Returns a 1-D array interpretation of a buffer.

The given ``buffer`` memory must be usable on the given device, otherwise,
an error is raised.

Note:
    The ``native`` backend requires a buffer of main memory, and
    the ``cuda`` backend requires a buffer of CUDA memory.
    No copy is performed.

Args:
    buffer: An object that exposes the buffer interface.
    dtype: Data type of the returned array.
    count (int): Number of items to read. -1 means all data in the buffer.
    offset (int): Start reading the buffer from this offset (in bytes).
    device (~chainerx.Device): Device of the returned array.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: 1-D array interpretation of ``buffer``.

.. seealso:: :func:`numpy.frombuffer`
""")

    _docs.set_doc(
        chainerx.arange,
        """arange([start=0, ]stop, [step=1, ]dtype=None, device=None)
Returns an array with  evenly spaced values within a given interval.

Values are generated within the half-open interval [``start``, ``stop``).
The first three arguments are mapped like the ``range`` built-in function,
i.e. ``start`` and ``step`` are optional.

Args:
    start: Start of the interval.
    stop: End of the interval.
    step: Step width between each pair of consecutive values.
    dtype: Data type specifier. It is inferred from other arguments by
        default.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: The 1-D array of range values.

.. seealso:: :func:`numpy.arange`
""")

    _docs.set_doc(
        chainerx.linspace,
        """linspace(start, stop, num=50, endpoint=True, dtype=None, device=None)
Returns an array with evenly spaced numbers over a specified interval.

Instead of specifying the step width like :func:`chainerx.arange()`,
this function requires the total number of elements specified.

Args:
    start: Start of the interval.
    stop: End of the interval.
    num: Number of elements.
    endpoint (bool): If ``True``, the stop value is included as the last
        element. Otherwise, the stop value is omitted.
    dtype: Data type specifier. It is inferred from the start and stop
        arguments by default.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: The 1-D array of ranged values.

.. seealso:: :func:`numpy.linspace`
""")  # NOQA

    _docs.set_doc(
        chainerx.diag,
        """diag(v, k=0, device=None)
Returns a diagonal or a diagonal array.

Args:
    v (~chainerx.ndarray): Array object.
    k (int): Index of diagonals. Zero indicates the main diagonal, a
        positive value an upper diagonal, and a negative value a lower
        diagonal.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: If ``v`` is a 1-D array, then it returns a 2-D
    array with the specified diagonal filled by ``v``. If ``v`` is a
    2-D array, then it returns the specified diagonal of ``v``. In latter
    case, if ``v`` is a :class:`chainerx.ndarray` object, then its view is
    returned.

Note:
    The argument ``v`` does not support array-like objects yet.

.. seealso:: :func:`numpy.diag`
""")

    _docs.set_doc(
        chainerx.diagflat,
        """diagflat(v, k=0, device=None)
Creates a diagonal array from the flattened input.

Args:
    v (~chainerx.ndarray): Array object.
    k (int): Index of diagonals. See :func:`chainerx.diag`.
    device (~chainerx.Device): Device on which the array is allocated.
        If omitted, :ref:`the default device <chainerx_device>` is chosen.

Returns:
    ~chainerx.ndarray: A 2-D diagonal array with the diagonal copied
    from ``v``.

Note:
    The argument ``v`` does not support array-like objects yet.

.. seealso:: :func:`numpy.diagflat`
""")

    _docs.set_doc(
        chainerx.meshgrid,
        """meshgrid(xi, indexing='xy')
Returns coordinate matrices from coordinate vectors.

Make N-D coordinate arrays for vectorized evaluations of N-D scalar/vector
fields over N-D grids, given one-dimensional coordinate arrays x1, x2,…, xn.

Args:
    xi (sequence of :class:`~chainerx.ndarray`\\ s): 1-D arrays
        representing the coordinates of a grid.
    indexing (str): {‘xy’, ‘ij’}, optional
        Cartesian (‘xy’, default) or matrix (‘ij’) indexing of output.

Returns:
    list of :class:`~chainerx.ndarray`\\ s: For vectors x1, x2,…, ‘xn’ with
    lengths Ni=len(xi), return (N1, N2, N3,...Nn) shaped arrays if
    indexing=’ij’ or (N2, N1, N3,...Nn) shaped arrays if indexing=’xy’
    with the elements of xi repeated to fill the matrix along the first
    dimension for x1, the second for x2 and so on.

.. seealso:: :func:`numpy.meshgrid`
""")


</source>
<source file="systems/chainer-7.8.1/chainerx/_docs/routines.py" startline="1274" endline="1770" pcid="10722">
def _docs_manipulation():
    _docs.set_doc(
        chainerx.reshape,
        """reshape(a, newshape)
Returns a reshaped array.

Args:
    a (~chainerx.ndarray): Array to be reshaped.
    newshape (int or tuple of ints): The new shape of the array to return.
        If it is an integer, then it is treated as a tuple of length one.
        It should be compatible with ``a.size``. One of the elements can be
        -1, which is automatically replaced with the appropriate value to
        make the shape compatible with ``a.size``.

Returns:
    :class:`~chainerx.ndarray`: A reshaped view of ``a`` if possible,
    otherwise a copy.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.reshape`
""")

    _docs.set_doc(
        chainerx.ravel,
        """ravel(a)
Returns a flattened array.

Args:
    a (~chainerx.ndarray): Array to be flattened.

Returns:
    :class:`~chainerx.ndarray`: A flattened view of ``a`` if possible,
    otherwise a copy.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.ravel`
""")

    _docs.set_doc(
        chainerx.transpose,
        """transpose(a, axes=None)
Permutes the dimensions of an array.

Args:
    a (~chainerx.ndarray): Array to permute the dimensions.
    axes (tuple of ints): Permutation of the dimensions. This function reverses
        the shape by default.

Returns:
    ~chainerx.ndarray: A view of ``a`` with the dimensions permuted.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.transpose`
""")

    _docs.set_doc(
        chainerx.broadcast_to,
        """broadcast_to(array, shape)
Broadcasts an array to a given shape.

Args:
    array (~chainerx.ndarray): Array to broadcast.
    shape (tuple of ints): The shape of the desired array.

Returns:
    ~chainerx.ndarray: Broadcasted view.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``array``.

.. seealso:: :func:`numpy.broadcast_to`
""")

    _docs.set_doc(
        chainerx.squeeze,
        """squeeze(a, axis=None)
Removes size-one axes from the shape of an array.

Args:
    a (~chainerx.ndarray): Array to be reshaped.
    axis (int or tuple of ints): Axes to be removed. This function removes all
        size-one axes by default. If one of the specified axes is not of size
        one, an exception is raised.

Returns:
    ~chainerx.ndarray: An array without (specified) size-one axes.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.squeeze`
""")

    _docs.set_doc(
        chainerx.concatenate,
        """concatenate(arrays, axis=0)
Joins arrays along an axis.

Args:
    arrays (sequence of :class:`~chainerx.ndarray`\\ s): Arrays to be joined.
        All of these should have the same dimensionalities except the specified
        axis.
    axis (int): The axis to join arrays along.


Returns:
    ~chainerx.ndarray: Joined array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays in ``arrays``.

.. seealso:: :func:`numpy.concatenate`
""")

    _docs.set_doc(
        chainerx.stack,
        """stack(arrays, axis=0)
Stacks arrays along a new axis.

Args:
    arrays (sequence of :class:`~chainerx.ndarray`\\ s): Arrays to be stacked.
    axis (int): Axis along which the arrays are stacked.

Returns:
    ~chainerx.ndarray: Stacked array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays in ``arrays``.

.. seealso:: :func:`numpy.stack`
""")

    _docs.set_doc(
        chainerx.hstack,
        """hstack(arrays)
Stack arrays in sequence horizontally (column wise).

Args:
    arrays (sequence of :class:`~chainerx.ndarray`\\ s): Arrays to be stacked.

Returns:
    ~chainerx.ndarray: Stacked array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays in ``arrays``.

.. seealso:: :func:`numpy.hstack`
""")

    _docs.set_doc(
        chainerx.vstack,
        """vstack(arrays)
Stack arrays in sequence vertically (row wise).

Args:
    arrays (sequence of :class:`~chainerx.ndarray`\\ s): Arrays to be stacked.

Returns:
    ~chainerx.ndarray: Stacked array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays in ``arrays``.

.. seealso:: :func:`numpy.vstack`
""")

    _docs.set_doc(
        chainerx.dstack,
        """dstack(arrays)
Stack arrays in sequence depth wise (along third axis).

Args:
    arrays (sequence of :class:`~chainerx.ndarray`\\ s): Arrays to be stacked.

Returns:
    ~chainerx.ndarray: Stacked array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input arrays in ``arrays``.

.. seealso:: :func:`numpy.dstack`
""")

    _docs.set_doc(
        chainerx.atleast_2d,
        """atleast_2d(a)
View inputs as arrays with at least two dimensions.

Args:
    a (~chainerx.ndarray): Array.

Returns:
    ~chainerx.ndarray: An array with a.ndim >= 2.
    Copies are avoided where possible, and views with
    two or more dimensions are returned.

Note:
    * Arrays that already have two or more dimensions are preserved.
    * During backpropagation, this function propagates the gradient of the
      output array to the input arrays in ``a``.

.. seealso:: :func:`numpy.atleast_2d`
""")

    _docs.set_doc(
        chainerx.atleast_3d,
        """atleast_3d(a)
View inputs as arrays with at least three dimensions.

Args:
    a (~chainerx.ndarray): Array.

Returns:
    ~chainerx.ndarray: An array with a.ndim >= 3.
    Copies are avoided where possible, and views with
    three or more dimensions are returned.

Note:
    * Arrays that already have three or more dimensions are preserved.
    * During backpropagation, this function propagates the gradient of the
      output array to the input arrays in ``a``.

.. seealso:: :func:`numpy.atleast_3d`
""")

    _docs.set_doc(
        chainerx.split,
        """split(ary, indices_or_sections, axis=0)
Splits an array into multiple sub arrays along a given axis.

Args:
    ary (~chainerx.ndarray): Array to split.
    indices_or_sections (int or sequence of ints): A value indicating how to
        divide the axis. If it is an integer, then is treated as the number of
        sections, and the axis is evenly divided. Otherwise, the integers
        indicate indices to split at. Note that a sequence on the device
        memory is not allowed.
    axis (int): Axis along which the array is split.

Returns:
    list of :class:`~chainerx.ndarray`\\ s: A list of sub arrays. Each array \
is a partial view of the input array.

Note:
    During backpropagation, this function propagates the gradients of the
    output arrays to the input array ``ary``.

.. seealso:: :func:`numpy.split`
""")

    _docs.set_doc(
        chainerx.dsplit,
        """dsplit(ary, indices_or_sections)
Split array into multiple sub-arrays along the 3rd axis (depth).

Args:
    ary (~chainerx.ndarray): Array to split.
    indices_or_sections (int or sequence of ints): A value indicating how to
        divide the axis. If it is an integer, then is treated as the number of
        sections, and the axis is evenly divided. Otherwise, the integers
        indicate indices to split at. Note that a sequence on the device
        memory is not allowed.

Returns:
    list of :class:`~chainerx.ndarray`\\ s: A list of sub arrays. Each array \
is a partial view of the input array.

Note:
    During backpropagation, this function propagates the gradients of the
    output arrays to the input array ``ary``.

.. seealso:: :func:`numpy.dsplit`
""")

    _docs.set_doc(
        chainerx.vsplit,
        """vsplit(ary, indices_or_sections)
Splits an array into multiple sub-arrays vertically (row-wise).

Args:
    ary (~chainerx.ndarray): Array to split.
    indices_or_sections (int or sequence of ints): A value indicating how to
        divide the axis. If it is an integer, then is treated as the number of
        sections, and the axis is evenly divided. Otherwise, the integers
        indicate indices to split at. Note that a sequence on the device
        memory is not allowed.

Returns:
    list of :class:`~chainerx.ndarray`\\ s: A list of sub arrays. Each array \
is a partial view of the input array.

Note:
    During backpropagation, this function propagates the gradients of the
    output arrays to the input array ``ary``.

.. seealso:: :func:`numpy.vsplit`
""")

    _docs.set_doc(
        chainerx.hsplit,
        """hsplit(ary, indices_or_sections)
Split an array into multiple sub-arrays horizontally (column-wise).

Args:
    ary (~chainerx.ndarray): Array to split.
    indices_or_sections (int or sequence of ints): A value indicating how to
        divide the axis. If it is an integer, then is treated as the number of
        sections, and the axis is evenly divided. Otherwise, the integers
        indicate indices to split at. Note that a sequence on the device
        memory is not allowed.

Returns:
    list of :class:`~chainerx.ndarray`\\ s: A list of sub arrays. Each array \
is a partial view of the input array.

Note:
    During backpropagation, this function propagates the gradients of the
    output arrays to the input array ``ary``.

.. seealso:: :func:`numpy.hsplit`
""")

    _docs.set_doc(
        chainerx.swapaxes,
        """swapaxes(a, axis1, axis2)
Interchange two axes of an array.

Args:
    a (~chainerx.ndarray): Array to swapaxes.
    axis1 (int): First Axis
    axis2 (int): Second Axis

Returns:
    ~chainerx.ndarray: Swaped array.

Note:
    * Output array is a view of the input array.
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``a``.


.. seealso:: :func:`numpy.swapaxes`
""")

    _docs.set_doc(
        chainerx.repeat,
        """repeat(a, repeats, axis=None)
Constructs an array by repeating a given array.

Args:
    a (~chainerx.ndarray): Array to repeat.
    repeats (int or tuple of ints): The number of times which each
        element of a is repeated.
    axis (int): The axis along which to repeat values.

Returns:
    ~chainerx.ndarray: The repeated output array.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.repeat`
""")

    _docs.set_doc(
        chainerx.expand_dims,
        """expand_dims(a, axis)
Expand the shape of an array.

Args:
    a (~chainerx.ndarray): Input Array.
    axis (int): Position in the expanded axes where the new axis is placed.

Returns:
    ~chainerx.ndarray: Output array.

Note:
    * Output array may or may not be a view of the input array.
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``a``.


.. seealso:: :func:`numpy.expand_dims`
""")

    _docs.set_doc(
        chainerx.flip,
        """flip(m, axis)
Reverse the order of elements in an array along the given axis.

Args:
    m (~chainerx.ndarray): Input Array.
    axis (int or tuple of ints): Axis or axes along which to flip over.
    The default, axis=None, will flip over all of the axes of the input array.
    If axis is negative it counts from the last to the first axis.
    If axis is a tuple of ints, flipping is performed on all of the
    axes specified in the tuple.

Returns:
    ~chainerx.ndarray: A view of m with the entries of axis reversed.
    Since a view is returned, this operation is done in constant time.

Note:
    * Output array is a view of the input array.
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``m``.


.. seealso:: :func:`numpy.flip`
""")

    _docs.set_doc(
        chainerx.fliplr,
        """fliplr(m)
Flip array in the left/right direction.

Args:
    m (~chainerx.ndarray): Input Array.

Returns:
    ~chainerx.ndarray: A view of m with the columns reversed.
    Since a view is returned, this operation is done in constant time.

Note:
    * Output array is a view of the input array.
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``m``.


.. seealso:: :func:`numpy.fliplr`
""")

    _docs.set_doc(
        chainerx.flipud,
        """flipud(m)
Flip array in the up/down direction.

Args:
    m (~chainerx.ndarray): Input Array.

Returns:
    ~chainerx.ndarray: A view of m with the rows reversed.
    Since a view is returned, this operation is done in constant time.

Note:
    * Output array is a view of the input array.
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``m``.


.. seealso:: :func:`numpy.flipud`
""")

    _docs.set_doc(
        chainerx.moveaxis,
        """moveaxis(a, source, destination)
Move axes of an array to new positions.

Other axes remain in their original order.

Args:
    a (~chainerx.ndarray): Input Array.
    source (int or tuple of ints): Original positions of the axes to move.
    These must be unique.
    destintation (int or tuple of ints): Destination positions for each of
    the original axes. These must also be unique.

Returns:
    ~chainerx.ndarray: Array with moved axes. This array is a view of the
    input array.

Note:
    * During backpropagation, this function propagates the gradients of the
      output arrays to the input array ``a``.


.. seealso:: :func:`numpy.moveaxis`
""")


</source>
</class>

<class classid="117" nclones="2" nlines="19" similarity="100">
<source file="systems/chainer-7.8.1/chainerx/_docs/routines.py" startline="1119" endline="1273" pcid="10721">
def _docs_loss():
    _docs.set_doc(
        chainerx.absolute_error,
        """Element-wise absolute error function.

Computes the element-wise absolute error :math:`L` between two inputs
:math:`x_1` and :math:`x_2` defined as follows.

.. math::
    L = |x_1 - x_2|

Args:
    x1 (~chainerx.ndarray): Input variable.
    x2 (~chainerx.ndarray): Input variable.

Returns:
    :class:`~chainerx.ndarray`: A variable holding an array representing
    the absolute error of two inputs.

.. seealso:: :func:`chainer.functions.absolute_error`
""")

    _docs.set_doc(
        chainerx.squared_error,
        """Element-wise squared error function.

Computes the element-wise squared error :math:`L` between two inputs
:math:`x_1` and :math:`x_2` defined as follows.

.. math::
    L = (x_1 - x_2)^2

Can be used to compute mean squared error by just calling `mean()`
on the output array.

Args:
    x0 (~chainerx.ndarray): Input variable.
    x1 (~chainerx.ndarray): Input variable.

Returns:
    :class:`~chainerx.ndarray`: A variable holding an array representing
    the squared error of two inputs.

.. seealso:: :func:`chainer.functions.squared_error`
""")

    _docs.set_doc(
        chainerx.huber_loss,
        """Element-wise Huber loss.

The Huber loss is similar to the squared error but is less sensitive to
outliers in the data. It is defined as

.. math::

    L_{\\delta}(a) = \\left \\{ \\begin{array}{cc}
    \\frac{1}{2} a^2 & {\\rm if~|a| \\leq \\delta} \\\\
    \\delta (|a| - \\frac{1}{2} \\delta) & {\\rm otherwise,}
    \\end{array} \\right.

where :math:`a = x - t` is the difference between the input :math:`x`
and the target :math:`t`.

See: `Huber loss - Wikipedia <https://en.wikipedia.org/wiki/Huber_loss>`_.

Args:
    x (~chainerx.ndarray): Input variable.
    t (~chainerx.ndarray): Target variable for regression.
    delta (float): Constant variable for Huber loss function as used in
        definition.

Returns:
    :class:`~chainerx.ndarray`:
        A variable object holding an array representing the Huber loss
        :math:`L_{\\delta}` of the two inputs.

.. seealso:: :func:`chainer.functions.huber_loss`
""")

    _docs.set_doc(
        chainerx.gaussian_kl_divergence,
        """Element-wise KL-divergence of Gaussian variables from the standard one.

Given two variable ``mean`` representing :math:`\\mu` and ``ln_var``
representing :math:`\\log(\\sigma^2)`, this function calculates
the element-wise KL-divergence between the given multi-dimensional
Gaussian :math:`N(\\mu, S)` and the standard Gaussian :math:`N(0, I)`

.. math::

   D_{\\mathbf{KL}}(N(\\mu, S) \\| N(0, I)),

where :math:`S` is a diagonal matrix such that :math:`S_{ii} = \\sigma_i^2`
and :math:`I` is an identity matrix.

Args:
    mean (~chainerx.ndarray):
        A variable representing mean of given
        gaussian distribution, :math:`\\mu`.
    ln_var (~chainerx.ndarray):
        A variable representing logarithm of
        variance of given gaussian distribution, :math:`\\log(\\sigma^2)`.

Returns:
    :class:`~chainerx.ndarray`:
        A variable representing KL-divergence between
        given gaussian distribution and the standard gaussian.

.. seealso:: :func:`chainer.functions.gaussian_kl_divergence`
""")

    _docs.set_doc(
        chainerx.sigmoid_cross_entropy,
        """sigmoid_cross_entropy(x1, x2)

Element-wise cross entropy loss for pre-sigmoid activations.

Args:
    x1 (~chainerx.ndarray): An array whose (i, j)-th element indicates the
        unnormalized log probability of the j-th unit at the i-th example.
    x2 (~chainerx.ndarray): An array whose (i, j)-th element indicates a signed
        integer vector of ground truth labels 0 or 1. If ``x2[i, j] == -1``,
        corresponding ``x1[i, j]`` is ignored. Loss is zero if all ground truth
        labels are -1.

Returns:
    :class:`~chainerx.ndarray`: An array of the cross entropy.

Note:
    During backpropagation, this function propagates the gradient of the output
    array to the input array ``x1`` only.
""")

    _docs.set_doc(
        chainerx.softmax_cross_entropy,
        """softmax_cross_entropy(x1, x2)

Element-wise cross entropy loss for pre-softmax activations.

Args:
    x1 (~chainerx.ndarray): An array whose element indicates unnormalized log
        probability: the first axis of the array represents the number of
        samples, and the second axis represents the number of classes.
    x2 (~chainerx.ndarray): A signed integer vector of ground truth labels. If
        ``x2[i] == -1``, corresponding ``x1[i]`` is ignored.

Returns:
    :class:`~chainerx.ndarray`: An array of the cross entropy.

Note:
    During backpropagation, this function propagates the gradient of the output
    array to the input array ``x1`` only.
""")


</source>
<source file="systems/chainer-7.8.1/chainerx/_docs/routines.py" startline="3362" endline="3945" pcid="10729">
def _docs_rnn():
    _docs.set_doc(
        chainerx.n_step_lstm,
        """n_step_lstm(n_layers, hx, cx, ws, bs, xs)
    Stacked Uni-directional Long Short-Term Memory function.

This function calculates stacked Uni-directional LSTM with sequences.
This function gets an initial hidden state :math:`h_0`, an initial cell
state :math:`c_0`, an input sequence :math:`x`, weight matrices :math:`W`,
and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` and :math:`c_t` for each
time :math:`t` from input :math:`x_t`.

.. math::
   i_t &= \\sigma(W_0 x_t + W_4 h_{t-1} + b_0 + b_4) \\\\
   f_t &= \\sigma(W_1 x_t + W_5 h_{t-1} + b_1 + b_5) \\\\
   o_t &= \\sigma(W_2 x_t + W_6 h_{t-1} + b_2 + b_6) \\\\
   a_t &= \\tanh(W_3 x_t + W_7 h_{t-1} + b_3 + b_7) \\\\
   c_t &= f_t \\cdot c_{t-1} + i_t \\cdot a_t \\\\
   h_t &= o_t \\cdot \\tanh(c_t)

As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Eight weight matrices and eight bias vectors are
required for each layer. So, when :math:`S` layers exist, you need to
prepare :math:`8S` weight matrices and :math:`8S` bias vectors.
If the number of layers ``n_layers`` is greater than :math:`1`, the input
of the ``k``-th layer is the hidden state ``h_t`` of the ``k-1``-th layer.
Note that all input variables except the first layer may have different
shape from the first layer.

Args:
    n_layers(int): The number of layers.
    hx (:class:`~chainerx.array`):
        Variable holding stacked hidden states.
        Its shape is ``(S, B, N)`` where ``S`` is the number of layers and
        is equal to ``n_layers``, ``B`` is the mini-batch size, and ``N``
        is the dimension of the hidden units.
    cx (:class:`~chainerx.array`): Variable holding stacked cell states.
        It has the same shape as ``hx``.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[i]`` represents the weights for the i-th layer.
        Each ``ws[i]`` is a list containing eight matrices.
        ``ws[i][j]`` corresponds to :math:`W_j` in the equation.
        Only ``ws[0][j]`` where ``0 <= j < 4`` are ``(N, I)``-shaped as
        they are multiplied with input variables, where ``I`` is the size
        of the input and ``N`` is the dimension of the hidden units. All
        other matrices are ``(N, N)``-shaped.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[i]`` represents the biases for the i-th layer.
        Each ``bs[i]`` is a list containing eight vectors.
        ``bs[i][j]`` corresponds to :math:`b_j` in the equation.
        The shape of each matrix is ``(N,)`` where ``N`` is the dimension
        of the hidden units.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array`
        holding input values. Each element ``xs[t]`` holds input value
        for time ``t``. Its shape is ``(B_t, I)``, where ``B_t`` is the
        mini-batch size for time ``t``.
        When sequences has different lengths, they must be
        sorted in descending order of their lengths.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] >= xs[t + 1].shape[0]``.

Returns:
    tuple: This function returns a tuple containing three elements,
    ``hy``, ``cy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is the same as
      ``hx``.
    - ``cy`` is an updated cell states whose shape is the same as
      ``cx``.
    - ``ys`` is a list of :class:`~chainerx.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, N)`` where ``B_t`` is
      the mini-batch size for time ``t``, and ``N`` is size of hidden
      units. Note that ``B_t`` is the same value as ``xs[t]``.

.. note::
   The dimension of hidden units is limited to only one size ``N``. If you
   want to use variable dimension of hidden units, please use
   :class:`chainerx.lstm`.

.. seealso::
   :func:`chainerx.lstm`

.. admonition:: Example

    >>> import chainerx as chx
    >>> batchs = [3, 2, 1]  # support variable length sequences
    >>> in_size, out_size, n_layers = 3, 2, 2
    >>> xs = [chx.ones((b, in_size)).astype(chx.float32) for b in batchs]
    >>> [x.shape for x in xs]
    [(3, 3), (2, 3), (1, 3)]
    >>> h_shape = (n_layers, batchs[0], out_size)
    >>> hx = chx.ones(h_shape).astype(chx.float32)
    >>> cx = chx.ones(h_shape).astype(chx.float32)
    >>> w_in = lambda i, j: in_size if i == 0 and j < 4 else out_size
    >>> ws = []
    >>> bs = []
    >>> for n in range(n_layers):
    ...     ws.append([chx.ones((out_size, w_in(n, i))).\
astype(np.float32) for i in range(8)])
    ...     bs.append([chx.ones((out_size,)).astype(chx.float32) \
for _ in range(8)])
    ...
    >>> ws[0][0].shape  # ws[0][:4].shape are (out_size, in_size)
    (2, 3)
    >>> ws[1][0].shape  # others are (out_size, out_size)
    (2, 2)
    >>> bs[0][0].shape
    (2,)
    >>> hy, cy, ys = chx.n_step_lstm(
    ...     n_layers, hx, cx, ws, bs, xs)
    >>> hy.shape
    (2, 3, 2)
    >>> cy.shape
    (2, 3, 2)
    >>> [y.shape for y in ys]
    [(3, 2), (2, 2), (1, 2)]
""")

    _docs.set_doc(
        chainerx.n_step_bilstm,
        """n_step_bilstm(n_layers, hx, cx, ws, bs, xs)
Stacked Bi-directional Long Short-Term Memory function.
This function calculates stacked Bi-directional LSTM with sequences.
This function gets an initial hidden state :math:`h_0`, an initial cell
state :math:`c_0`, an input sequence :math:`x`, weight matrices :math:`W`,
and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` and :math:`c_t` for each
time :math:`t` from input :math:`x_t`.

.. math::
    i^{f}_t &=& \\sigma(W^{f}_0 x_t + W^{f}_4 h_{t-1} + b^{f}_0 + b^{f}_4),
    \\\\
    f^{f}_t &=& \\sigma(W^{f}_1 x_t + W^{f}_5 h_{t-1} + b^{f}_1 + b^{f}_5),
    \\\\
    o^{f}_t &=& \\sigma(W^{f}_2 x_t + W^{f}_6 h_{t-1} + b^{f}_2 + b^{f}_6),
    \\\\
    a^{f}_t &=& \\tanh(W^{f}_3 x_t + W^{f}_7 h_{t-1} + b^{f}_3 + b^{f}_7),
    \\\\
    c^{f}_t &=& f^{f}_t \\cdot c^{f}_{t-1} + i^{f}_t \\cdot a^{f}_t,
    \\\\
    h^{f}_t &=& o^{f}_t \\cdot \\tanh(c^{f}_t),
    \\\\
    i^{b}_t &=& \\sigma(W^{b}_0 x_t + W^{b}_4 h_{t-1} + b^{b}_0 + b^{b}_4),
    \\\\
    f^{b}_t &=& \\sigma(W^{b}_1 x_t + W^{b}_5 h_{t-1} + b^{b}_1 + b^{b}_5),
    \\\\
    o^{b}_t &=& \\sigma(W^{b}_2 x_t + W^{b}_6 h_{t-1} + b^{b}_2 + b^{b}_6),
    \\\\
    a^{b}_t &=& \\tanh(W^{b}_3 x_t + W^{b}_7 h_{t-1} + b^{b}_3 + b^{b}_7),
    \\\\
    c^{b}_t &=& f^{b}_t \\cdot c^{b}_{t-1} + i^{b}_t \\cdot a^{b}_t, \\\\
    h^{b}_t &=& o^{b}_t \\cdot \\tanh(c^{b}_t), \\\\
    h_t &=& [h^{f}_t; h^{b}_t]

where :math:`W^{f}` is the weight matrices for forward-LSTM, :math:`W^{b}`
is weight matrices for backward-LSTM.
As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Eight weight matrices and eight bias vectors are
required for each layer of each direction. So, when :math:`S` layers
exist, you need to prepare :math:`16S` weight matrices and :math:`16S`
bias vectors.
If the number of layers ``n_layers`` is greater than :math:`1`, the input
of the ``k``-th layer is the hidden state ``h_t`` of the ``k-1``-th layer.
Note that all input variables except the first layer may have different
shape from the first layer.

Args:
    n_layers(int): The number of layers.
    hx (:class:`~chainerx.array`):
        Variable holding stacked hidden states.
        Its shape is ``(2S, B, N)`` where ``S`` is the number of layers and
        is equal to ``n_layers``, ``B`` is the mini-batch size, and ``N``
        is the dimension of the hidden units. Because of bi-direction, the
        first dimension length is ``2S``.
    cx (:class:`~chainerx.array`): Variable holding stacked cell states.
        It has the same shape as ``hx``.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[2 * l + m]`` represents the weights for the l-th layer of
        the m-th direction. (``m == 0`` means the forward direction and
        ``m == 1`` means the backward direction.) Each ``ws[i]`` is a
        list containing eight matrices. ``ws[i][j]`` corresponds to
        :math:`W_j` in the equation. ``ws[0][j]`` and ``ws[1][j]`` where
        ``0 <= j < 4`` are ``(N, I)``-shaped because they are multiplied
        with input variables, where ``I`` is the size of the input.
        ``ws[i][j]`` where ``2 <= i`` and ``0 <= j < 4`` are
        ``(N, 2N)``-shaped because they are multiplied with two hidden
        layers :math:`h_t = [h^{f}_t; h^{b}_t]`. All other matrices are
        ``(N, N)``-shaped.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[2 * l + m]`` represents the weights for the l-th layer of
        m-th direction. (``m == 0`` means the forward direction and
        ``m == 1`` means the backward direction.)
        Each ``bs[i]`` is a list containing eight vectors.
        ``bs[i][j]`` corresponds to :math:`b_j` in the equation.
        The shape of each matrix is ``(N,)``.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array`
        holding input values. Each element ``xs[t]`` holds input value
        for time ``t``. Its shape is ``(B_t, I)``, where ``B_t`` is the
        mini-batch size for time ``t``.
        When sequences has different lengths, they must be
        sorted in descending order of their lengths.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] >= xs[t + 1].shape[0]``.

Returns:
    tuple: This function returns a tuple containing three elements,
    ``hy``, ``cy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is the same as
      ``hx``.
    - ``cy`` is an updated cell states whose shape is the same as
      ``cx``.
    - ``ys`` is a list of :class:`~chainer.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, 2N)`` where ``B_t``
      is the mini-batch size for time ``t``, and ``N`` is size of
      hidden units. Note that ``B_t`` is the same value as ``xs[t]``.

.. admonition:: Example

    >>> import chainerx as chx
    >>> batchs = [3, 2, 1]  # support variable length sequences
    >>> in_size, out_size, n_layers = 3, 2, 2
    >>> dropout_ratio = 0.0
    >>> xs = [chx.ones((b, in_size)).astype(chx.float32) for b in batchs]
    >>> [x.shape for x in xs]
    [(3, 3), (2, 3), (1, 3)]
    >>> h_shape = (n_layers * 2, batchs[0], out_size)
    >>> hx = chx.ones(h_shape).astype(chx.float32)
    >>> cx = chx.ones(h_shape).astype(chx.float32)
    >>> def w_in(i, j):
    ...     if i == 0 and j < 4:
    ...         return in_size
    ...     elif i > 0 and j < 4:
    ...         return out_size * 2
    ...     else:
    ...         return out_size
    ...
    >>> ws = []
    >>> bs = []
    >>> for n in range(n_layers):
    ...     for direction in (0, 1):
    ...         ws.append([chx.ones((out_size, w_in(n, i))).\
astype(np.float32) for i in range(8)])
    ...         bs.append([chx.ones((out_size,)).astype(chx.float32) \
for _ in range(8)])
    ...
    >>> ws[0][0].shape  # ws[0:2][:4].shape are (out_size, in_size)
    (2, 3)
    >>> ws[2][0].shape  # ws[2:][:4].shape are (out_size, 2 * out_size)
    (2, 4)
    >>> ws[0][4].shape  # others are (out_size, out_size)
    (2, 2)
    >>> bs[0][0].shape
    (2,)
    >>> hy, cy, ys = chx.n_step_bilstm(
    ...     n_layers, hx, cx, ws, bs, xs)
    >>> hy.shape
    (4, 3, 2)
    >>> cy.shape
    (4, 3, 2)
    >>> [y.shape for y in ys]
    [(3, 4), (2, 4), (1, 4)]
    """)

    _docs.set_doc(
        chainerx.n_step_gru,
        """n_step_gru(n_layers, hx, ws, bs, xs)
Stacked Uni-directional Gated Recurrent Unit function.
This function calculates stacked Uni-directional GRU with sequences.
This function gets an initial hidden state :math:`h_0`, an input
sequence :math:`x`, weight matrices :math:`W`, and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` for each time :math:`t`
from input :math:`x_t`.

.. math::
   r_t &= \\sigma(W_0 x_t + W_3 h_{t-1} + b_0 + b_3) \\\\
   z_t &= \\sigma(W_1 x_t + W_4 h_{t-1} + b_1 + b_4) \\\\
   h'_t &= \\tanh(W_2 x_t + b_2 + r_t \\cdot (W_5 h_{t-1} + b_5)) \\\\
   h_t &= (1 - z_t) \\cdot h'_t + z_t \\cdot h_{t-1}

As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Six weight matrices and six bias vectors are
required for each layers. So, when :math:`S` layers exists, you need to
prepare :math:`6S` weight matrices and :math:`6S` bias vectors.
If the number of layers ``n_layers`` is greather than :math:`1`, input
of ``k``-th layer is hidden state ``h_t`` of ``k-1``-th layer.
Note that all input variables except first layer may have different shape
from the first layer.

Args:
    n_layers(int): Number of layers.
    hx (~chainerx.array):
        Variable holding stacked hidden states.
        Its shape is ``(S, B, N)`` where ``S`` is number of layers and is
        equal to ``n_layers``, ``B`` is mini-batch size, and ``N`` is
        dimension of hidden units.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[i]`` represents weights for i-th layer.
        Each ``ws[i]`` is a list containing six matrices.
        ``ws[i][j]`` is corresponding with ``W_j`` in the equation.
        Only ``ws[0][j]`` where ``0 <= j < 3`` is ``(N, I)`` shape as they
        are multiplied with input variables. All other matrices has
        ``(N, N)`` shape.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[i]`` represnents biases for i-th layer.
        Each ``bs[i]`` is a list containing six vectors.
        ``bs[i][j]`` is corresponding with ``b_j`` in the equation.
        Shape of each matrix is ``(N,)`` where ``N`` is dimension of
        hidden units.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array`
        holding input values. Each element ``xs[t]`` holds input value
        for time ``t``. Its shape is ``(B_t, I)``, where ``B_t`` is
        mini-batch size for time ``t``, and ``I`` is size of input units.
        Note that this function supports variable length sequences.
        When sequneces has different lengths, sort sequences in descending
        order by length.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] >= xs[t + 1].shape[0]``.

Returns:
    tuple: This function returns a tuple containing two elements,
    ``hy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is same as ``hx``.
    - ``ys`` is a list of :class:`~chainerx.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, N)`` where ``B_t`` is
      mini-batch size for time ``t``, and ``N`` is size of hidden
      units. Note that ``B_t`` is the same value as ``xs[t]``
    """)

    _docs.set_doc(
        chainerx.n_step_bigru,
        """n_step_bigru(n_layers, hx, ws, bs, xs)
Stacked Bi-directional Gated Recurrent Unit function.
This function calculates stacked Bi-directional GRU with sequences.
This function gets an initial hidden state :math:`h_0`, an input
sequence :math:`x`, weight matrices :math:`W`, and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` for each time :math:`t`
from input :math:`x_t`.

.. math::
   r^{f}_t &= \\sigma(W^{f}_0 x_t + W^{f}_3 h_{t-1} + b^{f}_0 + b^{f}_3)
   \\\\
   z^{f}_t &= \\sigma(W^{f}_1 x_t + W^{f}_4 h_{t-1} + b^{f}_1 + b^{f}_4)
   \\\\
   h^{f'}_t &= \\tanh(W^{f}_2 x_t + b^{f}_2 + r^{f}_t \\cdot (W^{f}_5
   h_{t-1} + b^{f}_5)) \\\\
   h^{f}_t &= (1 - z^{f}_t) \\cdot h^{f'}_t + z^{f}_t \\cdot h_{t-1}
   \\\\
   r^{b}_t &= \\sigma(W^{b}_0 x_t + W^{b}_3 h_{t-1} + b^{b}_0 + b^{b}_3)
   \\\\
   z^{b}_t &= \\sigma(W^{b}_1 x_t + W^{b}_4 h_{t-1} + b^{b}_1 + b^{b}_4)
   \\\\
   h^{b'}_t &= \\tanh(W^{b}_2 x_t + b^{b}_2 + r^{b}_t \\cdot (W^{b}_5
   h_{t-1} + b^{b}_5)) \\\\
   h^{b}_t &= (1 - z^{b}_t) \\cdot h^{b'}_t + z^{b}_t \\cdot h_{t-1}
   \\\\
   h_t  &= [h^{f}_t; h^{b}_t] \\\\

where :math:`W^{f}` is weight matrices for forward-GRU, :math:`W^{b}` is
weight matrices for backward-GRU.
As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Six weight matrices and six bias vectors are
required for each layers. So, when :math:`S` layers exists, you need to
prepare :math:`6S` weight matrices and :math:`6S` bias vectors.
If the number of layers ``n_layers`` is greather than :math:`1`, input
of ``k``-th layer is hidden state ``h_t`` of ``k-1``-th layer.
Note that all input variables except first layer may have different shape
from the first layer.

Args:
    n_layers(int): Number of layers.
    hx (:class:`~chainerx.array`):
        Variable holding stacked hidden states.
        Its shape is ``(2S, B, N)`` where ``S`` is number of layers and is
        equal to ``n_layers``, ``B`` is mini-batch size, and ``N`` is
        dimension of hidden units.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[i]`` represents weights for i-th layer.
        Each ``ws[i]`` is a list containing six matrices.
        ``ws[i][j]`` is corresponding with ``W_j`` in the equation.
        Only ``ws[0][j]`` where ``0 <= j < 3`` is ``(N, I)`` shape as they
        are multiplied with input variables. All other matrices has
        ``(N, N)`` shape.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[i]`` represnents biases for i-th layer.
        Each ``bs[i]`` is a list containing six vectors.
        ``bs[i][j]`` is corresponding with ``b_j`` in the equation.
        Shape of each matrix is ``(N,)`` where ``N`` is dimension of
        hidden units.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array` holding input values.
        Each element ``xs[t]`` holds input value
        for time ``t``. Its shape is ``(B_t, I)``, where ``B_t`` is
        mini-batch size for time ``t``, and ``I`` is size of input units.
        Note that this function supports variable length sequences.
        When sequneces has different lengths, sort sequences in descending
        order by length.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] >= xs[t + 1].shape[0]``.

Returns:
    tuple: This function returns a tuple containing two elements,
    ``hy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is same as ``hx``.
    - ``ys`` is a list of :class:`~chainerx.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, N)`` where ``B_t`` is
      mini-batch size for time ``t``, and ``N`` is size of hidden
      units. Note that ``B_t`` is the same value as ``xs[t]``.
    """)

    _docs.set_doc(
        chainerx.n_step_rnn,
        """n_step_rnn(n_layers, hx, ws, bs, xs, activation='tanh')
Stacked Uni-directional RNN function for sequence inputs.
This function calculates stacked Uni-directional RNN with sequences.
This function gets an initial hidden state :math:`h_0`,
an initial cell state :math:`c_0`, an input sequence :math:`x`,
weight matrices :math:`W`, and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` and :math:`c_t` for each
time :math:`t` from input :math:`x_t`.

.. math::
   h_t = f(W_0 x_t + W_1 h_{t-1} + b_0 + b_1)

where :math:`f` is an activation function.
Weight matrices :math:`W` contains two matrices :math:`W_0` and
:math:`W_1`. :math:`W_0` is a parameter for an input sequence.
:math:`W_1` is a parameter for a hidden state.
Bias matrices :math:`b` contains two matrices :math:`b_0` and :math:`b_1`.
:math:`b_0` is a parameter for an input sequence.
:math:`b_1` is a parameter for a hidden state.
As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Two weight matrices and two bias vectors are
required for each layer. So, when :math:`S` layers exist, you need to
prepare :math:`2S` weight matrices and :math:`2S` bias vectors.
If the number of layers ``n_layers`` is greather than :math:`1`, input
of ``k``-th layer is hidden state ``h_t`` of ``k-1``-th layer.
Note that all input variables except first layer may have different shape
from the first layer.

Args:
    n_layers(int): Number of layers.
    hx (:class:`~chainerx.array`):
        Variable holding stacked hidden states.
        Its shape is ``(S, B, N)`` where ``S`` is number of layers and is
        equal to ``n_layers``, ``B`` is mini-batch size, and ``N`` is
        dimension of hidden units.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[i]`` represents weights for i-th layer.
        Each ``ws[i]`` is a list containing two matrices.
        ``ws[i][j]`` is corresponding with ``W_j`` in the equation.
        Only ``ws[0][j]`` where ``0 <= j < 1`` is ``(N, I)`` shape as they
        are multiplied with input variables. All other matrices has
        ``(N, N)`` shape.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[i]`` represnents biases for i-th layer.
        Each ``bs[i]`` is a list containing two vectors.
        ``bs[i][j]`` is corresponding with ``b_j`` in the equation.
        Shape of each matrix is ``(N,)`` where ``N`` is dimension of
        hidden units.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array` holding input values.
        Each element ``xs[t]`` holds input value for time ``t``.
        Its shape is ``(B_t, I)``, where ``B_t`` is
        mini-batch size for time ``t``, and ``I`` is size of input units.
        Note that this function supports variable length sequences.
        When sequneces has different lengths, sort sequences in descending
        order by length.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] >= xs[t + 1].shape[0]``.
    activation (str): Activation function name.
        Please select ``tanh`` or ``relu``.

Returns:
    tuple: This function returns a tuple containing two elements,
    ``hy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is same as ``hx``.
    - ``ys`` is a list of :class:`~chainerx.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, N)`` where ``B_t`` is
      mini-batch size for time ``t``, and ``N`` is size of hidden
      units. Note that ``B_t`` is the same value as ``xs[t]``.
    """)

    _docs.set_doc(
        chainerx.n_step_birnn,
        """n_step_birnn(n_layers, hx, ws, bs, xs, activation='tanh')
Stacked Bi-directional RNN function for sequence inputs.
This function calculates stacked Bi-directional RNN with sequences.
This function gets an initial hidden state :math:`h_0`, an initial
cell state :math:`c_0`, an input sequence :math:`x`,
weight matrices :math:`W`, and bias vectors :math:`b`.
This function calculates hidden states :math:`h_t` and :math:`c_t` for each
time :math:`t` from input :math:`x_t`.

.. math::
    h^{f}_t &=& f(W^{f}_0 x_t + W^{f}_1 h_{t-1} + b^{f}_0 + b^{f}_1), \\\\
    h^{b}_t &=& f(W^{b}_0 x_t + W^{b}_1 h_{t-1} + b^{b}_0 + b^{b}_1), \\\\
    h_t  &=& [h^{f}_t; h^{f}_t], \\\\

where :math:`f` is an activation function.
Weight matrices :math:`W` contains two matrices :math:`W^{f}` and
:math:`W^{b}`. :math:`W^{f}` is weight matrices for forward directional
RNN. :math:`W^{b}` is weight matrices for backward directional RNN.
:math:`W^{f}` contains :math:`W^{f}_0` for an input sequence and
:math:`W^{f}_1` for a hidden state.
:math:`W^{b}` contains :math:`W^{b}_0` for an input sequence and
:math:`W^{b}_1` for a hidden state.
Bias matrices :math:`b` contains two matrices :math:`b^{f}` and
:math:`b^{f}`. :math:`b^{f}` contains :math:`b^{f}_0` for an input sequence
and :math:`b^{f}_1` for a hidden state.
:math:`b^{b}` contains :math:`b^{b}_0` for an input sequence and
:math:`b^{b}_1` for a hidden state.
As the function accepts a sequence, it calculates :math:`h_t` for all
:math:`t` with one call. Two weight matrices and two bias vectors are
required for each layer. So, when :math:`S` layers exist, you need to
prepare :math:`2S` weight matrices and :math:`2S` bias vectors.
If the number of layers ``n_layers`` is greather than :math:`1`, input
of ``k``-th layer is hidden state ``h_t`` of ``k-1``-th layer.
Note that all input variables except first layer may have different shape
from the first layer.

Args:
    n_layers(int): Number of layers.
    hx (:class:`~chainerx.array`):
        Variable holding stacked hidden states.
        Its shape is ``(2S, B, N)`` where ``S`` is number of layers and is
        equal to ``n_layers``, ``B`` is mini-batch size, and ``N`` is
        dimension of hidden units. Because of bi-direction, the
        first dimension length is ``2S``.
    ws (list of list of :class:`~chainerx.array`): Weight matrices.
        ``ws[i + di]`` represents weights for i-th layer.
        Note that ``di = 0`` for forward-RNN and ``di = 1`` for
        backward-RNN.
        Each ``ws[i + di]`` is a list containing two matrices.
        ``ws[i + di][j]`` is corresponding with ``W^{f}_j`` if ``di = 0``
        and corresponding with ``W^{b}_j`` if ``di = 1`` in the equation.
        Only ``ws[0][j]`` and ``ws[1][j]`` where ``0 <= j < 1`` are
        ``(I, N)`` shape as they are multiplied with input variables.
        All other matrices has ``(N, N)`` shape.
    bs (list of list of :class:`~chainerx.array`): Bias vectors.
        ``bs[i + di]`` represnents biases for i-th layer.
        Note that ``di = 0`` for forward-RNN and ``di = 1`` for
        backward-RNN.
        Each ``bs[i + di]`` is a list containing two vectors.
        ``bs[i + di][j]`` is corresponding with ``b^{f}_j`` if ``di = 0``
        and corresponding with ``b^{b}_j`` if ``di = 1`` in the equation.
        Shape of each matrix is ``(N,)`` where ``N`` is dimension of
        hidden units.
    xs (list of :class:`~chainerx.array`):
        A list of :class:`~chainerx.array` holding input values.
        Each element ``xs[t]`` holds input value
        for time ``t``. Its shape is ``(B_t, I)``, where ``B_t`` is
        mini-batch size for time ``t``, and ``I`` is size of input units.
        Note that this function supports variable length sequences.
        When sequneces has different lengths, sort sequences in descending
        order by length.
        So ``xs`` needs to satisfy
        ``xs[t].shape[0] >= xs[t + 1].shape[0]``.
    activation (str): Activation function name.
        Please select ``tanh`` or ``relu``.

Returns:
    tuple: This function returns a tuple containing two elements,
    ``hy`` and ``ys``.

    - ``hy`` is an updated hidden states whose shape is same as ``hx``.
    - ``ys`` is a list of :class:`~chainerx.array` . Each element
      ``ys[t]`` holds hidden states of the last layer corresponding
      to an input ``xs[t]``. Its shape is ``(B_t, N)`` where ``B_t``
      is mini-batch size for time ``t``, and ``N`` is size of hidden
      units. Note that ``B_t`` is the same value as ``xs[t]``.
    """)
</source>
</class>

<class classid="118" nclones="2" nlines="13" similarity="100">
<source file="systems/chainer-7.8.1/chainerx/_docs/routines.py" startline="2751" endline="2852" pcid="10725">
def _docs_statistics():
    _docs.set_doc(
        chainerx.amax,
        """amax(a, axis=None, keepdims=False)
Returns the maximum of an array or the maximum along an axis.

Note:
    When at least one element is NaN, the corresponding max value will be NaN.

Args:
    a (~chainerx.ndarray): Array to take the maximum.
    axis (None or int or tuple of ints): Along which axis to take the maximum.
        The flattened array is used by default.
        If this is a tuple of ints, the maximum is selected over multiple
        axes, instead of a single axis or all the axes.
    keepdims (bool): If ``True``, the axis is remained as an axis of size one.

Returns:
    :class:`~chainerx.ndarray`: The maximum of ``a``, along the axis if
    specified.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.amax`
""")

    _docs.set_doc(
        chainerx.amin,
        """amin(a, axis=None, keepdims=False)
Returns the minimum of an array or the minimum along an axis.

Note:
    When at least one element is NaN, the corresponding min value will be NaN.

Args:
    a (~chainerx.ndarray): Array to take the minimum.
    axis (None or int or tuple of ints): Along which axis to take the minimum.
        The flattened array is used by default.
        If this is a tuple of ints, the minimum is selected over multiple
        axes, instead of a single axis or all the axes.
    keepdims (bool): If ``True``, the axis is remained as an axis of size one.

Returns:
    :class:`~chainerx.ndarray`: The minimum of ``a``, along the axis if
    specified.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to the input array ``a``.

.. seealso:: :func:`numpy.amin`
""")

    _docs.set_doc(
        chainerx.mean,
        """mean(a, axis=None, keepdims=False)
Compute the arithmetic mean along the specified axis.

Returns the average of the array elements. The average is taken over the
flattened array by default, otherwise over the specified axis.

Args:
    a (~chainerx.ndarray): Array to take the mean of.
    axis (None or int or tuple of ints): Along which axis or axes to compute
    the mean. The flattened array is used by default.
    keepdims (bool): If this is set to True, the axes which are reduced are
    left in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

Returns:
    :class:`~chainerx.ndarray`: The mean of ``a``, along the axis or axes if
    specified.

.. seealso:: :func:`numpy.mean`
""")

    _docs.set_doc(
        chainerx.var,
        """var(a, axis=None, keepdims=False)
Compute the arithmetic var along the specified axis.

Returns the var of the array elements. The var is taken over the flattened
array by default, otherwise over the specified axis.

Args:
    a (~chainerx.ndarray): Array to take the var of.
    axis (None or int or tuple of ints): Along which axis or axes to compute
    the var. The flattened array is used by default.
    keepdims (bool): If this is set to True, the axes which are reduced are
    left in the result as dimensions with size one. With this option,
    the result will broadcast correctly against the input array.

Returns:
    :class:`~chainerx.ndarray`: The var of ``a``, along the axis or axes if
    specified.

.. seealso:: :func:`numpy.var`
""")


</source>
<source file="systems/chainer-7.8.1/chainerx/_docs/routines.py" startline="2853" endline="3224" pcid="10726">
def _docs_connection():
    _docs.set_doc(
        chainerx.conv,
        """conv(x, w, b=None, stride=1, pad=0, cover_all=False)
N-dimensional convolution.

This is an implementation of N-dimensional convolution which is generalized
two-dimensional convolution in ConvNets. It takes three arrays: the
input ``x``, the filter weight ``w`` and the bias vector ``b``.

Notation: here is a notation for dimensionalities.

- :math:`N` is the number of spatial dimensions.
- :math:`n` is the batch size.
- :math:`c_I` and :math:`c_O` are the number of the input and output
  channels, respectively.
- :math:`d_1, d_2, ..., d_N` are the size of each axis of the input's
  spatial dimensions, respectively.
- :math:`k_1, k_2, ..., k_N` are the size of each axis of the filters,
  respectively.
- :math:`l_1, l_2, ..., l_N` are the size of each axis of the output's
  spatial dimensions, respectively.
- :math:`p_1, p_2, ..., p_N` are the size of each axis of the spatial
  padding size, respectively.

Then the ``conv`` function computes correlations between filters
and patches of size :math:`(k_1, k_2, ..., k_N)` in ``x``.
Note that correlation here is equivalent to the inner product between
expanded tensors.
Patches are extracted at positions shifted by multiples of ``stride`` from
the first position ``(-p_1, -p_2, ..., -p_N)`` for each spatial axis.

Let :math:`(s_1, s_2, ..., s_N)` be the stride of filter application.
Then, the output size :math:`(l_1, l_2, ..., l_N)` is determined by the
following equations:

.. math::

   l_n = (d_n + 2p_n - k_n) / s_n + 1 \\ \\ (n = 1, ..., N)

If ``cover_all`` option is ``True``, the filter will cover the all
spatial locations. So, if the last stride of filter does not cover the
end of spatial locations, an additional stride will be applied to the end
part of spatial locations. In this case, the output size is determined by
the following equations:

.. math::

   l_n = (d_n + 2p_n - k_n + s_n - 1) / s_n + 1 \\ \\ (n = 1, ..., N)

Args:
    x (:class:`~chainerx.ndarray`):
        Input array of shape :math:`(n, c_I, d_1, d_2, ..., d_N)`.
    w (:class:`~chainerx.ndarray`):
        Weight array of shape :math:`(c_O, c_I, k_1, k_2, ..., k_N)`.
    b (None or :class:`~chainerx.ndarray`):
        One-dimensional bias array with length :math:`c_O` (optional).
    stride (:class:`int` or :class:`tuple` of :class:`int` s):
        Stride of filter applications :math:`(s_1, s_2, ..., s_N)`.
        ``stride=s`` is equivalent to ``(s, s, ..., s)``.
    pad (:class:`int` or :class:`tuple` of :class:`int` s):
        Spatial padding width for input arrays
        :math:`(p_1, p_2, ..., p_N)`. ``pad=p`` is equivalent to
        ``(p, p, ..., p)``.
    cover_all (bool): If ``True``, all spatial locations are convoluted
        into some output pixels. It may make the output size larger.
        `cover_all` needs to be ``False`` if you want to use ``cuda`` backend.

Returns:
    ~chainerx.ndarray:
        Output array of shape :math:`(n, c_O, l_1, l_2, ..., l_N)`.

Note:

    In ``cuda`` backend, this function uses cuDNN implementation for its
    forward and backward computation.

Note:

    In ``cuda`` backend, this function has following limitations yet:

    - The ``cover_all=True`` option is not supported yet.
    - The ``dtype`` must be ``float32`` or ``float64`` (``float16`` is not
      supported yet.)

Note:

    During backpropagation, this function propagates the gradient of the
    output array to input arrays ``x``, ``w``, and ``b``.

.. seealso:: :func:`chainer.functions.convolution_nd`

.. admonition:: Example

    >>> n = 10
    >>> c_i, c_o = 3, 1
    >>> d1, d2, d3 = 30, 40, 50
    >>> k1, k2, k3 = 10, 10, 10
    >>> p1, p2, p3 = 5, 5, 5
    >>> x = chainerx.random.uniform(0, 1, (n, c_i, d1, d2, d3)).\
astype(np.float32)
    >>> x.shape
    (10, 3, 30, 40, 50)
    >>> w = chainerx.random.uniform(0, 1, (c_o, c_i, k1, k2, k3)).\
astype(np.float32)
    >>> w.shape
    (1, 3, 10, 10, 10)
    >>> b = chainerx.random.uniform(0, 1, (c_o)).astype(np.float32)
    >>> b.shape
    (1,)
    >>> s1, s2, s3 = 2, 4, 6
    >>> y = chainerx.conv(x, w, b, stride=(s1, s2, s3),\
 pad=(p1, p2, p3))
    >>> y.shape
    (10, 1, 16, 11, 9)
    >>> l1 = int((d1 + 2 * p1 - k1) / s1 + 1)
    >>> l2 = int((d2 + 2 * p2 - k2) / s2 + 1)
    >>> l3 = int((d3 + 2 * p3 - k3) / s3 + 1)
    >>> y.shape == (n, c_o, l1, l2, l3)
    True
    >>> y = chainerx.conv(x, w, b, stride=(s1, s2, s3),\
 pad=(p1, p2, p3), cover_all=True)
    >>> y.shape == (n, c_o, l1, l2, l3 + 1)
    True
""")

    _docs.set_doc(
        chainerx.conv_transpose,
        """conv_transpose(x, w, b=None, stride=1, pad=0, outsize=None)
N-dimensional transposed convolution.

This is an implementation of N-dimensional transposed convolution, which is
previously known as **deconvolution** in Chainer.

.. _Deconvolutional Networks: \
://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf

It takes three arrays: the input ``x``, the filter weight ``w``, and the
bias vector ``b``.

Notation: here is a notation for dimensionalities.

- :math:`N` is the number of spatial dimensions.
- :math:`n` is the batch size.
- :math:`c_I` and :math:`c_O` are the number of the input and output
  channels, respectively.
- :math:`d_1, d_2, ..., d_N` are the size of each axis of the input's
  spatial dimensions, respectively.
- :math:`k_1, k_2, ..., k_N` are the size of each axis of the filters,
  respectively.
- :math:`p_1, p_2, ..., p_N` are the size of each axis of the spatial
  padding size, respectively.
- :math:`s_1, s_2, ..., s_N` are the stride of each axis of filter
  application, respectively.

If ``outsize`` option is ``None``, the output size
:math:`(l_1, l_2, ..., l_N)` is determined by the following equations with
the items in the above list:

.. math::

   l_n = s_n (d_n - 1)  + k_n - 2 p_n \\ \\ (n = 1, ..., N)

If ``outsize`` option is given, the output size is determined by
``outsize``. In this case, the ``outsize`` :math:`(l_1, l_2, ..., l_N)`
must satisfy the following equations:

.. math::

   d_n = \\lfloor (l_n + 2p_n - k_n) / s_n \\rfloor + 1 \\ \\ \
   (n = 1, ..., N)

Args:
    x (:class:`~chainerx.ndarray`):
        Input array of shape :math:`(n, c_I, d_1, d_2, ..., d_N)`.
    w (:class:`~chainerx.ndarray`):
        Weight array of shape :math:`(c_I, c_O, k_1, k_2, ..., k_N)`.
    b (None or :class:`~chainerx.ndarray`):
        One-dimensional bias array with length :math:`c_O` (optional).
    stride (:class:`int` or :class:`tuple` of :class:`int` s):
        Stride of filter applications :math:`(s_1, s_2, ..., s_N)`.
        ``stride=s`` is equivalent to ``(s, s, ..., s)``.
    pad (:class:`int` or :class:`tuple` of :class:`int` s):
        Spatial padding width for input arrays
        :math:`(p_1, p_2, ..., p_N)`. ``pad=p`` is equivalent to
        ``(p, p, ..., p)``.
    outsize (None or :class:`tuple` of :class:`int` s):
        Expected output size of deconvolutional operation. It should be a
        tuple of ints :math:`(l_1, l_2, ..., l_N)`. Default value is
        ``None`` and the outsize is estimated by input size, stride and
        pad.

Returns:
    ~chainerx.ndarray:
        Output array of shape :math:`(n, c_O, l_1, l_2, ..., l_N)`.

Note:

    During backpropagation, this function propagates the gradient of the
    output array to input arrays ``x``, ``w``, and ``b``.

.. seealso:: :func:`chainer.functions.deconvolution_nd`

.. admonition:: Example

    **Example1**: the case when ``outsize`` is not given.

    >>> n = 10
    >>> c_i, c_o = 3, 1
    >>> d1, d2, d3 = 5, 10, 15
    >>> k1, k2, k3 = 10, 10, 10
    >>> p1, p2, p3 = 5, 5, 5
    >>> x = chainerx.random.uniform(0, 1, (n, c_i, d1, d2, d3)).\
astype(np.float32)
    >>> x.shape
    (10, 3, 5, 10, 15)
    >>> w = chainerx.random.uniform(0, 1, (c_i, c_o, k1, k2, k3)).\
astype(np.float32)
    >>> w.shape
    (3, 1, 10, 10, 10)
    >>> b = chainerx.random.uniform(0, 1, (c_o)).astype(np.float32)
    >>> b.shape
    (1,)
    >>> s1, s2, s3 = 2, 4, 6
    >>> y = chainerx.conv_transpose(x, w, b, stride=(s1, s2, s3), \
pad=(p1, p2, p3))
    >>> y.shape
    (10, 1, 8, 36, 84)
    >>> l1 = s1 * (d1 - 1) + k1 - 2 * p1
    >>> l2 = s2 * (d2 - 1) + k2 - 2 * p2
    >>> l3 = s3 * (d3 - 1) + k3 - 2 * p3
    >>> y.shape == (n, c_o, l1, l2, l3)
    True

    **Example2**: the case when ``outsize`` is given.

    >>> n = 10
    >>> c_i, c_o = 3, 1
    >>> d1, d2, d3 = 5, 10, 15
    >>> k1, k2, k3 = 10, 10, 10
    >>> p1, p2, p3 = 5, 5, 5
    >>> x = chainerx.array(np.random.uniform(0, 1, (n, c_i, d1, d2, d3)).\
astype(np.float32))
    >>> x.shape
    (10, 3, 5, 10, 15)
    >>> w = chainerx.array(np.random.uniform(0, 1, (c_i, c_o, k1, k2, k3)).\
astype(np.float32))
    >>> w.shape
    (3, 1, 10, 10, 10)
    >>> b = chainerx.array(np.random.uniform(0, 1, (c_o)).astype(np.float32))
    >>> b.shape
    (1,)
    >>> s1, s2, s3 = 2, 4, 6
    >>> l1, l2, l3 = 9, 38, 87
    >>> d1 == int((l1 + 2 * p1 - k1) / s1) + 1
    True
    >>> d2 == int((l2 + 2 * p2 - k2) / s2) + 1
    True
    >>> d3 == int((l3 + 2 * p3 - k3) / s3) + 1
    True
    >>> y = chainerx.conv_transpose(x, w, b, stride=(s1, s2, s3), \
pad=(p1, p2, p3), outsize=(l1, l2, l3))
    >>> y.shape
    (10, 1, 9, 38, 87)
    >>> y.shape == (n, c_o, l1, l2, l3)
    True
""")

    _docs.set_doc(
        chainerx.linear,
        """linear(x, W, b=None, n_batch_axis=1)
Linear function, or affine transformation.

It accepts two or three arguments: an input minibatch ``x``, a weight
matrix ``W``, and optionally a bias vector ``b``. It computes

.. math:: Y = xW^\\top + b.

Args:
    x (~chainerx.ndarray):
        Input array, which is a :math:`(s_1, s_2, ..., s_n)`-shaped array.
    W (~chainerx.ndarray):
        Weight variable of shape :math:`(M, N)`,
        where :math:`(N = s_{\\rm n\\_batch\\_axes} * ... * s_n)`.
    b (~chainerx.ndarray):
        Bias variable (optional) of shape :math:`(M,)`.
    n_batch_axes (int):
        The number of batch axes. The default is 1. The input variable is
        reshaped into (:math:`{\\rm n\\_batch\\_axes} + 1`)-dimensional
        tensor. This should be greater than 0.

Returns:
    :class:`~chainerx.ndarray`:
        Output array with shape of
        :math:`(s_1, ..., s_{\\rm n\\_batch\\_axes}, M)`.

Note:
    During backpropagation, this function propagates the gradient of the
    output array to input arrays ``x``, ``W`` and ``b``.
""")

    _docs.set_doc(
        chainerx.lstm,
        """lstm(c_prev, x)
Long Short-Term Memory units as an activation function.

This function implements LSTM units with forget gates. Let the previous
cell state ``c_prev`` and the input array ``x``.
First, the input array ``x`` is split into four arrays
:math:`a, i, f, o` of the same shapes along the second axis. It means that
``x`` 's second axis must have 4 times the ``c_prev`` 's second axis.
The split input arrays are corresponding to:

    - :math:`a` : sources of cell input
    - :math:`i` : sources of input gate
    - :math:`f` : sources of forget gate
    - :math:`o` : sources of output gate

Second, it computes the updated cell state ``c`` and the outgoing signal
``h`` as

.. math::
    c &= \\tanh(a) \\sigma(i)
       + c_{\\text{prev}} \\sigma(f), \\\\
    h &= \\tanh(c) \\sigma(o),

where :math:`\\sigma` is the elementwise sigmoid function.
These are returned as a tuple of two variables.
This function supports variable length inputs. The mini-batch size of
the current input must be equal to or smaller than that of the previous
one. When mini-batch size of ``x`` is smaller than that of ``c``, this
function only updates ``c[0:len(x)]`` and doesn't change the rest of ``c``,
``c[len(x):]``. So,
please sort input sequences in descending order of lengths before
applying the function.

Args:
    c_prev (:class:`~chainerx.array`):
        Variable that holds the previous cell state. The cell state
        should be a zero array or the output of the previous call of LSTM.
    x (:class:`~chainer.array`):
        Variable that holds the sources of cell input, input gate, forget
        gate and output gate. It must have the second dimension whose size
        is four times of that of the cell state.

Returns:
    tuple: Two :class:`~chainerx.array` objects ``c`` and ``h``.
    ``c`` is the updated cell state. ``h`` indicates the outgoing signal.

See the original paper proposing LSTM with forget gates:
`Long Short-Term Memory in Recurrent Neural Networks
<http://www.felixgers.de/papers/phd.pdf>`_.

.. admonition:: Example

    Assuming ``y`` is the current incoming signal, ``c`` is the previous
    cell state, and ``h`` is the previous outgoing signal from an ``lstm``
    function. Each of ``y``, ``c`` and ``h`` has ``n_units`` channels.
    Most typical preparation of ``x`` is

    >>> n_units = 100
    >>> c_prev = chainerx.zeros((1, n_units), chainerx.float32)
    >>> x = chainerx.zeros((1, 4 * n_units), chainerx.float32)
    >>> c, h = chainerx.lstm(c_prev, x)

    It corresponds to calculate the input array ``x``, or the input
    sources :math:`a, i, f, o`, from the current incoming signal ``y`` and
    the previous outgoing signal ``h``. Different parameters are used for
    different kind of input sources.
""")


</source>
</class>

</clones>

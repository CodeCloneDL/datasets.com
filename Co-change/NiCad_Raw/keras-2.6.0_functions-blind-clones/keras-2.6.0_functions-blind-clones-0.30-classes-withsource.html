<html>
<head>
    <title>NiCad6 Clone Report</title>
    <style type="text/css">
        body {font-family:sans-serif;}
        table {background-color:white; border:0px; padding:0px; border-spacing:4px; width:auto; margin-left:30px; margin-right:auto;}
        td {background-color:rgba(192,212,238,0.8); border:0px; padding:8px; width:auto; vertical-align:top; border-radius:8px}
        pre {background-color:white; padding:4px;}
        a {color:darkblue;}
    </style>
</head>
<body>
<h2>NiCad6 Clone Report</h2>
<table>
<tr style="font-size:14pt">
<td><b>System:</b> &nbsp; keras-2.6.0</td>
<td><b>Clone pairs:</b> &nbsp; 2118</td>
<td><b>Clone classes:</b> &nbsp; 298</td>
</tr>
<tr style="font-size:12pt">
<td style="background-color:white">Clone type: &nbsp; 3-2</td>
<td style="background-color:white">Granularity: &nbsp; functions-blind</td>
<td style="background-color:white">Max diff threshold: &nbsp; 30%</td>
<td style="background-color:white">Clone size: &nbsp; 10 - 2500 lines</td>
<td style="background-color:white">Total functions-blind: &nbsp; 5370</td>
</tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 1:</b> &nbsp; 7 fragments, nominal size 15 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adam.py: 97-112
</a>
<div class="mid" id="frag4" style="display:none"><pre>
  def __init__(self,
               learning_rate=0.001,
               beta_1=0.9,
               beta_2=0.999,
               epsilon=1e-7,
               amsgrad=False,
               name='Adam',
               **kwargs):
    super(Adam, self).__init__(name, **kwargs)
    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
    self._set_hyper('decay', self._initial_decay)
    self._set_hyper('beta_1', beta_1)
    self._set_hyper('beta_2', beta_2)
    self.epsilon = epsilon or backend_config.epsilon()
    self.amsgrad = amsgrad

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag213')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adamax.py: 83-96
</a>
<div class="mid" id="frag213" style="display:none"><pre>
  def __init__(self,
               learning_rate=0.001,
               beta_1=0.9,
               beta_2=0.999,
               epsilon=1e-7,
               name='Adamax',
               **kwargs):
    super(Adamax, self).__init__(name, **kwargs)
    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
    self._set_hyper('decay', self._initial_decay)
    self._set_hyper('beta_1', beta_1)
    self._set_hyper('beta_2', beta_2)
    self.epsilon = epsilon or backend_config.epsilon()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag66')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/nadam.py: 58-80
</a>
<div class="mid" id="frag66" style="display:none"><pre>
  def __init__(self,
               learning_rate=0.001,
               beta_1=0.9,
               beta_2=0.999,
               epsilon=1e-7,
               name='Nadam',
               **kwargs):
    # Backwards compatibility with keras NAdam optimizer.
    kwargs['decay'] = kwargs.pop('schedule_decay', 0.004)
    learning_rate = kwargs.get('lr', learning_rate)
    if isinstance(learning_rate, learning_rate_schedule.LearningRateSchedule):
      raise ValueError('The Nadam optimizer does not support '
                       'tf.keras.optimizers.LearningRateSchedules as the '
                       'learning rate.')

    super(Nadam, self).__init__(name, **kwargs)
    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
    self._set_hyper('decay', self._initial_decay)
    self._set_hyper('beta_1', beta_1)
    self._set_hyper('beta_2', beta_2)
    self.epsilon = epsilon or backend_config.epsilon()
    self._m_cache = None

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag192')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/rmsprop.py: 90-144
</a>
<div class="mid" id="frag192" style="display:none"><pre>
  def __init__(self,
               learning_rate=0.001,
               rho=0.9,
               momentum=0.0,
               epsilon=1e-7,
               centered=False,
               name="RMSprop",
               **kwargs):
    """Construct a new RMSprop optimizer.

    Args:
      learning_rate: A `Tensor`, floating point value, or a schedule that is a
        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable
        that takes no arguments and returns the actual value to use. The
        learning rate. Defaults to 0.001.
      rho: Discounting factor for the history/coming gradient. Defaults to 0.9.
      momentum: A scalar or a scalar `Tensor`. Defaults to 0.0.
      epsilon: A small constant for numerical stability. This epsilon is
        "epsilon hat" in the Kingma and Ba paper (in the formula just before
        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to
        1e-7.
      centered: Boolean. If `True`, gradients are normalized by the estimated
        variance of the gradient; if False, by the uncentered second moment.
        Setting this to `True` may help with training, but is slightly more
        expensive in terms of computation and memory. Defaults to `False`.
      name: Optional name prefix for the operations created when applying
        gradients. Defaults to "RMSprop".
      **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,
        `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip
        gradients by value, `decay` is included for backward compatibility to
        allow time inverse decay of learning rate. `lr` is included for backward
        compatibility, recommended to use `learning_rate` instead.

    @compatibility(eager)
    When eager execution is enabled, `learning_rate`, `decay`, `momentum`, and
    `epsilon` can each be a callable that takes no arguments and returns the
    actual value to use. This can be useful for changing these values across
    different invocations of optimizer functions.
    @end_compatibility
    """
    super(RMSprop, self).__init__(name, **kwargs)
    self._set_hyper("learning_rate", kwargs.get("lr", learning_rate))
    self._set_hyper("decay", self._initial_decay)
    self._set_hyper("rho", rho)

    self._momentum = False
    if isinstance(momentum, tf.Tensor) or callable(momentum) or momentum &gt; 0:
      self._momentum = True
    if isinstance(momentum, (int, float)) and (momentum &lt; 0 or momentum &gt; 1):
      raise ValueError("`momentum` must be between [0, 1].")
    self._set_hyper("momentum", momentum)

    self.epsilon = epsilon or backend_config.epsilon()
    self.centered = centered

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag11')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adam.py: 321-364
</a>
<div class="mid" id="frag11" style="display:none"><pre>
  def __init__(self,
               learning_rate=0.001,
               beta_1=0.9,
               beta_2=0.999,
               epsilon=1e-7,
               amsgrad=False,
               name='Adam',
               **kwargs):
    """Construct a new Adam optimizer.

    Args:
      learning_rate: A `Tensor`, floating point value, or a schedule that is a
        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable that
        takes no arguments and returns the actual value to use, The learning
        rate. Defaults to 0.001.
      beta_1: A float value or a constant float tensor, or a callable that takes
        no arguments and returns the actual value to use. The exponential decay
        rate for the 1st moment estimates. Defaults to 0.9.
      beta_2: A float value or a constant float tensor, or a callable that takes
        no arguments and returns the actual value to use, The exponential decay
        rate for the 2nd moment estimates. Defaults to 0.999.
      epsilon: A small constant for numerical stability. This epsilon is
        "epsilon hat" in the Kingma and Ba paper (in the formula just before
        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to
        1e-7.
      amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from
        the paper "On the Convergence of Adam and beyond". Defaults to `False`.
      name: Optional name for the operations created when applying gradients.
        Defaults to "Adam".
      **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,
        `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip
        gradients by value, `decay` is included for backward compatibility to
        allow time inverse decay of learning rate. `lr` is included for backward
        compatibility, recommended to use `learning_rate` instead.
    """

    super(NonFusedAdam, self).__init__(name, **kwargs)
    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
    self._set_hyper('decay', self._initial_decay)
    self._set_hyper('beta_1', beta_1)
    self._set_hyper('beta_2', beta_2)
    self.epsilon = epsilon or backend_config.epsilon()
    self.amsgrad = amsgrad

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag258')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adadelta.py: 68-79
</a>
<div class="mid" id="frag258" style="display:none"><pre>
  def __init__(self,
               learning_rate=0.001,
               rho=0.95,
               epsilon=1e-7,
               name='Adadelta',
               **kwargs):
    super(Adadelta, self).__init__(name, **kwargs)
    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))
    self._set_hyper('decay', self._initial_decay)
    self._set_hyper('rho', rho)
    self.epsilon = epsilon or backend_config.epsilon()

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag206')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/gradient_descent.py: 96-114
</a>
<div class="mid" id="frag206" style="display:none"><pre>
  def __init__(self,
               learning_rate=0.01,
               momentum=0.0,
               nesterov=False,
               name="SGD",
               **kwargs):
    super(SGD, self).__init__(name, **kwargs)
    self._set_hyper("learning_rate", kwargs.get("lr", learning_rate))
    self._set_hyper("decay", self._initial_decay)

    self._momentum = False
    if isinstance(momentum, tf.Tensor) or callable(momentum) or momentum &gt; 0:
      self._momentum = True
    if isinstance(momentum, (int, float)) and (momentum &lt; 0 or momentum &gt; 1):
      raise ValueError("`momentum` must be between [0, 1].")
    self._set_hyper("momentum", momentum)

    self.nesterov = nesterov

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 2:</b> &nbsp; 3 fragments, nominal size 19 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag6')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adam.py: 124-145
</a>
<div class="mid" id="frag6" style="display:none"><pre>
  def _prepare_local(self, var_device, var_dtype, apply_state):
    super(Adam, self)._prepare_local(var_device, var_dtype, apply_state)

    local_step = tf.cast(self.iterations + 1, var_dtype)
    beta_1_t = tf.identity(self._get_hyper('beta_1', var_dtype))
    beta_2_t = tf.identity(self._get_hyper('beta_2', var_dtype))
    beta_1_power = tf.pow(beta_1_t, local_step)
    beta_2_power = tf.pow(beta_2_t, local_step)
    lr = (apply_state[(var_device, var_dtype)]['lr_t'] *
          (tf.sqrt(1 - beta_2_power) / (1 - beta_1_power)))
    apply_state[(var_device, var_dtype)].update(
        dict(
            lr=lr,
            epsilon=tf.convert_to_tensor(
                self.epsilon, var_dtype),
            beta_1_t=beta_1_t,
            beta_1_power=beta_1_power,
            one_minus_beta_1_t=1 - beta_1_t,
            beta_2_t=beta_2_t,
            beta_2_power=beta_2_power,
            one_minus_beta_2_t=1 - beta_2_t))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag215')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adamax.py: 104-123
</a>
<div class="mid" id="frag215" style="display:none"><pre>
  def _prepare_local(self, var_device, var_dtype, apply_state):
    super(Adamax, self)._prepare_local(var_device, var_dtype, apply_state)

    local_step = tf.cast(self.iterations + 1, var_dtype)
    beta_1_t = tf.identity(self._get_hyper('beta_1', var_dtype))
    beta_2_t = tf.identity(self._get_hyper('beta_2', var_dtype))
    beta_1_power = tf.pow(beta_1_t, local_step)
    lr_t = apply_state[(var_device, var_dtype)]['lr_t']

    apply_state[(var_device, var_dtype)].update(
        dict(
            neg_scaled_lr=-lr_t / (1 - beta_1_power),
            epsilon=tf.convert_to_tensor(
                self.epsilon, var_dtype),
            beta_1_t=beta_1_t,
            beta_1_power=beta_1_power,
            one_minus_beta_1_t=1 - beta_1_t,
            beta_2_t=beta_2_t,
            zero=tf.zeros((), dtype=tf.int64)))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag13')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adam.py: 376-398
</a>
<div class="mid" id="frag13" style="display:none"><pre>
  def _prepare_local(self, var_device, var_dtype, apply_state):
    super(NonFusedAdam, self)._prepare_local(var_device, var_dtype, apply_state)

    local_step = tf.cast(self.iterations + 1, var_dtype)
    beta_1_t = tf.identity(self._get_hyper('beta_1', var_dtype))
    beta_2_t = tf.identity(self._get_hyper('beta_2', var_dtype))
    beta_1_power = tf.pow(beta_1_t, local_step)
    beta_2_power = tf.pow(beta_2_t, local_step)
    lr = (
        apply_state[(var_device, var_dtype)]['lr_t'] *
        (tf.sqrt(1 - beta_2_power) / (1 - beta_1_power)))
    apply_state[(var_device, var_dtype)].update(
        dict(
            lr=lr,
            epsilon=tf.convert_to_tensor(
                self.epsilon, var_dtype),
            beta_1_t=beta_1_t,
            beta_1_power=beta_1_power,
            one_minus_beta_1_t=1 - beta_1_t,
            beta_2_t=beta_2_t,
            beta_2_power=beta_2_power,
            one_minus_beta_2_t=1 - beta_2_t))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 3:</b> &nbsp; 3 fragments, nominal size 33 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag8')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adam.py: 156-192
</a>
<div class="mid" id="frag8" style="display:none"><pre>
  def _resource_apply_dense(self, grad, var, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    m = self.get_slot(var, 'm')
    v = self.get_slot(var, 'v')

    if not self.amsgrad:
      return tf.raw_ops.ResourceApplyAdam(
          var=var.handle,
          m=m.handle,
          v=v.handle,
          beta1_power=coefficients['beta_1_power'],
          beta2_power=coefficients['beta_2_power'],
          lr=coefficients['lr_t'],
          beta1=coefficients['beta_1_t'],
          beta2=coefficients['beta_2_t'],
          epsilon=coefficients['epsilon'],
          grad=grad,
          use_locking=self._use_locking)
    else:
      vhat = self.get_slot(var, 'vhat')
      return tf.raw_ops.ResourceApplyAdamWithAmsgrad(
          var=var.handle,
          m=m.handle,
          v=v.handle,
          vhat=vhat.handle,
          beta1_power=coefficients['beta_1_power'],
          beta2_power=coefficients['beta_2_power'],
          lr=coefficients['lr_t'],
          beta1=coefficients['beta_1_t'],
          beta2=coefficients['beta_2_t'],
          epsilon=coefficients['epsilon'],
          grad=grad,
          use_locking=self._use_locking)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag268')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/ftrl.py: 165-202
</a>
<div class="mid" id="frag268" style="display:none"><pre>
  def _resource_apply_dense(self, grad, var, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    # Adjust L2 regularization strength to include beta to avoid the underlying
    # TensorFlow ops needing to include it.
    adjusted_l2_regularization_strength = (
        coefficients['l2_regularization_strength'] + coefficients['beta'] /
        (2. * coefficients['lr_t']))

    accum = self.get_slot(var, 'accumulator')
    linear = self.get_slot(var, 'linear')

    if self._l2_shrinkage_regularization_strength &lt;= 0.0:
      return tf.raw_ops.ResourceApplyFtrl(
          var=var.handle,
          accum=accum.handle,
          linear=linear.handle,
          grad=grad,
          lr=coefficients['lr_t'],
          l1=coefficients['l1_regularization_strength'],
          l2=adjusted_l2_regularization_strength,
          lr_power=coefficients['learning_rate_power'],
          use_locking=self._use_locking)
    else:
      return tf.raw_ops.ResourceApplyFtrlV2(
          var=var.handle,
          accum=accum.handle,
          linear=linear.handle,
          grad=grad,
          lr=coefficients['lr_t'],
          l1=coefficients['l1_regularization_strength'],
          l2=adjusted_l2_regularization_strength,
          l2_shrinkage=coefficients['l2_shrinkage_regularization_strength'],
          lr_power=coefficients['learning_rate_power'],
          use_locking=self._use_locking)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag269')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/ftrl.py: 203-242
</a>
<div class="mid" id="frag269" style="display:none"><pre>
  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    # Adjust L2 regularization strength to include beta to avoid the underlying
    # TensorFlow ops needing to include it.
    adjusted_l2_regularization_strength = (
        coefficients['l2_regularization_strength'] + coefficients['beta'] /
        (2. * coefficients['lr_t']))

    accum = self.get_slot(var, 'accumulator')
    linear = self.get_slot(var, 'linear')

    if self._l2_shrinkage_regularization_strength &lt;= 0.0:
      return tf.raw_ops.ResourceSparseApplyFtrl(
          var=var.handle,
          accum=accum.handle,
          linear=linear.handle,
          grad=grad,
          indices=indices,
          lr=coefficients['lr_t'],
          l1=coefficients['l1_regularization_strength'],
          l2=adjusted_l2_regularization_strength,
          lr_power=coefficients['learning_rate_power'],
          use_locking=self._use_locking)
    else:
      return tf.raw_ops.ResourceSparseApplyFtrlV2(
          var=var.handle,
          accum=accum.handle,
          linear=linear.handle,
          grad=grad,
          indices=indices,
          lr=coefficients['lr_t'],
          l1=coefficients['l1_regularization_strength'],
          l2=adjusted_l2_regularization_strength,
          l2_shrinkage=coefficients['l2_shrinkage_regularization_strength'],
          lr_power=coefficients['learning_rate_power'],
          use_locking=self._use_locking)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 4:</b> &nbsp; 5 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag10')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adam.py: 233-245
</a>
<div class="mid" id="frag10" style="display:none"><pre>
  def get_config(self):
    config = super(Adam, self).get_config()
    config.update({
        'learning_rate': self._serialize_hyperparameter('learning_rate'),
        'decay': self._initial_decay,
        'beta_1': self._serialize_hyperparameter('beta_1'),
        'beta_2': self._serialize_hyperparameter('beta_2'),
        'epsilon': self.epsilon,
        'amsgrad': self.amsgrad,
    })
    return config


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag198')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/rmsprop.py: 281-293
</a>
<div class="mid" id="frag198" style="display:none"><pre>
  def get_config(self):
    config = super(RMSprop, self).get_config()
    config.update({
        "learning_rate": self._serialize_hyperparameter("learning_rate"),
        "decay": self._initial_decay,
        "rho": self._serialize_hyperparameter("rho"),
        "momentum": self._serialize_hyperparameter("momentum"),
        "epsilon": self.epsilon,
        "centered": self.centered,
    })
    return config


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag17')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adam.py: 457-467
</a>
<div class="mid" id="frag17" style="display:none"><pre>
  def get_config(self):
    config = super(NonFusedAdam, self).get_config()
    config.update({
        'learning_rate': self._serialize_hyperparameter('learning_rate'),
        'decay': self._initial_decay,
        'beta_1': self._serialize_hyperparameter('beta_1'),
        'beta_2': self._serialize_hyperparameter('beta_2'),
        'epsilon': self.epsilon,
        'amsgrad': self.amsgrad,
    })
    return config
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag72')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/nadam.py: 204-213
</a>
<div class="mid" id="frag72" style="display:none"><pre>
  def get_config(self):
    config = super(Nadam, self).get_config()
    config.update({
        'learning_rate': self._serialize_hyperparameter('learning_rate'),
        'decay': self._initial_decay,
        'beta_1': self._serialize_hyperparameter('beta_1'),
        'beta_2': self._serialize_hyperparameter('beta_2'),
        'epsilon': self.epsilon,
    })
    return config
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag218')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adamax.py: 170-179
</a>
<div class="mid" id="frag218" style="display:none"><pre>
  def get_config(self):
    config = super(Adamax, self).get_config()
    config.update({
        'learning_rate': self._serialize_hyperparameter('learning_rate'),
        'decay': self._initial_decay,
        'beta_1': self._serialize_hyperparameter('beta_1'),
        'beta_2': self._serialize_hyperparameter('beta_2'),
        'epsilon': self.epsilon,
    })
    return config
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 5:</b> &nbsp; 3 fragments, nominal size 34 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag20')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adagrad_test.py: 62-104
</a>
<div class="mid" id="frag20" style="display:none"><pre>
  def doTestBasic(self, use_callable_params=False):
    for dtype in _DATA_TYPES:
      var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
      var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
      grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
      grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)
      var0 = tf.Variable(var0_np)
      var1 = tf.Variable(var1_np)
      grads0 = tf.constant(grads0_np)
      grads1 = tf.constant(grads1_np)

      learning_rate = lambda: 3.0
      if not use_callable_params:
        learning_rate = learning_rate()

      ada_opt = adagrad.Adagrad(learning_rate)

      accum0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
      accum1_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)

      if not tf.executing_eagerly():
        ada_update = ada_opt.apply_gradients(
            zip([grads0, grads1], [var0, var1]))
        self.evaluate(tf.compat.v1.global_variables_initializer())

      # Fetch params to validate initial values
      v0_val, v1_val = self.evaluate([var0, var1])
      self.assertAllClose([1.0, 2.0], v0_val)
      self.assertAllClose([3.0, 4.0], v1_val)

      # Run 3 steps of adagrad
      for _ in range(3):
        if not tf.executing_eagerly():
          self.evaluate(ada_update)
        else:
          ada_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))
        var0_np, accum0_np = adagrad_update_numpy(var0_np, accum0_np, grads0_np,
                                                  3.0)
        var1_np, accum1_np = adagrad_update_numpy(var1_np, accum1_np, grads1_np,
                                                  3.0)
        self.assertAllCloseAccordingToType(var0_np, self.evaluate(var0))
        self.assertAllCloseAccordingToType(var1_np, self.evaluate(var1))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag25')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adagrad_test.py: 195-239
</a>
<div class="mid" id="frag25" style="display:none"><pre>
  def testBasicWithLearningRateInverseTimeDecay(self):
    for dtype in _DATA_TYPES:
      var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
      var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
      grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
      grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)
      var0 = tf.Variable(var0_np)
      var1 = tf.Variable(var1_np)
      grads0 = tf.constant(grads0_np)
      grads1 = tf.constant(grads1_np)

      learning_rate = 3.0
      decay = 0.5
      lr_schedule = learning_rate_schedule.InverseTimeDecay(
          learning_rate, decay_steps=1.0, decay_rate=decay)

      ada_opt = adagrad.Adagrad(lr_schedule)

      accum0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
      accum1_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)

      if not tf.executing_eagerly():
        ada_update = ada_opt.apply_gradients(
            zip([grads0, grads1], [var0, var1]))
        self.evaluate(tf.compat.v1.global_variables_initializer())

      # Fetch params to validate initial values
      v0_val, v1_val = self.evaluate([var0, var1])
      self.assertAllClose([1.0, 2.0], v0_val)
      self.assertAllClose([3.0, 4.0], v1_val)

      # Run 3 steps of adagrad
      for t in range(3):
        if not tf.executing_eagerly():
          self.evaluate(ada_update)
        else:
          ada_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))
        lr_np = learning_rate / (1 + decay * t)
        var0_np, accum0_np = adagrad_update_numpy(var0_np, accum0_np, grads0_np,
                                                  lr_np)
        var1_np, accum1_np = adagrad_update_numpy(var1_np, accum1_np, grads1_np,
                                                  lr_np)
        self.assertAllCloseAccordingToType(var0_np, self.evaluate(var0))
        self.assertAllCloseAccordingToType(var1_np, self.evaluate(var1))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag23')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adagrad_test.py: 113-155
</a>
<div class="mid" id="frag23" style="display:none"><pre>
  def testBasicWithLearningRateDecay(self):
    for dtype in _DATA_TYPES:
      var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
      var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
      grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
      grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)
      var0 = tf.Variable(var0_np)
      var1 = tf.Variable(var1_np)
      grads0 = tf.constant(grads0_np)
      grads1 = tf.constant(grads1_np)

      learning_rate = 3.0
      decay = 0.5

      ada_opt = adagrad.Adagrad(learning_rate, decay=decay)

      accum0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
      accum1_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)

      if not tf.executing_eagerly():
        ada_update = ada_opt.apply_gradients(
            zip([grads0, grads1], [var0, var1]))
        self.evaluate(tf.compat.v1.global_variables_initializer())

      # Fetch params to validate initial values
      v0_val, v1_val = self.evaluate([var0, var1])
      self.assertAllClose([1.0, 2.0], v0_val)
      self.assertAllClose([3.0, 4.0], v1_val)

      # Run 3 steps of adagrad
      for t in range(3):
        if not tf.executing_eagerly():
          self.evaluate(ada_update)
        else:
          ada_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))
        lr_np = learning_rate / (1 + decay * t)
        var0_np, accum0_np = adagrad_update_numpy(var0_np, accum0_np, grads0_np,
                                                  lr_np)
        var1_np, accum1_np = adagrad_update_numpy(var1_np, accum1_np, grads1_np,
                                                  lr_np)
        self.assertAllCloseAccordingToType(var0_np, self.evaluate(var0))
        self.assertAllCloseAccordingToType(var1_np, self.evaluate(var1))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 6:</b> &nbsp; 2 fragments, nominal size 32 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag28')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adagrad_test.py: 263-295
</a>
<div class="mid" id="frag28" style="display:none"><pre>
  def testTensorLearningRate(self):
    # TODO(tanzheny, omalleyt): Fix test in eager mode.
    with tf.Graph().as_default():
      for dtype in _DATA_TYPES:
        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)
        var0 = tf.Variable(var0_np)
        var1 = tf.Variable(var1_np)
        grads0 = tf.constant(grads0_np)
        grads1 = tf.constant(grads1_np)

        learning_rate = tf.constant(3.0)
        ada_opt = adagrad.Adagrad(learning_rate)
        ada_update = ada_opt.apply_gradients(zip([grads0, grads1],
                                                 [var0, var1]))
        self.evaluate(tf.compat.v1.global_variables_initializer())
        # Fetch params to validate initial values
        self.assertAllClose([1.0, 2.0], self.evaluate(var0))
        self.assertAllClose([3.0, 4.0], self.evaluate(var1))
        accum0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        accum1_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        # Run 3 steps of adagrad
        for _ in range(3):
          self.evaluate(ada_update)
          var0_np, accum0_np = adagrad_update_numpy(
              var0_np, accum0_np, grads0_np, learning_rate)
          var1_np, accum1_np = adagrad_update_numpy(
              var1_np, accum1_np, grads1_np, learning_rate)
          self.assertAllCloseAccordingToType(var0_np, self.evaluate(var0))
          self.assertAllCloseAccordingToType(var1_np, self.evaluate(var1))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag34')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adagrad_test.py: 464-509
</a>
<div class="mid" id="frag34" style="display:none"><pre>
  def testSharing(self):
    # TODO(tanzheny, omalleyt): Fix test in eager mode.
    with tf.Graph().as_default():
      for dtype in _DATA_TYPES:
        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)

        var0 = tf.Variable(var0_np)
        var1 = tf.Variable(var1_np)
        grads0 = tf.constant(grads0_np)
        grads1 = tf.constant(grads1_np)

        learning_rate = 3.0
        ada_opt = adagrad.Adagrad(learning_rate)
        # Apply the optimizer twice.  Both applications will use
        # the same accums.
        ada_update1 = ada_opt.apply_gradients(zip([grads0, grads1],
                                                  [var0, var1]))
        ada_update2 = ada_opt.apply_gradients(zip([grads0, grads1],
                                                  [var0, var1]))
        slot0 = ada_opt.get_slot(var0, "accumulator")
        self.assertEqual(slot0.shape, var0.shape)
        slot1 = ada_opt.get_slot(var1, "accumulator")
        self.assertEqual(slot1.shape, var1.shape)
        self.evaluate(tf.compat.v1.global_variables_initializer())

        # Fetch params to validate initial values.
        self.assertAllClose([1.0, 2.0], self.evaluate(var0))
        self.assertAllClose([3.0, 4.0], self.evaluate(var1))
        # Mix the first and the second adagrad for 3 steps.
        self.evaluate(ada_update1)
        self.evaluate(ada_update2)
        self.evaluate(ada_update1)

        accum0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        accum1_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        for _ in range(3):
          var0_np, accum0_np = adagrad_update_numpy(
              var0_np, accum0_np, grads0_np, learning_rate)
          var1_np, accum1_np = adagrad_update_numpy(
              var1_np, accum1_np, grads1_np, learning_rate)
        self.assertAllCloseAccordingToType(var0_np, self.evaluate(var0))
        self.assertAllCloseAccordingToType(var1_np, self.evaluate(var1))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 7:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag35')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adagrad_test.py: 510-523
</a>
<div class="mid" id="frag35" style="display:none"><pre>
  def testConstructAdagradWithLR(self):
    opt = adagrad.Adagrad(lr=1.0)
    opt_2 = adagrad.Adagrad(learning_rate=0.1, lr=1.0)
    opt_3 = adagrad.Adagrad(learning_rate=0.1)
    self.assertIsInstance(opt.lr, tf.Variable)
    self.assertIsInstance(opt_2.lr, tf.Variable)
    self.assertIsInstance(opt_3.lr, tf.Variable)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.assertAllClose(self.evaluate(opt.lr), (1.0))
    self.assertAllClose(self.evaluate(opt_2.lr), (1.0))
    self.assertAllClose(self.evaluate(opt_3.lr), (0.1))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag204')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adadelta_test.py: 164-176
</a>
<div class="mid" id="frag204" style="display:none"><pre>
  def testConstructAdadeltaWithLR(self):
    opt = adadelta.Adadelta(lr=1.0, rho=0.9, epsilon=1.)
    opt_2 = adadelta.Adadelta(learning_rate=0.1, rho=0.9, epsilon=1., lr=1.0)
    opt_3 = adadelta.Adadelta(learning_rate=0.1, rho=0.9, epsilon=1.)
    self.assertIsInstance(opt.lr, tf.Variable)
    self.assertIsInstance(opt_2.lr, tf.Variable)
    self.assertIsInstance(opt_3.lr, tf.Variable)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.assertAllClose(self.evaluate(opt.lr), (1.0))
    self.assertAllClose(self.evaluate(opt_2.lr), (1.0))
    self.assertAllClose(self.evaluate(opt_3.lr), (0.1))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 8:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag40')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule_test.py: 91-110
</a>
<div class="mid" id="frag40" style="display:none"><pre>
  def testPiecewiseConstant(self, serialize):
    x = tf.Variable(-999)
    decayed_lr = learning_rate_schedule.PiecewiseConstantDecay(
        [100, 110, 120], [1.0, 0.1, 0.01, 0.001])
    decayed_lr = _maybe_serialized(decayed_lr, serialize)

    self.evaluate(tf.compat.v1.global_variables_initializer())

    self.assertAllClose(self.evaluate(decayed_lr(x)), 1.0, 1e-6)
    self.evaluate(x.assign(100))
    self.assertAllClose(self.evaluate(decayed_lr(x)), 1.0, 1e-6)
    self.evaluate(x.assign(105))
    self.assertAllClose(self.evaluate(decayed_lr(x)), 0.1, 1e-6)
    self.evaluate(x.assign(110))
    self.assertAllClose(self.evaluate(decayed_lr(x)), 0.1, 1e-6)
    self.evaluate(x.assign(120))
    self.assertAllClose(self.evaluate(decayed_lr(x)), 0.01, 1e-6)
    self.evaluate(x.assign(999))
    self.assertAllClose(self.evaluate(decayed_lr(x)), 0.001, 1e-6)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag44')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule_test.py: 133-155
</a>
<div class="mid" id="frag44" style="display:none"><pre>
  def testPiecewiseConstantEdgeCases(self, serialize):
    # Test casting boundaries from int32 to int64.
    x_int64 = tf.Variable(0, dtype=tf.int64)
    boundaries, values = [1, 2, 3], [0.4, 0.5, 0.6, 0.7]
    decayed_lr = learning_rate_schedule.PiecewiseConstantDecay(
        boundaries, values)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.assertAllClose(self.evaluate(decayed_lr(x_int64)), 0.4, 1e-6)
    self.evaluate(x_int64.assign(1))
    self.assertAllClose(self.evaluate(decayed_lr(x_int64)), 0.4, 1e-6)
    self.evaluate(x_int64.assign(2))
    self.assertAllClose(self.evaluate(decayed_lr(x_int64)), 0.5, 1e-6)
    self.evaluate(x_int64.assign(3))
    self.assertAllClose(self.evaluate(decayed_lr(x_int64)), 0.6, 1e-6)
    self.evaluate(x_int64.assign(4))
    self.assertAllClose(self.evaluate(decayed_lr(x_int64)), 0.7, 1e-6)


# @parameterized.named_parameters(
#     ("NotSerialized", False),
#     ("Serialized", True))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 9:</b> &nbsp; 5 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag50')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule_test.py: 215-225
</a>
<div class="mid" id="frag50" style="display:none"><pre>
  def testHalfWay(self, serialize):
    step = 5
    lr = 0.05
    end_lr = 0.0
    power = 0.5
    decayed_lr = learning_rate_schedule.PolynomialDecay(
        lr, 10, end_lr, power=power)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)
    expected = lr * 0.5**power
    self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag51')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule_test.py: 226-236
</a>
<div class="mid" id="frag51" style="display:none"><pre>
  def testEnd(self, serialize):
    step = 10
    lr = 0.05
    end_lr = 0.001
    power = 0.5
    decayed_lr = learning_rate_schedule.PolynomialDecay(
        lr, 10, end_lr, power=power)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)
    expected = end_lr
    self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag52')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule_test.py: 237-247
</a>
<div class="mid" id="frag52" style="display:none"><pre>
  def testHalfWayWithEnd(self, serialize):
    step = 5
    lr = 0.05
    end_lr = 0.001
    power = 0.5
    decayed_lr = learning_rate_schedule.PolynomialDecay(
        lr, 10, end_lr, power=power)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)
    expected = (lr - end_lr) * 0.5**power + end_lr
    self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag53')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule_test.py: 248-258
</a>
<div class="mid" id="frag53" style="display:none"><pre>
  def testBeyondEnd(self, serialize):
    step = 15
    lr = 0.05
    end_lr = 0.001
    power = 0.5
    decayed_lr = learning_rate_schedule.PolynomialDecay(
        lr, 10, end_lr, power=power)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)
    expected = end_lr
    self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag54')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule_test.py: 259-273
</a>
<div class="mid" id="frag54" style="display:none"><pre>
  def testBeyondEndWithCycle(self, serialize):
    step = 15
    lr = 0.05
    end_lr = 0.001
    power = 0.5
    decayed_lr = learning_rate_schedule.PolynomialDecay(
        lr, 10, end_lr, power=power, cycle=True)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)
    expected = (lr - end_lr) * 0.25**power + end_lr
    self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)


# @parameterized.named_parameters(
#     ("NotSerialized", False),
#     ("Serialized", True))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 10:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag56')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule_test.py: 297-311
</a>
<div class="mid" id="frag56" style="display:none"><pre>
  def testDecay(self, serialize):
    initial_lr = 0.1
    k = 10
    decay_rate = 0.96
    step = tf.Variable(0)
    decayed_lr = learning_rate_schedule.InverseTimeDecay(initial_lr, k,
                                                         decay_rate)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    for i in range(k + 1):
      expected = initial_lr / (1 + i / k * decay_rate)
      self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)
      self.evaluate(step.assign_add(1))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag57')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule_test.py: 312-327
</a>
<div class="mid" id="frag57" style="display:none"><pre>
  def testStaircase(self, serialize):
    initial_lr = 0.1
    k = 10
    decay_rate = 0.96
    step = tf.Variable(0)
    decayed_lr = learning_rate_schedule.InverseTimeDecay(
        initial_lr, k, decay_rate, staircase=True)
    decayed_lr = _maybe_serialized(decayed_lr, serialize)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    for i in range(k + 1):
      expected = initial_lr / (1 + decay_rate * (i // k))
      self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)
      self.evaluate(step.assign_add(1))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 11:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag61')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule_test.py: 366-377
</a>
<div class="mid" id="frag61" style="display:none"><pre>
  def np_cosine_decay_restarts(self, step, decay_steps, t_mul=2.0, m_mul=1.0,
                               alpha=0.0):
    fac = 1.0
    while step &gt;= decay_steps:
      step -= decay_steps
      decay_steps *= t_mul
      fac *= m_mul

    completed_fraction = step / decay_steps
    decay = fac * 0.5 * (1.0 + math.cos(math.pi * completed_fraction))
    return (1.0 - alpha) * decay + alpha

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag182')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py: 347-358
</a>
<div class="mid" id="frag182" style="display:none"><pre>
  def np_cosine_decay_restarts(self, step, decay_steps, t_mul=2.0, m_mul=1.0,
                               alpha=0.0):
    fac = 1.0
    while step &gt;= decay_steps:
      step -= decay_steps
      decay_steps *= t_mul
      fac *= m_mul

    completed_fraction = step / decay_steps
    decay = fac * 0.5 * (1.0 + math.cos(math.pi * completed_fraction))
    return (1.0 - alpha) * decay + alpha

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 12:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag63')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule_test.py: 388-399
</a>
<div class="mid" id="frag63" style="display:none"><pre>
  def testAlpha(self, serialize):
    num_training_steps = 1000
    initial_lr = 1.0
    alpha = 0.1
    for step in range(0, 1500, 250):
      decayed_lr = learning_rate_schedule.CosineDecayRestarts(
          initial_lr, num_training_steps, alpha=alpha)
      decayed_lr = _maybe_serialized(decayed_lr, serialize)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, alpha=alpha)
      self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag64')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule_test.py: 400-411
</a>
<div class="mid" id="frag64" style="display:none"><pre>
  def testMMul(self, serialize):
    num_training_steps = 1000
    initial_lr = 1.0
    m_mul = 0.9
    for step in range(0, 1500, 250):
      decayed_lr = learning_rate_schedule.CosineDecayRestarts(
          initial_lr, num_training_steps, m_mul=m_mul)
      decayed_lr = _maybe_serialized(decayed_lr, serialize)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, m_mul=m_mul)
      self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag65')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule_test.py: 412-424
</a>
<div class="mid" id="frag65" style="display:none"><pre>
  def testTMul(self, serialize):
    num_training_steps = 1000
    initial_lr = 1.0
    t_mul = 1.0
    for step in range(0, 1500, 250):
      decayed_lr = learning_rate_schedule.CosineDecayRestarts(
          initial_lr, num_training_steps, t_mul=t_mul)
      decayed_lr = _maybe_serialized(decayed_lr, serialize)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, t_mul=t_mul)
      self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 13:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag94')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/optimizer_v2_test.py: 426-448
</a>
<div class="mid" id="frag94" style="display:none"><pre>
  def testGettingHyperParameters(self):
    with self.test_session():
      opt = adam.Adam(learning_rate=1.0)
      var = tf.Variable([1.0, 2.0], dtype=tf.float32)
      loss = lambda: 3 * var
      opt_op = opt.minimize(loss, [var])
      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(opt_op)

      lr = self.evaluate(opt.lr)
      self.assertEqual(1.0, lr)

      opt.lr = 2.0
      lr = self.evaluate(opt.lr)
      self.assertEqual(2.0, lr)

      self.evaluate(opt.lr.assign(3.0))
      lr = self.evaluate(opt.lr)
      self.assertEqual(3.0, lr)

      with self.assertRaises(AttributeError):
        opt.not_an_attr += 3

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag95')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/optimizer_v2_test.py: 450-472
</a>
<div class="mid" id="frag95" style="display:none"><pre>
  def testGettingHyperParametersWithLrInConstructor(self):
    with self.test_session():
      opt = gradient_descent.SGD(lr=3.0)
      var = tf.Variable([1.0, 2.0], dtype=tf.float32)
      loss = lambda: 3 * var
      opt_op = opt.minimize(loss, [var])
      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(opt_op)

      self.assertIsInstance(opt.lr, tf.Variable)
      self.assertIsInstance(opt.learning_rate, tf.Variable)

      lr = self.evaluate(opt.lr)
      self.assertEqual(3.0, lr)

      opt.lr = 2.0
      lr = self.evaluate(opt.lr)
      self.assertEqual(2.0, lr)

      self.evaluate(opt.lr.assign(4.0))
      lr = self.evaluate(opt.lr)
      self.assertEqual(4.0, lr)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 14:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag104')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/optimizer_v2_test.py: 640-653
</a>
<div class="mid" id="frag104" style="display:none"><pre>
  def testAggregationTrue(self):
    # Test that experimental_aggregate_gradients=True works without distributed
    # strategy.
    var = tf.Variable([1., 2.])
    opt = gradient_descent.SGD(3.0)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.assertAllClose([1., 2.], self.evaluate(var))
    opt_op = opt.apply_gradients([([0.1, 0.1], var)],
                                 experimental_aggregate_gradients=True)
    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.evaluate(opt_op)
    self.assertAllClose([0.7, 1.7], self.evaluate(var))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag105')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/optimizer_v2_test.py: 655-668
</a>
<div class="mid" id="frag105" style="display:none"><pre>
  def testAggregationFalse(self):
    # Test that experimental_aggregate_gradients=False works without distributed
    # strategy.
    var = tf.Variable([1., 2.])
    opt = gradient_descent.SGD(3.0)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.assertAllClose([1., 2.], self.evaluate(var))
    opt_op = opt.apply_gradients([([0.1, 0.1], var)],
                                 experimental_aggregate_gradients=False)
    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.evaluate(opt_op)
    self.assertAllClose([0.7, 1.7], self.evaluate(var))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 15:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag108')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/optimizer_v2_test.py: 707-723
</a>
<div class="mid" id="frag108" style="display:none"><pre>
  def test_gradient_aggregator(self):
    def gradient_aggregator(grads_and_vars):
      # Simulate an all-reduce where the other replica has zeros for gradients,
      # by dividing each gradient by 2.
      grads = [g for g, _ in grads_and_vars]
      vars = [v for _, v in grads_and_vars]  # pylint: disable=redefined-builtin
      all_reduced_grads = [g / 2 for g in grads]
      return list(zip(all_reduced_grads, vars))

    var = tf.Variable(2.0)
    sgd = gradient_descent.SGD(1.0, gradient_aggregator=gradient_aggregator)
    loss = lambda: 2 * var
    opt_op = sgd.minimize(loss, var_list=[var])
    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.evaluate(opt_op)
    self.assertEqual(self.evaluate(var), 1.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag110')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/optimizer_v2_test.py: 725-744
</a>
<div class="mid" id="frag110" style="display:none"><pre>
  def test_override_aggregate_gradients(self):
    class MyOptimizer(gradient_descent.SGD):

      def _aggregate_gradients(self, grads_and_vars):
        # Simulate an all-reduce where the other replica has zeros for
        # gradients, by dividing each gradient by 2.
        grads = [g for g, _ in grads_and_vars]
        vars = [v for _, v in grads_and_vars]  # pylint: disable=redefined-builtin
        all_reduced_grads = [g / 2 for g in grads]
        return list(zip(all_reduced_grads, vars))

    var = tf.Variable(2.0)
    sgd = MyOptimizer(1.0)
    loss = lambda: 2 * var
    opt_op = sgd.minimize(loss, var_list=[var])
    self.evaluate(tf.compat.v1.global_variables_initializer())
    self.evaluate(opt_op)
    self.assertEqual(self.evaluate(var), 1.0)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 16:</b> &nbsp; 2 fragments, nominal size 45 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag121')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/optimizer_v2_test.py: 834-890
</a>
<div class="mid" id="frag121" style="display:none"><pre>
  def testNumericEquivalenceForNesterovMomentum(self):
    if tf.executing_eagerly():
      self.skipTest(
          'v1 optimizer does not run in eager mode')
    np.random.seed(1331)
    with testing_utils.use_gpu():
      train_samples = 20
      input_dim = 3
      num_classes = 2
      (x, y), _ = testing_utils.get_test_data(
          train_samples=train_samples,
          test_samples=10,
          input_shape=(input_dim,),
          num_classes=num_classes)
      y = np_utils.to_categorical(y)

      num_hidden = 5
      model_k_v1 = testing_utils.get_small_sequential_mlp(
          num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)
      model_k_v2 = testing_utils.get_small_sequential_mlp(
          num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)
      model_k_v2.set_weights(model_k_v1.get_weights())
      model_tf = testing_utils.get_small_sequential_mlp(
          num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)
      model_tf.set_weights(model_k_v2.get_weights())

      opt_k_v1 = optimizer_v1.SGD(momentum=0.9, nesterov=True)
      opt_k_v2 = gradient_descent.SGD(momentum=0.9, nesterov=True)
      opt_tf = tf.compat.v1.train.MomentumOptimizer(
          learning_rate=0.01, momentum=0.9, use_nesterov=True)

      model_k_v1.compile(
          opt_k_v1,
          loss='categorical_crossentropy',
          metrics=[],
          run_eagerly=testing_utils.should_run_eagerly())
      model_k_v2.compile(
          opt_k_v2,
          loss='categorical_crossentropy',
          metrics=[],
          run_eagerly=testing_utils.should_run_eagerly())
      model_tf.compile(
          opt_tf,
          loss='categorical_crossentropy',
          metrics=[],
          run_eagerly=testing_utils.should_run_eagerly())

      hist_k_v1 = model_k_v1.fit(x, y, batch_size=5, epochs=10, shuffle=False)
      hist_k_v2 = model_k_v2.fit(x, y, batch_size=5, epochs=10, shuffle=False)
      hist_tf = model_tf.fit(x, y, batch_size=5, epochs=10, shuffle=False)

      self.assertAllClose(model_k_v1.get_weights(), model_tf.get_weights())
      self.assertAllClose(model_k_v1.get_weights(), model_k_v2.get_weights())
      self.assertAllClose(opt_k_v1.get_weights(), opt_k_v2.get_weights())
      self.assertAllClose(hist_k_v1.history['loss'], hist_tf.history['loss'])
      self.assertAllClose(hist_k_v1.history['loss'], hist_k_v2.history['loss'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag122')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/optimizer_v2_test.py: 891-937
</a>
<div class="mid" id="frag122" style="display:none"><pre>
  def testNumericEquivalenceForAmsgrad(self):
    if tf.executing_eagerly():
      self.skipTest(
          'v1 optimizer does not run in eager mode')
    np.random.seed(1331)
    with testing_utils.use_gpu():
      train_samples = 20
      input_dim = 3
      num_classes = 2
      (x, y), _ = testing_utils.get_test_data(
          train_samples=train_samples,
          test_samples=10,
          input_shape=(input_dim,),
          num_classes=num_classes)
      y = np_utils.to_categorical(y)

      num_hidden = 5
      model_k_v1 = testing_utils.get_small_sequential_mlp(
          num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)
      model_k_v2 = testing_utils.get_small_sequential_mlp(
          num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)
      model_k_v2.set_weights(model_k_v1.get_weights())

      opt_k_v1 = optimizer_v1.Adam(amsgrad=True)
      opt_k_v2 = adam.Adam(amsgrad=True)

      model_k_v1.compile(
          opt_k_v1,
          loss='categorical_crossentropy',
          metrics=[],
          run_eagerly=testing_utils.should_run_eagerly())
      model_k_v2.compile(
          opt_k_v2,
          loss='categorical_crossentropy',
          metrics=[],
          run_eagerly=testing_utils.should_run_eagerly())

      hist_k_v1 = model_k_v1.fit(x, y, batch_size=5, epochs=10, shuffle=False)
      hist_k_v2 = model_k_v2.fit(x, y, batch_size=5, epochs=10, shuffle=False)

      self.assertAllClose(model_k_v1.get_weights(), model_k_v2.get_weights())
      self.assertAllClose(opt_k_v1.get_weights(), opt_k_v2.get_weights())
      self.assertAllClose(hist_k_v1.history['loss'], hist_k_v2.history['loss'])


# Note: These tests are kept in a separate class to avoid bugs in some
# distributions of Python that break AutoGraph which is used by tf.function.
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 17:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag123')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/optimizer_v2_test.py: 941-953
</a>
<div class="mid" id="frag123" style="display:none"><pre>
  def testBasic(self):
    var = tf.Variable([1.0, 2.0], dtype=tf.float32)
    loss = lambda: 3 * var
    opt = adam.Adam(learning_rate=1.0)

    @tf.function
    def fn():
      opt.minimize(loss, [var])
      return var

    self.assertAllClose([0., 1.], fn(), atol=1e-4)
    self.assertAllClose([-1, 0.], fn(), atol=1e-4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag125')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/optimizer_v2_test.py: 954-966
</a>
<div class="mid" id="frag125" style="display:none"><pre>
  def testBasicWithConstantDecay(self):
    var = tf.Variable([1.0, 2.0], dtype=tf.float32)
    loss = lambda: 3 * var
    opt = adam.Adam(learning_rate=1.0)

    @tf.function
    def fn():
      opt.minimize(loss, [var])
      return var

    self.assertAllClose([0., 1.], fn(), atol=1e-4)
    self.assertAllClose([-1, 0.], fn(), atol=1e-4)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 18:</b> &nbsp; 6 fragments, nominal size 14 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag146')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adagrad.py: 128-141
</a>
<div class="mid" id="frag146" style="display:none"><pre>
  def _resource_apply_dense(self, grad, var, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    acc = self.get_slot(var, 'accumulator')
    return tf.raw_ops.ResourceApplyAdagradV2(
        var=var.handle,
        accum=acc.handle,
        lr=coefficients['lr_t'],
        epsilon=coefficients['epsilon'],
        grad=grad,
        use_locking=self._use_locking)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag262')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adadelta.py: 104-120
</a>
<div class="mid" id="frag262" style="display:none"><pre>
  def _resource_apply_dense(self, grad, var, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    accum_grad = self.get_slot(var, 'accum_grad')
    accum_var = self.get_slot(var, 'accum_var')
    return tf.raw_ops.ResourceApplyAdadelta(
        var=var.handle,
        accum=accum_grad.handle,
        accum_update=accum_var.handle,
        lr=coefficients['lr_t'],
        rho=coefficients['rho'],
        epsilon=coefficients['epsilon'],
        grad=grad,
        use_locking=self._use_locking)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag211')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/gradient_descent.py: 162-178
</a>
<div class="mid" id="frag211" style="display:none"><pre>
  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):
    # This method is only needed for momentum optimization.
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    momentum_var = self.get_slot(var, "momentum")
    return tf.raw_ops.ResourceSparseApplyKerasMomentum(
        var=var.handle,
        accum=momentum_var.handle,
        lr=coefficients["lr_t"],
        grad=grad,
        indices=indices,
        momentum=coefficients["momentum"],
        use_locking=self._use_locking,
        use_nesterov=self.nesterov)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag216')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adamax.py: 124-142
</a>
<div class="mid" id="frag216" style="display:none"><pre>
  def _resource_apply_dense(self, grad, var, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    m = self.get_slot(var, 'm')
    v = self.get_slot(var, 'v')
    return tf.raw_ops.ResourceApplyAdaMax(
        var=var.handle,
        m=m.handle,
        v=v.handle,
        beta1_power=coefficients['beta_1_power'],
        lr=coefficients['lr_t'],
        beta1=coefficients['beta_1_t'],
        beta2=coefficients['beta_2_t'],
        epsilon=coefficients['epsilon'],
        grad=grad,
        use_locking=self._use_locking)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag147')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adagrad.py: 142-156
</a>
<div class="mid" id="frag147" style="display:none"><pre>
  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    acc = self.get_slot(var, 'accumulator')
    return tf.raw_ops.ResourceSparseApplyAdagradV2(
        var=var.handle,
        accum=acc.handle,
        lr=coefficients['lr_t'],
        epsilon=coefficients['epsilon'],
        grad=grad,
        indices=indices,
        use_locking=self._use_locking)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag263')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/adadelta.py: 121-138
</a>
<div class="mid" id="frag263" style="display:none"><pre>
  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):
    var_device, var_dtype = var.device, var.dtype.base_dtype
    coefficients = ((apply_state or {}).get((var_device, var_dtype))
                    or self._fallback_apply_state(var_device, var_dtype))

    accum_grad = self.get_slot(var, 'accum_grad')
    accum_var = self.get_slot(var, 'accum_var')
    return tf.raw_ops.ResourceSparseApplyAdadelta(
        var=var.handle,
        accum=accum_grad.handle,
        accum_update=accum_var.handle,
        lr=coefficients['lr_t'],
        rho=coefficients['rho'],
        epsilon=coefficients['epsilon'],
        grad=grad,
        indices=indices,
        use_locking=self._use_locking)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 19:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag151')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/utils.py: 81-101
</a>
<div class="mid" id="frag151" style="display:none"><pre>
def make_gradient_clipnorm_fn(clipnorm):
  """Creates a gradient transformation function for clipping by norm."""
  if clipnorm is None:
    return lambda grads_and_vars: grads_and_vars

  def gradient_clipnorm_fn(grads_and_vars):

    if isinstance(tf.distribute.get_strategy(),
                  (tf.distribute.experimental.CentralStorageStrategy,
                   tf.compat.v1.distribute.experimental.CentralStorageStrategy)):
      raise ValueError(
          "`clipnorm` is not supported with `CenteralStorageStrategy`")

    clipped_grads_and_vars = [
        (tf.clip_by_norm(g, clipnorm), v) for g, v in grads_and_vars
    ]
    return clipped_grads_and_vars

  return gradient_clipnorm_fn


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag155')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/utils.py: 123-143
</a>
<div class="mid" id="frag155" style="display:none"><pre>
def make_gradient_clipvalue_fn(clipvalue):
  """Creates a gradient transformation function for clipping by value."""
  if clipvalue is None:
    return lambda grads_and_vars: grads_and_vars

  def gradient_clipvalue_fn(grads_and_vars):

    if isinstance(tf.distribute.get_strategy(),
                  (tf.distribute.experimental.CentralStorageStrategy,
                   tf.compat.v1.distribute.experimental.CentralStorageStrategy)):
      raise ValueError(
          "`clipvalue` is not supported with `CenteralStorageStrategy`")

    clipped_grads_and_vars = [(tf.clip_by_value(g, -clipvalue,
                                                      clipvalue), v)
                              for g, v in grads_and_vars]
    return clipped_grads_and_vars

  return gradient_clipvalue_fn


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag153')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/utils.py: 102-122
</a>
<div class="mid" id="frag153" style="display:none"><pre>
def make_global_gradient_clipnorm_fn(clipnorm):
  """Creates a gradient transformation function for clipping by norm."""
  if clipnorm is None:
    return lambda grads_and_vars: grads_and_vars

  def gradient_clipnorm_fn(grads_and_vars):

    if isinstance(tf.distribute.get_strategy(),
                  (tf.distribute.experimental.CentralStorageStrategy,
                   tf.compat.v1.distribute.experimental.CentralStorageStrategy)):
      raise ValueError(
          "`global_clipnorm` is not supported with `CenteralStorageStrategy`")

    grads, variables = zip(*grads_and_vars)
    clipped_grads, _ = tf.clip_by_global_norm(grads, clipnorm)
    clipped_grads_and_vars = list(zip(clipped_grads, variables))
    return clipped_grads_and_vars

  return gradient_clipnorm_fn


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 20:</b> &nbsp; 4 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag175')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py: 254-267
</a>
<div class="mid" id="frag175" style="display:none"><pre>
  def testDecay(self):
    initial_lr = 0.1
    k = 10
    decay_rate = 0.96
    step = tf.Variable(0)
    decayed_lr = tf.compat.v1.train.natural_exp_decay(initial_lr, step, k,
                                                       decay_rate)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    for i in range(k + 1):
      expected = initial_lr * math.exp(-i / k * decay_rate)
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)
      self.evaluate(step.assign_add(1))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag178')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py: 300-314
</a>
<div class="mid" id="frag178" style="display:none"><pre>
  def testStaircase(self):
    initial_lr = 0.1
    k = 10
    decay_rate = 0.96
    step = tf.Variable(0)
    decayed_lr = tf.compat.v1.train.inverse_time_decay(
        initial_lr, step, k, decay_rate, staircase=True)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    for i in range(k + 1):
      expected = initial_lr / (1 + decay_rate * (i // k))
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)
      self.evaluate(step.assign_add(1))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag177')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py: 286-299
</a>
<div class="mid" id="frag177" style="display:none"><pre>
  def testDecay(self):
    initial_lr = 0.1
    k = 10
    decay_rate = 0.96
    step = tf.Variable(0)
    decayed_lr = tf.compat.v1.train.inverse_time_decay(initial_lr, step, k,
                                                        decay_rate)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    for i in range(k + 1):
      expected = initial_lr / (1 + i / k * decay_rate)
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)
      self.evaluate(step.assign_add(1))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag176')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py: 268-282
</a>
<div class="mid" id="frag176" style="display:none"><pre>
  def testStaircase(self):
    initial_lr = 0.1
    k = 10
    decay_rate = 0.96
    step = tf.Variable(0)
    decayed_lr = tf.compat.v1.train.natural_exp_decay(
        initial_lr, step, k, decay_rate, staircase=True)

    self.evaluate(tf.compat.v1.global_variables_initializer())
    for i in range(k + 1):
      expected = initial_lr * math.exp(-decay_rate * (i // k))
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)
      self.evaluate(step.assign_add(1))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 21:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag184')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py: 368-378
</a>
<div class="mid" id="frag184" style="display:none"><pre>
  def testAlpha(self):
    num_training_steps = 1000
    initial_lr = 1.0
    alpha = 0.1
    for step in range(0, 1500, 250):
      decayed_lr = tf.compat.v1.train.cosine_decay_restarts(
          initial_lr, step, num_training_steps, alpha=alpha)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, alpha=alpha)
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag186')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py: 390-401
</a>
<div class="mid" id="frag186" style="display:none"><pre>
  def testTMul(self):
    num_training_steps = 1000
    initial_lr = 1.0
    t_mul = 1.0
    for step in range(0, 1500, 250):
      decayed_lr = tf.compat.v1.train.cosine_decay_restarts(
          initial_lr, step, num_training_steps, t_mul=t_mul)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, t_mul=t_mul)
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag185')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py: 379-389
</a>
<div class="mid" id="frag185" style="display:none"><pre>
  def testMMul(self):
    num_training_steps = 1000
    initial_lr = 1.0
    m_mul = 0.9
    for step in range(0, 1500, 250):
      decayed_lr = tf.compat.v1.train.cosine_decay_restarts(
          initial_lr, step, num_training_steps, m_mul=m_mul)
      expected = self.np_cosine_decay_restarts(
          step, num_training_steps, m_mul=m_mul)
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 22:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag189')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py: 426-441
</a>
<div class="mid" id="frag189" style="display:none"><pre>
  def testNonDefaultDecay(self):
    num_training_steps = 1000
    initial_lr = 1.0
    for step in range(0, 1500, 250):
      decayed_lr = tf.compat.v1.train.linear_cosine_decay(
          initial_lr,
          step,
          num_training_steps,
          alpha=0.1,
          beta=1e-4,
          num_periods=5)
      expected = self.np_linear_cosine_decay(
          step, num_training_steps, alpha=0.1, beta=1e-4, num_periods=5)
      self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag191')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay_test.py: 455-472
</a>
<div class="mid" id="frag191" style="display:none"><pre>
  def testNonDefaultNoisyLinearCosine(self):
    num_training_steps = 1000
    initial_lr = 1.0
    for step in range(0, 1500, 250):
      # No numerical check because of noise
      decayed_lr = tf.compat.v1.train.noisy_linear_cosine_decay(
          initial_lr,
          step,
          num_training_steps,
          initial_variance=0.5,
          variance_decay=0.1,
          alpha=0.1,
          beta=1e-4,
          num_periods=5)
      # Cannot be deterministically tested
      self.evaluate(decayed_lr)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 23:</b> &nbsp; 8 fragments, nominal size 13 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag222')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule.py: 142-169
</a>
<div class="mid" id="frag222" style="display:none"><pre>
  def __init__(
      self,
      initial_learning_rate,
      decay_steps,
      decay_rate,
      staircase=False,
      name=None):
    """Applies exponential decay to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The initial learning rate.
      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.
        Must be positive.  See the decay computation above.
      decay_rate: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The decay rate.
      staircase: Boolean.  If `True` decay the learning rate at discrete
        intervals
      name: String.  Optional name of the operation.  Defaults to
        'ExponentialDecay'.
    """
    super(ExponentialDecay, self).__init__()
    self.initial_learning_rate = initial_learning_rate
    self.decay_steps = decay_steps
    self.decay_rate = decay_rate
    self.staircase = staircase
    self.name = name

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag228')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule.py: 369-400
</a>
<div class="mid" id="frag228" style="display:none"><pre>
  def __init__(
      self,
      initial_learning_rate,
      decay_steps,
      end_learning_rate=0.0001,
      power=1.0,
      cycle=False,
      name=None):
    """Applies a polynomial decay to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The initial learning rate.
      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.
        Must be positive.  See the decay computation above.
      end_learning_rate: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The minimal end learning rate.
      power: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The power of the polynomial. Defaults to linear, 1.0.
      cycle: A boolean, whether or not it should cycle beyond decay_steps.
      name: String.  Optional name of the operation. Defaults to
        'PolynomialDecay'.
    """
    super(PolynomialDecay, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.decay_steps = decay_steps
    self.end_learning_rate = end_learning_rate
    self.power = power
    self.cycle = cycle
    self.name = name

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag241')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule.py: 822-852
</a>
<div class="mid" id="frag241" style="display:none"><pre>
  def __init__(
      self,
      initial_learning_rate,
      decay_steps,
      num_periods=0.5,
      alpha=0.0,
      beta=0.001,
      name=None):
    """Applies linear cosine decay to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` Tensor or a Python
        number. The initial learning rate.
      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.
        Number of steps to decay over.
      num_periods: Number of periods in the cosine part of the decay.
        See computation above.
      alpha: See computation above.
      beta: See computation above.
      name: String.  Optional name of the operation.  Defaults to
        'LinearCosineDecay'.
    """
    super(LinearCosineDecay, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.decay_steps = decay_steps
    self.num_periods = num_periods
    self.alpha = alpha
    self.beta = beta
    self.name = name

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag237')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule.py: 684-715
</a>
<div class="mid" id="frag237" style="display:none"><pre>
  def __init__(
      self,
      initial_learning_rate,
      first_decay_steps,
      t_mul=2.0,
      m_mul=1.0,
      alpha=0.0,
      name=None):
    """Applies cosine decay with restarts to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` Tensor or a Python
        number. The initial learning rate.
      first_decay_steps: A scalar `int32` or `int64` `Tensor` or a Python
        number. Number of steps to decay over.
      t_mul: A scalar `float32` or `float64` `Tensor` or a Python number.
        Used to derive the number of iterations in the i-th period
      m_mul: A scalar `float32` or `float64` `Tensor` or a Python number.
        Used to derive the initial learning rate of the i-th period:
      alpha: A scalar `float32` or `float64` Tensor or a Python number.
        Minimum learning rate value as a fraction of the initial_learning_rate.
      name: String. Optional name of the operation.  Defaults to 'SGDRDecay'.
    """
    super(CosineDecayRestarts, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.first_decay_steps = first_decay_steps
    self._t_mul = t_mul
    self._m_mul = m_mul
    self.alpha = alpha
    self.name = name

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag231')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule.py: 494-520
</a>
<div class="mid" id="frag231" style="display:none"><pre>
  def __init__(
      self,
      initial_learning_rate,
      decay_steps,
      decay_rate,
      staircase=False,
      name=None):
    """Applies inverse time decay to the initial learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a
        Python number.  The initial learning rate.
      decay_steps: How often to apply decay.
      decay_rate: A Python number.  The decay rate.
      staircase: Whether to apply decay in a discrete staircase, as opposed to
        continuous, fashion.
      name: String.  Optional name of the operation.  Defaults to
        'InverseTimeDecay'.
    """
    super(InverseTimeDecay, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.decay_steps = decay_steps
    self.decay_rate = decay_rate
    self.staircase = staircase
    self.name = name

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag234')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule.py: 592-615
</a>
<div class="mid" id="frag234" style="display:none"><pre>
  def __init__(
      self,
      initial_learning_rate,
      decay_steps,
      alpha=0.0,
      name=None):
    """Applies cosine decay to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` Tensor or a
        Python number. The initial learning rate.
      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.
        Number of steps to decay over.
      alpha: A scalar `float32` or `float64` Tensor or a Python number.
        Minimum learning rate value as a fraction of initial_learning_rate.
      name: String. Optional name of the operation.  Defaults to 'CosineDecay'.
    """
    super(CosineDecay, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.decay_steps = decay_steps
    self.alpha = alpha
    self.name = name

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag244')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule.py: 942-978
</a>
<div class="mid" id="frag244" style="display:none"><pre>
  def __init__(
      self,
      initial_learning_rate,
      decay_steps,
      initial_variance=1.0,
      variance_decay=0.55,
      num_periods=0.5,
      alpha=0.0,
      beta=0.001,
      name=None):
    """Applies noisy linear cosine decay to the learning rate.

    Args:
      initial_learning_rate: A scalar `float32` or `float64` Tensor or a Python
        number. The initial learning rate.
      decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.
        Number of steps to decay over.
      initial_variance: initial variance for the noise. See computation above.
      variance_decay: decay for the noise's variance. See computation above.
      num_periods: Number of periods in the cosine part of the decay.
        See computation above.
      alpha: See computation above.
      beta: See computation above.
      name: String.  Optional name of the operation.  Defaults to
        'NoisyLinearCosineDecay'.
    """
    super(NoisyLinearCosineDecay, self).__init__()

    self.initial_learning_rate = initial_learning_rate
    self.decay_steps = decay_steps
    self.initial_variance = initial_variance
    self.variance_decay = variance_decay
    self.num_periods = num_periods
    self.alpha = alpha
    self.beta = beta
    self.name = name

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag225')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule.py: 233-263
</a>
<div class="mid" id="frag225" style="display:none"><pre>
  def __init__(
      self,
      boundaries,
      values,
      name=None):
    """Piecewise constant from boundaries and interval values.

    Args:
      boundaries: A list of `Tensor`s or `int`s or `float`s with strictly
        increasing entries, and with all elements having the same type as the
        optimizer step.
      values: A list of `Tensor`s or `float`s or `int`s that specifies the
        values for the intervals defined by `boundaries`. It should have one
        more element than `boundaries`, and all elements should have the same
        type.
      name: A string. Optional name of the operation. Defaults to
        'PiecewiseConstant'.

    Raises:
      ValueError: if the number of elements in the lists do not match.
    """
    super(PiecewiseConstantDecay, self).__init__()

    if len(boundaries) != len(values) - 1:
      raise ValueError(
          "The length of boundaries should be 1 less than the length of values")

    self.boundaries = boundaries
    self.values = values
    self.name = name

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 24:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag223')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule.py: 170-184
</a>
<div class="mid" id="frag223" style="display:none"><pre>
  def __call__(self, step):
    with tf.name_scope(self.name or "ExponentialDecay") as name:
      initial_learning_rate = tf.convert_to_tensor(
          self.initial_learning_rate, name="initial_learning_rate")
      dtype = initial_learning_rate.dtype
      decay_steps = tf.cast(self.decay_steps, dtype)
      decay_rate = tf.cast(self.decay_rate, dtype)

      global_step_recomp = tf.cast(step, dtype)
      p = global_step_recomp / decay_steps
      if self.staircase:
        p = tf.floor(p)
      return tf.multiply(
          initial_learning_rate, tf.pow(decay_rate, p), name=name)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag232')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/learning_rate_schedule.py: 521-536
</a>
<div class="mid" id="frag232" style="display:none"><pre>
  def __call__(self, step):
    with tf.name_scope(self.name or "InverseTimeDecay") as name:
      initial_learning_rate = tf.convert_to_tensor(
          self.initial_learning_rate, name="initial_learning_rate")
      dtype = initial_learning_rate.dtype
      decay_steps = tf.cast(self.decay_steps, dtype)
      decay_rate = tf.cast(self.decay_rate, dtype)

      global_step_recomp = tf.cast(step, dtype)
      p = global_step_recomp / decay_steps
      if self.staircase:
        p = tf.floor(p)
      const = tf.cast(tf.constant(1), dtype)
      denom = tf.add(const, tf.multiply(decay_rate, p))
      return tf.divide(initial_learning_rate, denom, name=name)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 25:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag249')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay.py: 25-99
</a>
<div class="mid" id="frag249" style="display:none"><pre>
def exponential_decay(learning_rate,
                      global_step,
                      decay_steps,
                      decay_rate,
                      staircase=False,
                      name=None):
  """Applies exponential decay to the learning rate.

  When training a model, it is often recommended to lower the learning rate as
  the training progresses.  This function applies an exponential decay function
  to a provided initial learning rate.  It requires a `global_step` value to
  compute the decayed learning rate.  You can just pass a TensorFlow variable
  that you increment at each training step.

  The function returns the decayed learning rate.  It is computed as:

  ```python
  decayed_learning_rate = learning_rate *
                          decay_rate ^ (global_step / decay_steps)
  ```

  If the argument `staircase` is `True`, then `global_step / decay_steps` is an
  integer division and the decayed learning rate follows a staircase function.

  Example: decay every 100000 steps with a base of 0.96:

  ```python
  ...
  global_step = tf.Variable(0, trainable=False)
  starter_learning_rate = 0.1
  learning_rate = tf.compat.v1.train.exponential_decay(starter_learning_rate,
  global_step,
                                             100000, 0.96, staircase=True)
  # Passing global_step to minimize() will increment it at each step.
  learning_step = (
      tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
      .minimize(...my loss..., global_step=global_step)
  )
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` `Tensor` or a Python number.
      The initial learning rate.
    global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global
      step to use for the decay computation.  Must not be negative.
    decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number. Must
      be positive.  See the decay computation above.
    decay_rate: A scalar `float32` or `float64` `Tensor` or a Python number.
      The decay rate.
    staircase: Boolean.  If `True` decay the learning rate at discrete intervals
    name: String.  Optional name of the operation.  Defaults to
      'ExponentialDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.

  Raises:
    ValueError: if `global_step` is not supplied.

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  decayed_lr = learning_rate_schedule.ExponentialDecay(
      learning_rate, decay_steps, decay_rate, staircase=staircase, name=name)
  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag253')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay.py: 368-449
</a>
<div class="mid" id="frag253" style="display:none"><pre>
def inverse_time_decay(learning_rate,
                       global_step,
                       decay_steps,
                       decay_rate,
                       staircase=False,
                       name=None):
  """Applies inverse time decay to the initial learning rate.

  When training a model, it is often recommended to lower the learning rate as
  the training progresses.  This function applies an inverse decay function
  to a provided initial learning rate.  It requires an `global_step` value to
  compute the decayed learning rate.  You can just pass a TensorFlow variable
  that you increment at each training step.

  The function returns the decayed learning rate.  It is computed as:

  ```python
  decayed_learning_rate = learning_rate / (1 + decay_rate * global_step /
  decay_step)
  ```

  or, if `staircase` is `True`, as:

  ```python
  decayed_learning_rate = learning_rate / (1 + decay_rate * floor(global_step /
  decay_step))
  ```

  Example: decay 1/t with a rate of 0.5:

  ```python
  ...
  global_step = tf.Variable(0, trainable=False)
  learning_rate = 0.1
  decay_steps = 1.0
  decay_rate = 0.5
  learning_rate = tf.compat.v1.train.inverse_time_decay(learning_rate,
  global_step,
  decay_steps, decay_rate)

  # Passing global_step to minimize() will increment it at each step.
  learning_step = (
      tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
      .minimize(...my loss..., global_step=global_step)
  )
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` `Tensor` or a Python number.
      The initial learning rate.
    global_step: A Python number. Global step to use for the decay computation.
      Must not be negative.
    decay_steps: How often to apply decay.
    decay_rate: A Python number.  The decay rate.
    staircase: Whether to apply decay in a discrete staircase, as opposed to
      continuous, fashion.
    name: String.  Optional name of the operation.  Defaults to
      'InverseTimeDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.

  Raises:
    ValueError: if `global_step` is not supplied.

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  decayed_lr = learning_rate_schedule.InverseTimeDecay(
      learning_rate, decay_steps, decay_rate, staircase=staircase, name=name)

  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 26:</b> &nbsp; 5 fragments, nominal size 19 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag251')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay.py: 179-278
</a>
<div class="mid" id="frag251" style="display:none"><pre>
def polynomial_decay(learning_rate,
                     global_step,
                     decay_steps,
                     end_learning_rate=0.0001,
                     power=1.0,
                     cycle=False,
                     name=None):
  """Applies a polynomial decay to the learning rate.

  It is commonly observed that a monotonically decreasing learning rate, whose
  degree of change is carefully chosen, results in a better performing model.
  This function applies a polynomial decay function to a provided initial
  `learning_rate` to reach an `end_learning_rate` in the given `decay_steps`.

  It requires a `global_step` value to compute the decayed learning rate.  You
  can just pass a TensorFlow variable that you increment at each training step.

  The function returns the decayed learning rate.  It is computed as:

  ```python
  global_step = min(global_step, decay_steps)
  decayed_learning_rate = (learning_rate - end_learning_rate) *
                          (1 - global_step / decay_steps) ^ (power) +
                          end_learning_rate

  ```

  If `cycle` is True then a multiple of `decay_steps` is used, the first one
  that is bigger than `global_steps`.

  ```python
  decay_steps = decay_steps * ceil(global_step / decay_steps)
  decayed_learning_rate = (learning_rate - end_learning_rate) *
                          (1 - global_step / decay_steps) ^ (power) +
                          end_learning_rate

  ```

  Example: decay from 0.1 to 0.01 in 10000 steps using sqrt (i.e. power=0.5):

  ```python
  ...
  global_step = tf.Variable(0, trainable=False)
  starter_learning_rate = 0.1
  end_learning_rate = 0.01
  decay_steps = 10000
  learning_rate = tf.compat.v1.train.polynomial_decay(starter_learning_rate,
  global_step,
                                            decay_steps, end_learning_rate,
                                            power=0.5)
  # Passing global_step to minimize() will increment it at each step.
  learning_step = (
      tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
      .minimize(...my loss..., global_step=global_step)
  )
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` `Tensor` or a Python number.
      The initial learning rate.
    global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global
      step to use for the decay computation.  Must not be negative.
    decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number. Must
      be positive.  See the decay computation above.
    end_learning_rate: A scalar `float32` or `float64` `Tensor` or a Python
      number.  The minimal end learning rate.
    power: A scalar `float32` or `float64` `Tensor` or a Python number.  The
      power of the polynomial. Defaults to linear, 1.0.
    cycle: A boolean, whether or not it should cycle beyond decay_steps.
    name: String.  Optional name of the operation. Defaults to
      'PolynomialDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.

  Raises:
    ValueError: if `global_step` is not supplied.

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  decayed_lr = learning_rate_schedule.PolynomialDecay(
      learning_rate,
      decay_steps,
      end_learning_rate=end_learning_rate,
      power=power,
      cycle=cycle,
      name=name)

  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag256')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay.py: 591-674
</a>
<div class="mid" id="frag256" style="display:none"><pre>
def linear_cosine_decay(learning_rate,
                        global_step,
                        decay_steps,
                        num_periods=0.5,
                        alpha=0.0,
                        beta=0.001,
                        name=None):
  """Applies linear cosine decay to the learning rate.

  Note that linear cosine decay is more aggressive than cosine decay and
  larger initial learning rates can typically be used.

  When training a model, it is often recommended to lower the learning rate as
  the training progresses.  This function applies a linear cosine decay function
  to a provided initial learning rate.  It requires a `global_step` value to
  compute the decayed learning rate.  You can just pass a TensorFlow variable
  that you increment at each training step.

  The function returns the decayed learning rate.  It is computed as:
  ```python
  global_step = min(global_step, decay_steps)
  linear_decay = (decay_steps - global_step) / decay_steps)
  cosine_decay = 0.5 * (
      1 + cos(pi * 2 * num_periods * global_step / decay_steps))
  decayed = (alpha + linear_decay) * cosine_decay + beta
  decayed_learning_rate = learning_rate * decayed
  ```

  Example usage:
  ```python
  decay_steps = 1000
  lr_decayed = linear_cosine_decay(learning_rate, global_step, decay_steps)
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` Tensor or a Python number.
      The initial learning rate.
    global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global
      step to use for the decay computation.
    decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number. Number
      of steps to decay over.
    num_periods: Number of periods in the cosine part of the decay. See
      computation above.
    alpha: See computation above.
    beta: See computation above.
    name: String.  Optional name of the operation.  Defaults to
      'LinearCosineDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.
  Raises:
    ValueError: if `global_step` is not supplied.

  References:
    Neural Optimizer Search with Reinforcement Learning:
      [Bello et al., 2017](http://proceedings.mlr.press/v70/bello17a.html)
      ([pdf](http://proceedings.mlr.press/v70/bello17a/bello17a.pdf))
    Stochastic Gradient Descent with Warm Restarts:
      [Loshchilov et al., 2017]
      (https://openreview.net/forum?id=Skq89Scxx&amp;noteId=Skq89Scxx)
      ([pdf](https://openreview.net/pdf?id=Skq89Scxx))

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  decayed_lr = learning_rate_schedule.LinearCosineDecay(
      learning_rate,
      decay_steps,
      num_periods=num_periods,
      alpha=alpha,
      beta=beta,
      name=name)

  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag257')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay.py: 676-767
</a>
<div class="mid" id="frag257" style="display:none"><pre>
def noisy_linear_cosine_decay(learning_rate,
                              global_step,
                              decay_steps,
                              initial_variance=1.0,
                              variance_decay=0.55,
                              num_periods=0.5,
                              alpha=0.0,
                              beta=0.001,
                              name=None):
  """Applies noisy linear cosine decay to the learning rate.

  Note that linear cosine decay is more aggressive than cosine decay and
  larger initial learning rates can typically be used.

  When training a model, it is often recommended to lower the learning rate as
  the training progresses.  This function applies a noisy linear
  cosine decay function to a provided initial learning rate.
  It requires a `global_step` value to compute the decayed learning rate.
  You can just pass a TensorFlow variable that you increment at each
  training step.

  The function returns the decayed learning rate.  It is computed as:
  ```python
  global_step = min(global_step, decay_steps)
  linear_decay = (decay_steps - global_step) / decay_steps)
  cosine_decay = 0.5 * (
      1 + cos(pi * 2 * num_periods * global_step / decay_steps))
  decayed = (alpha + linear_decay + eps_t) * cosine_decay + beta
  decayed_learning_rate = learning_rate * decayed
  ```
  where eps_t is 0-centered gaussian noise with variance
  initial_variance / (1 + global_step) ** variance_decay

  Example usage:
  ```python
  decay_steps = 1000
  lr_decayed = noisy_linear_cosine_decay(
    learning_rate, global_step, decay_steps)
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` Tensor or a Python number.
      The initial learning rate.
    global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global
      step to use for the decay computation.
    decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number. Number
      of steps to decay over.
    initial_variance: initial variance for the noise. See computation above.
    variance_decay: decay for the noise's variance. See computation above.
    num_periods: Number of periods in the cosine part of the decay. See
      computation above.
    alpha: See computation above.
    beta: See computation above.
    name: String.  Optional name of the operation.  Defaults to
      'NoisyLinearCosineDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.
  Raises:
    ValueError: if `global_step` is not supplied.

  References:
    Neural Optimizer Search with Reinforcement Learning:
      [Bello et al., 2017](http://proceedings.mlr.press/v70/bello17a.html)
      ([pdf](http://proceedings.mlr.press/v70/bello17a/bello17a.pdf))
    Stochastic Gradient Descent with Warm Restarts:
      [Loshchilov et al., 2017]
      (https://openreview.net/forum?id=Skq89Scxx&amp;noteId=Skq89Scxx)
      ([pdf](https://openreview.net/pdf?id=Skq89Scxx))

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  decayed_lr = learning_rate_schedule.NoisyLinearCosineDecay(
      learning_rate,
      decay_steps,
      initial_variance=initial_variance,
      variance_decay=variance_decay,
      num_periods=num_periods,
      alpha=alpha,
      beta=beta,
      name=name)

  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag255')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay.py: 514-589
</a>
<div class="mid" id="frag255" style="display:none"><pre>
def cosine_decay_restarts(learning_rate,
                          global_step,
                          first_decay_steps,
                          t_mul=2.0,
                          m_mul=1.0,
                          alpha=0.0,
                          name=None):
  """Applies cosine decay with restarts to the learning rate.

  When training a model, it is often recommended to lower the learning rate as
  the training progresses.  This function applies a cosine decay function with
  restarts to a provided initial learning rate.  It requires a `global_step`
  value to compute the decayed learning rate.  You can just pass a TensorFlow
  variable that you increment at each training step.

  The function returns the decayed learning rate while taking into account
  possible warm restarts. The learning rate multiplier first decays
  from 1 to `alpha` for `first_decay_steps` steps. Then, a warm
  restart is performed. Each new warm restart runs for `t_mul` times more steps
  and with `m_mul` times smaller initial learning rate.

  Example usage:
  ```python
  first_decay_steps = 1000
  lr_decayed = cosine_decay_restarts(learning_rate, global_step,
                                     first_decay_steps)
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` Tensor or a Python number.
      The initial learning rate.
    global_step: A scalar `int32` or `int64` `Tensor` or a Python number. Global
      step to use for the decay computation.
    first_decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.
      Number of steps to decay over.
    t_mul: A scalar `float32` or `float64` `Tensor` or a Python number. Used to
      derive the number of iterations in the i-th period
    m_mul: A scalar `float32` or `float64` `Tensor` or a Python number.
      Used to derive the initial learning rate of the i-th period:
    alpha: A scalar `float32` or `float64` Tensor or a Python number. Minimum
      learning rate value as a fraction of the learning_rate.
    name: String. Optional name of the operation.  Defaults to 'SGDRDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.
  Raises:
    ValueError: if `global_step` is not supplied.

  References:
    Stochastic Gradient Descent with Warm Restarts:
      [Loshchilov et al., 2017]
      (https://openreview.net/forum?id=Skq89Scxx&amp;noteId=Skq89Scxx)
      ([pdf](https://openreview.net/pdf?id=Skq89Scxx))

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  decayed_lr = learning_rate_schedule.CosineDecayRestarts(
      learning_rate,
      first_decay_steps,
      t_mul=t_mul,
      m_mul=m_mul,
      alpha=alpha,
      name=name)

  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag252')" href="javascript:;">
keras-2.6.0/keras/optimizer_v2/legacy_learning_rate_decay.py: 280-366
</a>
<div class="mid" id="frag252" style="display:none"><pre>
def natural_exp_decay(learning_rate,
                      global_step,
                      decay_steps,
                      decay_rate,
                      staircase=False,
                      name=None):
  """Applies natural exponential decay to the initial learning rate.

  When training a model, it is often recommended to lower the learning rate as
  the training progresses.  This function applies an exponential decay function
  to a provided initial learning rate.  It requires an `global_step` value to
  compute the decayed learning rate.  You can just pass a TensorFlow variable
  that you increment at each training step.

  The function returns the decayed learning rate.  It is computed as:

  ```python
  decayed_learning_rate = learning_rate * exp(-decay_rate * global_step /
  decay_step)
  ```

  or, if `staircase` is `True`, as:

  ```python
  decayed_learning_rate = learning_rate * exp(-decay_rate * floor(global_step /
  decay_step))
  ```

  Example: decay exponentially with a base of 0.96:

  ```python
  ...
  global_step = tf.Variable(0, trainable=False)
  learning_rate = 0.1
  decay_steps = 5
  k = 0.5
  learning_rate = tf.compat.v1.train.natural_exp_decay(learning_rate,
  global_step,
                                             decay_steps, k)

  # Passing global_step to minimize() will increment it at each step.
  learning_step = (
      tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
      .minimize(...my loss..., global_step=global_step)
  )
  ```

  Args:
    learning_rate: A scalar `float32` or `float64` `Tensor` or a Python number.
      The initial learning rate.
    global_step: A Python number. Global step to use for the decay computation.
      Must not be negative.
    decay_steps: How often to apply decay.
    decay_rate: A Python number.  The decay rate.
    staircase: Whether to apply decay in a discrete staircase, as opposed to
      continuous, fashion.
    name: String.  Optional name of the operation.  Defaults to
      'ExponentialTimeDecay'.

  Returns:
    A scalar `Tensor` of the same type as `learning_rate`.  The decayed
    learning rate.

  Raises:
    ValueError: if `global_step` is not supplied.

  @compatibility(eager)
  When eager execution is enabled, this function returns a function which in
  turn returns the decayed learning rate Tensor. This can be useful for changing
  the learning rate value across different invocations of optimizer functions.
  @end_compatibility
  """
  natural_exp_rate = tf.exp(tf.negative(decay_rate))
  decayed_lr = learning_rate_schedule.ExponentialDecay(
      learning_rate,
      decay_steps,
      natural_exp_rate,
      staircase=staircase,
      name=name)

  if not tf.executing_eagerly():
    decayed_lr = decayed_lr(global_step)
  else:
    decayed_lr = functools.partial(decayed_lr, global_step)
  return decayed_lr


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 27:</b> &nbsp; 2 fragments, nominal size 24 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag326')" href="javascript:;">
keras-2.6.0/keras/distribute/saved_model_test_base.py: 200-229
</a>
<div class="mid" id="frag326" style="display:none"><pre>
  def run_test_save_strategy_restore_no_strategy(self, model_and_input,
                                                 distribution, save_in_scope):
    """Save a model with DS, and restore it without DS."""

    saved_dir = os.path.join(self.get_temp_dir(), '1')

    with distribution.scope():
      model = model_and_input.get_model()
      x_train, y_train, x_predict = model_and_input.get_data()
      batch_size = model_and_input.get_batch_size()

      self._train_model(model, x_train, y_train, batch_size)
      predict_dataset = self._get_predict_dataset(x_predict, batch_size)
      result_before_save = self._predict_with_model(
          distribution, model, predict_dataset)

    if save_in_scope:
      with distribution.scope():
        self._save_model(model, saved_dir)
    else:
      self._save_model(model, saved_dir)

    load_result = self._load_and_run_model(
        distribution=None,
        saved_dir=saved_dir,
        predict_dataset=predict_dataset)

    tolerance = get_tolerance(distribution, None)
    self.assertAllClose(result_before_save, load_result, atol=tolerance)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag327')" href="javascript:;">
keras-2.6.0/keras/distribute/saved_model_test_base.py: 230-263
</a>
<div class="mid" id="frag327" style="display:none"><pre>
  def run_test_save_strategy_restore_strategy(self, model_and_input,
                                              distribution_for_saving,
                                              distribution_for_restoring,
                                              save_in_scope):
    """Save a model with DS, and restore it with potentially different DS."""
    saved_dir = os.path.join(self.get_temp_dir(), '2')

    with distribution_for_saving.scope():
      model = model_and_input.get_model()
      x_train, y_train, x_predict = model_and_input.get_data()
      batch_size = model_and_input.get_batch_size()

      self._train_model(model, x_train, y_train, batch_size)
      predict_dataset = self._get_predict_dataset(x_predict, batch_size)
      result_before_save = self._predict_with_model(
          distribution_for_saving, model, predict_dataset)

    if save_in_scope:
      with distribution_for_saving.scope():
        self._save_model(model, saved_dir)
    else:
      self._save_model(model, saved_dir)

    with distribution_for_restoring.scope():

      load_result = self._load_and_run_model(
          distribution=distribution_for_restoring,
          saved_dir=saved_dir,
          predict_dataset=predict_dataset)

    tolerance = get_tolerance(distribution_for_saving,
                              distribution_for_restoring)
    self.assertAllClose(result_before_save, load_result, atol=tolerance)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 28:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag352')" href="javascript:;">
keras-2.6.0/keras/distribute/sidecar_evaluator_test.py: 115-131
</a>
<div class="mid" id="frag352" style="display:none"><pre>
  def testIterationsNotSavedWillRaiseError(self, model_type):
    model = _test_model_builder(
        model_type=model_type, compile_model=False, build_model=True)

    checkpoint_dir = self.get_temp_dir()
    checkpoint = tf.train.Checkpoint(model=model)
    checkpoint_manager = tf.train.CheckpointManager(
        checkpoint, checkpoint_dir, max_to_keep=2)
    checkpoint_manager.save()

    sidecar_evaluator = sidecar_evaluator_lib.SidecarEvaluator(
        model, data=None, checkpoint_dir=checkpoint_dir)
    with self.assertRaisesRegex(
        RuntimeError, '`iterations` cannot be loaded '
        'from the checkpoint file.'):
      sidecar_evaluator.start()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag353')" href="javascript:;">
keras-2.6.0/keras/distribute/sidecar_evaluator_test.py: 136-150
</a>
<div class="mid" id="frag353" style="display:none"><pre>
  def testModelNotBuiltRaiseError(self, model_type):
    model = _test_model_builder(
        model_type=model_type, compile_model=False, build_model=False)

    checkpoint_dir = self.get_temp_dir()
    checkpoint = tf.train.Checkpoint(model=model)
    checkpoint_manager = tf.train.CheckpointManager(
        checkpoint, checkpoint_dir, max_to_keep=2)
    checkpoint_manager.save()

    sidecar_evaluator = sidecar_evaluator_lib.SidecarEvaluator(
        model, data=None, checkpoint_dir=checkpoint_dir)
    with self.assertRaisesRegex(AssertionError, 'Nothing to load.'):
      sidecar_evaluator.start()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 29:</b> &nbsp; 4 fragments, nominal size 10 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag360')" href="javascript:;">
keras-2.6.0/keras/distribute/dataset_creator_model_fit_test.py: 72-83
</a>
<div class="mid" id="frag360" style="display:none"><pre>
  def testModelFitWithNumpyData(self, strategy):
    x = np.random.rand(100, 10)
    y = np.random.rand(100, 1)
    model = self._model_fit(
        strategy,
        x=x,
        y=y,
        batch_size=1,
        validation_data=(x, y),
    )
    self.assertEqual(model.optimizer.iterations, 100)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag367')" href="javascript:;">
keras-2.6.0/keras/distribute/dataset_creator_model_fit_test.py: 120-130
</a>
<div class="mid" id="frag367" style="display:none"><pre>
  def testModelEvaluateWithNumpyData(self, strategy):
    x = np.random.rand(100, 10)
    y = np.random.rand(100, 1)
    self._model_evaluate(
        strategy,
        x=x,
        y=y,
        batch_size=1,
    )
    self.assertGreaterEqual(self._accuracy_metric.result(), 0.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag361')" href="javascript:;">
keras-2.6.0/keras/distribute/dataset_creator_model_fit_test.py: 84-95
</a>
<div class="mid" id="frag361" style="display:none"><pre>
  def testModelFitWithTensorData(self, strategy):
    x = tf.random.uniform((100, 10))
    y = tf.random.uniform((100,))
    model = self._model_fit(
        strategy,
        x=x,
        y=y,
        batch_size=1,
        validation_data=(x, y),
    )
    self.assertEqual(model.optimizer.iterations, 100)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag368')" href="javascript:;">
keras-2.6.0/keras/distribute/dataset_creator_model_fit_test.py: 131-141
</a>
<div class="mid" id="frag368" style="display:none"><pre>
  def testModelEvaluateWithTensorData(self, strategy):
    x = tf.random.uniform((100, 10))
    y = tf.random.uniform((100,))
    self._model_evaluate(
        strategy,
        x=x,
        y=y,
        batch_size=1,
    )
    self.assertGreaterEqual(self._accuracy_metric.result(), 0.0)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 30:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag390')" href="javascript:;">
keras-2.6.0/keras/distribute/optimizer_combinations.py: 71-83
</a>
<div class="mid" id="frag390" style="display:none"><pre>
def distributions_and_v1_optimizers():
  """A common set of combination with DistributionStrategies and Optimizers."""
  return tf.__internal__.test.combinations.combine(
      distribution=[
          tf.__internal__.distribute.combinations.one_device_strategy,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,
          tf.__internal__.distribute.combinations
          .mirrored_strategy_with_two_gpus_no_merge_call,
      ],
      optimizer_fn=optimizers_v1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag391')" href="javascript:;">
keras-2.6.0/keras/distribute/optimizer_combinations.py: 84-96
</a>
<div class="mid" id="frag391" style="display:none"><pre>
def distributions_and_v2_optimizers():
  """A common set of combination with DistributionStrategies and Optimizers."""
  return tf.__internal__.test.combinations.combine(
      distribution=[
          tf.__internal__.distribute.combinations.one_device_strategy,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,
          tf.__internal__.distribute.combinations
          .mirrored_strategy_with_two_gpus_no_merge_call,
      ],
      optimizer_fn=optimizers_v2)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag392')" href="javascript:;">
keras-2.6.0/keras/distribute/optimizer_combinations.py: 97-107
</a>
<div class="mid" id="frag392" style="display:none"><pre>
def distributions_and_v1_and_v2_optimizers():
  """A common set of combination with DistributionStrategies and Optimizers."""
  return tf.__internal__.test.combinations.combine(
      distribution=[
          tf.__internal__.distribute.combinations.one_device_strategy,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_gpu_and_cpu,
          tf.__internal__.distribute.combinations.mirrored_strategy_with_two_gpus,
          tf.__internal__.distribute.combinations
          .mirrored_strategy_with_two_gpus_no_merge_call,
      ],
      optimizer_fn=optimizers_v1_and_v2)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 31:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag400')" href="javascript:;">
keras-2.6.0/keras/distribute/keras_embedding_model_correctness_test.py: 30-55
</a>
<div class="mid" id="frag400" style="display:none"><pre>
  def get_model(self,
                max_words=10,
                initial_weights=None,
                distribution=None,
                input_shapes=None):
    del input_shapes
    with keras_correctness_test_base.MaybeDistributionScope(distribution):
      word_ids = keras.layers.Input(
          shape=(max_words,), dtype=np.int32, name='words')
      word_embed = keras.layers.Embedding(input_dim=20, output_dim=10)(word_ids)
      if self.use_distributed_dense:
        word_embed = keras.layers.TimeDistributed(keras.layers.Dense(4))(
            word_embed)
      avg = keras.layers.GlobalAveragePooling1D()(word_embed)
      preds = keras.layers.Dense(2, activation='softmax')(avg)
      model = keras.Model(inputs=[word_ids], outputs=[preds])

      if initial_weights:
        model.set_weights(initial_weights)

      model.compile(
          optimizer=gradient_descent_keras.SGD(learning_rate=0.1),
          loss='sparse_categorical_crossentropy',
          metrics=['sparse_categorical_accuracy'])
    return model

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag748')" href="javascript:;">
keras-2.6.0/keras/distribute/keras_rnn_model_correctness_test.py: 37-66
</a>
<div class="mid" id="frag748" style="display:none"><pre>
  def get_model(self,
                max_words=10,
                initial_weights=None,
                distribution=None,
                input_shapes=None):
    del input_shapes
    rnn_cls = self._get_layer_class()

    with keras_correctness_test_base.MaybeDistributionScope(distribution):
      word_ids = keras.layers.Input(
          shape=(max_words,), dtype=np.int32, name='words')
      word_embed = keras.layers.Embedding(input_dim=20, output_dim=10)(word_ids)
      rnn_embed = rnn_cls(units=4, return_sequences=False)(word_embed)

      dense_output = keras.layers.Dense(2)(rnn_embed)
      preds = keras.layers.Softmax(dtype='float32')(dense_output)
      model = keras.Model(inputs=[word_ids], outputs=[preds])

      if initial_weights:
        model.set_weights(initial_weights)

      optimizer_fn = gradient_descent_keras.SGD

      model.compile(
          optimizer=optimizer_fn(learning_rate=0.1),
          loss='sparse_categorical_crossentropy',
          metrics=['sparse_categorical_accuracy'])
    return model


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 32:</b> &nbsp; 4 fragments, nominal size 19 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag436')" href="javascript:;">
keras-2.6.0/keras/distribute/custom_training_loop_models_test.py: 55-78
</a>
<div class="mid" id="frag436" style="display:none"><pre>
  def test_single_keras_layer_run(self, distribution):
    dataset = _get_dataset()
    input_iterator = iter(distribution.experimental_distribute_dataset(dataset))

    with distribution.scope():
      model = keras.layers.Dense(4, name="dense")

    @tf.function
    def train_step(iterator):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        return grads

      outputs = distribution.run(
          step_fn, args=(next(iterator),))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

    train_step(input_iterator)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag476')" href="javascript:;">
keras-2.6.0/keras/distribute/custom_training_loop_models_test.py: 390-414
</a>
<div class="mid" id="frag476" style="display:none"><pre>
  def test_customized_tf_module_run(self, distribution):
    dataset = _get_dataset()
    input_iterator = iter(distribution.experimental_distribute_dataset(dataset))

    with distribution.scope():
      model = CustomModel()

    @tf.function
    def train_step(iterator):

      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        return grads

      outputs = distribution.run(
          step_fn, args=(next(iterator),))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

    train_step(input_iterator)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag448')" href="javascript:;">
keras-2.6.0/keras/distribute/custom_training_loop_models_test.py: 142-165
</a>
<div class="mid" id="frag448" style="display:none"><pre>
  def test_keras_model_optimizer_run_loop(self, distribution):
    dataset = _get_dataset()
    input_iterator = iter(distribution.experimental_distribute_dataset(dataset))

    with distribution.scope():
      model = _get_model()
      optimizer = keras.optimizer_v2.rmsprop.RMSprop()

    @tf.function
    def train_step(iterator):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        optimizer.apply_gradients(zip(grads, model.variables))
        return loss

      for _ in tf.range(4):
        distribution.run(step_fn, args=(next(iterator),))

    train_step(input_iterator)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag439')" href="javascript:;">
keras-2.6.0/keras/distribute/custom_training_loop_models_test.py: 79-104
</a>
<div class="mid" id="frag439" style="display:none"><pre>
  def test_keras_model_optimizer_run(self, distribution):
    dataset = _get_dataset()
    input_iterator = iter(distribution.experimental_distribute_dataset(dataset))

    with distribution.scope():
      model = _get_model()
      optimizer = keras.optimizer_v2.rmsprop.RMSprop()

    @tf.function
    def train_step(replicated_inputs):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        optimizer.apply_gradients(zip(grads, model.variables))
        return loss

      outputs = distribution.run(step_fn, args=(replicated_inputs,))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

    for x in input_iterator:
      train_step(x)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 33:</b> &nbsp; 7 fragments, nominal size 11 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag437')" href="javascript:;">
keras-2.6.0/keras/distribute/custom_training_loop_models_test.py: 63-76
</a>
<div class="mid" id="frag437" style="display:none"><pre>
    def train_step(iterator):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        return grads

      outputs = distribution.run(
          step_fn, args=(next(iterator),))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag489')" href="javascript:;">
keras-2.6.0/keras/distribute/custom_training_loop_models_test.py: 503-516
</a>
<div class="mid" id="frag489" style="display:none"><pre>
    def train_step(iterator):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        return grads

      outputs = distribution.run(
          step_fn, args=(next(iterator),))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag446')" href="javascript:;">
keras-2.6.0/keras/distribute/custom_training_loop_models_test.py: 126-139
</a>
<div class="mid" id="frag446" style="display:none"><pre>
    def train_step(iterator):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        optimizer.apply_gradients(zip(grads, model.variables))
        return loss

      outputs = distribution.run(step_fn, args=(next(iterator),))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag477')" href="javascript:;">
keras-2.6.0/keras/distribute/custom_training_loop_models_test.py: 398-412
</a>
<div class="mid" id="frag477" style="display:none"><pre>
    def train_step(iterator):

      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        return grads

      outputs = distribution.run(
          step_fn, args=(next(iterator),))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag440')" href="javascript:;">
keras-2.6.0/keras/distribute/custom_training_loop_models_test.py: 88-101
</a>
<div class="mid" id="frag440" style="display:none"><pre>
    def train_step(replicated_inputs):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        optimizer.apply_gradients(zip(grads, model.variables))
        return loss

      outputs = distribution.run(step_fn, args=(replicated_inputs,))
      return tf.nest.map_structure(distribution.experimental_local_results,
                                outputs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag449')" href="javascript:;">
keras-2.6.0/keras/distribute/custom_training_loop_models_test.py: 151-163
</a>
<div class="mid" id="frag449" style="display:none"><pre>
    def train_step(iterator):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        optimizer.apply_gradients(zip(grads, model.variables))
        return loss

      for _ in tf.range(4):
        distribution.run(step_fn, args=(next(iterator),))

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag452')" href="javascript:;">
keras-2.6.0/keras/distribute/custom_training_loop_models_test.py: 183-194
</a>
<div class="mid" id="frag452" style="display:none"><pre>
    def train_step(iterator):
      def step_fn(inputs):
        images, targets = inputs
        with tf.GradientTape() as tape:
          outputs = model(images, training=True)
          loss = keras.losses.mean_squared_error(targets, outputs)
        grads = tape.gradient(loss, model.variables)
        optimizer.apply_gradients(zip(grads, model.variables))
        return loss

      distribution.run(step_fn, args=(next(iterator),))

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 34:</b> &nbsp; 2 fragments, nominal size 37 lines, similarity 89%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag506')" href="javascript:;">
keras-2.6.0/keras/distribute/mirrored_strategy_test.py: 81-130
</a>
<div class="mid" id="frag506" style="display:none"><pre>
  def testTrainAndServeWithKPL(self, distribution):
    use_adapt = False
    test_utils_obj = kpl_test_utils.DistributeKplTestUtils()
    with distribution.scope():
      feature_mapper, label_mapper = test_utils_obj.define_kpls_for_training(
          use_adapt)
      model = test_utils_obj.define_model()
      optimizer = rmsprop.RMSprop(learning_rate=0.1)
      accuracy = keras.metrics.Accuracy()

      def dataset_fn(_):
        return test_utils_obj.dataset_fn(feature_mapper, label_mapper)

      @tf.function
      def train_step(iterator):
        """The step function for one training step."""

        def step_fn(inputs):
          """The computation to run on each replica(GPU)."""
          features, labels = inputs
          with tf.GradientTape() as tape:
            pred = model(features, training=True)
            loss = keras.losses.binary_crossentropy(labels, pred)
            loss = tf.nn.compute_average_loss(loss)
          grads = tape.gradient(loss, model.trainable_variables)
          optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))

          actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)
          accuracy.update_state(labels, actual_pred)

        distribution.run(step_fn, args=(next(iterator),))

      distributed_dataset = distribution.distribute_datasets_from_function(
          dataset_fn)
      distributed_iterator = iter(distributed_dataset)
      num_epochs = 4
      num_steps = 7
      for _ in range(num_epochs):
        accuracy.reset_state()
        for _ in range(num_steps):
          train_step(distributed_iterator)

      self.assertGreater(accuracy.result().numpy(), 0.5)
      self.assertEqual(optimizer.iterations.numpy(), num_epochs * num_steps)

    # Test save/load/serving the trained model.
    test_utils_obj.test_save_load_serving_model(
        model, feature_mapper, test_utils_obj.define_reverse_lookup_layer())


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1600')" href="javascript:;">
keras-2.6.0/keras/integration_test/central_storage_strategy_test.py: 35-84
</a>
<div class="mid" id="frag1600" style="display:none"><pre>
  def testTrainAndServeWithKPL(self, distribution):
    use_adapt = False
    test_utils_obj = kpl_test_utils.DistributeKplTestUtils()
    with distribution.scope():
      feature_mapper, label_mapper = test_utils_obj.define_kpls_for_training(
          use_adapt)
      model = test_utils_obj.define_model()
      optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.1)
      accuracy = tf.keras.metrics.Accuracy()

      def dataset_fn(_):
        return test_utils_obj.dataset_fn(feature_mapper, label_mapper)

      @tf.function
      def train_step(iterator):
        """The step function for one training step."""

        def step_fn(inputs):
          """The computation to run on each replica."""
          features, labels = inputs
          with tf.GradientTape() as tape:
            pred = model(features, training=True)
            loss = tf.keras.losses.binary_crossentropy(labels, pred)
            loss = tf.nn.compute_average_loss(loss)
          grads = tape.gradient(loss, model.trainable_variables)
          optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))

          actual_pred = tf.cast(tf.math.greater(pred, 0.5), tf.dtypes.int64)
          accuracy.update_state(labels, actual_pred)

        distribution.run(step_fn, args=(next(iterator),))

      distributed_dataset = distribution.distribute_datasets_from_function(
          dataset_fn)
      distributed_iterator = iter(distributed_dataset)
      num_epochs = 4
      num_steps = 7
      for _ in range(num_epochs):
        accuracy.reset_state()
        for _ in range(num_steps):
          train_step(distributed_iterator)

      self.assertGreater(accuracy.result().numpy(), 0.5)
      self.assertEqual(optimizer.iterations.numpy(), num_epochs * num_steps)

    # Test save/load/serving the trained model.
    test_utils_obj.test_save_load_serving_model(
        model, feature_mapper, test_utils_obj.define_reverse_lookup_layer())


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 35:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag508')" href="javascript:;">
keras-2.6.0/keras/distribute/mirrored_strategy_test.py: 95-112
</a>
<div class="mid" id="frag508" style="display:none"><pre>
      def train_step(iterator):
        """The step function for one training step."""

        def step_fn(inputs):
          """The computation to run on each replica(GPU)."""
          features, labels = inputs
          with tf.GradientTape() as tape:
            pred = model(features, training=True)
            loss = keras.losses.binary_crossentropy(labels, pred)
            loss = tf.nn.compute_average_loss(loss)
          grads = tape.gradient(loss, model.trainable_variables)
          optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))

          actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)
          accuracy.update_state(labels, actual_pred)

        distribution.run(step_fn, args=(next(iterator),))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1617')" href="javascript:;">
keras-2.6.0/keras/integration_test/tpu_strategy_test.py: 163-180
</a>
<div class="mid" id="frag1617" style="display:none"><pre>
      def train_step(iterator):
        """The step function for one training step."""

        def step_fn(inputs):
          """The computation to run on each TPU device."""
          features, labels = inputs
          with tf.GradientTape() as tape:
            pred = model(features, training=True)
            loss = tf.keras.losses.binary_crossentropy(labels, pred)
            loss = tf.nn.compute_average_loss(loss)
          grads = tape.gradient(loss, model.trainable_variables)
          optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))

          actual_pred = tf.cast(tf.math.greater(pred, 0.5), tf.dtypes.int64)
          accuracy.update_state(labels, actual_pred)

        strategy.run(step_fn, args=(next(iterator),))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1602')" href="javascript:;">
keras-2.6.0/keras/integration_test/central_storage_strategy_test.py: 49-66
</a>
<div class="mid" id="frag1602" style="display:none"><pre>
      def train_step(iterator):
        """The step function for one training step."""

        def step_fn(inputs):
          """The computation to run on each replica."""
          features, labels = inputs
          with tf.GradientTape() as tape:
            pred = model(features, training=True)
            loss = tf.keras.losses.binary_crossentropy(labels, pred)
            loss = tf.nn.compute_average_loss(loss)
          grads = tape.gradient(loss, model.trainable_variables)
          optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))

          actual_pred = tf.cast(tf.math.greater(pred, 0.5), tf.dtypes.int64)
          accuracy.update_state(labels, actual_pred)

        distribution.run(step_fn, args=(next(iterator),))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 36:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag631')" href="javascript:;">
keras-2.6.0/keras/distribute/keras_utils_test.py: 122-142
</a>
<div class="mid" id="frag631" style="display:none"><pre>
  def test_callbacks_in_eval(self, distribution):
    with distribution.scope():
      model = keras_test_lib.get_model()
      model.compile(
          optimizer='sgd',
          loss='mse',
          metrics=['mae'])

    dataset = keras_test_lib.get_dataset(distribution)
    counter = Counter()

    model.evaluate(dataset, steps=5, callbacks=[counter])

    self.assertDictEqual(
        counter.method_counts, {
            'on_test_batch_begin': 5,
            'on_test_batch_end': 5,
            'on_test_begin': 1,
            'on_test_end': 1
        })

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag632')" href="javascript:;">
keras-2.6.0/keras/distribute/keras_utils_test.py: 146-170
</a>
<div class="mid" id="frag632" style="display:none"><pre>
  def test_callbacks_in_predict(self, distribution):
    with distribution.scope():
      model = keras_test_lib.get_model()
      model.compile(
          optimizer='sgd',
          loss='mse',
          metrics=['mae'])

    dataset = keras_test_lib.get_dataset(distribution)
    counter = Counter()

    model.predict(
        keras_test_lib.get_predict_dataset(dataset),
        steps=5,
        callbacks=[counter])

    self.assertDictEqual(
        counter.method_counts, {
            'on_predict_batch_begin': 5,
            'on_predict_batch_end': 5,
            'on_predict_begin': 1,
            'on_predict_end': 1
        })


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 37:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag633')" href="javascript:;">
keras-2.6.0/keras/distribute/keras_utils_test.py: 179-196
</a>
<div class="mid" id="frag633" style="display:none"><pre>
  def test_validating_dataset_input_tensors_with_shape_mismatch(
      self, distribution):
    with self.cached_session():
      a = tf.constant([1, 2], shape=(1, 2))
      b = tf.constant([[1, 2], [1, 2]], shape=(2, 2))
      x = tf.distribute.DistributedValues((a, b))
      y = tf.distribute.DistributedValues((a, a))
      # Removed device and input tensor shape details from the error message
      # since the order of the device and the corresponding input tensor shape
      # is not deterministic over different runs.
      with self.assertRaisesRegex(
          ValueError, 'Input tensor shapes do not match for '
          'distributed tensor inputs '
          'DistributedValues:.+'):
        with distribution.scope():
          distributed_training_utils_v1.validate_distributed_dataset_inputs(
              distribution, x, y)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag634')" href="javascript:;">
keras-2.6.0/keras/distribute/keras_utils_test.py: 203-220
</a>
<div class="mid" id="frag634" style="display:none"><pre>
  def test_validating_dataset_input_tensors_with_dtype_mismatch(
      self, distribution):
    with self.cached_session():
      a = tf.constant([1, 2], shape=(1, 2), dtype=tf.int32)
      b = tf.constant([1, 2], shape=(1, 2), dtype=tf.float64)
      x = tf.distribute.DistributedValues((a, b))
      y = tf.distribute.DistributedValues((a, a))
      # Removed device and input tensor dtype details from the error message
      # since the order of the device and the corresponding input tensor dtype
      # is not deterministic over different runs.
      with self.assertRaisesRegex(
          ValueError, 'Input tensor dtypes do not match for '
          'distributed tensor inputs '
          'DistributedValues:.+'):
        with distribution.scope():
          distributed_training_utils_v1.validate_distributed_dataset_inputs(
              distribution, x, y)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 38:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag643')" href="javascript:;">
keras-2.6.0/keras/distribute/keras_utils_test.py: 433-454
</a>
<div class="mid" id="frag643" style="display:none"><pre>
  def test_save_load_h5(self, distribution, optimizer):
    with self.cached_session():
      dataset = keras_test_lib.get_dataset(distribution)
      with distribution.scope():
        model = keras_test_lib.get_model()
        model.compile(
            optimizer(),
            'mse')
        model.fit(dataset, epochs=1, steps_per_epoch=1)

        weights_file = tempfile.mktemp('.h5')
        model.save_weights(weights_file)

        model_2 = keras_test_lib.get_model()
        model_2.compile(
            optimizer(),
            'mse')
        model_2.load_weights(weights_file)
        model_2.predict(
            keras_test_lib.get_predict_dataset(distribution), steps=2)
        model_2.fit(dataset, epochs=1, steps_per_epoch=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag644')" href="javascript:;">
keras-2.6.0/keras/distribute/keras_utils_test.py: 460-487
</a>
<div class="mid" id="frag644" style="display:none"><pre>
  def test_save_load_trackable(self, distribution, optimizer):
    # TODO(b/123533246): Enable the test for TPU once bug is fixed
    if (isinstance(distribution,
                   (tf.distribute.experimental.TPUStrategy, tf.compat.v1.distribute.experimental.TPUStrategy)) and
        distribution.extended.steps_per_run &gt; 1):
      self.skipTest('MultiStep TPU Strategy deadlocks with optimizer restore.')
    with self.cached_session():
      dataset = keras_test_lib.get_dataset(distribution)
      with distribution.scope():
        model = keras_test_lib.get_model()
        model.compile(
            optimizer(),
            'mse')
        model.fit(dataset, epochs=1, steps_per_epoch=1)

        weights_file = tempfile.mktemp()
        model.save_weights(weights_file)

        model_2 = keras_test_lib.get_model()
        model_2.compile(
            optimizer(),
            'mse')
        model_2.load_weights(weights_file)
        model_2.predict(
            keras_test_lib.get_predict_dataset(distribution), steps=2)
        model_2.fit(dataset, epochs=1, steps_per_epoch=1)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 39:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag658')" href="javascript:;">
keras-2.6.0/keras/distribute/multi_worker_callback_tf2_test.py: 147-171
</a>
<div class="mid" id="frag658" style="display:none"><pre>
  def test_model_checkpoint_works_with_same_file_path(self, mode):

    def proc_model_checkpoint_works_with_same_file_path(
        test_obj, saving_filepath):
      model, _, train_ds, steps = _model_setup(test_obj, file_format='')
      num_epoch = 2

      # The saving_filepath shouldn't exist at the beginning (as it's unique).
      test_obj.assertFalse(tf.io.gfile.exists(saving_filepath))

      model.fit(
          x=train_ds,
          epochs=num_epoch,
          steps_per_epoch=steps,
          callbacks=[callbacks.ModelCheckpoint(filepath=saving_filepath)])

      test_obj.assertTrue(tf.io.gfile.exists(saving_filepath))

    saving_filepath = os.path.join(self.get_temp_dir(), 'checkpoint')

    tf.__internal__.distribute.multi_process_runner.run(
        proc_model_checkpoint_works_with_same_file_path,
        cluster_spec=tf.__internal__.distribute.multi_process_runner.create_cluster_spec(num_workers=2),
        args=(self, saving_filepath))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag668')" href="javascript:;">
keras-2.6.0/keras/distribute/multi_worker_callback_tf2_test.py: 305-332
</a>
<div class="mid" id="frag668" style="display:none"><pre>
  def test_tensorboard_works_with_same_file_path(self, mode):

    def proc_tensorboard_works_with_same_file_path(test_obj, saving_filepath):
      model, _, train_ds, steps = _model_setup(test_obj, file_format='')
      num_epoch = 2

      # The saving_filepath shouldn't exist at the beginning (as it's unique).
      test_obj.assertFalse(tf.io.gfile.exists(saving_filepath))

      tf.__internal__.distribute.multi_process_runner.get_barrier().wait()

      model.fit(
          x=train_ds,
          epochs=num_epoch,
          steps_per_epoch=steps,
          callbacks=[callbacks.TensorBoard(log_dir=saving_filepath)])

      tf.__internal__.distribute.multi_process_runner.get_barrier().wait()

      test_obj.assertTrue(tf.io.gfile.listdir(saving_filepath))

    saving_filepath = os.path.join(self.get_temp_dir(), 'logfile')

    tf.__internal__.distribute.multi_process_runner.run(
        proc_tensorboard_works_with_same_file_path,
        cluster_spec=tf.__internal__.distribute.multi_process_runner.create_cluster_spec(num_workers=2),
        args=(self, saving_filepath))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 40:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag664')" href="javascript:;">
keras-2.6.0/keras/distribute/multi_worker_callback_tf2_test.py: 238-273
</a>
<div class="mid" id="frag664" style="display:none"><pre>
  def test_tensorboard_saves_on_chief_but_not_otherwise(self, mode):

    def proc_tensorboard_saves_on_chief_but_not_otherwise(test_obj):
      model, _, train_ds, steps = _model_setup(test_obj, file_format='')
      num_epoch = 2

      # Incorporate type/index information and thread id in saving_filepath to
      # ensure every worker has a unique path. Note that in normal use case the
      # saving_filepath will be the same for all workers, but we use different
      # ones here just to test out chief saves summaries but non-chief doesn't.
      task_config = get_tf_config_task()
      saving_filepath = os.path.join(
          test_obj.get_temp_dir(),
          'logfile_%s_%d' % (task_config['type'], task_config['index']))

      # The saving_filepath shouldn't exist at the beginning (as it's unique).
      test_obj.assertFalse(tf.io.gfile.exists(saving_filepath))

      model.fit(
          x=train_ds,
          epochs=num_epoch,
          steps_per_epoch=steps,
          callbacks=[callbacks.TensorBoard(log_dir=saving_filepath)])

      # If it's chief, the summaries should be saved in the filepath; if not,
      # the directory should be empty (although created). Using
      # `file_io.list_directory()` since the directory may be created at this
      # point.
      test_obj.assertEqual(
          bool(tf.io.gfile.listdir(saving_filepath)), is_chief())

    tf.__internal__.distribute.multi_process_runner.run(
        proc_tensorboard_saves_on_chief_but_not_otherwise,
        cluster_spec=tf.__internal__.distribute.multi_process_runner.create_cluster_spec(num_workers=2),
        args=(self,))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag666')" href="javascript:;">
keras-2.6.0/keras/distribute/multi_worker_callback_tf2_test.py: 275-303
</a>
<div class="mid" id="frag666" style="display:none"><pre>
  def test_tensorboard_can_still_save_to_temp_even_if_it_exists(self, mode):

    def proc_tensorboard_can_still_save_to_temp_even_if_it_exists(test_obj):
      model, _, train_ds, steps = _model_setup(test_obj, file_format='')
      num_epoch = 2

      saving_filepath = os.path.join(
          test_obj.get_temp_dir(),
          'logfile_%s' % (get_tf_config_task()['type']))

      saving_filepath_for_temp = os.path.join(saving_filepath, 'workertemp_1')
      os.mkdir(saving_filepath)
      os.mkdir(saving_filepath_for_temp)

      # Verifies that even if `saving_filepath_for_temp` exists, tensorboard
      # can still save to temporary directory.
      test_obj.assertTrue(tf.io.gfile.exists(saving_filepath_for_temp))

      model.fit(
          x=train_ds,
          epochs=num_epoch,
          steps_per_epoch=steps,
          callbacks=[callbacks.TensorBoard(log_dir=saving_filepath)])

    tf.__internal__.distribute.multi_process_runner.run(
        proc_tensorboard_can_still_save_to_temp_even_if_it_exists,
        cluster_spec=tf.__internal__.distribute.multi_process_runner.create_cluster_spec(num_workers=2),
        args=(self,))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 41:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag712')" href="javascript:;">
keras-2.6.0/keras/distribute/distributed_file_utils_test.py: 74-85
</a>
<div class="mid" id="frag712" style="display:none"><pre>
  def testChiefDoesNotRemoveDirAndFilePath(self):
    temp_dir = self.get_temp_dir()
    strategy = DistributedFileUtilsTest.MockedChiefStrategy()
    dir_to_write = distributed_file_utils.write_dirpath(temp_dir, strategy)
    file_to_write = os.path.join(dir_to_write, 'tmp')
    self.assertFalse(os.path.exists(file_to_write))
    self._write_dummy_file(file_to_write)
    self.assertTrue(os.path.exists(file_to_write))
    distributed_file_utils.remove_temp_dir_with_filepath(
        file_to_write, strategy)
    self.assertTrue(os.path.exists(file_to_write))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag713')" href="javascript:;">
keras-2.6.0/keras/distribute/distributed_file_utils_test.py: 86-97
</a>
<div class="mid" id="frag713" style="display:none"><pre>
  def testWorkerDoesRemoveFilePath(self):
    temp_dir = self.get_temp_dir()
    strategy = DistributedFileUtilsTest.MockedWorkerStrategy()
    dir_to_write = distributed_file_utils.write_dirpath(temp_dir, strategy)
    file_to_write = os.path.join(dir_to_write, 'tmp')
    self.assertFalse(os.path.exists(file_to_write))
    self._write_dummy_file(file_to_write)
    self.assertTrue(os.path.exists(file_to_write))
    distributed_file_utils.remove_temp_dir_with_filepath(
        file_to_write, strategy)
    self.assertFalse(os.path.exists(file_to_write))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag714')" href="javascript:;">
keras-2.6.0/keras/distribute/distributed_file_utils_test.py: 98-109
</a>
<div class="mid" id="frag714" style="display:none"><pre>
  def testWorkerDoesRemoveDirPath(self):
    temp_dir = self.get_temp_dir()
    strategy = DistributedFileUtilsTest.MockedWorkerStrategy()
    dir_to_write = distributed_file_utils.write_dirpath(temp_dir, strategy)
    file_to_write = os.path.join(dir_to_write, 'tmp')
    self.assertFalse(os.path.exists(file_to_write))
    self._write_dummy_file(file_to_write)
    self.assertTrue(os.path.exists(file_to_write))
    distributed_file_utils.remove_temp_dirpath(temp_dir, strategy)
    self.assertFalse(os.path.exists(file_to_write))
    self.assertFalse(os.path.exists(os.path.dirname(file_to_write)))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 42:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag755')" href="javascript:;">
keras-2.6.0/keras/distribute/dataset_creator_model_fit_test_base.py: 46-56
</a>
<div class="mid" id="frag755" style="display:none"><pre>
      def dataset_fn(input_context):
        del input_context
        lookup_layer = string_lookup.StringLookup(
            num_oov_indices=1, vocabulary=filepath)
        x = np.array([["earth", "wind", "and", "fire"],
                      ["fire", "and", "earth", "michigan"]])
        y = np.array([0, 1])
        map_fn = lambda x, y: (lookup_layer(x), y)
        return tf.data.Dataset.from_tensor_slices(
            (x, y)).shuffle(10).repeat().batch(2).map(map_fn)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1636')" href="javascript:;">
keras-2.6.0/keras/integration_test/parameter_server_keras_preprocessing_test.py: 271-281
</a>
<div class="mid" id="frag1636" style="display:none"><pre>
      def dataset_fn(input_context):
        del input_context
        lookup_layer = tf.keras.layers.StringLookup(
            num_oov_indices=1, vocabulary=filepath)
        x = np.array([["earth", "wind", "and", "fire"],
                      ["fire", "and", "earth", "michigan"]])
        y = np.array([0, 1])
        map_fn = lambda x, y: (lookup_layer(x), y)
        return tf.data.Dataset.from_tensor_slices(
            (x, y)).shuffle(10).repeat().batch(2).map(map_fn)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 43:</b> &nbsp; 3 fragments, nominal size 25 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag765')" href="javascript:;">
keras-2.6.0/keras/applications/resnet_v2.py: 30-58
</a>
<div class="mid" id="frag765" style="display:none"><pre>
def ResNet50V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'):
  """Instantiates the ResNet50V2 architecture."""
  def stack_fn(x):
    x = resnet.stack2(x, 64, 3, name='conv2')
    x = resnet.stack2(x, 128, 4, name='conv3')
    x = resnet.stack2(x, 256, 6, name='conv4')
    return resnet.stack2(x, 512, 3, stride1=1, name='conv5')

  return resnet.ResNet(
      stack_fn,
      True,
      True,
      'resnet50v2',
      include_top,
      weights,
      input_tensor,
      input_shape,
      pooling,
      classes,
      classifier_activation=classifier_activation)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag769')" href="javascript:;">
keras-2.6.0/keras/applications/resnet_v2.py: 92-120
</a>
<div class="mid" id="frag769" style="display:none"><pre>
def ResNet152V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'):
  """Instantiates the ResNet152V2 architecture."""
  def stack_fn(x):
    x = resnet.stack2(x, 64, 3, name='conv2')
    x = resnet.stack2(x, 128, 8, name='conv3')
    x = resnet.stack2(x, 256, 36, name='conv4')
    return resnet.stack2(x, 512, 3, stride1=1, name='conv5')

  return resnet.ResNet(
      stack_fn,
      True,
      True,
      'resnet152v2',
      include_top,
      weights,
      input_tensor,
      input_shape,
      pooling,
      classes,
      classifier_activation=classifier_activation)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag767')" href="javascript:;">
keras-2.6.0/keras/applications/resnet_v2.py: 61-89
</a>
<div class="mid" id="frag767" style="display:none"><pre>
def ResNet101V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'):
  """Instantiates the ResNet101V2 architecture."""
  def stack_fn(x):
    x = resnet.stack2(x, 64, 3, name='conv2')
    x = resnet.stack2(x, 128, 4, name='conv3')
    x = resnet.stack2(x, 256, 23, name='conv4')
    return resnet.stack2(x, 512, 3, stride1=1, name='conv5')

  return resnet.ResNet(
      stack_fn,
      True,
      True,
      'resnet101v2',
      include_top,
      weights,
      input_tensor,
      input_shape,
      pooling,
      classes,
      classifier_activation=classifier_activation)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 44:</b> &nbsp; 4 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag802')" href="javascript:;">
keras-2.6.0/keras/regularizers.py: 367-379
</a>
<div class="mid" id="frag802" style="display:none"><pre>
def get(identifier):
  """Retrieve a regularizer instance from a config or identifier."""
  if identifier is None:
    return None
  if isinstance(identifier, dict):
    return deserialize(identifier)
  elif isinstance(identifier, str):
    return deserialize(str(identifier))
  elif callable(identifier):
    return identifier
  else:
    raise ValueError(
        'Could not interpret regularizer identifier: {}'.format(identifier))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5370')" href="javascript:;">
keras-2.6.0/keras/activations.py: 564-604
</a>
<div class="mid" id="frag5370" style="display:none"><pre>
def get(identifier):
  """Returns function.

  Args:
      identifier: Function or string

  Returns:
      Function corresponding to the input string or input function.

  For example:

  &gt;&gt;&gt; tf.keras.activations.get('softmax')
   &lt;function softmax at 0x1222a3d90&gt;
  &gt;&gt;&gt; tf.keras.activations.get(tf.keras.activations.softmax)
   &lt;function softmax at 0x1222a3d90&gt;
  &gt;&gt;&gt; tf.keras.activations.get(None)
   &lt;function linear at 0x1239596a8&gt;
  &gt;&gt;&gt; tf.keras.activations.get(abs)
   &lt;built-in function abs&gt;
  &gt;&gt;&gt; tf.keras.activations.get('abcd')
  Traceback (most recent call last):
  ...
  ValueError: Unknown activation function:abcd

  Raises:
      ValueError: Input is an unknown function or string, i.e., the input does
      not denote any defined function.
  """
  if identifier is None:
    return linear
  if isinstance(identifier, str):
    identifier = str(identifier)
    return deserialize(identifier)
  elif isinstance(identifier, dict):
    return deserialize(identifier)
  elif callable(identifier):
    return identifier
  else:
    raise TypeError(
        'Could not interpret activation function identifier: {}'.format(
            identifier))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag829')" href="javascript:;">
keras-2.6.0/keras/constraints.py: 334-346
</a>
<div class="mid" id="frag829" style="display:none"><pre>
def get(identifier):
  if identifier is None:
    return None
  if isinstance(identifier, dict):
    return deserialize(identifier)
  elif isinstance(identifier, str):
    config = {'class_name': str(identifier), 'config': {}}
    return deserialize(config)
  elif callable(identifier):
    return identifier
  else:
    raise ValueError('Could not interpret constraint identifier: ' +
                     str(identifier))
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2195')" href="javascript:;">
keras-2.6.0/keras/initializers/__init__.py: 147-193
</a>
<div class="mid" id="frag2195" style="display:none"><pre>
def get(identifier):
  """Retrieve a Keras initializer by the identifier.

  The `identifier` may be the string name of a initializers function or class (
  case-sensitively).

  &gt;&gt;&gt; identifier = 'Ones'
  &gt;&gt;&gt; tf.keras.initializers.deserialize(identifier)
  &lt;...keras.initializers.initializers_v2.Ones...&gt;

  You can also specify `config` of the initializer to this function by passing
  dict containing `class_name` and `config` as an identifier. Also note that the
  `class_name` must map to a `Initializer` class.

  &gt;&gt;&gt; cfg = {'class_name': 'Ones', 'config': {}}
  &gt;&gt;&gt; tf.keras.initializers.deserialize(cfg)
  &lt;...keras.initializers.initializers_v2.Ones...&gt;

  In the case that the `identifier` is a class, this method will return a new
  instance of the class by its constructor.

  Args:
    identifier: String or dict that contains the initializer name or
      configurations.

  Returns:
    Initializer instance base on the input identifier.

  Raises:
    ValueError: If the input identifier is not a supported type or in a bad
      format.
  """

  if identifier is None:
    return None
  if isinstance(identifier, dict):
    return deserialize(identifier)
  elif isinstance(identifier, str):
    identifier = str(identifier)
    return deserialize(identifier)
  elif callable(identifier):
    if inspect.isclass(identifier):
      identifier = identifier()
    return identifier
  else:
    raise ValueError('Could not interpret initializer identifier: ' +
                     str(identifier))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 45:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag803')" href="javascript:;">
keras-2.6.0/keras/estimator/__init__.py: 32-180
</a>
<div class="mid" id="frag803" style="display:none"><pre>
def model_to_estimator(
    keras_model=None,
    keras_model_path=None,
    custom_objects=None,
    model_dir=None,
    config=None,
    checkpoint_format='saver',
    metric_names_map=None,
    export_outputs=None):
  """Constructs an `Estimator` instance from given keras model.

  If you use infrastructure or other tooling that relies on Estimators, you can
  still build a Keras model and use model_to_estimator to convert the Keras
  model to an Estimator for use with downstream systems.

  For usage example, please see:
  [Creating estimators from Keras Models](
    https://www.tensorflow.org/guide/estimators#creating_estimators_from_keras_models).

  Sample Weights:
  Estimators returned by `model_to_estimator` are configured so that they can
  handle sample weights (similar to `keras_model.fit(x, y, sample_weights)`).

  To pass sample weights when training or evaluating the Estimator, the first
  item returned by the input function should be a dictionary with keys
  `features` and `sample_weights`. Example below:

  ```python
  keras_model = tf.keras.Model(...)
  keras_model.compile(...)

  estimator = tf.keras.estimator.model_to_estimator(keras_model)

  def input_fn():
    return dataset_ops.Dataset.from_tensors(
        ({'features': features, 'sample_weights': sample_weights},
         targets))

  estimator.train(input_fn, steps=1)
  ```

  Example with customized export signature:
  ```python
  inputs = {'a': tf.keras.Input(..., name='a'),
            'b': tf.keras.Input(..., name='b')}
  outputs = {'c': tf.keras.layers.Dense(..., name='c')(inputs['a']),
             'd': tf.keras.layers.Dense(..., name='d')(inputs['b'])}
  keras_model = tf.keras.Model(inputs, outputs)
  keras_model.compile(...)
  export_outputs = {'c': tf.estimator.export.RegressionOutput,
                    'd': tf.estimator.export.ClassificationOutput}

  estimator = tf.keras.estimator.model_to_estimator(
      keras_model, export_outputs=export_outputs)

  def input_fn():
    return dataset_ops.Dataset.from_tensors(
        ({'features': features, 'sample_weights': sample_weights},
         targets))

  estimator.train(input_fn, steps=1)
  ```

  Args:
    keras_model: A compiled Keras model object. This argument is mutually
      exclusive with `keras_model_path`. Estimator's `model_fn` uses the
      structure of the model to clone the model. Defaults to `None`.
    keras_model_path: Path to a compiled Keras model saved on disk, in HDF5
      format, which can be generated with the `save()` method of a Keras model.
      This argument is mutually exclusive with `keras_model`.
      Defaults to `None`.
    custom_objects: Dictionary for cloning customized objects. This is
      used with classes that is not part of this pip package. For example, if
      user maintains a `relu6` class that inherits from `tf.keras.layers.Layer`,
      then pass `custom_objects={'relu6': relu6}`. Defaults to `None`.
    model_dir: Directory to save `Estimator` model parameters, graph, summary
      files for TensorBoard, etc. If unset a directory will be created with
      `tempfile.mkdtemp`
    config: `RunConfig` to config `Estimator`. Allows setting up things in
      `model_fn` based on configuration such as `num_ps_replicas`, or
      `model_dir`. Defaults to `None`. If both `config.model_dir` and the
      `model_dir` argument (above) are specified the `model_dir` **argument**
      takes precedence.
    checkpoint_format: Sets the format of the checkpoint saved by the estimator
      when training. May be `saver` or `checkpoint`, depending on whether to
      save checkpoints from `tf.train.Saver` or `tf.train.Checkpoint`. This
      argument currently defaults to `saver`. When 2.0 is released, the default
      will be `checkpoint`. Estimators use name-based `tf.train.Saver`
      checkpoints, while Keras models use object-based checkpoints from
      `tf.train.Checkpoint`. Currently, saving object-based checkpoints from
      `model_to_estimator` is only supported by Functional and Sequential
      models. Defaults to 'saver'.
    metric_names_map: Optional dictionary mapping Keras model output metric
      names to custom names. This can be used to override the default Keras
      model output metrics names in a multi IO model use case and provide custom
      names for the `eval_metric_ops` in Estimator.
      The Keras model metric names can be obtained using `model.metrics_names`
      excluding any loss metrics such as total loss and output losses.
      For example, if your Keras model has two outputs `out_1` and `out_2`,
      with `mse` loss and `acc` metric, then `model.metrics_names` will be
      `['loss', 'out_1_loss', 'out_2_loss', 'out_1_acc', 'out_2_acc']`.
      The model metric names excluding the loss metrics will be
      `['out_1_acc', 'out_2_acc']`.
    export_outputs: Optional dictionary. This can be used to override the
      default Keras model output exports in a multi IO model use case and
      provide custom names for the `export_outputs` in
      `tf.estimator.EstimatorSpec`. Default is None, which is equivalent to
      {'serving_default': `tf.estimator.export.PredictOutput`}. If not None,
      the keys must match the keys of `model.output_names`.
      A dict `{name: output}` where:
        * name: An arbitrary name for this output.
        * output: an `ExportOutput` class such as `ClassificationOutput`,
          `RegressionOutput`, or `PredictOutput`. Single-headed models only need
          to specify one entry in this dictionary. Multi-headed models should
          specify one entry for each head, one of which must be named using
          `tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`
          If no entry is provided, a default `PredictOutput` mapping to
          `predictions` will be created.

  Returns:
    An Estimator from given keras model.

  Raises:
    ValueError: If neither keras_model nor keras_model_path was given.
    ValueError: If both keras_model and keras_model_path was given.
    ValueError: If the keras_model_path is a GCS URI.
    ValueError: If keras_model has not been compiled.
    ValueError: If an invalid checkpoint_format was given.
  """

  try:
    from tensorflow_estimator.python.estimator import keras_lib  # pylint: disable=g-import-not-at-top
  except ImportError:
    raise NotImplementedError(
        'tf.keras.estimator.model_to_estimator function not available in your '
        'installation.')
  _model_to_estimator_usage_gauge.get_cell('v1').set(True)
  return keras_lib.model_to_estimator(  # pylint:disable=unexpected-keyword-arg
      keras_model=keras_model,
      keras_model_path=keras_model_path,
      custom_objects=custom_objects,
      model_dir=model_dir,
      config=config,
      checkpoint_format=checkpoint_format,
      use_v2_estimator=False,
      metric_names_map=metric_names_map,
      export_outputs=export_outputs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag804')" href="javascript:;">
keras-2.6.0/keras/estimator/__init__.py: 182-367
</a>
<div class="mid" id="frag804" style="display:none"><pre>
def model_to_estimator_v2(keras_model=None,
                          keras_model_path=None,
                          custom_objects=None,
                          model_dir=None,
                          config=None,
                          checkpoint_format='checkpoint',
                          metric_names_map=None,
                          export_outputs=None):
  """Constructs an `Estimator` instance from given keras model.

  If you use infrastructure or other tooling that relies on Estimators, you can
  still build a Keras model and use model_to_estimator to convert the Keras
  model to an Estimator for use with downstream systems.

  For usage example, please see:
  [Creating estimators from Keras Models](
    https://www.tensorflow.org/guide/estimators#creating_estimators_from_keras_models).

  Sample Weights:
  Estimators returned by `model_to_estimator` are configured so that they can
  handle sample weights (similar to `keras_model.fit(x, y, sample_weights)`).

  To pass sample weights when training or evaluating the Estimator, the first
  item returned by the input function should be a dictionary with keys
  `features` and `sample_weights`. Example below:

  ```python
  keras_model = tf.keras.Model(...)
  keras_model.compile(...)

  estimator = tf.keras.estimator.model_to_estimator(keras_model)

  def input_fn():
    return dataset_ops.Dataset.from_tensors(
        ({'features': features, 'sample_weights': sample_weights},
         targets))

  estimator.train(input_fn, steps=1)
  ```

  Example with customized export signature:
  ```python
  inputs = {'a': tf.keras.Input(..., name='a'),
            'b': tf.keras.Input(..., name='b')}
  outputs = {'c': tf.keras.layers.Dense(..., name='c')(inputs['a']),
             'd': tf.keras.layers.Dense(..., name='d')(inputs['b'])}
  keras_model = tf.keras.Model(inputs, outputs)
  keras_model.compile(...)
  export_outputs = {'c': tf.estimator.export.RegressionOutput,
                    'd': tf.estimator.export.ClassificationOutput}

  estimator = tf.keras.estimator.model_to_estimator(
      keras_model, export_outputs=export_outputs)

  def input_fn():
    return dataset_ops.Dataset.from_tensors(
        ({'features': features, 'sample_weights': sample_weights},
         targets))

  estimator.train(input_fn, steps=1)
  ```

  Note: We do not support creating weighted metrics in Keras and converting them
  to weighted metrics in the Estimator API using `model_to_estimator`.
  You will have to create these metrics directly on the estimator spec using the
  `add_metrics` function.

  To customize the estimator `eval_metric_ops` names, you can pass in the
  `metric_names_map` dictionary mapping the keras model output metric names
  to the custom names as follows:

  ```python
    input_a = tf.keras.layers.Input(shape=(16,), name='input_a')
    input_b = tf.keras.layers.Input(shape=(16,), name='input_b')
    dense = tf.keras.layers.Dense(8, name='dense_1')
    interm_a = dense(input_a)
    interm_b = dense(input_b)
    merged = tf.keras.layers.concatenate([interm_a, interm_b], name='merge')
    output_a = tf.keras.layers.Dense(3, activation='softmax', name='dense_2')(
            merged)
    output_b = tf.keras.layers.Dense(2, activation='softmax', name='dense_3')(
            merged)
    keras_model = tf.keras.models.Model(
        inputs=[input_a, input_b], outputs=[output_a, output_b])
    keras_model.compile(
        loss='categorical_crossentropy',
        optimizer='rmsprop',
        metrics={
            'dense_2': 'categorical_accuracy',
            'dense_3': 'categorical_accuracy'
        })

    metric_names_map = {
        'dense_2_categorical_accuracy': 'acc_1',
        'dense_3_categorical_accuracy': 'acc_2',
    }
    keras_est = tf.keras.estimator.model_to_estimator(
        keras_model=keras_model,
        config=config,
        metric_names_map=metric_names_map)
  ```

  Args:
    keras_model: A compiled Keras model object. This argument is mutually
      exclusive with `keras_model_path`. Estimator's `model_fn` uses the
      structure of the model to clone the model. Defaults to `None`.
    keras_model_path: Path to a compiled Keras model saved on disk, in HDF5
      format, which can be generated with the `save()` method of a Keras model.
      This argument is mutually exclusive with `keras_model`.
      Defaults to `None`.
    custom_objects: Dictionary for cloning customized objects. This is
      used with classes that is not part of this pip package. For example, if
      user maintains a `relu6` class that inherits from `tf.keras.layers.Layer`,
      then pass `custom_objects={'relu6': relu6}`. Defaults to `None`.
    model_dir: Directory to save `Estimator` model parameters, graph, summary
      files for TensorBoard, etc. If unset a directory will be created with
      `tempfile.mkdtemp`
    config: `RunConfig` to config `Estimator`. Allows setting up things in
      `model_fn` based on configuration such as `num_ps_replicas`, or
      `model_dir`. Defaults to `None`. If both `config.model_dir` and the
      `model_dir` argument (above) are specified the `model_dir` **argument**
      takes precedence.
    checkpoint_format: Sets the format of the checkpoint saved by the estimator
      when training. May be `saver` or `checkpoint`, depending on whether to
      save checkpoints from `tf.compat.v1.train.Saver` or `tf.train.Checkpoint`.
      The default is `checkpoint`. Estimators use name-based `tf.train.Saver`
      checkpoints, while Keras models use object-based checkpoints from
      `tf.train.Checkpoint`. Currently, saving object-based checkpoints from
      `model_to_estimator` is only supported by Functional and Sequential
      models. Defaults to 'checkpoint'.
    metric_names_map: Optional dictionary mapping Keras model output metric
      names to custom names. This can be used to override the default Keras
      model output metrics names in a multi IO model use case and provide custom
      names for the `eval_metric_ops` in Estimator.
      The Keras model metric names can be obtained using `model.metrics_names`
      excluding any loss metrics such as total loss and output losses.
      For example, if your Keras model has two outputs `out_1` and `out_2`,
      with `mse` loss and `acc` metric, then `model.metrics_names` will be
      `['loss', 'out_1_loss', 'out_2_loss', 'out_1_acc', 'out_2_acc']`.
      The model metric names excluding the loss metrics will be
      `['out_1_acc', 'out_2_acc']`.
    export_outputs: Optional dictionary. This can be used to override the
      default Keras model output exports in a multi IO model use case and
      provide custom names for the `export_outputs` in
      `tf.estimator.EstimatorSpec`. Default is None, which is equivalent to
      {'serving_default': `tf.estimator.export.PredictOutput`}. If not None,
      the keys must match the keys of `model.output_names`.
      A dict `{name: output}` where:
        * name: An arbitrary name for this output.
        * output: an `ExportOutput` class such as `ClassificationOutput`,
          `RegressionOutput`, or `PredictOutput`. Single-headed models only need
          to specify one entry in this dictionary. Multi-headed models should
          specify one entry for each head, one of which must be named using
          `tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`
          If no entry is provided, a default `PredictOutput` mapping to
          `predictions` will be created.

  Returns:
    An Estimator from given keras model.

  Raises:
    ValueError: If neither keras_model nor keras_model_path was given.
    ValueError: If both keras_model and keras_model_path was given.
    ValueError: If the keras_model_path is a GCS URI.
    ValueError: If keras_model has not been compiled.
    ValueError: If an invalid checkpoint_format was given.
  """

  try:
    from tensorflow_estimator.python.estimator import keras_lib  # pylint: disable=g-import-not-at-top
  except ImportError:
    raise NotImplementedError(
        'tf.keras.estimator.model_to_estimator function not available in your '
        'installation.')
  _model_to_estimator_usage_gauge.get_cell('v2').set(True)
  return keras_lib.model_to_estimator(  # pylint:disable=unexpected-keyword-arg
      keras_model=keras_model,
      keras_model_path=keras_model_path,
      custom_objects=custom_objects,
      model_dir=model_dir,
      config=config,
      checkpoint_format=checkpoint_format,
      use_v2_estimator=True,
      metric_names_map=metric_names_map,
      export_outputs=export_outputs)
# LINT.ThenChange(//tensorflow_estimator/python/estimator/keras_lib.py)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 46:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag830')" href="javascript:;">
keras-2.6.0/keras/wrappers/scikit_learn_test.py: 34-46
</a>
<div class="mid" id="frag830" style="display:none"><pre>
def build_fn_clf(hidden_dim):
  model = keras.models.Sequential()
  model.add(keras.layers.Dense(INPUT_DIM, input_shape=(INPUT_DIM,)))
  model.add(keras.layers.Activation('relu'))
  model.add(keras.layers.Dense(hidden_dim))
  model.add(keras.layers.Activation('relu'))
  model.add(keras.layers.Dense(NUM_CLASSES))
  model.add(keras.layers.Activation('softmax'))
  model.compile(
      optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])
  return model


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag832')" href="javascript:;">
keras-2.6.0/keras/wrappers/scikit_learn_test.py: 70-82
</a>
<div class="mid" id="frag832" style="display:none"><pre>
def build_fn_reg(hidden_dim):
  model = keras.models.Sequential()
  model.add(keras.layers.Dense(INPUT_DIM, input_shape=(INPUT_DIM,)))
  model.add(keras.layers.Activation('relu'))
  model.add(keras.layers.Dense(hidden_dim))
  model.add(keras.layers.Activation('relu'))
  model.add(keras.layers.Dense(1))
  model.add(keras.layers.Activation('linear'))
  model.compile(
      optimizer='sgd', loss='mean_absolute_error', metrics=['accuracy'])
  return model


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 47:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag831')" href="javascript:;">
keras-2.6.0/keras/wrappers/scikit_learn_test.py: 47-69
</a>
<div class="mid" id="frag831" style="display:none"><pre>
def assert_classification_works(clf):
  np.random.seed(42)
  (x_train, y_train), (x_test, _) = testing_utils.get_test_data(
      train_samples=TRAIN_SAMPLES,
      test_samples=TEST_SAMPLES,
      input_shape=(INPUT_DIM,),
      num_classes=NUM_CLASSES)

  clf.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS)

  score = clf.score(x_train, y_train, batch_size=BATCH_SIZE)
  assert np.isscalar(score) and np.isfinite(score)

  preds = clf.predict(x_test, batch_size=BATCH_SIZE)
  assert preds.shape == (TEST_SAMPLES,)
  for prediction in np.unique(preds):
    assert prediction in range(NUM_CLASSES)

  proba = clf.predict_proba(x_test, batch_size=BATCH_SIZE)
  assert proba.shape == (TEST_SAMPLES, NUM_CLASSES)
  assert np.allclose(np.sum(proba, axis=1), np.ones(TEST_SAMPLES))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag833')" href="javascript:;">
keras-2.6.0/keras/wrappers/scikit_learn_test.py: 83-99
</a>
<div class="mid" id="frag833" style="display:none"><pre>
def assert_regression_works(reg):
  np.random.seed(42)
  (x_train, y_train), (x_test, _) = testing_utils.get_test_data(
      train_samples=TRAIN_SAMPLES,
      test_samples=TEST_SAMPLES,
      input_shape=(INPUT_DIM,),
      num_classes=NUM_CLASSES)

  reg.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS)

  score = reg.score(x_train, y_train, batch_size=BATCH_SIZE)
  assert np.isscalar(score) and np.isfinite(score)

  preds = reg.predict(x_test, batch_size=BATCH_SIZE)
  assert preds.shape == (TEST_SAMPLES,)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 48:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag835')" href="javascript:;">
keras-2.6.0/keras/wrappers/scikit_learn_test.py: 112-127
</a>
<div class="mid" id="frag835" style="display:none"><pre>
  def test_classify_class_build_fn(self):

    class ClassBuildFnClf(object):

      def __call__(self, hidden_dim):
        return build_fn_clf(hidden_dim)

    with self.cached_session():
      clf = scikit_learn.KerasClassifier(
          build_fn=ClassBuildFnClf(),
          hidden_dim=HIDDEN_DIM,
          batch_size=BATCH_SIZE,
          epochs=EPOCHS)

      assert_classification_works(clf)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag840')" href="javascript:;">
keras-2.6.0/keras/wrappers/scikit_learn_test.py: 154-169
</a>
<div class="mid" id="frag840" style="display:none"><pre>
  def test_regression_class_build_fn(self):

    class ClassBuildFnReg(object):

      def __call__(self, hidden_dim):
        return build_fn_reg(hidden_dim)

    with self.cached_session():
      reg = scikit_learn.KerasRegressor(
          build_fn=ClassBuildFnReg(),
          hidden_dim=HIDDEN_DIM,
          batch_size=BATCH_SIZE,
          epochs=EPOCHS)

      assert_regression_works(reg)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag842')" href="javascript:;">
keras-2.6.0/keras/wrappers/scikit_learn_test.py: 170-186
</a>
<div class="mid" id="frag842" style="display:none"><pre>
  def test_regression_inherit_class_build_fn(self):

    class InheritClassBuildFnReg(scikit_learn.KerasRegressor):

      def __call__(self, hidden_dim):
        return build_fn_reg(hidden_dim)

    with self.cached_session():
      reg = InheritClassBuildFnReg(
          build_fn=None,
          hidden_dim=HIDDEN_DIM,
          batch_size=BATCH_SIZE,
          epochs=EPOCHS)

      assert_regression_works(reg)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag837')" href="javascript:;">
keras-2.6.0/keras/wrappers/scikit_learn_test.py: 128-143
</a>
<div class="mid" id="frag837" style="display:none"><pre>
  def test_classify_inherit_class_build_fn(self):

    class InheritClassBuildFnClf(scikit_learn.KerasClassifier):

      def __call__(self, hidden_dim):
        return build_fn_clf(hidden_dim)

    with self.cached_session():
      clf = InheritClassBuildFnClf(
          build_fn=None,
          hidden_dim=HIDDEN_DIM,
          batch_size=BATCH_SIZE,
          epochs=EPOCHS)

      assert_classification_works(clf)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 49:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag865')" href="javascript:;">
keras-2.6.0/keras/models_test.py: 280-291
</a>
<div class="mid" id="frag865" style="display:none"><pre>
  def test_functional_cloning_does_not_create_unnecessary_placeholders(self):
    with tf.Graph().as_default():
      x = keras.Input((4,))
      y = keras.layers.Dense(4)(x)
      model = keras.models.Model(x, y)
    graph = tf.Graph()
    with graph.as_default():
      x = tf.ones((10, 4))
      _ = keras.models.clone_model(model, input_tensors=[x])
      has_placeholder = _has_placeholder(graph)
      self.assertFalse(has_placeholder)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag866')" href="javascript:;">
keras-2.6.0/keras/models_test.py: 292-302
</a>
<div class="mid" id="frag866" style="display:none"><pre>
  def test_sequential_cloning_does_not_create_unnecessary_placeholders(self):
    with tf.Graph().as_default():
      model = keras.models.Sequential()
      model.add(keras.layers.Dense(4, input_shape=(4,)))
    graph = tf.Graph()
    with graph.as_default():
      x = tf.ones((10, 4))
      _ = keras.models.clone_model(model, input_tensors=[x])
      has_placeholder = _has_placeholder(graph)
      self.assertFalse(has_placeholder)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 50:</b> &nbsp; 5 fragments, nominal size 10 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1232')" href="javascript:;">
keras-2.6.0/keras/utils/data_utils_test.py: 168-180
</a>
<div class="mid" id="frag1232" style="display:none"><pre>
  def test_generator_enqueuer_threads(self):
    enqueuer = keras.utils.data_utils.GeneratorEnqueuer(
        create_generator_from_sequence_threads(TestSequence([3, 200, 200, 3])),
        use_multiprocessing=False)
    enqueuer.start(3, 10)
    gen_output = enqueuer.get()
    acc = []
    for _ in range(100):
      acc.append(int(next(gen_output)[0, 0, 0, 0]))

    self.assertEqual(len(set(acc) - set(range(100))), 0)
    enqueuer.stop()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1233')" href="javascript:;">
keras-2.6.0/keras/utils/data_utils_test.py: 182-193
</a>
<div class="mid" id="frag1233" style="display:none"><pre>
  def test_generator_enqueuer_processes(self):
    enqueuer = keras.utils.data_utils.GeneratorEnqueuer(
        create_generator_from_sequence_threads(TestSequence([3, 200, 200, 3])),
        use_multiprocessing=True)
    enqueuer.start(4, 10)
    gen_output = enqueuer.get()
    acc = []
    for _ in range(300):
      acc.append(int(next(gen_output)[0, 0, 0, 0]))
    self.assertNotEqual(acc, list(range(100)))
    enqueuer.stop()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1237')" href="javascript:;">
keras-2.6.0/keras/utils/data_utils_test.py: 225-235
</a>
<div class="mid" id="frag1237" style="display:none"><pre>
  def test_ordered_enqueuer_processes(self):
    enqueuer = keras.utils.data_utils.OrderedEnqueuer(
        TestSequence([3, 200, 200, 3]), use_multiprocessing=True)
    enqueuer.start(3, 10)
    gen_output = enqueuer.get()
    acc = []
    for _ in range(100):
      acc.append(next(gen_output)[0, 0, 0, 0])
    self.assertEqual(acc, list(range(100)))
    enqueuer.stop()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1236')" href="javascript:;">
keras-2.6.0/keras/utils/data_utils_test.py: 213-223
</a>
<div class="mid" id="frag1236" style="display:none"><pre>
  def test_ordered_enqueuer_threads(self):
    enqueuer = keras.utils.data_utils.OrderedEnqueuer(
        TestSequence([3, 200, 200, 3]), use_multiprocessing=False)
    enqueuer.start(3, 10)
    gen_output = enqueuer.get()
    acc = []
    for _ in range(100):
      acc.append(next(gen_output)[0, 0, 0, 0])
    self.assertEqual(acc, list(range(100)))
    enqueuer.stop()

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1240')" href="javascript:;">
keras-2.6.0/keras/utils/data_utils_test.py: 254-265
</a>
<div class="mid" id="frag1240" style="display:none"><pre>
  def test_on_epoch_end_processes(self):
    enqueuer = keras.utils.data_utils.OrderedEnqueuer(
        TestSequence([3, 200, 200, 3]), use_multiprocessing=True)
    enqueuer.start(3, 10)
    gen_output = enqueuer.get()
    acc = []
    for _ in range(200):
      acc.append(next(gen_output)[0, 0, 0, 0])
    # Check that order was keep in GeneratorEnqueuer with processes
    self.assertEqual(acc[100:], list([k * 5 for k in range(100)]))
    enqueuer.stop()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 51:</b> &nbsp; 4 fragments, nominal size 15 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1275')" href="javascript:;">
keras-2.6.0/keras/utils/conv_utils_test.py: 163-179
</a>
<div class="mid" id="frag1275" style="display:none"><pre>
  def test_conv_kernel_mask_fc(self, *input_shape):
    padding = 'valid'
    kernel_shape = input_shape
    ndims = len(input_shape)
    strides = (1,) * ndims
    output_shape = _get_const_output_shape(input_shape, dim=1)
    mask = np.ones(input_shape + output_shape, np.bool)
    self.assertAllEqual(
        mask,
        conv_utils.conv_kernel_mask(
            input_shape,
            kernel_shape,
            strides,
            padding
        )
    )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1277')" href="javascript:;">
keras-2.6.0/keras/utils/conv_utils_test.py: 198-218
</a>
<div class="mid" id="frag1277" style="display:none"><pre>
  def test_conv_kernel_mask_full_stride(self, *input_shape):
    padding = 'valid'
    ndims = len(input_shape)
    kernel_shape = (1,) * ndims
    strides = tuple([max(d, 1) for d in input_shape])
    output_shape = _get_const_output_shape(input_shape, dim=1)

    mask = np.zeros(input_shape + output_shape, np.bool)
    if all(d &gt; 0 for d in mask.shape):  # pylint: disable=not-an-iterable
      mask[(0,) * len(output_shape)] = True

    self.assertAllEqual(
        mask,
        conv_utils.conv_kernel_mask(
            input_shape,
            kernel_shape,
            strides,
            padding
        )
    )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1278')" href="javascript:;">
keras-2.6.0/keras/utils/conv_utils_test.py: 219-241
</a>
<div class="mid" id="frag1278" style="display:none"><pre>
  def test_conv_kernel_mask_almost_full_stride(self, *input_shape):
    padding = 'valid'
    ndims = len(input_shape)
    kernel_shape = (1,) * ndims
    strides = tuple([max(d - 1, 1) for d in input_shape])
    output_shape = _get_const_output_shape(input_shape, dim=2)

    mask = np.zeros(input_shape + output_shape, np.bool)
    if all(d &gt; 0 for d in mask.shape):  # pylint: disable=not-an-iterable
      for in_position in itertools.product(*[[0, d - 1] for d in input_shape]):
        out_position = tuple([min(p, 1) for p in in_position])
        mask[in_position + out_position] = True

    self.assertAllEqual(
        mask,
        conv_utils.conv_kernel_mask(
            input_shape,
            kernel_shape,
            strides,
            padding
        )
    )

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1276')" href="javascript:;">
keras-2.6.0/keras/utils/conv_utils_test.py: 180-197
</a>
<div class="mid" id="frag1276" style="display:none"><pre>
  def test_conv_kernel_mask_diag(self, *input_shape):
    ndims = len(input_shape)
    kernel_shape = (1,) * ndims
    strides = (1,) * ndims

    for padding in ['valid', 'same']:
      mask = np.identity(int(np.prod(input_shape)), np.bool)
      mask = np.reshape(mask, input_shape * 2)
      self.assertAllEqual(
          mask,
          conv_utils.conv_kernel_mask(
              input_shape,
              kernel_shape,
              strides,
              padding
          )
      )

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 52:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1336')" href="javascript:;">
keras-2.6.0/keras/utils/kpl_test_utils.py: 77-109
</a>
<div class="mid" id="frag1336" style="display:none"><pre>
  def dataset_fn(self, feature_mapper, label_mapper):
    """Function that generates dataset for test of tf.distribute + KPL.

    Args:
      feature_mapper: a simple keras model with one keras StringLookup layer
        which maps feature to index.
      label_mapper: similar to feature_mapper, but maps label to index.

    Returns:
      Generated dataset for test of tf.distribute + KPL.

    """

    def feature_and_label_gen():
      # Generator of dataset.
      while True:
        features = random.sample(self.FEATURE_VOCAB, 3)
        label = ["yes"] if self.FEATURE_VOCAB[0] in features else ["no"]
        yield {"features": features, "label": label}

    raw_dataset = tf.data.Dataset.from_generator(
        feature_and_label_gen,
        output_signature={
            "features": tf.TensorSpec([3], tf.string),
            "label": tf.TensorSpec([1], tf.string)
        }).shuffle(100).batch(32)

    train_dataset = raw_dataset.map(lambda x: (  # pylint: disable=g-long-lambda
        {
            "features": feature_mapper(x["features"])
        }, label_mapper(x["label"])))
    return train_dataset

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1627')" href="javascript:;">
keras-2.6.0/keras/integration_test/parameter_server_keras_preprocessing_test.py: 141-163
</a>
<div class="mid" id="frag1627" style="display:none"><pre>
      def dataset_fn():

        def feature_and_label_gen():
          while True:
            features = random.sample(FEATURE_VOCAB, 3)
            label = ["yes"] if "avenger" in features else ["no"]
            yield {"features": features, "label": label}

        # The dataset will be created on the coordinator.
        raw_dataset = tf.data.Dataset.from_generator(
            feature_and_label_gen,
            output_signature={
                "features": tf.TensorSpec([3], tf.string),
                "label": tf.TensorSpec([1], tf.string)
            }).shuffle(100).batch(32)

        train_dataset = raw_dataset.map(lambda x: (  # pylint: disable=g-long-lambda
            {
                "features": feature_ps(x["features"])
            }, label_ps(x["label"])))
        return train_dataset

      # Create the model. The input needs to be compatible with KPLs.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1615')" href="javascript:;">
keras-2.6.0/keras/integration_test/tpu_strategy_test.py: 123-145
</a>
<div class="mid" id="frag1615" style="display:none"><pre>
      def dataset_fn(_):

        def feature_and_label_gen():
          # Generator of dataset.
          while True:
            features = random.sample(FEATURE_VOCAB, 3)
            label = ["yes"] if "avenger" in features else ["no"]
            yield {"features": features, "label": label}

        raw_dataset = tf.data.Dataset.from_generator(
            feature_and_label_gen,
            output_signature={
                "features": tf.TensorSpec([3], tf.dtypes.string),
                "label": tf.TensorSpec([1], tf.dtypes.string)
            }).shuffle(100).batch(32)

        train_dataset = raw_dataset.map(lambda x: (  # pylint: disable=g-long-lambda
            {
                "features": feature_mapper(x["features"])
            }, label_mapper(x["label"])))
        return train_dataset

      # Create the model. The input needs to be compatible with KPLs.
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 53:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1399')" href="javascript:;">
keras-2.6.0/keras/utils/tf_utils_test.py: 34-53
</a>
<div class="mid" id="frag1399" style="display:none"><pre>
  def test_default_behavior(self):
    if tf.executing_eagerly():
      self.assertFalse(tf_utils.is_symbolic_tensor(
          tf.Variable(name='blah', initial_value=0.)))
      self.assertFalse(
          tf_utils.is_symbolic_tensor(
              tf.convert_to_tensor(0.)))
      self.assertFalse(tf_utils.is_symbolic_tensor(
          tf.SparseTensor(
              indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])))
    else:
      self.assertTrue(tf_utils.is_symbolic_tensor(
          tf.Variable(name='blah', initial_value=0.)))
      self.assertTrue(
          tf_utils.is_symbolic_tensor(
              tf.convert_to_tensor(0.)))
      self.assertTrue(tf_utils.is_symbolic_tensor(
          tf.SparseTensor(
              indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1400')" href="javascript:;">
keras-2.6.0/keras/utils/tf_utils_test.py: 54-86
</a>
<div class="mid" id="frag1400" style="display:none"><pre>
  def test_works_with_registered(self):

    class CustomClass(object):

      def value(self):
        return tf.convert_to_tensor(42.)

    tf.register_tensor_conversion_function(
        CustomClass, lambda value, **_: value.value())

    tf_utils.register_symbolic_tensor_type(CustomClass)

    if tf.executing_eagerly():
      self.assertFalse(tf_utils.is_symbolic_tensor(
          tf.Variable(name='blah', initial_value=0.)))
      self.assertFalse(
          tf_utils.is_symbolic_tensor(
              tf.convert_to_tensor(0.)))
      self.assertFalse(tf_utils.is_symbolic_tensor(
          tf.SparseTensor(
              indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])))
      self.assertFalse(tf_utils.is_symbolic_tensor(CustomClass()))
    else:
      self.assertTrue(tf_utils.is_symbolic_tensor(
          tf.Variable(name='blah', initial_value=0.)))
      self.assertTrue(
          tf_utils.is_symbolic_tensor(
              tf.convert_to_tensor(0.)))
      self.assertTrue(tf_utils.is_symbolic_tensor(
          tf.SparseTensor(
              indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])))
      self.assertTrue(tf_utils.is_symbolic_tensor(CustomClass()))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 54:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1557')" href="javascript:;">
keras-2.6.0/keras/utils/composite_tensor_support_test.py: 407-437
</a>
<div class="mid" id="frag1557" style="display:none"><pre>
  def test_sparse_scipy_predict_input_dicts_via_input_layer_args(self):
    # Create a model that accepts a sparse input and converts the sparse tensor
    # back to a dense tensor. Scipy sparse matrices are limited to 2D, so use
    # a one-dimensional shape; note also that scipy's default dtype is int64.
    if testing_utils.get_model_type() == "subclass":
      input_name = "input_1"  # Subclass models don"t support input names.
    else:
      input_name = "test_input_name"
    model_input = input_layer.Input(
        shape=(3,), sparse=True, name=input_name, dtype=tf.int64)
    layers = [ToDense(default_value=-1)]
    model = get_model_from_layers_with_input(layers, model_input=model_input)

    input_data = {
        input_name:
            scipy.sparse.coo_matrix(([1, 2, 3], ([0, 1, 1], [0, 0, 1])),
                                    shape=[2, 3])
    }
    expected_output = np.array([[1, -1, -1], [2, 3, -1]])
    output = model.predict(input_data, steps=1)
    self.assertAllEqual(expected_output, output)

    input_data_2 = {
        input_name:
            scipy.sparse.coo_matrix(
                ([5, 6, 7, 8], ([0, 1, 1, 2], [0, 0, 1, 1])), shape=[3, 3])
    }
    expected_output_2 = np.array([[5, -1, -1], [6, 7, -1], [-1, 8, -1]])
    output_2 = model.predict(input_data_2, steps=1)
    self.assertAllEqual(expected_output_2, output_2)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1558')" href="javascript:;">
keras-2.6.0/keras/utils/composite_tensor_support_test.py: 438-473
</a>
<div class="mid" id="frag1558" style="display:none"><pre>
  def test_sparse_scipy_eval_input_dicts(self):
    # Create a model that accepts a sparse input and converts the sparse tensor
    # back to a dense tensor. Scipy sparse matrices are limited to 2D, so use
    # a one-dimensional shape; note also that scipy's default dtype is int64.
    if testing_utils.get_model_type() == "subclass":
      input_name = "input_1"  # Subclass models don"t support input names.
    else:
      input_name = "test_input_name"
    model_input = input_layer.Input(
        shape=(3,), sparse=True, name=input_name, dtype=tf.int64)
    layers = [ToDense(default_value=-1)]
    model = get_model_from_layers_with_input(layers, model_input=model_input)
    model.compile(
        optimizer="sgd",
        loss="mse",
        metrics=["accuracy"])

    input_data = {
        input_name:
            scipy.sparse.coo_matrix(([1, 2, 3], ([0, 1, 1], [0, 0, 1])),
                                    shape=[2, 3])
    }
    expected_output = np.array([[1, -1, -1], [2, 3, -1]])
    output = model.evaluate(input_data, expected_output, steps=1)
    self.assertAllEqual(1.0, output[-1])

    input_data_2 = {
        input_name:
            scipy.sparse.coo_matrix(
                ([5, 6, 7, 8], ([0, 1, 1, 2], [0, 0, 1, 1])), shape=[3, 3])
    }
    expected_output_2 = np.array([[5, -1, -1], [6, 7, -1], [-1, 8, -1]])
    output_2 = model.evaluate(input_data_2, expected_output_2, steps=1)
    self.assertAllEqual(1.0, output_2[-1])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 55:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1560')" href="javascript:;">
keras-2.6.0/keras/utils/composite_tensor_support_test.py: 529-557
</a>
<div class="mid" id="frag1560" style="display:none"><pre>
  def test_ragged_tensor_input_with_one_none_dimension(self, use_dict,
                                                       use_dataset):
    # Define some input data.
    data = [(tf.ragged.constant([[[1, 0]], [[2, 3]]], ragged_rank=1),
             np.array([[[1, 0]], [[2, 3]]]))]

    # Prepare the model to test.
    input_shape = (None, 2)  # RaggedTensorInputTest uses (None, None).
    input_name = get_input_name(use_dict)
    model_input = input_layer.Input(
        shape=input_shape, ragged=True, name=input_name, dtype=tf.int32)
    layers = [ToDense(default_value=-1)]
    model = get_model_from_layers_with_input(layers, model_input=model_input)
    model.compile(
        optimizer="sgd",
        loss="mse",
        metrics=["accuracy"],
        **get_test_mode_kwargs())

    for data_element in data:
      input_data, expected_output = prepare_inputs(
          data_element,
          use_dict,
          use_dataset,
          action="predict",
          input_name=input_name)
      result = model.predict(input_data)
      self.assertAllEqual(expected_output, result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1561')" href="javascript:;">
keras-2.6.0/keras/utils/composite_tensor_support_test.py: 558-588
</a>
<div class="mid" id="frag1561" style="display:none"><pre>
  def test_ragged_tensor_input_with_no_none_dimension(self, use_dict,
                                                      use_dataset):
    # Define some input data.
    data = [(tf.ragged.constant([[[1, 0]], [[2, 3]]], ragged_rank=0),
             np.array([[[1, 0]], [[2, 3]]]))]

    # Prepare the model to test.
    input_shape = (1, 2)  # RaggedTensorInputTest uses (None, None).
    input_name = get_input_name(use_dict)
    model_input = input_layer.Input(
        shape=input_shape, ragged=True, name=input_name, dtype=tf.int32)
    layers = [ToDense(default_value=-1)]
    model = get_model_from_layers_with_input(layers, model_input=model_input)
    model.compile(
        optimizer="sgd",
        loss="mse",
        metrics=["accuracy"],
        **get_test_mode_kwargs())
    kwargs = get_kwargs(use_dataset)

    for data_element in data:
      input_data, expected_output = prepare_inputs(
          data_element,
          use_dict,
          use_dataset,
          action="predict",
          input_name=input_name)
      result = model.predict(input_data, **kwargs)
      self.assertAllEqual(expected_output, result)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 56:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1579')" href="javascript:;">
keras-2.6.0/keras/keras_parameterized.py: 42-153
</a>
<div class="mid" id="frag1579" style="display:none"><pre>
def run_with_all_saved_model_formats(
    test_or_class=None,
    exclude_formats=None):
  """Execute the decorated test with all Keras saved model formats).

  This decorator is intended to be applied either to individual test methods in
  a `keras_parameterized.TestCase` class, or directly to a test class that
  extends it. Doing so will cause the contents of the individual test
  method (or all test methods in the class) to be executed multiple times - once
  for each Keras saved model format.

  The Keras saved model formats include:
  1. HDF5: 'h5'
  2. SavedModel: 'tf'

  Note: if stacking this decorator with absl.testing's parameterized decorators,
  those should be at the bottom of the stack.

  Various methods in `testing_utils` to get file path for saved models will
  auto-generate a string of the two saved model formats. This allows unittests
  to confirm the equivalence between the two Keras saved model formats.

  For example, consider the following unittest:

  ```python
  class MyTests(testing_utils.KerasTestCase):

    @testing_utils.run_with_all_saved_model_formats
    def test_foo(self):
      save_format = testing_utils.get_save_format()
      saved_model_dir = '/tmp/saved_model/'
      model = keras.models.Sequential()
      model.add(keras.layers.Dense(2, input_shape=(3,)))
      model.add(keras.layers.Dense(3))
      model.compile(loss='mse', optimizer='sgd', metrics=['acc'])

      keras.models.save_model(model, saved_model_dir, save_format=save_format)
      model = keras.models.load_model(saved_model_dir)

  if __name__ == "__main__":
    tf.test.main()
  ```

  This test tries to save the model into the formats of 'hdf5', 'h5', 'keras',
  'tensorflow', and 'tf'.

  We can also annotate the whole class if we want this to apply to all tests in
  the class:
  ```python
  @testing_utils.run_with_all_saved_model_formats
  class MyTests(testing_utils.KerasTestCase):

    def test_foo(self):
      save_format = testing_utils.get_save_format()
      saved_model_dir = '/tmp/saved_model/'
      model = keras.models.Sequential()
      model.add(keras.layers.Dense(2, input_shape=(3,)))
      model.add(keras.layers.Dense(3))
      model.compile(loss='mse', optimizer='sgd', metrics=['acc'])

      keras.models.save_model(model, saved_model_dir, save_format=save_format)
      model = tf.keras.models.load_model(saved_model_dir)

  if __name__ == "__main__":
    tf.test.main()
  ```

  Args:
    test_or_class: test method or class to be annotated. If None,
      this method returns a decorator that can be applied to a test method or
      test class. If it is not None this returns the decorator applied to the
      test or class.
    exclude_formats: A collection of Keras saved model formats to not run.
      (May also be a single format not wrapped in a collection).
      Defaults to None.

  Returns:
    Returns a decorator that will run the decorated test method multiple times:
    once for each desired Keras saved model format.

  Raises:
    ImportError: If abseil parameterized is not installed or not included as
      a target dependency.
  """
  # Exclude h5 save format if H5py isn't available.
  if h5py is None:
    exclude_formats.append(['h5'])
  saved_model_formats = ['h5', 'tf', 'tf_no_traces']
  params = [('_%s' % saved_format, saved_format)
            for saved_format in saved_model_formats
            if saved_format not in tf.nest.flatten(exclude_formats)]

  def single_method_decorator(f):
    """Decorator that constructs the test cases."""
    # Use named_parameters so it can be individually run from the command line
    @parameterized.named_parameters(*params)
    @functools.wraps(f)
    def decorated(self, saved_format, *args, **kwargs):
      """A run of a single test case w/ the specified model type."""
      if saved_format == 'h5':
        _test_h5_saved_model_format(f, self, *args, **kwargs)
      elif saved_format == 'tf':
        _test_tf_saved_model_format(f, self, *args, **kwargs)
      elif saved_format == 'tf_no_traces':
        _test_tf_saved_model_format_no_traces(f, self, *args, **kwargs)
      else:
        raise ValueError('Unknown model type: %s' % (saved_format,))
    return decorated

  return _test_or_class_decorator(test_or_class, single_method_decorator)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1586')" href="javascript:;">
keras-2.6.0/keras/keras_parameterized.py: 178-293
</a>
<div class="mid" id="frag1586" style="display:none"><pre>
def run_with_all_model_types(
    test_or_class=None,
    exclude_models=None):
  """Execute the decorated test with all Keras model types.

  This decorator is intended to be applied either to individual test methods in
  a `keras_parameterized.TestCase` class, or directly to a test class that
  extends it. Doing so will cause the contents of the individual test
  method (or all test methods in the class) to be executed multiple times - once
  for each Keras model type.

  The Keras model types are: ['functional', 'subclass', 'sequential']

  Note: if stacking this decorator with absl.testing's parameterized decorators,
  those should be at the bottom of the stack.

  Various methods in `testing_utils` to get models will auto-generate a model
  of the currently active Keras model type. This allows unittests to confirm
  the equivalence between different Keras models.

  For example, consider the following unittest:

  ```python
  class MyTests(testing_utils.KerasTestCase):

    @testing_utils.run_with_all_model_types(
      exclude_models = ['sequential'])
    def test_foo(self):
      model = testing_utils.get_small_mlp(1, 4, input_dim=3)
      optimizer = RMSPropOptimizer(learning_rate=0.001)
      loss = 'mse'
      metrics = ['mae']
      model.compile(optimizer, loss, metrics=metrics)

      inputs = np.zeros((10, 3))
      targets = np.zeros((10, 4))
      dataset = dataset_ops.Dataset.from_tensor_slices((inputs, targets))
      dataset = dataset.repeat(100)
      dataset = dataset.batch(10)

      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)

  if __name__ == "__main__":
    tf.test.main()
  ```

  This test tries building a small mlp as both a functional model and as a
  subclass model.

  We can also annotate the whole class if we want this to apply to all tests in
  the class:
  ```python
  @testing_utils.run_with_all_model_types(exclude_models = ['sequential'])
  class MyTests(testing_utils.KerasTestCase):

    def test_foo(self):
      model = testing_utils.get_small_mlp(1, 4, input_dim=3)
      optimizer = RMSPropOptimizer(learning_rate=0.001)
      loss = 'mse'
      metrics = ['mae']
      model.compile(optimizer, loss, metrics=metrics)

      inputs = np.zeros((10, 3))
      targets = np.zeros((10, 4))
      dataset = dataset_ops.Dataset.from_tensor_slices((inputs, targets))
      dataset = dataset.repeat(100)
      dataset = dataset.batch(10)

      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)

  if __name__ == "__main__":
    tf.test.main()
  ```


  Args:
    test_or_class: test method or class to be annotated. If None,
      this method returns a decorator that can be applied to a test method or
      test class. If it is not None this returns the decorator applied to the
      test or class.
    exclude_models: A collection of Keras model types to not run.
      (May also be a single model type not wrapped in a collection).
      Defaults to None.

  Returns:
    Returns a decorator that will run the decorated test method multiple times:
    once for each desired Keras model type.

  Raises:
    ImportError: If abseil parameterized is not installed or not included as
      a target dependency.
  """
  model_types = ['functional', 'subclass', 'sequential']
  params = [('_%s' % model, model) for model in model_types
            if model not in tf.nest.flatten(exclude_models)]

  def single_method_decorator(f):
    """Decorator that constructs the test cases."""
    # Use named_parameters so it can be individually run from the command line
    @parameterized.named_parameters(*params)
    @functools.wraps(f)
    def decorated(self, model_type, *args, **kwargs):
      """A run of a single test case w/ the specified model type."""
      if model_type == 'functional':
        _test_functional_model_type(f, self, *args, **kwargs)
      elif model_type == 'subclass':
        _test_subclass_model_type(f, self, *args, **kwargs)
      elif model_type == 'sequential':
        _test_sequential_model_type(f, self, *args, **kwargs)
      else:
        raise ValueError('Unknown model type: %s' % (model_type,))
    return decorated

  return _test_or_class_decorator(test_or_class, single_method_decorator)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 57:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1580')" href="javascript:;">
keras-2.6.0/keras/keras_parameterized.py: 134-150
</a>
<div class="mid" id="frag1580" style="display:none"><pre>
  def single_method_decorator(f):
    """Decorator that constructs the test cases."""
    # Use named_parameters so it can be individually run from the command line
    @parameterized.named_parameters(*params)
    @functools.wraps(f)
    def decorated(self, saved_format, *args, **kwargs):
      """A run of a single test case w/ the specified model type."""
      if saved_format == 'h5':
        _test_h5_saved_model_format(f, self, *args, **kwargs)
      elif saved_format == 'tf':
        _test_tf_saved_model_format(f, self, *args, **kwargs)
      elif saved_format == 'tf_no_traces':
        _test_tf_saved_model_format_no_traces(f, self, *args, **kwargs)
      else:
        raise ValueError('Unknown model type: %s' % (saved_format,))
    return decorated

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1593')" href="javascript:;">
keras-2.6.0/keras/keras_parameterized.py: 391-409
</a>
<div class="mid" id="frag1593" style="display:none"><pre>
  def single_method_decorator(f):
    """Decorator that constructs the test cases."""

    # Use named_parameters so it can be individually run from the command line
    @parameterized.named_parameters(*params)
    @functools.wraps(f)
    def decorated(self, run_mode, *args, **kwargs):
      """A run of a single test case w/ specified run mode."""
      if run_mode == 'v1_session':
        _v1_session_test(f, self, config, *args, **kwargs)
      elif run_mode == 'v2_eager':
        _v2_eager_test(f, self, *args, **kwargs)
      elif run_mode == 'v2_function':
        _v2_function_test(f, self, *args, **kwargs)
      else:
        return ValueError('Unknown run mode %s' % run_mode)

    return decorated

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1587')" href="javascript:;">
keras-2.6.0/keras/keras_parameterized.py: 274-290
</a>
<div class="mid" id="frag1587" style="display:none"><pre>
  def single_method_decorator(f):
    """Decorator that constructs the test cases."""
    # Use named_parameters so it can be individually run from the command line
    @parameterized.named_parameters(*params)
    @functools.wraps(f)
    def decorated(self, model_type, *args, **kwargs):
      """A run of a single test case w/ the specified model type."""
      if model_type == 'functional':
        _test_functional_model_type(f, self, *args, **kwargs)
      elif model_type == 'subclass':
        _test_subclass_model_type(f, self, *args, **kwargs)
      elif model_type == 'sequential':
        _test_sequential_model_type(f, self, *args, **kwargs)
      else:
        raise ValueError('Unknown model type: %s' % (model_type,))
    return decorated

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 58:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1604')" href="javascript:;">
keras-2.6.0/keras/integration_test/preprocessing_applied_in_model_test.py: 49-70
</a>
<div class="mid" id="frag1604" style="display:none"><pre>
  def testDistributedModelFit(self, strategy):
    with strategy.scope():
      preprocessing_model = utils.make_preprocessing_model(self.get_temp_dir())
      training_model = utils.make_training_model()
      # Merge the two separate models into a single model for training.
      inputs = preprocessing_model.inputs
      outputs = training_model(preprocessing_model(inputs))
      merged_model = tf.keras.Model(inputs, outputs)
      merged_model.compile(optimizer="sgd", loss="binary_crossentropy")

    def dataset_fn(input_context):
      dataset = utils.make_dataset()
      dataset = dataset.shard(input_context.num_input_pipelines,
                              input_context.input_pipeline_id)
      batch_size = input_context.get_per_replica_batch_size(
          global_batch_size=utils.BATCH_SIZE)
      return dataset.batch(batch_size).repeat().prefetch(2)

    dataset_creator = tf.keras.utils.experimental.DatasetCreator(dataset_fn)
    merged_model.fit(dataset_creator, epochs=2, steps_per_epoch=utils.STEPS)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1690')" href="javascript:;">
keras-2.6.0/keras/integration_test/preprocessing_applied_in_dataset_creator_test.py: 48-66
</a>
<div class="mid" id="frag1690" style="display:none"><pre>
  def testDistributedModelFit(self, strategy):
    with strategy.scope():
      preprocessing_model = utils.make_preprocessing_model(self.get_temp_dir())
      training_model = utils.make_training_model()
      training_model.compile(optimizer="sgd", loss="binary_crossentropy")

    def dataset_fn(input_context):
      dataset = utils.make_dataset()
      dataset = dataset.shard(input_context.num_input_pipelines,
                              input_context.input_pipeline_id)
      batch_size = input_context.get_per_replica_batch_size(
          global_batch_size=utils.BATCH_SIZE)
      dataset = dataset.batch(batch_size).repeat().prefetch(2)
      return dataset.map(lambda x, y: (preprocessing_model(x), y))

    dataset_creator = tf.keras.utils.experimental.DatasetCreator(dataset_fn)
    training_model.fit(dataset_creator, epochs=2, steps_per_epoch=utils.STEPS)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 59:</b> &nbsp; 2 fragments, nominal size 28 lines, similarity 82%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1610')" href="javascript:;">
keras-2.6.0/keras/integration_test/tpu_strategy_test.py: 56-86
</a>
<div class="mid" id="frag1610" style="display:none"><pre>
  def define_kpls_for_training(self, use_adapt):
    if use_adapt:
      feature_lookup_layer = (
          tf.keras.layers.StringLookup(
              num_oov_indices=1))
      feature_lookup_layer.adapt(FEATURE_VOCAB)
      label_lookup_layer = (
          tf.keras.layers.StringLookup(
              num_oov_indices=0, mask_token=None))
      label_lookup_layer.adapt(LABEL_VOCAB)
    else:
      feature_lookup_layer = (
          tf.keras.layers.StringLookup(
              vocabulary=FEATURE_VOCAB, num_oov_indices=1))
      label_lookup_layer = (
          tf.keras.layers.StringLookup(
              vocabulary=LABEL_VOCAB, num_oov_indices=0, mask_token=None))

    raw_feature_input = tf.keras.layers.Input(
        shape=(3,), dtype=tf.dtypes.string, name="feature", ragged=True)
    feature_id_input = feature_lookup_layer(raw_feature_input)
    feature_mapper = tf.keras.Model({"features": raw_feature_input},
                                    feature_id_input)

    raw_label_input = tf.keras.layers.Input(
        shape=(1,), dtype=tf.dtypes.string, name="label")
    label_id_input = label_lookup_layer(raw_label_input)
    label_mapper = tf.keras.Model({"label": raw_label_input}, label_id_input)

    return feature_mapper, label_mapper

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1624')" href="javascript:;">
keras-2.6.0/keras/integration_test/parameter_server_keras_preprocessing_test.py: 83-121
</a>
<div class="mid" id="frag1624" style="display:none"><pre>
  def define_kpls_for_training(self, use_adapt):
    # Define KPLs under strategy's scope. Right now, if they have look up
    # tables, they will be created on the client. Their variables will be
    # created on PS. Ideally they should be cached on each worker since they
    # will not be changed in a training step.
    if use_adapt:
      feature_lookup_layer = (
          tf.keras.layers.StringLookup(
              num_oov_indices=1))
      feature_lookup_layer.adapt(FEATURE_VOCAB)
      label_lookup_layer = (
          tf.keras.layers.StringLookup(
              num_oov_indices=0, mask_token=None))
      label_lookup_layer.adapt(LABEL_VOCAB)
    else:
      # Do vocab shuffling.
      shuffled_vocab = FEATURE_VOCAB.copy()
      random.shuffle(shuffled_vocab)
      feature_lookup_layer = (
          tf.keras.layers.StringLookup(
              vocabulary=shuffled_vocab, num_oov_indices=1))
      label_lookup_layer = (
          tf.keras.layers.StringLookup(
              vocabulary=LABEL_VOCAB, num_oov_indices=0, mask_token=None))

    raw_feature_input = tf.keras.Input(
        shape=(3,), dtype=tf.string, name="feature", ragged=True)
    feature_id_input = feature_lookup_layer(raw_feature_input)

    # Model creates variables as well.
    feature_ps = tf.keras.Model({"features": raw_feature_input},
                                feature_id_input)

    raw_label_input = tf.keras.Input(shape=(1,), dtype=tf.string, name="label")
    label_id_input = label_lookup_layer(raw_label_input)
    label_ps = tf.keras.Model({"label": raw_label_input}, label_id_input)

    return feature_ps, label_ps

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 60:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1619')" href="javascript:;">
keras-2.6.0/keras/integration_test/tpu_strategy_test.py: 199-214
</a>
<div class="mid" id="frag1619" style="display:none"><pre>
      def create_serving_signature(model):

        @tf.function
        def serve_fn(raw_features):
          raw_features = tf.expand_dims(raw_features, axis=0)
          transformed_features = model.feature_mapper(raw_features)
          outputs = model(transformed_features)
          outputs = tf.squeeze(outputs, axis=0)
          outputs = tf.cast(tf.math.greater(outputs, 0.5), tf.dtypes.int64)
          decoded_outputs = model.label_inverse_lookup_layer(outputs)
          return tf.squeeze(decoded_outputs, axis=0)

        # Serving does NOT have batch dimension
        return serve_fn.get_concrete_function(
            tf.TensorSpec(shape=(3), dtype=tf.dtypes.string, name="example"))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1631')" href="javascript:;">
keras-2.6.0/keras/integration_test/parameter_server_keras_preprocessing_test.py: 213-228
</a>
<div class="mid" id="frag1631" style="display:none"><pre>
    def create_serving_signature(model):

      @tf.function
      def serve_fn(raw_features):
        raw_features = tf.expand_dims(raw_features, axis=0)
        transformed_features = model.feature_ps(raw_features)
        outputs = model(transformed_features)
        outputs = tf.squeeze(outputs, axis=0)
        outputs = tf.cast(tf.greater(outputs, 0.5), tf.int64)
        decoded_outputs = model.label_inverse_lookup_layer(outputs)
        return tf.squeeze(decoded_outputs, axis=0)

      # serving does NOT have batch dimension
      return serve_fn.get_concrete_function(
          tf.TensorSpec(shape=(3), dtype=tf.string, name="example"))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 61:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1622')" href="javascript:;">
keras-2.6.0/keras/integration_test/parameter_server_keras_preprocessing_test.py: 36-68
</a>
<div class="mid" id="frag1622" style="display:none"><pre>
def create_in_process_cluster(num_workers, num_ps):
  """Creates and starts local servers and returns the cluster_resolver."""

  worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]
  ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]

  cluster_dict = {}
  cluster_dict["worker"] = ["localhost:%s" % port for port in worker_ports]
  if num_ps &gt; 0:
    cluster_dict["ps"] = ["localhost:%s" % port for port in ps_ports]

  cluster_spec = tf.train.ClusterSpec(cluster_dict)

  # Workers need some inter_ops threads to work properly.
  worker_config = tf.compat.v1.ConfigProto()
  if multiprocessing.cpu_count() &lt; num_workers + 1:
    worker_config.inter_op_parallelism_threads = num_workers + 1

  for i in range(num_workers):
    tf.distribute.Server(
        cluster_spec,
        job_name="worker",
        task_index=i,
        config=worker_config,
        protocol="grpc")

  for i in range(num_ps):
    tf.distribute.Server(
        cluster_spec, job_name="ps", task_index=i, protocol="grpc")

  return cluster_spec


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1698')" href="javascript:;">
keras-2.6.0/keras/integration_test/parameter_server_custom_training_loop_test.py: 32-62
</a>
<div class="mid" id="frag1698" style="display:none"><pre>
  def create_in_process_cluster(self, num_workers, num_ps):
    """Creates and starts local servers and returns the cluster_resolver."""
    worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]
    ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]

    cluster_dict = {}
    cluster_dict["worker"] = ["localhost:%s" % port for port in worker_ports]
    if num_ps &gt; 0:
      cluster_dict["ps"] = ["localhost:%s" % port for port in ps_ports]

    cluster_spec = tf.train.ClusterSpec(cluster_dict)

    # Workers need some inter_ops threads to work properly.
    worker_config = tf.compat.v1.ConfigProto()
    if multiprocessing.cpu_count() &lt; num_workers + 1:
      worker_config.inter_op_parallelism_threads = num_workers + 1

    for i in range(num_workers):
      tf.distribute.Server(
          cluster_spec,
          job_name="worker",
          task_index=i,
          config=worker_config,
          protocol="grpc")

    for i in range(num_ps):
      tf.distribute.Server(
          cluster_spec, job_name="ps", task_index=i, protocol="grpc")

    return cluster_spec

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 62:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1623')" href="javascript:;">
keras-2.6.0/keras/integration_test/parameter_server_keras_preprocessing_test.py: 71-82
</a>
<div class="mid" id="frag1623" style="display:none"><pre>
  def setUp(self):
    super(KPLTest, self).setUp()

    cluster_spec = create_in_process_cluster(num_workers=3, num_ps=2)
    cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(
        cluster_spec, rpc_layer="grpc")
    self.strategy = tf.distribute.experimental.ParameterServerStrategy(
        cluster_resolver)
    self.coordinator = (
        tf.distribute.experimental.coordinator.ClusterCoordinator(
            self.strategy))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1633')" href="javascript:;">
keras-2.6.0/keras/integration_test/parameter_server_keras_preprocessing_test.py: 251-262
</a>
<div class="mid" id="frag1633" style="display:none"><pre>
  def setUp(self):
    super(KPLCreatedInDatasetsFromFunctionTest, self).setUp()

    cluster_spec = create_in_process_cluster(num_workers=3, num_ps=2)
    cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(
        cluster_spec, rpc_layer="grpc")
    self.strategy = tf.distribute.experimental.ParameterServerStrategy(
        cluster_resolver)
    self.coordinator = (
        tf.distribute.experimental.coordinator.ClusterCoordinator(
            self.strategy))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1699')" href="javascript:;">
keras-2.6.0/keras/integration_test/parameter_server_custom_training_loop_test.py: 63-74
</a>
<div class="mid" id="frag1699" style="display:none"><pre>
  def setUp(self):
    super(ParameterServerCustomTrainingLoopTest, self).setUp()

    cluster_spec = self.create_in_process_cluster(num_workers=3, num_ps=2)
    cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(
        cluster_spec, rpc_layer="grpc")
    self.strategy = tf.distribute.experimental.ParameterServerStrategy(
        cluster_resolver)
    self.coordinator = (
        tf.distribute.experimental.coordinator.ClusterCoordinator(
            self.strategy))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 63:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1724')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 72-95
</a>
<div class="mid" id="frag1724" style="display:none"><pre>
  def test_categorical_crossentropy_loss(self):
    target = backend.variable(np.random.randint(0, 1, (5, 1)))
    logits = backend.variable(np.random.random((5, 1)))
    softmax_output = backend.softmax(logits)
    output_from_logit = losses.categorical_crossentropy(
        target, logits, from_logits=True)
    output_from_softmax = losses.categorical_crossentropy(
        target, softmax_output)
    np.testing.assert_allclose(
        backend.eval(output_from_logit),
        backend.eval(output_from_softmax),
        atol=1e-5)

    axis = 0
    output_from_logit_axis = losses.categorical_crossentropy(
        target, logits, from_logits=True, axis=axis)
    output_from_softmax_axis = losses.categorical_crossentropy(
        target, softmax_output, axis=axis)

    np.testing.assert_allclose(
        backend.eval(output_from_logit_axis),
        backend.eval(output_from_softmax_axis),
        atol=1e-5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1728')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 162-184
</a>
<div class="mid" id="frag1728" style="display:none"><pre>
  def test_binary_crossentropy_loss(self):
    target = backend.variable(np.random.randint(0, 1, (5, 1)))
    logits = backend.variable(np.random.random((5, 1)))
    sigmoid_output = backend.sigmoid(logits)
    output_from_logit = losses.binary_crossentropy(
        target, logits, from_logits=True)
    output_from_sigmoid = losses.binary_crossentropy(target, sigmoid_output)
    np.testing.assert_allclose(
        backend.eval(output_from_logit),
        backend.eval(output_from_sigmoid),
        atol=1e-5)

    axis = 0
    output_from_logit_axis = losses.binary_crossentropy(
        target, logits, from_logits=True, axis=axis)
    output_from_sigmoid_axis = losses.binary_crossentropy(
        target, sigmoid_output, axis=axis)

    np.testing.assert_allclose(
        backend.eval(output_from_logit_axis),
        backend.eval(output_from_sigmoid_axis),
        atol=1e-5)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 64:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1725')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 97-120
</a>
<div class="mid" id="frag1725" style="display:none"><pre>
  def test_categorical_crossentropy_loss_with_unknown_rank_tensor(self):
    t = backend.placeholder()
    p = backend.placeholder()
    o = losses.categorical_crossentropy(t, p)

    t_val = tf.convert_to_tensor([[1., 0., 0.], [0., 1., 0.],
                                                    [0., 0., 1.]])
    p_val = tf.convert_to_tensor([[.9, .05, .05],
                                                    [.05, .89, .06],
                                                    [.05, .01, .94]])
    f = backend.function([t, p], o)

    result = f([t_val, p_val])
    self.assertArrayNear(result, [.105, .116, .062], 1e-3)

    # from logits
    p_val = tf.convert_to_tensor([[8., 1., 1.], [0., 9., 1.],
                                                    [2., 3., 5.]])
    o = losses.categorical_crossentropy(t, p, from_logits=True)
    f = backend.function([t, p], o)

    result = f([t_val, p_val])
    self.assertArrayNear(result, [.002, 0, .17], 1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1727')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 136-160
</a>
<div class="mid" id="frag1727" style="display:none"><pre>
  def test_sparse_categorical_crossentropy_loss_with_unknown_rank_tensor(self):
    # This test only runs in graph because the TF op layer is not supported yet
    # for sparse ops.
    t = backend.placeholder()
    p = backend.placeholder()
    o = losses.sparse_categorical_crossentropy(t, p)

    t_val = tf.convert_to_tensor([0, 1, 2])
    p_val = tf.convert_to_tensor([[.9, .05, .05],
                                                    [.05, .89, .06],
                                                    [.05, .01, .94]])
    f = backend.function([t, p], o)

    result = f([t_val, p_val])
    self.assertArrayNear(result, [.105, .116, .062], 1e-3)

    # from logits
    p_val = tf.convert_to_tensor([[8., 1., 1.], [0., 9., 1.],
                                                    [2., 3., 5.]])
    o = losses.sparse_categorical_crossentropy(t, p, from_logits=True)
    f = backend.function([t, p], o)

    result = f([t_val, p_val])
    self.assertArrayNear(result, [.002, 0, .17], 1e-3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 65:</b> &nbsp; 9 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1792')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 774-788
</a>
<div class="mid" id="frag1792" style="display:none"><pre>
  def test_all_correct_unweighted(self):
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]],
                                  dtype=tf.float32)
    bce_obj = losses.BinaryCrossentropy()
    loss = bce_obj(y_true, y_true)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

    # Test with logits.
    logits = tf.constant([[100.0, -100.0, -100.0],
                                   [-100.0, 100.0, -100.0],
                                   [-100.0, -100.0, 100.0]])
    bce_obj = losses.BinaryCrossentropy(from_logits=True)
    loss = bce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1800')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 969-983
</a>
<div class="mid" id="frag1800" style="display:none"><pre>
  def test_all_correct_unweighted(self):
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]],
                                  dtype=tf.int64)
    y_pred = tf.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],
                                  dtype=tf.float32)
    cce_obj = losses.CategoricalCrossentropy()
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

    # Test with logits.
    logits = tf.constant([[10., 0., 0.], [0., 10., 0.], [0., 0., 10.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1809')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1096-1109
</a>
<div class="mid" id="frag1809" style="display:none"><pre>
  def test_all_correct_unweighted(self):
    y_true = tf.constant([[0], [1], [2]], dtype=tf.int64)
    y_pred = tf.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],
                                  dtype=tf.float32)
    cce_obj = losses.SparseCategoricalCrossentropy()
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

    # Test with logits.
    logits = tf.constant([[10., 0., 0.], [0., 10., 0.], [0., 0., 10.]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1810')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1110-1123
</a>
<div class="mid" id="frag1810" style="display:none"><pre>
  def test_unweighted(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.constant([0, 1, 2])
    y_pred = tf.constant(
        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), .3239, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), .0573, 3)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1801')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 984-997
</a>
<div class="mid" id="frag1801" style="display:none"><pre>
  def test_unweighted(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
    y_pred = tf.constant(
        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)
    loss = cce_obj(y_true, y_pred)
    self.assertAlmostEqual(self.evaluate(loss), .3239, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits)
    self.assertAlmostEqual(self.evaluate(loss), .0573, 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1811')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1124-1137
</a>
<div class="mid" id="frag1811" style="display:none"><pre>
  def test_scalar_weighted(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.constant([[0], [1], [2]])
    y_pred = tf.constant(
        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)
    loss = cce_obj(y_true, y_pred, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .7449, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .1317, 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1802')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 998-1011
</a>
<div class="mid" id="frag1802" style="display:none"><pre>
  def test_scalar_weighted(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
    y_pred = tf.constant(
        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)
    loss = cce_obj(y_true, y_pred, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .7449, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=2.3)
    self.assertAlmostEqual(self.evaluate(loss), .1317, 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1803')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1012-1026
</a>
<div class="mid" id="frag1803" style="display:none"><pre>
  def test_sample_weighted(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
    y_pred = tf.constant(
        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)
    sample_weight = tf.constant([[1.2], [3.4], [5.6]], shape=(3, 1))
    loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 1.0696, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 0.31829, 3)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1812')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1138-1152
</a>
<div class="mid" id="frag1812" style="display:none"><pre>
  def test_sample_weighted(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.constant([[0], [1], [2]])
    y_pred = tf.constant(
        [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]], dtype=tf.float32)
    sample_weight = tf.constant([[1.2], [3.4], [5.6]], shape=(3, 1))
    loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 1.0696, 3)

    # Test with logits.
    logits = tf.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    loss = cce_obj(y_true, logits, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 0.31829, 3)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 66:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1793')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 789-826
</a>
<div class="mid" id="frag1793" style="display:none"><pre>
  def test_unweighted(self):
    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])
    y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])
    bce_obj = losses.BinaryCrossentropy()
    loss = bce_obj(y_true, y_pred)

    # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999
    # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)
    # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]

    # Loss = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))
    #      = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),
    #         -log(Y_MAX + EPSILON), -log(1)]
    #      = [0, 15.33, 0, 0]
    # Reduced loss = 15.33 / 4

    self.assertAlmostEqual(self.evaluate(loss), 3.833, 3)

    # Test with logits.
    y_true = tf.constant([[1, 0, 1], [0, 1, 1]])
    logits = tf.constant([[100.0, -100.0, 100.0],
                                   [100.0, 100.0, -100.0]])
    bce_obj = losses.BinaryCrossentropy(from_logits=True)
    loss = bce_obj(y_true, logits)

    # Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))
    #            (where x = logits and z = y_true)
    #      = [((100 - 100 * 1 + log(1 + exp(-100))) +
    #          (0 + 100 * 0 + log(1 + exp(-100))) +
    #          (100 - 100 * 1 + log(1 + exp(-100))),
    #         ((100 - 100 * 0 + log(1 + exp(-100))) +
    #          (100 - 100 * 1 + log(1 + exp(-100))) +
    #          (0 + 100 * 1 + log(1 + exp(-100))))]
    #      = [(0 + 0 + 0) / 3, 200 / 3]
    # Reduced loss = (0 + 66.666) / 2

    self.assertAlmostEqual(self.evaluate(loss), 33.333, 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1794')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 827-860
</a>
<div class="mid" id="frag1794" style="display:none"><pre>
  def test_scalar_weighted(self):
    bce_obj = losses.BinaryCrossentropy()
    y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])
    y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])
    loss = bce_obj(y_true, y_pred, sample_weight=2.3)

    # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999
    # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)
    # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]

    # Loss = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))
    #      = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),
    #         -log(Y_MAX + EPSILON), -log(1)]
    #      = [0, 15.33, 0, 0]
    # Weighted loss = [0, 15.33 * 2.3, 0, 0]
    # Reduced loss = 15.33 * 2.3 / 4

    self.assertAlmostEqual(self.evaluate(loss), 8.817, 3)

    # Test with logits.
    y_true = tf.constant([[1, 0, 1], [0, 1, 1]])
    logits = tf.constant([[100.0, -100.0, 100.0],
                                   [100.0, 100.0, -100.0]])
    bce_obj = losses.BinaryCrossentropy(from_logits=True)
    loss = bce_obj(y_true, logits, sample_weight=2.3)

    # Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))
    #            (where x = logits and z = y_true)
    # Loss = [(0 + 0 + 0) / 3, 200 / 3]
    # Weighted loss = [0 * 2.3, 66.666 * 2.3]
    # Reduced loss = (0 + 66.666 * 2.3) / 2

    self.assertAlmostEqual(self.evaluate(loss), 76.667, 3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 67:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1807')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1066-1086
</a>
<div class="mid" id="frag1807" style="display:none"><pre>
  def test_ragged_tensors(self):
    cce_obj = losses.CategoricalCrossentropy()
    y_true = tf.ragged.constant([[[1, 0, 0], [0, 1, 0]], [[0, 0, 1]]])
    y_pred = tf.ragged.constant(
        [[[.9, .05, .05], [.5, .89, .6]], [[.05, .01, .94]]],
        dtype=tf.float32)
    # batch losses [[0.1054, 0.8047], [0.0619]]
    sample_weight = tf.constant([[1.2], [3.4]], shape=(2, 1))
    loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)
    # sum([0.1054, 0.8047, 0.0619]) / 3
    self.assertAlmostEqual(self.evaluate(loss), 0.4341, 3)

    # Test with logits.
    logits = tf.ragged.constant([[[8., 1., 1.], [0., 9., 1.]],
                                          [[2., 3., 5.]]])
    cce_obj = losses.CategoricalCrossentropy(from_logits=True)
    # batch losses [[0.0018, 0.0004], [0.1698]]
    loss = cce_obj(y_true, logits, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 0.1934, 3)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1815')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1169-1188
</a>
<div class="mid" id="frag1815" style="display:none"><pre>
  def test_ragged_tensors(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.ragged.constant([[0, 1], [2]])
    y_pred = tf.ragged.constant(
        [[[.9, .05, .05], [.5, .89, .6]], [[.05, .01, .94]]],
        dtype=tf.float32)
    # batch losses [[0.1054, 0.8047], [0.0619]]
    sample_weight = tf.constant([[1.2], [3.4]], shape=(2, 1))
    loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)
    # sum([0.1054, 0.8047, 0.0619]) / 3
    self.assertAlmostEqual(self.evaluate(loss), 0.4341, 3)

    # Test with logits.
    logits = tf.ragged.constant([[[8., 1., 1.], [0., 9., 1.]],
                                          [[2., 3., 5.]]])
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    # batch losses [[0.0018, 0.0004], [0.1698]]
    loss = cce_obj(y_true, logits, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 0.1934, 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1816')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1189-1209
</a>
<div class="mid" id="frag1816" style="display:none"><pre>
  def test_ragged_tensors_rank_1(self):
    cce_obj = losses.SparseCategoricalCrossentropy()
    y_true = tf.ragged.constant([[0, 1], [2]])
    y_pred = tf.ragged.constant(
        [[[.9, .05, .05], [.5, .89, .6]], [[.05, .01, .94]]],
        ragged_rank=1,
        dtype=tf.float32)
    # batch losses [[0.1054, 0.8047], [0.0619]]
    sample_weight = tf.constant([[1.2], [3.4]], shape=(2, 1))
    loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)
    # sum([0.1054, 0.8047, 0.0619]) / 3
    self.assertAlmostEqual(self.evaluate(loss), 0.4341, 3)

    # Test with logits.
    logits = tf.ragged.constant(
        [[[8., 1., 1.], [0., 9., 1.]], [[2., 3., 5.]]], ragged_rank=1)
    cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)
    # batch losses [[0.0018, 0.0004], [0.1698]]
    loss = cce_obj(y_true, logits, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), 0.1934, 3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 68:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1839')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1524-1537
</a>
<div class="mid" id="frag1839" style="display:none"><pre>
  def test_scalar_weighted(self):
    self.setup()
    logcosh_obj = losses.LogCosh()
    sample_weight = 2.3

    loss = logcosh_obj(self.y_true, self.y_pred, sample_weight=sample_weight)
    expected_loss = sample_weight * np.sum(
        self.expected_losses) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

    # Verify we get the same output when the same input is given
    loss_2 = logcosh_obj(self.y_true, self.y_pred, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), self.evaluate(loss_2), 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1846')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1605-1619
</a>
<div class="mid" id="frag1846" style="display:none"><pre>
  def test_scalar_weighted(self):
    self.setup()
    poisson_obj = losses.Poisson()
    sample_weight = 2.3
    loss = poisson_obj(self.y_true, self.y_pred, sample_weight=sample_weight)

    expected_loss = sample_weight * np.sum(
        self.expected_losses) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

    # Verify we get the same output when the same input is given
    loss_2 = poisson_obj(self.y_true, self.y_pred, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), self.evaluate(loss_2), 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1853')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1686-1699
</a>
<div class="mid" id="frag1853" style="display:none"><pre>
  def test_scalar_weighted(self):
    self.setup()
    k_obj = losses.KLDivergence()
    sample_weight = 2.3

    loss = k_obj(self.y_true, self.y_pred, sample_weight=sample_weight)
    expected_loss = sample_weight * np.sum(
        self.expected_losses) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

    # Verify we get the same output when the same input is given
    loss_2 = k_obj(self.y_true, self.y_pred, sample_weight=sample_weight)
    self.assertAlmostEqual(self.evaluate(loss), self.evaluate(loss_2), 3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 69:</b> &nbsp; 4 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1840')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1538-1550
</a>
<div class="mid" id="frag1840" style="display:none"><pre>
  def test_sample_weighted(self):
    self.setup()
    logcosh_obj = losses.LogCosh()

    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))
    loss = logcosh_obj(self.y_true, self.y_pred, sample_weight=sample_weight)

    expected_loss = np.multiply(
        self.expected_losses,
        np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape((2, 3)))
    expected_loss = np.sum(expected_loss) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1863')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1792-1803
</a>
<div class="mid" id="frag1863" style="display:none"><pre>
  def test_sample_weighted(self):
    self.setup()
    h_obj = losses.Huber()
    sample_weight = tf.constant((1.2, 3.4), shape=(2, 1))

    loss = h_obj(self.y_true, self.y_pred, sample_weight=sample_weight)
    actual_loss = np.multiply(
        self.expected_losses,
        np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape((2, 3)))
    actual_loss = np.sum(actual_loss) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), actual_loss, 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1847')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1620-1632
</a>
<div class="mid" id="frag1847" style="display:none"><pre>
  def test_sample_weighted(self):
    self.setup()
    poisson_obj = losses.Poisson()

    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))
    loss = poisson_obj(self.y_true, self.y_pred, sample_weight=sample_weight)

    expected_loss = np.multiply(
        self.expected_losses,
        np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape((2, 3)))
    expected_loss = np.sum(expected_loss) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1854')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1700-1711
</a>
<div class="mid" id="frag1854" style="display:none"><pre>
  def test_sample_weighted(self):
    self.setup()
    k_obj = losses.KLDivergence()
    sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))
    loss = k_obj(self.y_true, self.y_pred, sample_weight=sample_weight)

    expected_loss = np.multiply(
        self.expected_losses,
        np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape(2, 3))
    expected_loss = np.sum(expected_loss) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 70:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1841')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1551-1568
</a>
<div class="mid" id="frag1841" style="display:none"><pre>
  def test_timestep_weighted(self):
    self.setup()
    logcosh_obj = losses.LogCosh()
    y_true = np.asarray([1, 9, 2, -5, -2, 6]).reshape(2, 3, 1)
    y_pred = np.asarray([4, 8, 12, 8, 1, 3]).reshape(2, 3, 1)
    error = y_pred - y_true
    expected_losses = np.log((np.exp(error) + np.exp(-error)) / 2)
    sample_weight = np.array([3, 6, 5, 0, 4, 2]).reshape((2, 3, 1))

    y_pred = tf.constant(y_pred, dtype=tf.float32)
    y_true = tf.constant(y_true)
    loss = logcosh_obj(
        y_true,
        y_pred,
        sample_weight=tf.constant(sample_weight, shape=(2, 3)))
    expected_loss = np.sum(expected_losses * sample_weight) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1848')" href="javascript:;">
keras-2.6.0/keras/losses_test.py: 1633-1650
</a>
<div class="mid" id="frag1848" style="display:none"><pre>
  def test_timestep_weighted(self):
    self.setup()
    poisson_obj = losses.Poisson()
    y_true = self.np_y_true.reshape(2, 3, 1)
    y_pred = self.np_y_pred.reshape(2, 3, 1)
    sample_weight = np.asarray([3, 6, 5, 0, 4, 2]).reshape(2, 3, 1)
    expected_losses = y_pred - np.multiply(y_true, np.log(y_pred))

    y_pred = tf.constant(y_pred, dtype=tf.float32)
    y_true = tf.constant(y_true)

    loss = poisson_obj(
        y_true,
        y_pred,
        sample_weight=tf.constant(sample_weight, shape=(2, 3)))
    expected_loss = np.sum(expected_losses * sample_weight) / self.batch_size
    self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 71:</b> &nbsp; 4 fragments, nominal size 27 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1873')" href="javascript:;">
keras-2.6.0/keras/tests/integration_test.py: 54-81
</a>
<div class="mid" id="frag1873" style="display:none"><pre>
  def test_vector_classification(self):
    np.random.seed(1337)
    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=100,
        test_samples=0,
        input_shape=(10,),
        num_classes=2)
    y_train = np_utils.to_categorical(y_train)

    model = testing_utils.get_model_from_layers(
        [keras.layers.Dense(16, activation='relu'),
         keras.layers.Dropout(0.1),
         keras.layers.Dense(y_train.shape[-1], activation='softmax')],
        input_shape=x_train.shape[1:])
    model.compile(
        loss='categorical_crossentropy',
        optimizer=keras.optimizer_v2.adam.Adam(0.005),
        metrics=['acc'],
        run_eagerly=testing_utils.should_run_eagerly())
    history = model.fit(x_train, y_train, epochs=10, batch_size=10,
                        validation_data=(x_train, y_train),
                        verbose=2)
    self.assertGreater(history.history['val_acc'][-1], 0.7)
    _, val_acc = model.evaluate(x_train, y_train)
    self.assertAlmostEqual(history.history['val_acc'][-1], val_acc)
    predictions = model.predict(x_train)
    self.assertEqual(predictions.shape, (x_train.shape[0], 2))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1877')" href="javascript:;">
keras-2.6.0/keras/tests/integration_test.py: 207-238
</a>
<div class="mid" id="frag1877" style="display:none"><pre>
  def test_timeseries_classification_sequential_tf_rnn(self):
    np.random.seed(1337)
    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=100,
        test_samples=0,
        input_shape=(4, 10),
        num_classes=2)
    y_train = np_utils.to_categorical(y_train)

    with base_layer.keras_style_scope():
      model = keras.models.Sequential()
      model.add(keras.layers.RNN(rnn_cell.LSTMCell(5), return_sequences=True,
                                 input_shape=x_train.shape[1:]))
      model.add(keras.layers.RNN(rnn_cell.GRUCell(y_train.shape[-1],
                                                  activation='softmax',
                                                  dtype=tf.float32)))
      model.compile(
          loss='categorical_crossentropy',
          optimizer=keras.optimizer_v2.adam.Adam(0.005),
          metrics=['acc'],
          run_eagerly=testing_utils.should_run_eagerly())

    history = model.fit(x_train, y_train, epochs=15, batch_size=10,
                        validation_data=(x_train, y_train),
                        verbose=2)
    self.assertGreater(history.history['val_acc'][-1], 0.7)
    _, val_acc = model.evaluate(x_train, y_train)
    self.assertAlmostEqual(history.history['val_acc'][-1], val_acc)
    predictions = model.predict(x_train)
    self.assertEqual(predictions.shape, (x_train.shape[0], 2))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1878')" href="javascript:;">
keras-2.6.0/keras/tests/integration_test.py: 243-276
</a>
<div class="mid" id="frag1878" style="display:none"><pre>
  def test_image_classification(self):
    np.random.seed(1337)
    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=100,
        test_samples=0,
        input_shape=(10, 10, 3),
        num_classes=2)
    y_train = np_utils.to_categorical(y_train)

    layers = [
        keras.layers.Conv2D(4, 3, padding='same', activation='relu'),
        keras.layers.Conv2D(8, 3, padding='same'),
        keras.layers.BatchNormalization(),
        keras.layers.Conv2D(8, 3, padding='same'),
        keras.layers.Flatten(),
        keras.layers.Dense(y_train.shape[-1], activation='softmax')
    ]
    model = testing_utils.get_model_from_layers(
        layers, input_shape=x_train.shape[1:])
    model.compile(
        loss='categorical_crossentropy',
        optimizer=keras.optimizer_v2.adam.Adam(0.005),
        metrics=['acc'],
        run_eagerly=testing_utils.should_run_eagerly())
    history = model.fit(x_train, y_train, epochs=10, batch_size=10,
                        validation_data=(x_train, y_train),
                        verbose=2)
    self.assertGreater(history.history['val_acc'][-1], 0.7)
    _, val_acc = model.evaluate(x_train, y_train)
    self.assertAlmostEqual(history.history['val_acc'][-1], val_acc)
    predictions = model.predict(x_train)
    self.assertEqual(predictions.shape, (x_train.shape[0], 2))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1876')" href="javascript:;">
keras-2.6.0/keras/tests/integration_test.py: 178-206
</a>
<div class="mid" id="frag1876" style="display:none"><pre>
  def test_timeseries_classification(self):
    np.random.seed(1337)
    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=100,
        test_samples=0,
        input_shape=(4, 10),
        num_classes=2)
    y_train = np_utils.to_categorical(y_train)

    layers = [
        keras.layers.LSTM(5, return_sequences=True),
        keras.layers.GRU(y_train.shape[-1], activation='softmax')
    ]
    model = testing_utils.get_model_from_layers(
        layers, input_shape=x_train.shape[1:])
    model.compile(
        loss='categorical_crossentropy',
        optimizer=keras.optimizer_v2.adam.Adam(0.005),
        metrics=['acc'],
        run_eagerly=testing_utils.should_run_eagerly())
    history = model.fit(x_train, y_train, epochs=15, batch_size=10,
                        validation_data=(x_train, y_train),
                        verbose=2)
    self.assertGreater(history.history['val_acc'][-1], 0.7)
    _, val_acc = model.evaluate(x_train, y_train)
    self.assertAlmostEqual(history.history['val_acc'][-1], val_acc)
    predictions = model.predict(x_train)
    self.assertEqual(predictions.shape, (x_train.shape[0], 2))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 72:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1893')" href="javascript:;">
keras-2.6.0/keras/tests/model_architectures.py: 178-199
</a>
<div class="mid" id="frag1893" style="display:none"><pre>
def nested_subclassed_model():
  """A subclass model nested in another subclass model."""

  class NestedSubclassModel(keras.Model):
    """A nested subclass model."""

    def __init__(self):
      super(NestedSubclassModel, self).__init__()
      self.dense1 = keras.layers.Dense(4, activation='relu')
      self.dense2 = keras.layers.Dense(2, activation='relu')
      self.bn = keras.layers.BatchNormalization()
      self.inner_subclass_model = MySubclassModel()

    def call(self, inputs):
      x = self.dense1(inputs)
      x = self.bn(x)
      x = self.inner_subclass_model(x)
      return self.dense2(x)

  return ModelFn(NestedSubclassModel(), (None, 3), (None, 2))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1901')" href="javascript:;">
keras-2.6.0/keras/tests/model_architectures.py: 237-257
</a>
<div class="mid" id="frag1901" style="display:none"><pre>
def shared_layer_subclassed_model():
  """Shared layer in a subclass model."""

  class SharedLayerSubclassModel(keras.Model):
    """A subclass model with shared layers."""

    def __init__(self):
      super(SharedLayerSubclassModel, self).__init__(
          name='shared_layer_subclass_model')
      self.dense = keras.layers.Dense(3, activation='relu')
      self.dp = keras.layers.Dropout(0.5)
      self.bn = keras.layers.BatchNormalization()

    def call(self, inputs):
      x = self.dense(inputs)
      x = self.dp(x)
      x = self.bn(x)
      return self.dense(x)
  return ModelFn(SharedLayerSubclassModel(), (None, 3), (None, 3))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 73:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1953')" href="javascript:;">
keras-2.6.0/keras/tests/temporal_sample_weights_correctness_test.py: 252-268
</a>
<div class="mid" id="frag1953" style="display:none"><pre>
  def test_fit_with_sample_weight(self):

    def _train_and_assert(model):
      history = model.fit([self.x, self.x], [self.y1, self.y2],
                          sample_weight={
                              'output_1': self.sample_weight_1,
                              'output_2': self.sample_weight_2,
                          },
                          batch_size=3,
                          epochs=2,
                          shuffle=False)
      for key, value in self.expected_fit_result_with_weights.items():
        self.assertAllClose(history.history[key], value, 1e-3)

    run_with_different_sample_weight_mode_inputs(
        _train_and_assert, partial_sw=False)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1955')" href="javascript:;">
keras-2.6.0/keras/tests/temporal_sample_weights_correctness_test.py: 269-283
</a>
<div class="mid" id="frag1955" style="display:none"><pre>
  def test_fit_with_partial_sample_weight(self):

    def _train_and_assert(model):
      history = model.fit([self.x, self.x], [self.y1, self.y2],
                          sample_weight={
                              'output_2': self.sample_weight_2,
                          },
                          batch_size=3,
                          epochs=2,
                          shuffle=False)
      for key, value in self.expected_fit_result_with_weights_output_2.items():
        self.assertAllClose(history.history[key], value, 1e-3)

    run_with_different_sample_weight_mode_inputs(_train_and_assert)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 74:</b> &nbsp; 7 fragments, nominal size 14 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1959')" href="javascript:;">
keras-2.6.0/keras/tests/temporal_sample_weights_correctness_test.py: 294-313
</a>
<div class="mid" id="frag1959" style="display:none"><pre>
  def test_eval_with_sample_weight(self):

    def _eval_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_1': self.sample_weight_1,
                               'output_2': self.sample_weight_2,
                           })
      eval_result = model.evaluate([self.x, self.x], [self.y1, self.y2],
                                   batch_size=3,
                                   sample_weight={
                                       'output_1': self.sample_weight_1,
                                       'output_2': self.sample_weight_2,
                                   })
      self.assertAllClose(eval_result, self.expected_batch_result_with_weights,
                          1e-3)

    run_with_different_sample_weight_mode_inputs(
        _eval_and_assert, partial_sw=False)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1961')" href="javascript:;">
keras-2.6.0/keras/tests/temporal_sample_weights_correctness_test.py: 314-331
</a>
<div class="mid" id="frag1961" style="display:none"><pre>
  def test_eval_with_partial_sample_weight(self):

    def _eval_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_2': self.sample_weight_2,
                           })
      eval_result = model.evaluate([self.x, self.x], [self.y1, self.y2],
                                   batch_size=3,
                                   sample_weight={
                                       'output_2': self.sample_weight_2,
                                   })
      self.assertAllClose(eval_result,
                          self.expected_batch_result_with_weights_output_2,
                          1e-3)

    run_with_different_sample_weight_mode_inputs(_eval_and_assert)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1971')" href="javascript:;">
keras-2.6.0/keras/tests/temporal_sample_weights_correctness_test.py: 378-395
</a>
<div class="mid" id="frag1971" style="display:none"><pre>
  def test_test_on_batch_with_sample_weight(self):

    def _test_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_1': self.sample_weight_1,
                               'output_2': self.sample_weight_2,
                           })
      result = model.test_on_batch([self.x, self.x], [self.y1, self.y2],
                                   sample_weight={
                                       'output_1': self.sample_weight_1,
                                       'output_2': self.sample_weight_2,
                                   })
      self.assertAllClose(result, self.expected_batch_result_with_weights, 1e-3)

    run_with_different_sample_weight_mode_inputs(
        _test_and_assert, partial_sw=False)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1973')" href="javascript:;">
keras-2.6.0/keras/tests/temporal_sample_weights_correctness_test.py: 396-412
</a>
<div class="mid" id="frag1973" style="display:none"><pre>
  def test_test_on_batch_with_partial_sample_weight(self):

    def _test_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_2': self.sample_weight_2,
                           })
      result = model.test_on_batch([self.x, self.x], [self.y1, self.y2],
                                   sample_weight={
                                       'output_2': self.sample_weight_2,
                                   })
      self.assertAllClose(result,
                          self.expected_batch_result_with_weights_output_2,
                          1e-3)

    run_with_different_sample_weight_mode_inputs(_test_and_assert)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1967')" href="javascript:;">
keras-2.6.0/keras/tests/temporal_sample_weights_correctness_test.py: 355-368
</a>
<div class="mid" id="frag1967" style="display:none"><pre>
  def test_train_on_batch_with_partial_sample_weight(self):

    def _train_and_assert(model):
      for _ in range(2):
        result = model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                                      sample_weight={
                                          'output_2': self.sample_weight_2,
                                      })
      self.assertAllClose(result,
                          self.expected_batch_result_with_weights_output_2,
                          1e-3)

    run_with_different_sample_weight_mode_inputs(_train_and_assert)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1985')" href="javascript:;">
keras-2.6.0/keras/tests/temporal_sample_weights_correctness_test.py: 480-496
</a>
<div class="mid" id="frag1985" style="display:none"><pre>
  def test_eval_generator_with_partial_sample_weight(self):

    def _test_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_2': self.sample_weight_2,
                           })
      eval_result = model.evaluate_generator(
          self.custom_generator_multi_io_temporal(
              sample_weights={'output_2': self.sample_weight_2}),
          steps=2)
      self.assertAllClose(eval_result,
                          self.expected_batch_result_with_weights_output_2,
                          1e-3)

    run_with_different_sample_weight_mode_inputs(_test_and_assert)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1960')" href="javascript:;">
keras-2.6.0/keras/tests/temporal_sample_weights_correctness_test.py: 296-310
</a>
<div class="mid" id="frag1960" style="display:none"><pre>
    def _eval_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_1': self.sample_weight_1,
                               'output_2': self.sample_weight_2,
                           })
      eval_result = model.evaluate([self.x, self.x], [self.y1, self.y2],
                                   batch_size=3,
                                   sample_weight={
                                       'output_1': self.sample_weight_1,
                                       'output_2': self.sample_weight_2,
                                   })
      self.assertAllClose(eval_result, self.expected_batch_result_with_weights,
                          1e-3)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 75:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1972')" href="javascript:;">
keras-2.6.0/keras/tests/temporal_sample_weights_correctness_test.py: 380-392
</a>
<div class="mid" id="frag1972" style="display:none"><pre>
    def _test_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_1': self.sample_weight_1,
                               'output_2': self.sample_weight_2,
                           })
      result = model.test_on_batch([self.x, self.x], [self.y1, self.y2],
                                   sample_weight={
                                       'output_1': self.sample_weight_1,
                                       'output_2': self.sample_weight_2,
                                   })
      self.assertAllClose(result, self.expected_batch_result_with_weights, 1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1974')" href="javascript:;">
keras-2.6.0/keras/tests/temporal_sample_weights_correctness_test.py: 398-410
</a>
<div class="mid" id="frag1974" style="display:none"><pre>
    def _test_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_2': self.sample_weight_2,
                           })
      result = model.test_on_batch([self.x, self.x], [self.y1, self.y2],
                                   sample_weight={
                                       'output_2': self.sample_weight_2,
                                   })
      self.assertAllClose(result,
                          self.expected_batch_result_with_weights_output_2,
                          1e-3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 76:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1977')" href="javascript:;">
keras-2.6.0/keras/tests/temporal_sample_weights_correctness_test.py: 425-438
</a>
<div class="mid" id="frag1977" style="display:none"><pre>
  def test_fit_generator_with_sample_weight(self):

    def _train_and_assert(model):
      history = model.fit_generator(
          self.custom_generator_multi_io_temporal(
              sample_weights=[self.sample_weight_1, self.sample_weight_2]),
          steps_per_epoch=1,
          epochs=2)
      for key, value in self.expected_fit_result_with_weights.items():
        self.assertAllClose(history.history[key], value, 1e-3)

    run_with_different_sample_weight_mode_inputs(
        _train_and_assert, partial_sw=False)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1979')" href="javascript:;">
keras-2.6.0/keras/tests/temporal_sample_weights_correctness_test.py: 439-451
</a>
<div class="mid" id="frag1979" style="display:none"><pre>
  def test_fit_generator_with_partial_sample_weight(self):

    def _train_and_assert(model):
      history = model.fit_generator(
          self.custom_generator_multi_io_temporal(
              sample_weights={'output_2': self.sample_weight_2}),
          steps_per_epoch=1,
          epochs=2)
      for key, value in self.expected_fit_result_with_weights_output_2.items():
        self.assertAllClose(history.history[key], value, 1e-3)

    run_with_different_sample_weight_mode_inputs(_train_and_assert)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 77:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1984')" href="javascript:;">
keras-2.6.0/keras/tests/temporal_sample_weights_correctness_test.py: 464-476
</a>
<div class="mid" id="frag1984" style="display:none"><pre>
    def _test_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_1': self.sample_weight_1,
                               'output_2': self.sample_weight_2,
                           })
      eval_result = model.evaluate_generator(
          self.custom_generator_multi_io_temporal(
              sample_weights=[self.sample_weight_1, self.sample_weight_2]),
          steps=2)
      self.assertAllClose(eval_result, self.expected_batch_result_with_weights,
                          1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag1986')" href="javascript:;">
keras-2.6.0/keras/tests/temporal_sample_weights_correctness_test.py: 482-494
</a>
<div class="mid" id="frag1986" style="display:none"><pre>
    def _test_and_assert(model):
      model.train_on_batch([self.x, self.x], [self.y1, self.y2],
                           sample_weight={
                               'output_2': self.sample_weight_2,
                           })
      eval_result = model.evaluate_generator(
          self.custom_generator_multi_io_temporal(
              sample_weights={'output_2': self.sample_weight_2}),
          steps=2)
      self.assertAllClose(eval_result,
                          self.expected_batch_result_with_weights_output_2,
                          1e-3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 78:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag1998')" href="javascript:;">
keras-2.6.0/keras/tests/model_subclassing_compiled_test.py: 37-55
</a>
<div class="mid" id="frag1998" style="display:none"><pre>
  def test_single_io_workflow_with_np_arrays(self):
    num_classes = 2
    num_samples = 100
    input_dim = 50

    model = testing_utils.SmallSubclassMLP(
        num_hidden=32, num_classes=num_classes, use_dp=True, use_bn=True)
    model.compile(
        loss='mse',
        optimizer='rmsprop',
        metrics=['acc', keras.metrics.CategoricalAccuracy()],
        run_eagerly=testing_utils.should_run_eagerly())

    x = np.ones((num_samples, input_dim))
    y = np.zeros((num_samples, num_classes))

    model.fit(x, y, epochs=2, batch_size=32, verbose=0)
    _ = model.evaluate(x, y, verbose=0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2013')" href="javascript:;">
keras-2.6.0/keras/tests/model_subclassing_compiled_test.py: 347-369
</a>
<div class="mid" id="frag2013" style="display:none"><pre>
  def test_subclass_nested_in_graph(self):
    num_classes = 2
    num_samples = 100
    input_dim = 50

    model = model_util.get_nested_model_3(
        input_dim=input_dim, num_classes=num_classes)
    model.compile(
        loss='mse',
        optimizer='rmsprop',
        metrics=['acc'],
        run_eagerly=testing_utils.should_run_eagerly())

    x = np.ones((num_samples, input_dim))
    y = np.zeros((num_samples, num_classes))

    model.fit(x, y, epochs=2, batch_size=32, verbose=0)
    _ = model.evaluate(x, y, verbose=0)

    self.assertEqual(len(model.weights), 16)
    self.assertEqual(len(model.non_trainable_weights), 4)
    self.assertEqual(len(model.trainable_weights), 12)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 79:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2005')" href="javascript:;">
keras-2.6.0/keras/tests/model_subclassing_compiled_test.py: 159-188
</a>
<div class="mid" id="frag2005" style="display:none"><pre>
  def test_training_and_inference_behavior(self):
    # test that dropout is applied in training and not inference

    num_samples = 100
    input_dim = 50

    class DPNet(keras.Model):

      def __init__(self):
        super(DPNet, self).__init__()
        self.dp = keras.layers.Dropout(0.5)
        self.dense = keras.layers.Dense(1,
                                        use_bias=False,
                                        kernel_initializer='ones')

      def call(self, inputs):
        x = self.dp(inputs)
        return self.dense(x)

    model = DPNet()
    x = np.ones((num_samples, input_dim))
    y = model.predict(x)
    self.assertEqual(np.sum(y), np.sum(x))
    model.compile(
        loss='mse',
        optimizer='rmsprop',
        run_eagerly=testing_utils.should_run_eagerly())
    loss = model.train_on_batch(x, y)
    self.assertGreater(loss, 0.1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2017')" href="javascript:;">
keras-2.6.0/keras/tests/model_subclassing_compiled_test.py: 404-436
</a>
<div class="mid" id="frag2017" style="display:none"><pre>
  def test_support_for_manual_training_arg(self):
    # In most cases, the `training` argument is left unspecified, in which
    # case it defaults to value corresponding to the Model method being used
    # (fit -&gt; True, predict -&gt; False, etc).
    # If the user writes their model `call` method to take
    # an explicit `training` argument, we must check that the correct value
    # is being passed to the model for each method call.

    class DPNet(keras.Model):

      def __init__(self):
        super(DPNet, self).__init__()
        self.dp = keras.layers.Dropout(0.5)
        self.dense = keras.layers.Dense(1,
                                        use_bias=False,
                                        kernel_initializer='ones')

      def call(self, inputs, training=False):
        x = self.dp(inputs, training=training)
        return self.dense(x)

    model = DPNet()
    x = np.ones((10, 10))
    y = model.predict(x)
    self.assertEqual(np.sum(y), np.sum(x))
    model.compile(
        loss='mse',
        optimizer='rmsprop',
        run_eagerly=testing_utils.should_run_eagerly())
    loss = model.train_on_batch(x, y)
    self.assertGreater(loss, 0.1)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 80:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2011')" href="javascript:;">
keras-2.6.0/keras/tests/model_subclassing_compiled_test.py: 299-322
</a>
<div class="mid" id="frag2011" style="display:none"><pre>
  def test_subclass_nested_in_subclass(self):
    num_classes = 2
    num_samples = 100
    input_dim = 50

    model = model_util.NestedTestModel1(num_classes=num_classes)
    model.compile(
        loss='mse',
        optimizer='rmsprop',
        metrics=['acc'],
        run_eagerly=testing_utils.should_run_eagerly())

    x = np.ones((num_samples, input_dim))
    y = np.zeros((num_samples, num_classes))

    model.fit(x, y, epochs=2, batch_size=32, verbose=0)
    _ = model.evaluate(x, y, verbose=0)

    self.assertEqual(len(model.weights), 8 + len(model.test_net.weights))
    self.assertEqual(len(model.non_trainable_weights),
                     2 + len(model.test_net.non_trainable_weights))
    self.assertEqual(len(model.trainable_weights),
                     6 + len(model.test_net.trainable_weights))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2012')" href="javascript:;">
keras-2.6.0/keras/tests/model_subclassing_compiled_test.py: 323-346
</a>
<div class="mid" id="frag2012" style="display:none"><pre>
  def test_graph_nested_in_subclass(self):
    num_classes = 2
    num_samples = 100
    input_dim = 50

    model = model_util.NestedTestModel2(num_classes=num_classes)
    model.compile(
        loss='mse',
        optimizer='rmsprop',
        metrics=['acc'],
        run_eagerly=testing_utils.should_run_eagerly())

    x = np.ones((num_samples, input_dim))
    y = np.zeros((num_samples, num_classes))

    model.fit(x, y, epochs=2, batch_size=32, verbose=0)
    _ = model.evaluate(x, y, verbose=0)

    self.assertEqual(len(model.weights), 8 + len(model.test_net.weights))
    self.assertEqual(len(model.non_trainable_weights),
                     2 + len(model.test_net.non_trainable_weights))
    self.assertEqual(len(model.trainable_weights),
                     6 + len(model.test_net.trainable_weights))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 81:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2059')" href="javascript:;">
keras-2.6.0/keras/tests/tracking_test.py: 109-127
</a>
<div class="mid" id="frag2059" style="display:none"><pre>
  def testSubSequentialTracking(self):

    class _Subclassed(training.Model):

      def __init__(self, wrapped):
        super(_Subclassed, self).__init__()
        self._wrapped = wrapped

      def call(self, x):
        return self._wrapped(x)

    model = sequential.Sequential()
    layer = core.Dense(1)
    model.add(layer)
    model2 = _Subclassed(model)
    model2(tf.ones([1, 2]))
    model2.m = [model]
    self.assertIn(layer.kernel, model2.trainable_weights)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2091')" href="javascript:;">
keras-2.6.0/keras/tests/tracking_test.py: 452-470
</a>
<div class="mid" id="frag2091" style="display:none"><pre>
  def testSubSequentialTracking(self):

    class _Subclassed(training.Model):

      def __init__(self, wrapped):
        super(_Subclassed, self).__init__()
        self._wrapped = wrapped

      def call(self, x):
        return self._wrapped(x)

    model = sequential.Sequential()
    layer = core.Dense(1)
    model.add(layer)
    model2 = _Subclassed(model)
    model2(tf.ones([1, 2]))
    model2.m = (model,)
    self.assertIn(layer.kernel, model2.trainable_weights)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 82:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2072')" href="javascript:;">
keras-2.6.0/keras/tests/tracking_test.py: 201-217
</a>
<div class="mid" id="frag2072" style="display:none"><pre>
  def testTensorConversion(self):

    class ListToTensor(training.Model):

      def __init__(self):
        super(ListToTensor, self).__init__()
        self.l = [1., 2., 3.]

    self.assertAllEqual(
        [1., 2., 3.],
        self.evaluate(tf.constant(ListToTensor().l)))

    self.assertAllEqual(
        [1., 2., 3.],
        self.evaluate(tf.raw_ops.Pack(values=ListToTensor().l)))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2098')" href="javascript:;">
keras-2.6.0/keras/tests/tracking_test.py: 514-530
</a>
<div class="mid" id="frag2098" style="display:none"><pre>
  def testTensorConversion(self):

    class TupleToTensor(training.Model):

      def __init__(self):
        super(TupleToTensor, self).__init__()
        self.l = (1., 2., 3.)

    self.assertAllEqual(
        (1., 2., 3.),
        self.evaluate(tf.constant(TupleToTensor().l)))

    self.assertAllEqual(
        (1., 2., 3.),
        self.evaluate(tf.raw_ops.Pack(values=TupleToTensor().l)))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 83:</b> &nbsp; 6 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2164')" href="javascript:;">
keras-2.6.0/keras/initializers/initializers_test.py: 110-120
</a>
<div class="mid" id="frag2164" style="display:none"><pre>
  def test_lecun_uniform(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, _ = _compute_fans(tensor_shape)
      std = np.sqrt(1. / fan_in)
      self._runner(
          initializers.LecunUniformV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2169')" href="javascript:;">
keras-2.6.0/keras/initializers/initializers_test.py: 165-175
</a>
<div class="mid" id="frag2169" style="display:none"><pre>
  def test_he_normal(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, _ = _compute_fans(tensor_shape)
      std = np.sqrt(2. / fan_in)
      self._runner(
          initializers.HeNormalV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2166')" href="javascript:;">
keras-2.6.0/keras/initializers/initializers_test.py: 132-142
</a>
<div class="mid" id="frag2166" style="display:none"><pre>
  def test_he_uniform(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, _ = _compute_fans(tensor_shape)
      std = np.sqrt(2. / fan_in)
      self._runner(
          initializers.HeUniformV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2168')" href="javascript:;">
keras-2.6.0/keras/initializers/initializers_test.py: 154-164
</a>
<div class="mid" id="frag2168" style="display:none"><pre>
  def test_glorot_normal(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, fan_out = _compute_fans(tensor_shape)
      std = np.sqrt(2. / (fan_in + fan_out))
      self._runner(
          initializers.GlorotNormalV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2167')" href="javascript:;">
keras-2.6.0/keras/initializers/initializers_test.py: 143-153
</a>
<div class="mid" id="frag2167" style="display:none"><pre>
  def test_lecun_normal(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, _ = _compute_fans(tensor_shape)
      std = np.sqrt(1. / fan_in)
      self._runner(
          initializers.LecunNormalV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2165')" href="javascript:;">
keras-2.6.0/keras/initializers/initializers_test.py: 121-131
</a>
<div class="mid" id="frag2165" style="display:none"><pre>
  def test_glorot_uniform(self):
    tensor_shape = (5, 6, 4, 2)
    with self.cached_session():
      fan_in, fan_out = _compute_fans(tensor_shape)
      std = np.sqrt(2. / (fan_in + fan_out))
      self._runner(
          initializers.GlorotUniformV2(seed=123),
          tensor_shape,
          target_mean=0.,
          target_std=std)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 84:</b> &nbsp; 4 fragments, nominal size 34 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2200')" href="javascript:;">
keras-2.6.0/keras/saving/metrics_serialization_test.py: 156-200
</a>
<div class="mid" id="frag2200" style="display:none"><pre>
  def test_serializing_model_with_metric_with_custom_object_scope(self, value):

    def get_instance(x):
      if isinstance(x, str):
        return x
      if isinstance(x, type) and issubclass(x, metrics.Metric):
        return x()
      return x

    metric_input = tf.nest.map_structure(get_instance, value)
    weighted_metric_input = tf.nest.map_structure(get_instance, value)

    with generic_utils.custom_object_scope({
        'MyMeanAbsoluteError': MyMeanAbsoluteError,
        '_my_mae': _my_mae,
        'Bias': testing_utils.Bias,
    }):
      model = _get_multi_io_model()
      model.compile(
          optimizer_v2.gradient_descent.SGD(0.1),
          'mae',
          metrics=metric_input,
          weighted_metrics=weighted_metric_input,
          run_eagerly=testing_utils.should_run_eagerly())
      history = model.fit([self.x, self.x], [self.y, self.y],
                          batch_size=3,
                          epochs=3,
                          sample_weight=[self.w, self.w])

      # Assert training.
      self.assertAllClose(history.history['loss'], [2., 1.6, 1.2], 1e-3)
      eval_results = model.evaluate([self.x, self.x], [self.y, self.y],
                                    sample_weight=[self.w, self.w])

      if h5py is None:
        return
      model.save(self.model_filename)
      loaded_model = keras.models.load_model(self.model_filename)
      loaded_model.predict([self.x, self.x])
      loaded_eval_results = loaded_model.evaluate(
          [self.x, self.x], [self.y, self.y], sample_weight=[self.w, self.w])

      # Assert all evaluation results are the same.
      self.assertAllClose(eval_results, loaded_eval_results, 1e-9)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2202')" href="javascript:;">
keras-2.6.0/keras/saving/metrics_serialization_test.py: 201-248
</a>
<div class="mid" id="frag2202" style="display:none"><pre>
  def test_serializing_model_with_metric_with_custom_objects(self, value):

    def get_instance(x):
      if isinstance(x, str):
        return x
      if isinstance(x, type) and issubclass(x, metrics.Metric):
        return x()
      return x

    metric_input = tf.nest.map_structure(get_instance, value)
    weighted_metric_input = tf.nest.map_structure(get_instance, value)

    model = _get_multi_io_model()
    model.compile(
        optimizer_v2.gradient_descent.SGD(0.1),
        'mae',
        metrics=metric_input,
        weighted_metrics=weighted_metric_input,
        run_eagerly=testing_utils.should_run_eagerly())
    history = model.fit([self.x, self.x], [self.y, self.y],
                        batch_size=3,
                        epochs=3,
                        sample_weight=[self.w, self.w])

    # Assert training.
    self.assertAllClose(history.history['loss'], [2., 1.6, 1.2], 1e-3)
    eval_results = model.evaluate([self.x, self.x], [self.y, self.y],
                                  sample_weight=[self.w, self.w])

    if h5py is None:
      return
    model.save(self.model_filename)
    loaded_model = keras.models.load_model(
        self.model_filename,
        custom_objects={
            'MyMeanAbsoluteError': MyMeanAbsoluteError,
            '_my_mae': _my_mae,
            'Bias': testing_utils.Bias,
        })
    loaded_model.predict([self.x, self.x])
    loaded_eval_results = loaded_model.evaluate([self.x, self.x],
                                                [self.y, self.y],
                                                sample_weight=[self.w, self.w])

    # Assert all evaluation results are the same.
    self.assertAllClose(eval_results, loaded_eval_results, 1e-9)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2228')" href="javascript:;">
keras-2.6.0/keras/saving/losses_serialization_test.py: 124-155
</a>
<div class="mid" id="frag2228" style="display:none"><pre>
  def test_serializing_model_with_loss_with_custom_object_scope(self, value):
    with generic_utils.custom_object_scope({
        'MyMeanAbsoluteError': MyMeanAbsoluteError,
        'my_mae': my_mae,
        'Bias': testing_utils.Bias,
    }):
      model = _get_multi_io_model()
      model.compile(
          optimizer_v2.gradient_descent.SGD(0.1),
          loss=value,
          run_eagerly=testing_utils.should_run_eagerly())
      history = model.fit([self.x, self.x], [self.y, self.y],
                          batch_size=3,
                          epochs=3,
                          sample_weight=[self.w, self.w])

      # Assert training.
      self.assertAllClose(history.history['loss'], [2., 1.6, 1.2], 1e-3)
      eval_results = model.evaluate([self.x, self.x], [self.y, self.y],
                                    sample_weight=[self.w, self.w])

      if h5py is None:
        return
      model.save(self.model_filename)
      loaded_model = keras.models.load_model(self.model_filename)
      loaded_model.predict([self.x, self.x])
      loaded_eval_results = loaded_model.evaluate(
          [self.x, self.x], [self.y, self.y], sample_weight=[self.w, self.w])

      # Assert all evaluation results are the same.
      self.assertAllClose(eval_results, loaded_eval_results, 1e-9)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2229')" href="javascript:;">
keras-2.6.0/keras/saving/losses_serialization_test.py: 156-190
</a>
<div class="mid" id="frag2229" style="display:none"><pre>
  def test_serializing_model_with_loss_with_custom_objects(self, value):
    model = _get_multi_io_model()
    model.compile(
        optimizer_v2.gradient_descent.SGD(0.1),
        loss=value,
        run_eagerly=testing_utils.should_run_eagerly())
    history = model.fit([self.x, self.x], [self.y, self.y],
                        batch_size=3,
                        epochs=3,
                        sample_weight=[self.w, self.w])

    # Assert training.
    self.assertAllClose(history.history['loss'], [2., 1.6, 1.2], 1e-3)
    eval_results = model.evaluate([self.x, self.x], [self.y, self.y],
                                  sample_weight=[self.w, self.w])

    if h5py is None:
      return
    model.save(self.model_filename)
    loaded_model = keras.models.load_model(
        self.model_filename,
        custom_objects={
            'MyMeanAbsoluteError': MyMeanAbsoluteError,
            'my_mae': my_mae,
            'Bias': testing_utils.Bias,
        })
    loaded_model.predict([self.x, self.x])
    loaded_eval_results = loaded_model.evaluate([self.x, self.x],
                                                [self.y, self.y],
                                                sample_weight=[self.w, self.w])

    # Assert all evaluation results are the same.
    self.assertAllClose(eval_results, loaded_eval_results, 1e-9)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 85:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2364')" href="javascript:;">
keras-2.6.0/keras/engine/base_preprocessing_layer_test.py: 206-220
</a>
<div class="mid" id="frag2364" style="display:none"><pre>
  def test_post_build_injected_update(self):
    """Test external update injection after build() is called."""
    input_dataset = np.array([1, 2, 3, 4, 5])
    input_data = keras.Input(shape=(1,))
    layer = AddingPreprocessingLayer()
    output = layer(input_data)
    model = keras.Model(input_data, output)
    model._run_eagerly = testing_utils.should_run_eagerly()

    combiner = layer._combiner
    updates = combiner.extract(combiner.compute(input_dataset))
    layer._set_state_variables(updates)

    self.assertAllEqual([[16], [17], [18]], model.predict([1., 2., 3.]))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2366')" href="javascript:;">
keras-2.6.0/keras/engine/base_preprocessing_layer_test.py: 236-250
</a>
<div class="mid" id="frag2366" style="display:none"><pre>
  def test_post_build_adapt_update_dataset(self):
    """Test that preproc layers can adapt() after build() is called."""
    input_dataset = tf.data.Dataset.from_tensor_slices(
        np.array([[1], [2], [3], [4], [5], [0]]))

    input_data = keras.Input(shape=(1,))
    layer = AddingPreprocessingLayer()
    output = layer(input_data)
    model = keras.Model(input_data, output)
    model._run_eagerly = testing_utils.should_run_eagerly()

    layer.adapt(input_dataset)

    self.assertAllEqual([[16], [17], [18]], model.predict([1., 2., 3.]))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2365')" href="javascript:;">
keras-2.6.0/keras/engine/base_preprocessing_layer_test.py: 221-235
</a>
<div class="mid" id="frag2365" style="display:none"><pre>
  def test_pre_build_adapt_update_dataset(self):
    """Test that preproc layers can adapt() before build() is called."""
    input_dataset = tf.data.Dataset.from_tensor_slices(
        np.array([[1], [2], [3], [4], [5], [0]]))

    layer = AddingPreprocessingLayer()
    layer.adapt(input_dataset)

    input_data = keras.Input(shape=(1,))
    output = layer(input_data)
    model = keras.Model(input_data, output)
    model._run_eagerly = testing_utils.should_run_eagerly()

    self.assertAllEqual([[16], [17], [18]], model.predict([1., 2., 3.]))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 86:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2409')" href="javascript:;">
keras-2.6.0/keras/engine/training_utils_v1_test.py: 269-287
</a>
<div class="mid" id="frag2409" style="display:none"><pre>
  def _run_without_steps(self):
    aggregator = training_utils_v1.OutputsAggregator(
        use_steps=False, num_samples=6)

    batch_start = 0
    for i, batch in enumerate(np.array_split(_TEST_DATA, 4)):
      if i == 0:
        aggregator.create(batch)

      batch_end = batch_start + batch.shape[0]
      aggregator.aggregate(batch, batch_start, batch_end)
      batch_start = batch_end

    assert len(aggregator.results) == 1
    assert isinstance(aggregator.results[0], training_utils_v1.SliceAggregator)

    aggregator.finalize()
    return aggregator.results

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2412')" href="javascript:;">
keras-2.6.0/keras/engine/training_utils_v1_test.py: 294-311
</a>
<div class="mid" id="frag2412" style="display:none"><pre>
  def test_nested_aggregation(self):
    aggregator = training_utils_v1.OutputsAggregator(
        use_steps=False, num_samples=6)

    batches = np.array_split(_TEST_DATA, 4)
    batch_start = 0
    for i, batch in enumerate(zip(batches, batches)):
      if i == 0:
        aggregator.create(batch)

      batch_end = batch_start + batch[0].shape[0]
      aggregator.aggregate(batch, batch_start, batch_end)
      batch_start = batch_end

    assert len(aggregator.results) == 2
    aggregator.finalize()
    self.assertAllEqual(aggregator.results, (_TEST_DATA, _TEST_DATA))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 87:</b> &nbsp; 2 fragments, nominal size 33 lines, similarity 79%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2433')" href="javascript:;">
keras-2.6.0/keras/engine/training_distributed_v1.py: 678-718
</a>
<div class="mid" id="frag2433" style="display:none"><pre>
  def evaluate(self,
               model,
               x=None,
               y=None,
               batch_size=None,
               verbose=1,
               sample_weight=None,
               steps=None,
               callbacks=None,
               **kwargs):
    """Evaluate loop for Distribution Strategies."""
    dist_utils.validate_inputs(x, y)
    batch_size, steps = dist_utils.process_batch_and_step_size(
        model._distribution_strategy, x, batch_size, steps, ModeKeys.TEST)
    batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)
    dataset = model._distribution_standardize_user_data(
        x, y,
        sample_weight=sample_weight,
        batch_size=batch_size,
        allow_partial_batch=True)

    if backend.is_tpu_strategy(model._distribution_strategy):
      steps = training_utils_v1.infer_steps_for_dataset(
          model, dataset, steps, steps_name='steps')
      if steps is None:
        raise ValueError('Number of steps could not be inferred from the data, '
                         'please pass the steps argument.')

      if not tf.executing_eagerly():
        # Run TPU evaluation in a custom loop in graph mode.
        return experimental_tpu_test_loop(
            model, dataset, verbose=verbose, steps=steps, callbacks=callbacks)

    return training_arrays_v1.test_loop(
        model,
        inputs=dataset,
        batch_size=batch_size,
        verbose=verbose,
        steps=steps,
        callbacks=callbacks)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2434')" href="javascript:;">
keras-2.6.0/keras/engine/training_distributed_v1.py: 719-753
</a>
<div class="mid" id="frag2434" style="display:none"><pre>
  def predict(self,
              model,
              x,
              batch_size=None,
              verbose=0,
              steps=None,
              callbacks=None,
              **kwargs):
    """Predict loop for Distribution Strategies."""
    dist_utils.validate_inputs(x=x, y=None)
    batch_size, steps = dist_utils.process_batch_and_step_size(
        model._distribution_strategy, x, batch_size, steps, ModeKeys.PREDICT)
    batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)
    dataset = model._distribution_standardize_user_data(
        x,
        batch_size=batch_size,
        allow_partial_batch=True)
    if backend.is_tpu_strategy(model._distribution_strategy):
      steps = training_utils_v1.infer_steps_for_dataset(
          model, dataset, steps, steps_name='steps')
      if steps is None:
        raise ValueError('Number of steps could not be inferred from the data, '
                         'please pass the steps argument.')
      if not tf.executing_eagerly():
        return experimental_tpu_predict_loop(
            model, dataset, verbose=verbose, steps=steps, callbacks=callbacks)
    return training_arrays_v1.predict_loop(
        model,
        dataset,
        batch_size=batch_size,
        verbose=verbose,
        steps=steps,
        callbacks=callbacks)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 88:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2445')" href="javascript:;">
keras-2.6.0/keras/engine/deferred_sequential_test.py: 120-134
</a>
<div class="mid" id="frag2445" style="display:none"><pre>
  def test_saving_savedmodel(self):
    model = get_model()
    model(np.random.random((3, 6)))  # Build model

    path = os.path.join(self.get_temp_dir(), 'model_path')
    model.save(path)
    new_model = keras.models.load_model(path)
    model_layers = model._flatten_layers(include_self=True, recursive=False)
    new_model_layers = new_model._flatten_layers(
        include_self=True, recursive=False)
    for layer1, layer2 in zip(model_layers, new_model_layers):
      self.assertEqual(layer1.name, layer2.name)
      for w1, w2 in zip(layer1.weights, layer2.weights):
        self.assertAllClose(w1, w2)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2446')" href="javascript:;">
keras-2.6.0/keras/engine/deferred_sequential_test.py: 137-152
</a>
<div class="mid" id="frag2446" style="display:none"><pre>
  def test_saving_h5(self):
    path = os.path.join(self.get_temp_dir(), 'model_path.h5')
    model = get_model()
    model(np.random.random((3, 6)))  # Build model

    path = os.path.join(self.get_temp_dir(), 'model_path.h5')
    model.save(path)
    new_model = keras.models.load_model(path)
    model_layers = model._flatten_layers(include_self=True, recursive=False)
    new_model_layers = new_model._flatten_layers(
        include_self=True, recursive=False)
    for layer1, layer2 in zip(model_layers, new_model_layers):
      self.assertEqual(layer1.name, layer2.name)
      for w1, w2 in zip(layer1.weights, layer2.weights):
        self.assertAllClose(w1, w2)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 89:</b> &nbsp; 4 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2482')" href="javascript:;">
keras-2.6.0/keras/engine/ragged_keras_tensor_test.py: 184-199
</a>
<div class="mid" id="frag2482" style="display:none"><pre>
  def test_from_value_rowids(self):
    inp = layers.Input(shape=[None])
    out = tf.RaggedTensor.from_value_rowids(
        inp, value_rowids=[0, 0, 0, 0, 2, 2, 2, 3], nrows=5)
    model = training.Model(inp, out)

    x = tf.constant([3, 1, 4, 1, 5, 9, 2, 6])
    expected = tf.RaggedTensor.from_value_rowids(
        x, value_rowids=[0, 0, 0, 0, 2, 2, 2, 3], nrows=5)
    self.assertAllEqual(model(x), expected)

    # Test that the model can serialize and deserialize as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2485')" href="javascript:;">
keras-2.6.0/keras/engine/ragged_keras_tensor_test.py: 232-247
</a>
<div class="mid" id="frag2485" style="display:none"><pre>
  def test_from_row_starts(self):
    inp = layers.Input(shape=[None])
    out = tf.RaggedTensor.from_row_starts(
        inp, row_starts=[0, 4, 4, 7, 8])
    model = training.Model(inp, out)

    x = tf.constant([3, 1, 4, 1, 5, 9, 2, 6])
    expected = tf.RaggedTensor.from_row_starts(
        x, row_starts=[0, 4, 4, 7, 8])
    self.assertAllEqual(model(x), expected)

    # Test that the model can serialize and deserialize as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2484')" href="javascript:;">
keras-2.6.0/keras/engine/ragged_keras_tensor_test.py: 216-231
</a>
<div class="mid" id="frag2484" style="display:none"><pre>
  def test_from_row_lengths(self):
    inp = layers.Input(shape=[None])
    out = tf.RaggedTensor.from_row_lengths(
        inp, row_lengths=[4, 0, 3, 1, 0])
    model = training.Model(inp, out)

    x = tf.constant([3, 1, 4, 1, 5, 9, 2, 6])
    expected = tf.RaggedTensor.from_row_lengths(
        x, row_lengths=[4, 0, 3, 1, 0])
    self.assertAllEqual(model(x), expected)

    # Test that the model can serialize and deserialize as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2483')" href="javascript:;">
keras-2.6.0/keras/engine/ragged_keras_tensor_test.py: 200-215
</a>
<div class="mid" id="frag2483" style="display:none"><pre>
  def test_from_row_splits(self):
    inp = layers.Input(shape=[None])
    out = tf.RaggedTensor.from_row_splits(
        inp, row_splits=[0, 4, 4, 7, 8, 8])
    model = training.Model(inp, out)

    x = tf.constant([3, 1, 4, 1, 5, 9, 2, 6])
    expected = tf.RaggedTensor.from_row_splits(
        x, row_splits=[0, 4, 4, 7, 8, 8])
    self.assertAllEqual(model(x), expected)

    # Test that the model can serialize and deserialize as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 90:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2488')" href="javascript:;">
keras-2.6.0/keras/engine/ragged_keras_tensor_test.py: 281-300
</a>
<div class="mid" id="frag2488" style="display:none"><pre>
  def test_from_nested_value_row_ids(self):
    nested_value_rowids = [
        tf.constant([0, 0, 1, 3, 3], tf.int64),
        tf.constant([0, 0, 2, 2, 2, 3, 4], tf.int64)
    ]
    inp = layers.Input(shape=[None], dtype=tf.string)
    out = tf.RaggedTensor.from_nested_value_rowids(
        inp, nested_value_rowids)
    model = training.Model(inp, out)

    x = tf.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])
    expected = tf.RaggedTensor.from_nested_value_rowids(
        x, nested_value_rowids)
    self.assertAllEqual(model(x), expected)

    # Test that the model can serialize and deserialize as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2489')" href="javascript:;">
keras-2.6.0/keras/engine/ragged_keras_tensor_test.py: 301-320
</a>
<div class="mid" id="frag2489" style="display:none"><pre>
  def test_from_nested_row_splits(self):
    nested_row_splits = [
        tf.constant([0, 2, 3, 3, 5], tf.int64),
        tf.constant([0, 2, 2, 5, 6, 7], tf.int64)
    ]
    inp = layers.Input(shape=[None], dtype=tf.string)
    out = tf.RaggedTensor.from_nested_row_splits(
        inp, nested_row_splits)
    model = training.Model(inp, out)

    x = tf.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])
    expected = tf.RaggedTensor.from_nested_row_splits(
        x, nested_row_splits)
    self.assertAllEqual(model(x), expected)

    # Test that the model can serialize and deserialize as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2490')" href="javascript:;">
keras-2.6.0/keras/engine/ragged_keras_tensor_test.py: 321-340
</a>
<div class="mid" id="frag2490" style="display:none"><pre>
  def test_from_nested_row_lengths(self):
    nested_row_lengths = [
        tf.constant([2, 1, 0, 2], tf.int64),
        tf.constant([2, 0, 3, 1, 1], tf.int64)
    ]
    inp = layers.Input(shape=[None], dtype=tf.string)
    out = tf.RaggedTensor.from_nested_row_lengths(
        inp, nested_row_lengths)
    model = training.Model(inp, out)

    x = tf.constant(['a', 'b', 'c', 'd', 'e', 'f', 'g'])
    expected = tf.RaggedTensor.from_nested_row_lengths(
        x, nested_row_lengths)
    self.assertAllEqual(model(x), expected)

    # Test that the model can serialize and deserialize as well
    model_config = model.get_config()
    model2 = training.Model.from_config(model_config)
    self.assertAllEqual(model2(x), expected)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 91:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2627')" href="javascript:;">
keras-2.6.0/keras/engine/data_adapter_test.py: 136-146
</a>
<div class="mid" id="frag2627" style="display:none"><pre>
  def test_epochs(self):
    num_epochs = 3
    adapter = self.adapter_cls(
        self.numpy_input, self.numpy_target, batch_size=5, epochs=num_epochs)
    ds_iter = iter(adapter.get_dataset())
    num_batches_per_epoch = self.numpy_input.shape[0] // 5
    for _ in range(num_batches_per_epoch * num_epochs):
      next(ds_iter)
    with self.assertRaises(StopIteration):
      next(ds_iter)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2644')" href="javascript:;">
keras-2.6.0/keras/engine/data_adapter_test.py: 428-439
</a>
<div class="mid" id="frag2644" style="display:none"><pre>
  def test_epochs(self):
    num_epochs = 3
    adapter = self.adapter_cls(
        self.arraylike_input,
        self.numpy_target, batch_size=5, epochs=num_epochs)
    ds_iter = iter(adapter.get_dataset())
    num_batches_per_epoch = self.numpy_input.shape[0] // 5
    for _ in range(num_batches_per_epoch * num_epochs):
      next(ds_iter)
    with self.assertRaises(StopIteration):
      next(ds_iter)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 92:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2634')" href="javascript:;">
keras-2.6.0/keras/engine/data_adapter_test.py: 263-294
</a>
<div class="mid" id="frag2634" style="display:none"><pre>
  def test_shuffle_correctness(self):
    num_samples = 100
    batch_size = 32
    x = np.arange(num_samples)
    np.random.seed(99)
    adapter = self.adapter_cls(
        x, y=None, batch_size=batch_size, shuffle=True, epochs=2)

    def _get_epoch(ds_iter):
      ds_data = []
      for _ in range(int(math.ceil(num_samples / batch_size))):
        ds_data.append(next(ds_iter).numpy())
      return np.concatenate(ds_data)

    ds_iter = iter(adapter.get_dataset())

    # First epoch.
    epoch_data = _get_epoch(ds_iter)
    # Check that shuffling occurred.
    self.assertNotAllClose(x, epoch_data)
    # Check that each elements appears, and only once.
    self.assertAllClose(x, np.sort(epoch_data))

    # Second epoch.
    second_epoch_data = _get_epoch(ds_iter)
    # Check that shuffling occurred.
    self.assertNotAllClose(x, second_epoch_data)
    # Check that shuffling is different across epochs.
    self.assertNotAllClose(epoch_data, second_epoch_data)
    # Check that each elements appears, and only once.
    self.assertAllClose(x, np.sort(second_epoch_data))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2648')" href="javascript:;">
keras-2.6.0/keras/engine/data_adapter_test.py: 494-525
</a>
<div class="mid" id="frag2648" style="display:none"><pre>
  def test_shuffle_correctness(self):
    num_samples = 100
    batch_size = 32
    x = DummyArrayLike(np.arange(num_samples))
    np.random.seed(99)
    adapter = self.adapter_cls(
        x, y=None, batch_size=batch_size, shuffle=True, epochs=2)

    def _get_epoch(ds_iter):
      ds_data = []
      for _ in range(int(math.ceil(num_samples / batch_size))):
        ds_data.append(next(ds_iter).numpy())
      return np.concatenate(ds_data)

    ds_iter = iter(adapter.get_dataset())

    # First epoch.
    epoch_data = _get_epoch(ds_iter)
    # Check that shuffling occurred.
    self.assertNotAllClose(x, epoch_data)
    # Check that each elements appears, and only once.
    self.assertAllClose(x, np.sort(epoch_data))

    # Second epoch.
    second_epoch_data = _get_epoch(ds_iter)
    # Check that shuffling occurred.
    self.assertNotAllClose(x, second_epoch_data)
    # Check that shuffling is different across epochs.
    self.assertNotAllClose(epoch_data, second_epoch_data)
    # Check that each elements appears, and only once.
    self.assertAllClose(x, np.sort(second_epoch_data))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 93:</b> &nbsp; 2 fragments, nominal size 31 lines, similarity 96%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2636')" href="javascript:;">
keras-2.6.0/keras/engine/data_adapter_test.py: 296-348
</a>
<div class="mid" id="frag2636" style="display:none"><pre>
  def test_batch_shuffle_correctness(self):
    num_samples = 100
    batch_size = 6
    x = np.arange(num_samples)
    np.random.seed(99)
    adapter = self.adapter_cls(
        x, y=None, batch_size=batch_size, shuffle='batch', epochs=2)

    def _get_epoch_batches(ds_iter):
      ds_data = []
      for _ in range(int(math.ceil(num_samples / batch_size))):
        ds_data.append(next(ds_iter)[0].numpy())
      return ds_data

    ds_iter = iter(adapter.get_dataset())

    # First epoch.
    epoch_batch_data = _get_epoch_batches(ds_iter)
    epoch_data = np.concatenate(epoch_batch_data)

    def _verify_batch(batch):
      # Verify that a batch contains only contiguous data, and that it has
      # been shuffled.
      shuffled_batch = np.sort(batch)
      self.assertNotAllClose(batch, shuffled_batch)
      for i in range(1, len(batch)):
        self.assertEqual(shuffled_batch[i-1] + 1, shuffled_batch[i])

    # Assert that the data within each batch remains contiguous
    for batch in epoch_batch_data:
      _verify_batch(batch)

    # Check that individual batches are unshuffled
    # Check that shuffling occurred.
    self.assertNotAllClose(x, epoch_data)
    # Check that each elements appears, and only once.
    self.assertAllClose(x, np.sort(epoch_data))

    # Second epoch.
    second_epoch_batch_data = _get_epoch_batches(ds_iter)
    second_epoch_data = np.concatenate(second_epoch_batch_data)

    # Assert that the data within each batch remains contiguous
    for batch in second_epoch_batch_data:
      _verify_batch(batch)

    # Check that shuffling occurred.
    self.assertNotAllClose(x, second_epoch_data)
    # Check that shuffling is different across epochs.
    self.assertNotAllClose(epoch_data, second_epoch_data)
    # Check that each elements appears, and only once.
    self.assertAllClose(x, np.sort(second_epoch_data))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2650')" href="javascript:;">
keras-2.6.0/keras/engine/data_adapter_test.py: 527-579
</a>
<div class="mid" id="frag2650" style="display:none"><pre>
  def test_batch_shuffle_correctness(self):
    num_samples = 100
    batch_size = 6
    x = DummyArrayLike(np.arange(num_samples))
    np.random.seed(99)
    adapter = self.adapter_cls(
        x, y=None, batch_size=batch_size, shuffle='batch', epochs=2)

    def _get_epoch_batches(ds_iter):
      ds_data = []
      for _ in range(int(math.ceil(num_samples / batch_size))):
        ds_data.append(next(ds_iter)[0].numpy())
      return ds_data

    ds_iter = iter(adapter.get_dataset())

    # First epoch.
    epoch_batch_data = _get_epoch_batches(ds_iter)
    epoch_data = np.concatenate(epoch_batch_data)

    def _verify_batch(batch):
      # Verify that a batch contains only contiguous data, but that it has
      # been shuffled.
      shuffled_batch = np.sort(batch)
      self.assertNotAllClose(batch, shuffled_batch)
      for i in range(1, len(batch)):
        self.assertEqual(shuffled_batch[i-1] + 1, shuffled_batch[i])

    # Assert that the data within each batch is shuffled contiguous data
    for batch in epoch_batch_data:
      _verify_batch(batch)

    # Check that individual batches are unshuffled
    # Check that shuffling occurred.
    self.assertNotAllClose(x, epoch_data)
    # Check that each elements appears, and only once.
    self.assertAllClose(x, np.sort(epoch_data))

    # Second epoch.
    second_epoch_batch_data = _get_epoch_batches(ds_iter)
    second_epoch_data = np.concatenate(second_epoch_batch_data)

    # Assert that the data within each batch remains contiguous
    for batch in second_epoch_batch_data:
      _verify_batch(batch)

    # Check that shuffling occurred.
    self.assertNotAllClose(x, second_epoch_data)
    # Check that shuffling is different across epochs.
    self.assertNotAllClose(epoch_data, second_epoch_data)
    # Check that each elements appears, and only once.
    self.assertAllClose(x, np.sort(second_epoch_data))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 94:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2646')" href="javascript:;">
keras-2.6.0/keras/engine/data_adapter_test.py: 464-477
</a>
<div class="mid" id="frag2646" style="display:none"><pre>
  def test_training_numpy_target(self):
    self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',
                       run_eagerly=testing_utils.should_run_eagerly())
    self.model.fit(self.arraylike_input,
                   self.numpy_target, batch_size=5)
    self.model.fit(self.arraylike_input,
                   self.numpy_target, shuffle=True,
                   batch_size=5)
    self.model.fit(self.arraylike_input,
                   self.numpy_target, shuffle='batch',
                   batch_size=5)
    self.model.evaluate(self.arraylike_input,
                        self.numpy_target, batch_size=5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2647')" href="javascript:;">
keras-2.6.0/keras/engine/data_adapter_test.py: 479-492
</a>
<div class="mid" id="frag2647" style="display:none"><pre>
  def test_training_tensor_target(self):
    self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',
                       run_eagerly=testing_utils.should_run_eagerly())
    self.model.fit(self.arraylike_input,
                   self.tensor_target, batch_size=5)
    self.model.fit(self.arraylike_input,
                   self.tensor_target, shuffle=True,
                   batch_size=5)
    self.model.fit(self.arraylike_input,
                   self.tensor_target, shuffle='batch',
                   batch_size=5)
    self.model.evaluate(self.arraylike_input,
                        self.tensor_target, batch_size=5)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 95:</b> &nbsp; 4 fragments, nominal size 12 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2683')" href="javascript:;">
keras-2.6.0/keras/engine/data_adapter_test.py: 783-797
</a>
<div class="mid" id="frag2683" style="display:none"><pre>
  def test_finite_dataset_with_steps_per_epoch(self):
    data = tf.data.Dataset.from_tensor_slices([0, 1, 2, 3]).batch(1)
    # User can choose to only partially consume `Dataset`.
    data_handler = data_adapter.DataHandler(
        data, initial_epoch=0, epochs=2, steps_per_epoch=2)
    self.assertEqual(data_handler.inferred_steps, 2)
    self.assertFalse(data_handler._adapter.should_recreate_iterator())
    returned_data = []
    for _, iterator in data_handler.enumerate_epochs():
      epoch_data = []
      for _ in data_handler.steps():
        epoch_data.append(next(iterator).numpy())
      returned_data.append(epoch_data)
    self.assertEqual(returned_data, [[0, 1], [2, 3]])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2685')" href="javascript:;">
keras-2.6.0/keras/engine/data_adapter_test.py: 810-824
</a>
<div class="mid" id="frag2685" style="display:none"><pre>
  def test_finite_dataset_with_steps_per_epoch_exact_size(self):
    data = tf.data.Dataset.from_tensor_slices([0, 1, 2, 3]).batch(1)
    # If user specifies exact size of `Dataset` as `steps_per_epoch`,
    # create a new iterator each epoch.
    data_handler = data_adapter.DataHandler(
        data, initial_epoch=0, epochs=2, steps_per_epoch=4)
    self.assertTrue(data_handler._adapter.should_recreate_iterator())
    returned_data = []
    for _, iterator in data_handler.enumerate_epochs():
      epoch_data = []
      for _ in data_handler.steps():
        epoch_data.append(next(iterator).numpy())
      returned_data.append(epoch_data)
    self.assertEqual(returned_data, [[0, 1, 2, 3], [0, 1, 2, 3]])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2686')" href="javascript:;">
keras-2.6.0/keras/engine/data_adapter_test.py: 825-836
</a>
<div class="mid" id="frag2686" style="display:none"><pre>
  def test_infinite_dataset_with_steps_per_epoch(self):
    data = tf.data.Dataset.from_tensor_slices([0, 1, 2]).batch(1).repeat()
    data_handler = data_adapter.DataHandler(
        data, initial_epoch=0, epochs=2, steps_per_epoch=3)
    returned_data = []
    for _, iterator in data_handler.enumerate_epochs():
      epoch_data = []
      for _ in data_handler.steps():
        epoch_data.append(next(iterator).numpy())
      returned_data.append(epoch_data)
    self.assertEqual(returned_data, [[0, 1, 2], [0, 1, 2]])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2684')" href="javascript:;">
keras-2.6.0/keras/engine/data_adapter_test.py: 798-809
</a>
<div class="mid" id="frag2684" style="display:none"><pre>
  def test_finite_dataset_without_steps_per_epoch(self):
    data = tf.data.Dataset.from_tensor_slices([0, 1, 2]).batch(1)
    data_handler = data_adapter.DataHandler(data, initial_epoch=0, epochs=2)
    self.assertEqual(data_handler.inferred_steps, 3)
    returned_data = []
    for _, iterator in data_handler.enumerate_epochs():
      epoch_data = []
      for _ in data_handler.steps():
        epoch_data.append(next(iterator).numpy())
      returned_data.append(epoch_data)
    self.assertEqual(returned_data, [[0, 1, 2], [0, 1, 2]])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 96:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2687')" href="javascript:;">
keras-2.6.0/keras/engine/data_adapter_test.py: 837-856
</a>
<div class="mid" id="frag2687" style="display:none"><pre>
  def test_unknown_cardinality_dataset_with_steps_per_epoch(self):
    ds = tf.data.Dataset.from_tensor_slices([0, 1, 2, 3, 4, 5, 6])
    filtered_ds = ds.filter(lambda x: x &lt; 4)
    self.assertEqual(
        tf.data.experimental.cardinality(filtered_ds).numpy(), tf.data.experimental.UNKNOWN_CARDINALITY)

    # User can choose to only partially consume `Dataset`.
    data_handler = data_adapter.DataHandler(
        filtered_ds, initial_epoch=0, epochs=2, steps_per_epoch=2)
    self.assertFalse(data_handler._adapter.should_recreate_iterator())
    returned_data = []
    for _, iterator in data_handler.enumerate_epochs():
      epoch_data = []
      for _ in data_handler.steps():
        epoch_data.append(next(iterator))
      returned_data.append(epoch_data)
    returned_data = self.evaluate(returned_data)
    self.assertEqual(returned_data, [[0, 1], [2, 3]])
    self.assertEqual(data_handler.inferred_steps, 2)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2688')" href="javascript:;">
keras-2.6.0/keras/engine/data_adapter_test.py: 857-877
</a>
<div class="mid" id="frag2688" style="display:none"><pre>
  def test_unknown_cardinality_dataset_without_steps_per_epoch(self):
    ds = tf.data.Dataset.from_tensor_slices([0, 1, 2, 3, 4, 5, 6])
    filtered_ds = ds.filter(lambda x: x &lt; 4)
    self.assertEqual(
        tf.data.experimental.cardinality(filtered_ds).numpy(), tf.data.experimental.UNKNOWN_CARDINALITY)

    data_handler = data_adapter.DataHandler(
        filtered_ds, initial_epoch=0, epochs=2)
    self.assertEqual(data_handler.inferred_steps, None)
    self.assertTrue(data_handler._adapter.should_recreate_iterator())
    returned_data = []
    for _, iterator in data_handler.enumerate_epochs():
      epoch_data = []
      with data_handler.catch_stop_iteration():
        for _ in data_handler.steps():
          epoch_data.append(next(iterator))
      returned_data.append(epoch_data)
    returned_data = self.evaluate(returned_data)
    self.assertEqual(returned_data, [[0, 1, 2, 3], [0, 1, 2, 3]])
    self.assertEqual(data_handler.inferred_steps, 4)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 97:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2691')" href="javascript:;">
keras-2.6.0/keras/engine/data_adapter_test.py: 911-929
</a>
<div class="mid" id="frag2691" style="display:none"><pre>
  def test_generator(self):

    def generator():
      for _ in range(2):
        for step in range(3):
          yield (tf.convert_to_tensor([step]),)

    data_handler = data_adapter.DataHandler(
        generator(), epochs=2, steps_per_epoch=3)
    returned_data = []
    for _, iterator in data_handler.enumerate_epochs():
      epoch_data = []
      for _ in data_handler.steps():
        epoch_data.append(next(iterator))
      returned_data.append(epoch_data)
    returned_data = self.evaluate(returned_data)
    self.assertEqual(returned_data, [[([0],), ([1],),
                                      ([2],)], [([0],), ([1],), ([2],)]])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2694')" href="javascript:;">
keras-2.6.0/keras/engine/data_adapter_test.py: 945-963
</a>
<div class="mid" id="frag2694" style="display:none"><pre>
  def test_iterator(self):
    def generator():
      for _ in range(2):
        for step in range(3):
          yield (tf.convert_to_tensor([step]),)

    it = iter(tf.data.Dataset.from_generator(
        generator, output_types=('float32',)))
    data_handler = data_adapter.DataHandler(it, epochs=2, steps_per_epoch=3)
    returned_data = []
    for _, iterator in data_handler.enumerate_epochs():
      epoch_data = []
      for _ in data_handler.steps():
        epoch_data.append(next(iterator))
      returned_data.append(epoch_data)
    returned_data = self.evaluate(returned_data)
    self.assertEqual(returned_data, [[([0],), ([1],), ([2],)],
                                     [([0],), ([1],), ([2],)]])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 98:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2709')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 112-132
</a>
<div class="mid" id="frag2709" style="display:none"><pre>
  def test_loss_partial_dict_with_output_names(self):
    loss_container = compile_utils.LossesContainer(
        {'out2': 'mae'}, {'out2': 1.}, output_names=['out1', 'out2'])

    y_t = [tf.ones((10, 1)), tf.zeros((10, 1))]
    y_p = [tf.ones((10, 1)), tf.ones((10, 1))]
    sw = tf.convert_to_tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

    total_loss = loss_container(y_t, y_p, sample_weight=sw)

    self.assertEqual(total_loss.numpy(), 0.5)
    self.assertLen(loss_container.metrics, 2)

    loss_metric = loss_container.metrics[0]
    self.assertEqual(loss_metric.name, 'loss')
    self.assertEqual(loss_metric.result().numpy(), 0.5)

    out2_metric = loss_container.metrics[1]
    self.assertEqual(out2_metric.name, 'out2_loss')
    self.assertEqual(out2_metric.result().numpy(), 0.5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2712')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 194-216
</a>
<div class="mid" id="frag2712" style="display:none"><pre>
  def test_broadcast_single_loss(self):
    loss_container = compile_utils.LossesContainer('mse')

    y_t = [tf.ones((10, 1)), tf.zeros((10, 1))]
    y_p = [tf.ones((10, 1)), tf.ones((10, 1))]
    sw = tf.convert_to_tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

    total_loss = loss_container(y_t, y_p, sample_weight=sw)
    self.assertEqual(total_loss.numpy(), 0.5)
    self.assertLen(loss_container.metrics, 3)

    loss_metric = loss_container.metrics[0]
    self.assertEqual(loss_metric.name, 'loss')
    self.assertEqual(loss_metric.result().numpy(), 0.5)

    output_1_metric = loss_container.metrics[1]
    self.assertEqual(output_1_metric.name, 'output_1_loss')
    self.assertEqual(output_1_metric.result().numpy(), 0.)

    output_2_metric = loss_container.metrics[2]
    self.assertEqual(output_2_metric.name, 'output_2_loss')
    self.assertEqual(output_2_metric.result().numpy(), 0.5)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 99:</b> &nbsp; 2 fragments, nominal size 28 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2711')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 156-193
</a>
<div class="mid" id="frag2711" style="display:none"><pre>
  def test_nested_structure(self):
    loss_container = compile_utils.LossesContainer(
        {
            'b': ['mse', None],
            'a': 'mae'
        }, loss_weights={
            'b': [0.5, 0],
            'a': 1
        })

    y_t = {
        'b': [tf.ones((10, 1)),
              tf.zeros((10, 1))],
        'a': tf.zeros((10, 1))
    }
    y_p = {
        'b': [tf.zeros((10, 1)),
              tf.zeros((10, 1))],
        'a': tf.ones((10, 1))
    }
    sw = tf.convert_to_tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

    total_loss = loss_container(y_t, y_p, sample_weight=sw)
    self.assertEqual(total_loss.numpy(), 0.75)
    self.assertLen(loss_container.metrics, 3)

    loss_metric = loss_container.metrics[0]
    self.assertEqual(loss_metric.name, 'loss')
    self.assertEqual(loss_metric.result().numpy(), 0.75)

    a_metric = loss_container.metrics[1]
    self.assertEqual(a_metric.name, 'a_loss')
    self.assertEqual(a_metric.result().numpy(), 0.5)

    b_1_metric = loss_container.metrics[2]
    self.assertEqual(b_1_metric.name, 'b_1_loss')
    self.assertEqual(b_1_metric.result().numpy(), 0.5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2735')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 544-581
</a>
<div class="mid" id="frag2735" style="display:none"><pre>
  def test_nested_structure(self):
    metric_container = compile_utils.MetricsContainer(
        metrics={
            'b': ['mse', None],
            'a': 'mae'
        },
        weighted_metrics={
            'b': [None, None],
            'a': 'mse'
        })

    y_t = {
        'b': [2 * tf.ones((10, 1)),
              tf.zeros((10, 1))],
        'a': tf.zeros((10, 1))
    }
    y_p = {
        'b': [tf.zeros((10, 1)),
              tf.zeros((10, 1))],
        'a': tf.ones((10, 1))
    }
    sw = tf.convert_to_tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

    metric_container.update_state(y_t, y_p, sample_weight=sw)
    self.assertLen(metric_container.metrics, 3)

    a_mae_metric = metric_container.metrics[0]
    self.assertEqual(a_mae_metric.name, 'a_mae')
    self.assertEqual(a_mae_metric.result().numpy(), 1.)

    weighted_a_mae_metric = metric_container.metrics[1]
    self.assertEqual(weighted_a_mae_metric.name, 'a_mse')
    self.assertEqual(weighted_a_mae_metric.result().numpy(), 1.)

    b_1_mse_metric = metric_container.metrics[2]
    self.assertEqual(b_1_mse_metric.name, 'b_1_mse')
    self.assertEqual(b_1_mse_metric.result().numpy(), 4.)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 100:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2713')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 217-250
</a>
<div class="mid" id="frag2713" style="display:none"><pre>
  def test_missing_label_with_no_loss(self):
    # It's ok to exclude a label if that label has no
    # losses or metrics associated with it.
    loss_container = compile_utils.LossesContainer({
        'output1': 'mse',
        'output3': 'mae'
    })

    y_p = {
        'output1': tf.convert_to_tensor([[0], [1], [2]]),
        'output2': tf.convert_to_tensor([[3], [4], [5]]),
        'output3': tf.convert_to_tensor([[6], [7], [8]])
    }
    y_t = {
        'output1': tf.convert_to_tensor([[1], [2], [3]]),
        'output3': tf.convert_to_tensor([[4], [5], [6]])
    }

    total_loss = loss_container(y_t, y_p)
    self.assertEqual(total_loss.numpy(), 3.)
    self.assertLen(loss_container.metrics, 3)

    loss_metric = loss_container.metrics[0]
    self.assertEqual(loss_metric.name, 'loss')
    self.assertEqual(loss_metric.result().numpy(), 3.)

    output_1_metric = loss_container.metrics[1]
    self.assertEqual(output_1_metric.name, 'output1_loss')
    self.assertEqual(output_1_metric.result().numpy(), 1.)

    output_3_metric = loss_container.metrics[2]
    self.assertEqual(output_3_metric.name, 'output3_loss')
    self.assertEqual(output_3_metric.result().numpy(), 2.)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2741')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 670-698
</a>
<div class="mid" id="frag2741" style="display:none"><pre>
  def test_missing_label_with_no_metrics(self):
    # It's ok to exclude a label if that label has no
    # losses or metrics associated with it.
    metric_container = compile_utils.MetricsContainer(metrics={
        'output1': 'mae',
        'output3': 'mse'
    })

    y_p = {
        'output1': tf.convert_to_tensor([[0], [1], [2]]),
        'output2': tf.convert_to_tensor([[3], [4], [5]]),
        'output3': tf.convert_to_tensor([[6], [7], [8]])
    }
    y_t = {
        'output1': tf.convert_to_tensor([[1], [2], [3]]),
        'output3': tf.convert_to_tensor([[4], [5], [6]])
    }

    metric_container.update_state(y_t, y_p)
    self.assertLen(metric_container.metrics, 2)

    mae_metric = metric_container.metrics[0]
    self.assertEqual(mae_metric.name, 'output1_mae')
    self.assertEqual(mae_metric.result().numpy(), 1.)

    mse_metric = metric_container.metrics[1]
    self.assertEqual(mse_metric.name, 'output3_mse')
    self.assertEqual(mse_metric.result().numpy(), 4.)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 101:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2714')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 251-266
</a>
<div class="mid" id="frag2714" style="display:none"><pre>
  def test_mismatched_dtypes(self):
    y_t = tf.constant([1, 9, 2, -5], shape=(2, 2))
    y_p = tf.constant([4, 8, 12, 8],
                               shape=(2, 2),
                               dtype=tf.float32)

    def my_mae(labels, preds):
      self.assertEqual(labels.dtype, tf.int32)
      self.assertEqual(preds.dtype, tf.float32)
      labels = tf.cast(labels, preds.dtype)
      return backend.mean(tf.abs(preds - labels), axis=-1)

    loss_container = compile_utils.LossesContainer(my_mae)
    total_loss = loss_container(y_t, y_p)
    self.assertEqual(total_loss.dtype, tf.float32)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2718')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 280-296
</a>
<div class="mid" id="frag2718" style="display:none"><pre>
  def test_float_dtypes(self):
    y_t = tf.constant([1, 9, 2, -5],
                               shape=(2, 2),
                               dtype=tf.float32)
    y_p = tf.constant([4, 8, 12, 8],
                               shape=(2, 2),
                               dtype=tf.float64)

    def my_mae(labels, preds):
      self.assertEqual(labels.dtype, tf.float64)
      self.assertEqual(preds.dtype, tf.float64)
      return backend.mean(tf.abs(preds - labels), axis=-1)

    loss_container = compile_utils.LossesContainer(my_mae)
    total_loss = loss_container(y_t, y_p)
    self.assertEqual(total_loss.dtype, tf.float64)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 102:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2720')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 297-311
</a>
<div class="mid" id="frag2720" style="display:none"><pre>
  def test_loss_masking(self):
    loss_container = compile_utils.LossesContainer('mae')
    y_p = tf.constant([[[1], [1]], [[0], [0]]], dtype=tf.float32)
    y_t = tf.constant([[[1], [1]], [[1], [1]]], dtype=tf.float32)
    y_p._keras_mask = tf.constant([[1, 0], [1, 0]],
                                           dtype=tf.float32)

    total_loss = loss_container(y_t, y_p)
    self.assertAlmostEqual(total_loss.numpy(), .25)  # sum over batch size

    self.assertLen(loss_container.metrics, 1)
    loss_metric = loss_container.metrics[0]
    self.assertEqual(loss_metric.name, 'loss')
    self.assertAlmostEqual(loss_metric.result().numpy(), .25)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2721')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 312-326
</a>
<div class="mid" id="frag2721" style="display:none"><pre>
  def test_loss_sample_weight(self):
    loss_container = compile_utils.LossesContainer('mae')
    y_p = tf.constant([[[1], [1]], [[0], [0]]], dtype=tf.float32)
    y_t = tf.constant([[[1], [1]], [[1], [1]]], dtype=tf.float32)
    sw = tf.constant([[.2, .3], [.5, 0]], dtype=tf.float32)

    total_loss = loss_container(y_t, y_p, sample_weight=sw)
    # (0 * .2 + 0 * .3 + 1 * .5 + 1 * 0) / 4
    self.assertAlmostEqual(total_loss.numpy(), .125)

    self.assertLen(loss_container.metrics, 1)
    loss_metric = loss_container.metrics[0]
    self.assertEqual(loss_metric.name, 'loss')
    self.assertAlmostEqual(loss_metric.result().numpy(), .125)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2722')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 327-343
</a>
<div class="mid" id="frag2722" style="display:none"><pre>
  def test_loss_masking_sample_weight(self):
    loss_container = compile_utils.LossesContainer('mae')
    y_p = tf.constant([[[1], [1]], [[0], [0]]], dtype=tf.float32)
    y_t = tf.constant([[[1], [1]], [[1], [1]]], dtype=tf.float32)
    sw = tf.constant([[.2, .3], [.5, 0]], dtype=tf.float32)
    y_p._keras_mask = tf.constant([[1, 0], [1, 0]],
                                           dtype=tf.float32)

    total_loss = loss_container(y_t, y_p, sample_weight=sw)
    # (0 * .2 + 1 * .5) / 4
    self.assertAlmostEqual(total_loss.numpy(), .125)  # sum over batch size

    self.assertLen(loss_container.metrics, 1)
    loss_metric = loss_container.metrics[0]
    self.assertEqual(loss_metric.name, 'loss')
    self.assertAlmostEqual(loss_metric.result().numpy(), .125)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 103:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2723')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 344-361
</a>
<div class="mid" id="frag2723" style="display:none"><pre>
  def test_custom_loss_callables(self):

    def custom_loss_fn(y_true, y_pred):
      return tf.reduce_sum(y_true - y_pred)

    class CustomLossClass(object):

      def __call__(self, y_true, y_pred):
        return tf.reduce_sum(y_true - y_pred)

    loss_container = compile_utils.LossesContainer(
        [custom_loss_fn, CustomLossClass()])
    y_t, y_p = tf.ones((10, 5)), tf.zeros((10, 5))
    loss_container(y_t, y_p)

    self.assertEqual(loss_container._losses[0].name, 'custom_loss_fn')
    self.assertEqual(loss_container._losses[1].name, 'custom_loss_class')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2746')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 769-786
</a>
<div class="mid" id="frag2746" style="display:none"><pre>
  def test_custom_metric_callables(self):

    def custom_metric_fn(y_true, y_pred):
      return tf.reduce_sum(y_true - y_pred)

    class CustomMetricClass(object):

      def __call__(self, y_true, y_pred):
        return tf.reduce_sum(y_true - y_pred)

    metric_container = compile_utils.MetricsContainer(
        [custom_metric_fn, CustomMetricClass()])
    y_t, y_p = tf.ones((10, 5)), tf.zeros((10, 5))
    metric_container.update_state(y_t, y_p)

    self.assertEqual(metric_container.metrics[0].name, 'custom_metric_fn')
    self.assertEqual(metric_container.metrics[1].name, 'custom_metric_class')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 104:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2736')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 582-601
</a>
<div class="mid" id="frag2736" style="display:none"><pre>
  def test_crossentropy(self):
    metric_container = compile_utils.MetricsContainer('crossentropy')
    y_t, y_p = tf.ones((10, 1)), tf.ones((10, 1))
    metric_container.update_state(y_t, y_p)
    self.assertEqual(metric_container.metrics[0]._fn,
                     metrics_mod.binary_crossentropy)

    metric_container = compile_utils.MetricsContainer('crossentropy')
    y_t, y_p = tf.ones((10, 1)), tf.ones((10, 20))
    self.assertEqual(y_p.shape.as_list()[-1], 20)
    metric_container.update_state(y_t, y_p)
    self.assertEqual(metric_container.metrics[0]._fn,
                     metrics_mod.sparse_categorical_crossentropy)

    metric_container = compile_utils.MetricsContainer('crossentropy')
    y_t, y_p = tf.ones((10, 20)), tf.ones((10, 20))
    metric_container.update_state(y_t, y_p)
    self.assertEqual(metric_container.metrics[0]._fn,
                     metrics_mod.categorical_crossentropy)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2737')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 602-627
</a>
<div class="mid" id="frag2737" style="display:none"><pre>
  def test_accuracy(self):
    metric_container = compile_utils.MetricsContainer('accuracy')
    y_t, y_p = tf.ones((10, 1)), tf.ones((10, 1))
    metric_container.update_state(y_t, y_p)
    self.assertEqual(metric_container.metrics[0]._fn,
                     metrics_mod.binary_accuracy)

    metric_container = compile_utils.MetricsContainer('Accuracy')
    y_t, y_p = tf.ones((10, 1)), tf.ones((10, 1))
    metric_container.update_state(y_t, y_p)
    self.assertEqual(metric_container.metrics[0]._fn,
                     metrics_mod.binary_accuracy)

    metric_container = compile_utils.MetricsContainer('accuracy')
    y_t, y_p = tf.ones((10, 1)), tf.ones((10, 20))
    self.assertEqual(y_p.shape.as_list()[-1], 20)
    metric_container.update_state(y_t, y_p)
    self.assertEqual(metric_container.metrics[0]._fn,
                     metrics_mod.sparse_categorical_accuracy)

    metric_container = compile_utils.MetricsContainer('accuracy')
    y_t, y_p = tf.ones((10, 20)), tf.ones((10, 20))
    metric_container.update_state(y_t, y_p)
    self.assertEqual(metric_container.metrics[0]._fn,
                     metrics_mod.categorical_accuracy)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 105:</b> &nbsp; 4 fragments, nominal size 14 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2738')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 628-646
</a>
<div class="mid" id="frag2738" style="display:none"><pre>
  def test_metric_weighting(self):
    metric_container = compile_utils.MetricsContainer(
        metrics=['mae'], weighted_metrics=['mae'])

    y_t = tf.convert_to_tensor([[0], [3], [0]])
    y_p = tf.convert_to_tensor([[0], [0], [0]])
    sw = tf.convert_to_tensor([[1], [0], [1]])

    metric_container.update_state(y_t, y_p, sample_weight=sw)
    self.assertLen(metric_container.metrics, 2)

    mae_metric = metric_container.metrics[0]
    self.assertEqual(mae_metric.name, 'mae')
    self.assertEqual(mae_metric.result().numpy(), 1.)

    weighted_mae_metric = metric_container.metrics[1]
    self.assertEqual(weighted_mae_metric.name, 'weighted_mae')
    self.assertEqual(weighted_mae_metric.result().numpy(), 0.)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2743')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 718-735
</a>
<div class="mid" id="frag2743" style="display:none"><pre>
  def test_metrics_sample_weight(self):
    metrics_container = compile_utils.MetricsContainer(
        metrics=['mae'], weighted_metrics=['mse'])
    y_p = tf.constant([[[1], [1]], [[0], [1]]], dtype=tf.float32)
    y_t = tf.constant([[[1], [1]], [[1], [1]]], dtype=tf.float32)
    sw = tf.constant([[.2, .3], [.5, 0]], dtype=tf.float32)

    metrics_container.update_state(y_t, y_p, sample_weight=sw)
    self.assertLen(metrics_container.metrics, 2)

    mae_metric = metrics_container.metrics[0]
    self.assertEqual(mae_metric.name, 'mae')
    self.assertAlmostEqual(mae_metric.result().numpy(), .25)  # 1 / 4

    weighted_mae_metric = metrics_container.metrics[1]
    self.assertEqual(weighted_mae_metric.name, 'mse')
    self.assertAlmostEqual(weighted_mae_metric.result().numpy(), .5)  # .5 / 1

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2744')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 736-755
</a>
<div class="mid" id="frag2744" style="display:none"><pre>
  def test_metrics_masking_sample_weight(self):
    metrics_container = compile_utils.MetricsContainer(
        metrics=['mae'], weighted_metrics=['mse'])
    y_p = tf.constant([[[1], [1]], [[0], [1]]], dtype=tf.float32)
    y_t = tf.constant([[[1], [1]], [[1], [1]]], dtype=tf.float32)
    sw = tf.constant([[.3, .2], [.2, .3]], dtype=tf.float32)
    y_p._keras_mask = tf.constant([[1, 0], [1, 0]],
                                           dtype=tf.float32)

    metrics_container.update_state(y_t, y_p, sample_weight=sw)
    self.assertLen(metrics_container.metrics, 2)

    mae_metric = metrics_container.metrics[0]
    self.assertEqual(mae_metric.name, 'mae')
    self.assertAlmostEqual(mae_metric.result().numpy(), .5)  # 1 / .5

    weighted_mae_metric = metrics_container.metrics[1]
    self.assertEqual(weighted_mae_metric.name, 'mse')
    self.assertAlmostEqual(weighted_mae_metric.result().numpy(), .2 / .5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2742')" href="javascript:;">
keras-2.6.0/keras/engine/compile_utils_test.py: 699-717
</a>
<div class="mid" id="frag2742" style="display:none"><pre>
  def test_metrics_masking(self):
    metrics_container = compile_utils.MetricsContainer(
        metrics=['mae'], weighted_metrics=['mse'])
    y_p = tf.constant([[[1], [1]], [[0], [0]]], dtype=tf.float32)
    y_t = tf.constant([[[1], [1]], [[1], [1]]], dtype=tf.float32)
    y_p._keras_mask = tf.constant([[1, 1], [0, 0]],
                                           dtype=tf.float32)

    metrics_container.update_state(y_t, y_p)
    self.assertLen(metrics_container.metrics, 2)

    mae_metric = metrics_container.metrics[0]
    self.assertEqual(mae_metric.name, 'mae')
    self.assertAlmostEqual(mae_metric.result().numpy(), 0)

    weighted_mae_metric = metrics_container.metrics[1]
    self.assertEqual(weighted_mae_metric.name, 'mse')
    self.assertAlmostEqual(weighted_mae_metric.result().numpy(), 0)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 106:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2770')" href="javascript:;">
keras-2.6.0/keras/engine/training_eager_test.py: 231-248
</a>
<div class="mid" id="frag2770" style="display:none"><pre>
  def test_loss_correctness(self, optimizer_kwargs):
    # Test that training loss is the same in eager and graph
    # (by comparing it to a reference value in a deterministic case)
    layers = [
        keras.layers.Dense(3, activation='relu',
                           kernel_initializer='ones'),
        keras.layers.Dense(2, activation='softmax', kernel_initializer='ones')]
    model = testing_utils.get_model_from_layers(layers, input_shape=(4,))
    model.compile(
        loss='sparse_categorical_crossentropy',
        optimizer=rmsprop.RMSprop(learning_rate=0.001, **optimizer_kwargs),
        run_eagerly=testing_utils.should_run_eagerly())
    x = np.ones((100, 4))
    np.random.seed(123)
    y = np.random.randint(0, 1, size=(100, 1))
    history = model.fit(x, y, epochs=1, batch_size=10)
    self.assertAlmostEqual(history.history['loss'][-1], 0.5836, 4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2771')" href="javascript:;">
keras-2.6.0/keras/engine/training_eager_test.py: 251-271
</a>
<div class="mid" id="frag2771" style="display:none"><pre>
  def test_loss_correctness_clipvalue_zero(self):
    # Test that training loss is the same in eager and graph
    # (by comparing it to a reference value in a deterministic case)
    # And confirm that setting clipvalue to zero stops all training
    layers = [
        keras.layers.Dense(3, activation='relu',
                           kernel_initializer='ones'),
        keras.layers.Dense(2, activation='softmax', kernel_initializer='ones')]
    model = testing_utils.get_model_from_layers(layers, input_shape=(4,))
    model.compile(
        loss='sparse_categorical_crossentropy',
        optimizer=rmsprop.RMSprop(learning_rate=0.001, clipvalue=0.0),
        run_eagerly=testing_utils.should_run_eagerly())
    x = np.ones((100, 4))
    np.random.seed(123)
    y = np.random.randint(0, 1, size=(100, 1))
    history = model.fit(x, y, epochs=3, batch_size=10)
    self.assertAlmostEqual(history.history['loss'][-3], 0.6931, 4)
    self.assertAlmostEqual(history.history['loss'][-2], 0.6931, 4)
    self.assertAlmostEqual(history.history['loss'][-1], 0.6931, 4)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 107:</b> &nbsp; 2 fragments, nominal size 40 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2785')" href="javascript:;">
keras-2.6.0/keras/engine/training_generator_v1.py: 547-587
</a>
<div class="mid" id="frag2785" style="display:none"><pre>
  def fit(self,
          model,
          x=None,
          y=None,
          batch_size=None,
          epochs=1,
          verbose=1,
          callbacks=None,
          validation_split=0.,
          validation_data=None,
          shuffle=True,
          class_weight=None,
          sample_weight=None,
          initial_epoch=0,
          steps_per_epoch=None,
          validation_steps=None,
          validation_freq=1,
          max_queue_size=10,
          workers=1,
          use_multiprocessing=False):
    model._validate_or_infer_batch_size(batch_size, steps_per_epoch, x)
    training_utils_v1.check_generator_arguments(
        y, sample_weight, validation_split=validation_split)
    return fit_generator(
        model,
        x,
        steps_per_epoch=steps_per_epoch,
        epochs=epochs,
        verbose=verbose,
        callbacks=callbacks,
        validation_data=validation_data,
        validation_steps=validation_steps,
        validation_freq=validation_freq,
        class_weight=class_weight,
        max_queue_size=max_queue_size,
        workers=workers,
        use_multiprocessing=use_multiprocessing,
        shuffle=shuffle,
        initial_epoch=initial_epoch,
        steps_name='steps_per_epoch')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2788')" href="javascript:;">
keras-2.6.0/keras/engine/training_generator_v1.py: 637-678
</a>
<div class="mid" id="frag2788" style="display:none"><pre>
  def fit(self,
          model,
          x=None,
          y=None,
          batch_size=None,
          epochs=1,
          verbose=1,
          callbacks=None,
          validation_split=0.,
          validation_data=None,
          shuffle=True,
          class_weight=None,
          sample_weight=None,
          initial_epoch=0,
          steps_per_epoch=None,
          validation_steps=None,
          validation_freq=1,
          **kwargs):
    model._validate_or_infer_batch_size(batch_size, steps_per_epoch, x)
    # Make sure that y, sample_weights, validation_split are not passed.
    training_utils_v1.validate_dataset_input(x, y, sample_weight,
                                             validation_split)
    if (isinstance(x, (tf.compat.v1.data.Dataset, tf.data.Dataset)) and
        shuffle):
      training_utils_v1.verify_dataset_shuffled(x)

    return fit_generator(
        model,
        x,
        steps_per_epoch=steps_per_epoch,
        epochs=epochs,
        verbose=verbose,
        callbacks=callbacks,
        validation_data=validation_data,
        validation_steps=validation_steps,
        validation_freq=validation_freq,
        class_weight=class_weight,
        workers=0,
        shuffle=shuffle,
        initial_epoch=initial_epoch,
        steps_name='steps_per_epoch')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 108:</b> &nbsp; 5 fragments, nominal size 21 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2786')" href="javascript:;">
keras-2.6.0/keras/engine/training_generator_v1.py: 588-611
</a>
<div class="mid" id="frag2786" style="display:none"><pre>
  def evaluate(self,
               model,
               x=None,
               y=None,
               batch_size=None,
               verbose=1,
               sample_weight=None,
               steps=None,
               callbacks=None,
               max_queue_size=10,
               workers=1,
               use_multiprocessing=False):
    model._validate_or_infer_batch_size(batch_size, steps, x)
    training_utils_v1.check_generator_arguments(y, sample_weight)
    return evaluate_generator(
        model,
        x,
        steps=steps,
        verbose=verbose,
        callbacks=callbacks,
        max_queue_size=max_queue_size,
        workers=workers,
        use_multiprocessing=use_multiprocessing)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3571')" href="javascript:;">
keras-2.6.0/keras/preprocessing/image.py: 833-897
</a>
<div class="mid" id="frag3571" style="display:none"><pre>
  def flow(self,
           x,
           y=None,
           batch_size=32,
           shuffle=True,
           sample_weight=None,
           seed=None,
           save_to_dir=None,
           save_prefix='',
           save_format='png',
           subset=None):
    """Takes data &amp; label arrays, generates batches of augmented data.

    Args:
        x: Input data. Numpy array of rank 4 or a tuple. If tuple, the first
          element should contain the images and the second element another numpy
          array or a list of numpy arrays that gets passed to the output without
          any modifications. Can be used to feed the model miscellaneous data
          along with the images. In case of grayscale data, the channels axis of
          the image array should have value 1, in case of RGB data, it should
          have value 3, and in case of RGBA data, it should have value 4.
        y: Labels.
        batch_size: Int (default: 32).
        shuffle: Boolean (default: True).
        sample_weight: Sample weights.
        seed: Int (default: None).
        save_to_dir: None or str (default: None). This allows you to optionally
          specify a directory to which to save the augmented pictures being
          generated (useful for visualizing what you are doing).
        save_prefix: Str (default: `''`). Prefix to use for filenames of saved
          pictures (only relevant if `save_to_dir` is set).
        save_format: one of "png", "jpeg", "bmp", "pdf", "ppm", "gif",
            "tif", "jpg"
            (only relevant if `save_to_dir` is set). Default: "png".
        subset: Subset of data (`"training"` or `"validation"`) if
          `validation_split` is set in `ImageDataGenerator`.

    Returns:
        An `Iterator` yielding tuples of `(x, y)`
            where `x` is a numpy array of image data
            (in the case of a single image input) or a list
            of numpy arrays (in the case with
            additional inputs) and `y` is a numpy array
            of corresponding labels. If 'sample_weight' is not None,
            the yielded tuples are of the form `(x, y, sample_weight)`.
            If `y` is None, only the numpy array `x` is returned.
    Raises:
      ValueError: If the Value of the argument, `subset` is other than
            "training" or "validation".

    """
    return NumpyArrayIterator(
        x,
        y,
        self,
        batch_size=batch_size,
        shuffle=shuffle,
        sample_weight=sample_weight,
        seed=seed,
        data_format=self.data_format,
        save_to_dir=save_to_dir,
        save_prefix=save_prefix,
        save_format=save_format,
        subset=subset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2787')" href="javascript:;">
keras-2.6.0/keras/engine/training_generator_v1.py: 612-633
</a>
<div class="mid" id="frag2787" style="display:none"><pre>
  def predict(self,
              model,
              x,
              batch_size=None,
              verbose=0,
              steps=None,
              callbacks=None,
              max_queue_size=10,
              workers=1,
              use_multiprocessing=False):
    model._validate_or_infer_batch_size(batch_size, steps, x)
    return predict_generator(
        model,
        x,
        steps=steps,
        verbose=verbose,
        callbacks=callbacks,
        max_queue_size=max_queue_size,
        workers=workers,
        use_multiprocessing=use_multiprocessing)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2793')" href="javascript:;">
keras-2.6.0/keras/engine/training_generator_v1.py: 806-824
</a>
<div class="mid" id="frag2793" style="display:none"><pre>
  def predict(self,
              model,
              x,
              batch_size=None,
              verbose=0,
              steps=None,
              callbacks=None,
              **kwargs):
    batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)
    x, _, _ = model._standardize_user_data(
        x, check_steps=True, steps_name='steps', steps=steps)
    return predict_generator(
        model,
        x,
        steps=steps,
        batch_size=batch_size,
        verbose=verbose,
        workers=0,
        callbacks=callbacks)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2854')" href="javascript:;">
keras-2.6.0/keras/engine/training_arrays_v1.py: 688-705
</a>
<div class="mid" id="frag2854" style="display:none"><pre>
  def predict(self,
              model,
              x,
              batch_size=None,
              verbose=0,
              steps=None,
              callbacks=None,
              **kwargs):
    batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)
    x, _, _ = model._standardize_user_data(
        x, check_steps=True, steps_name='steps', steps=steps)
    return predict_loop(
        model,
        x,
        batch_size=batch_size,
        verbose=verbose,
        steps=steps,
        callbacks=callbacks)
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 109:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2789')" href="javascript:;">
keras-2.6.0/keras/engine/training_generator_v1.py: 679-694
</a>
<div class="mid" id="frag2789" style="display:none"><pre>
  def evaluate(self,
               model,
               x=None,
               y=None,
               batch_size=None,
               verbose=1,
               sample_weight=None,
               steps=None,
               callbacks=None,
               **kwargs):
    model._validate_or_infer_batch_size(batch_size, steps, x)
    # Make sure that y, sample_weights, validation_split are not passed.
    training_utils_v1.validate_dataset_input(x, y, sample_weight)
    return evaluate_generator(
        model, x, steps=steps, verbose=verbose, workers=0, callbacks=callbacks)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2790')" href="javascript:;">
keras-2.6.0/keras/engine/training_generator_v1.py: 695-707
</a>
<div class="mid" id="frag2790" style="display:none"><pre>
  def predict(self,
              model,
              x,
              batch_size=None,
              verbose=0,
              steps=None,
              callbacks=None,
              **kwargs):
    model._validate_or_infer_batch_size(batch_size, steps, x)
    return predict_generator(
        model, x, steps=steps, verbose=verbose, workers=0, callbacks=callbacks)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 110:</b> &nbsp; 2 fragments, nominal size 59 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2791')" href="javascript:;">
keras-2.6.0/keras/engine/training_generator_v1.py: 717-778
</a>
<div class="mid" id="frag2791" style="display:none"><pre>
  def fit(self,
          model,
          x=None,
          y=None,
          batch_size=None,
          epochs=1,
          verbose=1,
          callbacks=None,
          validation_split=0.,
          validation_data=None,
          shuffle=True,
          class_weight=None,
          sample_weight=None,
          initial_epoch=0,
          steps_per_epoch=None,
          validation_steps=None,
          validation_freq=1,
          **kwargs):
    batch_size = model._validate_or_infer_batch_size(batch_size,
                                                     steps_per_epoch, x)
    x, y, sample_weights = model._standardize_user_data(
        x,
        y,
        sample_weight=sample_weight,
        class_weight=class_weight,
        batch_size=batch_size,
        check_steps=True,
        steps_name='steps_per_epoch',
        steps=steps_per_epoch,
        validation_split=validation_split,
        shuffle=shuffle)

    if validation_data:
      validation_data = model._prepare_validation_data(validation_data,
                                                       batch_size,
                                                       validation_steps)
    elif validation_split and 0. &lt; validation_split &lt; 1.:
      (x, y, sample_weights, val_x, val_y,
       val_sample_weights) = (
           training_utils_v1.split_training_and_validation_data(
               x, y, sample_weights, validation_split))
      validation_data = (val_x, val_y, val_sample_weights)
    else:
      if validation_steps:
        raise ValueError('`validation_steps` should not be specified if '
                         '`validation_data` is None.')

    return fit_generator(
        model, (x, y, sample_weights),
        steps_per_epoch=steps_per_epoch,
        batch_size=batch_size,
        epochs=epochs,
        verbose=verbose,
        callbacks=callbacks,
        validation_data=validation_data,
        validation_steps=validation_steps,
        validation_freq=validation_freq,
        workers=0,
        shuffle=shuffle,
        initial_epoch=initial_epoch,
        steps_name='steps_per_epoch')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2852')" href="javascript:;">
keras-2.6.0/keras/engine/training_arrays_v1.py: 594-658
</a>
<div class="mid" id="frag2852" style="display:none"><pre>
  def fit(self,
          model,
          x=None,
          y=None,
          batch_size=None,
          epochs=1,
          verbose=1,
          callbacks=None,
          validation_split=0.,
          validation_data=None,
          shuffle=True,
          class_weight=None,
          sample_weight=None,
          initial_epoch=0,
          steps_per_epoch=None,
          validation_steps=None,
          validation_freq=1,
          **kwargs):
    batch_size = model._validate_or_infer_batch_size(batch_size,
                                                     steps_per_epoch, x)

    x, y, sample_weights = model._standardize_user_data(
        x,
        y,
        sample_weight=sample_weight,
        class_weight=class_weight,
        batch_size=batch_size,
        check_steps=True,
        steps_name='steps_per_epoch',
        steps=steps_per_epoch,
        validation_split=validation_split,
        shuffle=shuffle)

    if validation_data:
      val_x, val_y, val_sample_weights = model._prepare_validation_data(
          validation_data, batch_size, validation_steps)
    elif validation_split and 0. &lt; validation_split &lt; 1.:
      (x, y, sample_weights, val_x, val_y, val_sample_weights
      ) = training_utils_v1.split_training_and_validation_data(
          x, y, sample_weights, validation_split)
    else:
      if validation_steps:
        raise ValueError('`validation_steps` should not be specified if '
                         '`validation_data` is None.')
      val_x, val_y, val_sample_weights = None, None, None

    return fit_loop(
        model,
        inputs=x,
        targets=y,
        sample_weights=sample_weights,
        batch_size=batch_size,
        epochs=epochs,
        verbose=verbose,
        callbacks=callbacks,
        val_inputs=val_x,
        val_targets=val_y,
        val_sample_weights=val_sample_weights,
        shuffle=shuffle,
        initial_epoch=initial_epoch,
        steps_per_epoch=steps_per_epoch,
        validation_steps=validation_steps,
        validation_freq=validation_freq,
        steps_name='steps_per_epoch')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 111:</b> &nbsp; 2 fragments, nominal size 27 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2792')" href="javascript:;">
keras-2.6.0/keras/engine/training_generator_v1.py: 779-805
</a>
<div class="mid" id="frag2792" style="display:none"><pre>
  def evaluate(self,
               model,
               x=None,
               y=None,
               batch_size=None,
               verbose=1,
               sample_weight=None,
               steps=None,
               callbacks=None,
               **kwargs):
    batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)
    x, y, sample_weights = model._standardize_user_data(
        x,
        y,
        sample_weight=sample_weight,
        batch_size=batch_size,
        check_steps=True,
        steps_name='steps',
        steps=steps)
    return evaluate_generator(
        model, (x, y, sample_weights),
        steps=steps,
        batch_size=batch_size,
        verbose=verbose,
        workers=0,
        callbacks=callbacks)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2853')" href="javascript:;">
keras-2.6.0/keras/engine/training_arrays_v1.py: 659-687
</a>
<div class="mid" id="frag2853" style="display:none"><pre>
  def evaluate(self,
               model,
               x=None,
               y=None,
               batch_size=None,
               verbose=1,
               sample_weight=None,
               steps=None,
               callbacks=None,
               **kwargs):
    batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)
    x, y, sample_weights = model._standardize_user_data(
        x,
        y,
        sample_weight=sample_weight,
        batch_size=batch_size,
        check_steps=True,
        steps_name='steps',
        steps=steps)
    return test_loop(
        model,
        inputs=x,
        targets=y,
        sample_weights=sample_weights,
        batch_size=batch_size,
        verbose=verbose,
        steps=steps,
        callbacks=callbacks)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 112:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2808')" href="javascript:;">
keras-2.6.0/keras/engine/input_layer_test.py: 162-180
</a>
<div class="mid" id="frag2808" style="display:none"><pre>
  def testInputTensorArgInTFFunction(self):
    # We use a mutable model container instead of a model python variable,
    # because python 2.7 does not have `nonlocal`
    model_container = {}

    @tf.function
    def run_model(inp):
      if not model_container:
        # Create a Keras Input
        x = input_layer_lib.Input(tensor=tf.zeros((10, 16)))
        self.assertAllEqual(x.shape.as_list(), [10, 16])

        # Verify you can construct and use a model w/ this input
        model_container['model'] = functional.Functional(x, x * 3.0)
      return model_container['model'](inp)

    self.assertAllEqual(run_model(tf.ones((10, 16))),
                        tf.ones((10, 16)) * 3.0)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2815')" href="javascript:;">
keras-2.6.0/keras/engine/input_layer_test.py: 269-288
</a>
<div class="mid" id="frag2815" style="display:none"><pre>
  def testTypeSpecArgInTFFunction(self):
    # We use a mutable model container instead of a model python variable,
    # because python 2.7 does not have `nonlocal`
    model_container = {}

    @tf.function
    def run_model(inp):
      if not model_container:
        # Create a Keras Input
        x = input_layer_lib.Input(
            type_spec=tf.TensorSpec((10, 16), tf.float32))
        self.assertAllEqual(x.shape.as_list(), [10, 16])

        # Verify you can construct and use a model w/ this input
        model_container['model'] = functional.Functional(x, x * 3.0)
      return model_container['model'](inp)

    self.assertAllEqual(run_model(tf.ones((10, 16))),
                        tf.ones((10, 16)) * 3.0)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 113:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2811')" href="javascript:;">
keras-2.6.0/keras/engine/input_layer_test.py: 197-218
</a>
<div class="mid" id="frag2811" style="display:none"><pre>
  def testCompositeInputTensorArgInTFFunction(self):
    # We use a mutable model container instead of a model python variable,
    # because python 2.7 does not have `nonlocal`
    model_container = {}

    @tf.function
    def run_model(inp):
      if not model_container:
        # Create a Keras Input
        rt = tf.RaggedTensor.from_row_splits(
            values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])
        x = input_layer_lib.Input(tensor=rt)

        # Verify you can construct and use a model w/ this input
        model_container['model'] = functional.Functional(x, x * 3)
      return model_container['model'](inp)

    # And verify the model works
    rt = tf.RaggedTensor.from_row_splits(
        values=[3, 21, 4, 1, 53, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])
    self.assertAllEqual(run_model(rt), rt * 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2818')" href="javascript:;">
keras-2.6.0/keras/engine/input_layer_test.py: 311-332
</a>
<div class="mid" id="frag2818" style="display:none"><pre>
  def testCompositeTypeSpecArgInTFFunction(self):
    # We use a mutable model container instead of a model pysthon variable,
    # because python 2.7 does not have `nonlocal`
    model_container = {}

    @tf.function
    def run_model(inp):
      if not model_container:
        # Create a Keras Input
        rt = tf.RaggedTensor.from_row_splits(
            values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])
        x = input_layer_lib.Input(type_spec=rt._type_spec)

        # Verify you can construct and use a model w/ this input
        model_container['model'] = functional.Functional(x, x * 3)
      return model_container['model'](inp)

    # And verify the model works
    rt = tf.RaggedTensor.from_row_splits(
        values=[3, 21, 4, 1, 53, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])
    self.assertAllEqual(run_model(rt), rt * 3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 114:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2835')" href="javascript:;">
keras-2.6.0/keras/engine/feature_columns_integration_test.py: 70-93
</a>
<div class="mid" id="frag2835" style="display:none"><pre>
  def test_sequential_model_with_ds_input(self):
    columns = [tf.feature_column.numeric_column('a')]
    model = keras.models.Sequential([
        df.DenseFeatures(columns),
        keras.layers.Dense(64, activation='relu'),
        keras.layers.Dense(20, activation='softmax')
    ])
    model.compile(
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'],
        run_eagerly=testing_utils.should_run_eagerly())

    y = np.random.randint(20, size=(100, 1))
    y = np_utils.to_categorical(y, num_classes=20)
    x = {'a': np.random.random((100, 1))}
    ds1 = tf.data.Dataset.from_tensor_slices(x)
    ds2 = tf.data.Dataset.from_tensor_slices(y)
    ds = tf.data.Dataset.zip((ds1, ds2)).batch(5)
    model.fit(ds, steps_per_epoch=1)
    model.fit(ds, steps_per_epoch=1)
    model.evaluate(ds, steps=1)
    model.predict(ds, steps=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2838')" href="javascript:;">
keras-2.6.0/keras/engine/feature_columns_integration_test.py: 155-178
</a>
<div class="mid" id="frag2838" style="display:none"><pre>
  def test_subclassed_model_with_feature_columns_with_ds_input(self):
    col_a = tf.feature_column.numeric_column('a')
    col_b = tf.feature_column.numeric_column('b')

    dnn_model = TestDNNModel([col_a, col_b], 20)

    dnn_model.compile(
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'],
        run_eagerly=testing_utils.should_run_eagerly())

    y = np.random.randint(20, size=(100, 1))
    y = np_utils.to_categorical(y, num_classes=20)
    x = {'a': np.random.random((100, 1)), 'b': np.random.random((100, 1))}
    ds1 = tf.data.Dataset.from_tensor_slices(x)
    ds2 = tf.data.Dataset.from_tensor_slices(y)
    ds = tf.data.Dataset.zip((ds1, ds2)).batch(5)
    dnn_model.fit(ds, steps_per_epoch=1)
    dnn_model.fit(ds, steps_per_epoch=1)
    dnn_model.evaluate(ds, steps=1)
    dnn_model.predict(ds, steps=1)

  # TODO(kaftan) seems to throw an error when enabled.
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 115:</b> &nbsp; 4 fragments, nominal size 15 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2859')" href="javascript:;">
keras-2.6.0/keras/feature_column/sequence_feature_column_test.py: 144-164
</a>
<div class="mid" id="frag2859" style="display:none"><pre>
  def test_embedding_column_with_non_sequence_categorical(self):
    """Tests that error is raised for non-sequence embedding column."""
    vocabulary_size = 3
    sparse_input = tf.compat.v1.SparseTensorValue(
        # example 0, ids [2]
        # example 1, ids [0, 1]
        indices=((0, 0), (1, 0), (1, 1)),
        values=(2, 0, 1),
        dense_shape=(2, 2))

    categorical_column_a = tf.feature_column.categorical_column_with_identity(
        key='aaa', num_buckets=vocabulary_size)
    embedding_column_a = tf.feature_column.embedding_column(
        categorical_column_a, dimension=2)
    sequence_input_layer = ksfc.SequenceFeatures([embedding_column_a])
    with self.assertRaisesRegex(
        ValueError,
        r'In embedding_column: aaa_embedding\. categorical_column must be of '
        r'type SequenceCategoricalColumn to use SequenceFeatures\.'):
      _, _ = sequence_input_layer({'aaa': sparse_input})

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2946')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 1097-1118
</a>
<div class="mid" id="frag2946" style="display:none"><pre>
  def test_indicator_column(self):
    """Tests that error is raised for sequence indicator column."""
    vocabulary_size = 3
    sparse_input = tf.compat.v1.SparseTensorValue(
        # example 0, ids [2]
        # example 1, ids [0, 1]
        indices=((0, 0), (1, 0), (1, 1)),
        values=(2, 0, 1),
        dense_shape=(2, 2))

    categorical_column_a = tf.feature_column.sequence_categorical_column_with_identity(
        key='aaa', num_buckets=vocabulary_size)
    indicator_column_a = tf.feature_column.indicator_column(categorical_column_a)

    input_layer = df.DenseFeatures([indicator_column_a])
    with self.assertRaisesRegex(
        ValueError,
        r'In indicator_column: aaa_indicator\. categorical_column must not be '
        r'of type SequenceCategoricalColumn\.'):
      _ = input_layer({'aaa': sparse_input})


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2865')" href="javascript:;">
keras-2.6.0/keras/feature_column/sequence_feature_column_test.py: 332-352
</a>
<div class="mid" id="frag2865" style="display:none"><pre>
  def test_indicator_column_with_non_sequence_categorical(self):
    """Tests that error is raised for non-sequence categorical column."""
    vocabulary_size = 3
    sparse_input = tf.compat.v1.SparseTensorValue(
        # example 0, ids [2]
        # example 1, ids [0, 1]
        indices=((0, 0), (1, 0), (1, 1)),
        values=(2, 0, 1),
        dense_shape=(2, 2))

    categorical_column_a = tf.feature_column.categorical_column_with_identity(
        key='aaa', num_buckets=vocabulary_size)
    indicator_column_a = tf.feature_column.indicator_column(categorical_column_a)

    sequence_input_layer = ksfc.SequenceFeatures([indicator_column_a])
    with self.assertRaisesRegex(
        ValueError,
        r'In indicator_column: aaa_indicator\. categorical_column must be of '
        r'type SequenceCategoricalColumn to use SequenceFeatures\.'):
      _, _ = sequence_input_layer({'aaa': sparse_input})

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2945')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 1075-1096
</a>
<div class="mid" id="frag2945" style="display:none"><pre>
  def test_embedding_column(self):
    """Tests that error is raised for sequence embedding column."""
    vocabulary_size = 3
    sparse_input = tf.compat.v1.SparseTensorValue(
        # example 0, ids [2]
        # example 1, ids [0, 1]
        indices=((0, 0), (1, 0), (1, 1)),
        values=(2, 0, 1),
        dense_shape=(2, 2))

    categorical_column_a = tf.feature_column.sequence_categorical_column_with_identity(
        key='aaa', num_buckets=vocabulary_size)
    embedding_column_a = tf.feature_column.embedding_column(
        categorical_column_a, dimension=2)

    input_layer = df.DenseFeatures([embedding_column_a])
    with self.assertRaisesRegex(
        ValueError,
        r'In embedding_column: aaa_embedding\. categorical_column must not be '
        r'of type SequenceCategoricalColumn\.'):
      _ = input_layer({'aaa': sparse_input})

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 116:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2876')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_v2.py: 61-89
</a>
<div class="mid" id="frag2876" style="display:none"><pre>
  def __init__(self,
               feature_columns,
               trainable=True,
               name=None,
               **kwargs):
    """Creates a DenseFeatures object.

    Args:
      feature_columns: An iterable containing the FeatureColumns to use as
        inputs to your model. All items should be instances of classes derived
        from `DenseColumn` such as `numeric_column`, `embedding_column`,
        `bucketized_column`, `indicator_column`. If you have categorical
        features, you can wrap them with an `embedding_column` or
        `indicator_column`.
      trainable:  Boolean, whether the layer's variables will be updated via
        gradient descent during training.
      name: Name to give to the DenseFeatures.
      **kwargs: Keyword arguments to construct a layer.

    Raises:
      ValueError: if an item in `feature_columns` is not a `DenseColumn`.
    """
    super(DenseFeatures, self).__init__(
        feature_columns=feature_columns,
        trainable=trainable,
        name=name,
        **kwargs)
    self._state_manager = _StateManagerImplV2(self, self.trainable)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2989')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features.py: 69-100
</a>
<div class="mid" id="frag2989" style="display:none"><pre>
  def __init__(self,
               feature_columns,
               trainable=True,
               name=None,
               partitioner=None,
               **kwargs):
    """Constructs a DenseFeatures layer.

    Args:
      feature_columns: An iterable containing the FeatureColumns to use as
        inputs to your model. All items should be instances of classes derived
        from `DenseColumn` such as `numeric_column`, `embedding_column`,
        `bucketized_column`, `indicator_column`. If you have categorical
        features, you can wrap them with an `embedding_column` or
        `indicator_column`.
      trainable:  Boolean, whether the layer's variables will be updated via
        gradient descent during training.
      name: Name to give to the DenseFeatures.
      partitioner: Partitioner for input layer. Defaults to None.
      **kwargs: Keyword arguments to construct a layer.

    Raises:
      ValueError: if an item in `feature_columns` is not a `DenseColumn`.
    """
    super(DenseFeatures, self).__init__(
        feature_columns=feature_columns,
        trainable=trainable,
        name=name,
        partitioner=partitioner,
        expected_column_type=tf.__internal__.feature_column.DenseColumn,
        **kwargs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2984')" href="javascript:;">
keras-2.6.0/keras/feature_column/sequence_feature_column.py: 81-108
</a>
<div class="mid" id="frag2984" style="display:none"><pre>
  def __init__(
      self,
      feature_columns,
      trainable=True,
      name=None,
      **kwargs):
    """"Constructs a SequenceFeatures layer.

    Args:
      feature_columns: An iterable of dense sequence columns. Valid columns are
        - `embedding_column` that wraps a `sequence_categorical_column_with_*`
        - `sequence_numeric_column`.
      trainable: Boolean, whether the layer's variables will be updated via
        gradient descent during training.
      name: Name to give to the SequenceFeatures.
      **kwargs: Keyword arguments to construct a layer.

    Raises:
      ValueError: If any of the `feature_columns` is not a
        `SequenceDenseColumn`.
    """
    super(SequenceFeatures, self).__init__(
        feature_columns=feature_columns,
        trainable=trainable,
        name=name,
        expected_column_type=tf.__internal__.feature_column.SequenceDenseColumn,
        **kwargs)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 117:</b> &nbsp; 4 fragments, nominal size 31 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2899')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 49-92
</a>
<div class="mid" id="frag2899" style="display:none"><pre>
  def test_reuses_variables(self):
    sparse_input = tf.SparseTensor(
        indices=((0, 0), (1, 0), (2, 0)),
        values=(0, 1, 2),
        dense_shape=(3, 3))

    # Create feature columns (categorical and embedding).
    categorical_column = tf.feature_column.categorical_column_with_identity(
        key='a', num_buckets=3)
    embedding_dimension = 2

    def _embedding_column_initializer(shape, dtype, partition_info=None):
      del shape  # unused
      del dtype  # unused
      del partition_info  # unused
      embedding_values = (
          (1, 0),  # id 0
          (0, 1),  # id 1
          (1, 1))  # id 2
      return embedding_values

    embedding_column = tf.feature_column.embedding_column(
        categorical_column,
        dimension=embedding_dimension,
        initializer=_embedding_column_initializer)

    dense_features = df.DenseFeatures([embedding_column])
    features = {'a': sparse_input}

    inputs = dense_features(features)
    variables = dense_features.variables

    # Sanity check: test that the inputs are correct.
    self.assertAllEqual([[1, 0], [0, 1], [1, 1]], inputs)

    # Check that only one variable was created.
    self.assertEqual(1, len(variables))

    # Check that invoking dense_features on the same features does not create
    # additional variables
    _ = dense_features(features)
    self.assertEqual(1, len(variables))
    self.assertIs(variables[0], dense_features.variables[0])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2903')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 146-190
</a>
<div class="mid" id="frag2903" style="display:none"><pre>
  def test_feature_column_dense_features_gradient(self):
    sparse_input = tf.SparseTensor(
        indices=((0, 0), (1, 0), (2, 0)),
        values=(0, 1, 2),
        dense_shape=(3, 3))

    # Create feature columns (categorical and embedding).
    categorical_column = tf.feature_column.categorical_column_with_identity(
        key='a', num_buckets=3)
    embedding_dimension = 2

    def _embedding_column_initializer(shape, dtype, partition_info=None):
      del shape  # unused
      del dtype  # unused
      del partition_info  # unused
      embedding_values = (
          (1, 0),  # id 0
          (0, 1),  # id 1
          (1, 1))  # id 2
      return embedding_values

    embedding_column = tf.feature_column.embedding_column(
        categorical_column,
        dimension=embedding_dimension,
        initializer=_embedding_column_initializer)

    dense_features = df.DenseFeatures([embedding_column])
    features = {'a': sparse_input}

    def scale_matrix():
      matrix = dense_features(features)
      return 2 * matrix

    # Sanity check: Verify that scale_matrix returns the correct output.
    self.assertAllEqual([[2, 0], [0, 2], [2, 2]], scale_matrix())

    # Check that the returned gradient is correct.
    grad_function = backprop.implicit_grad(scale_matrix)
    grads_and_vars = grad_function()
    indexed_slice = grads_and_vars[0][0]
    gradient = grads_and_vars[0][0].values

    self.assertAllEqual([0, 1, 2], indexed_slice.indices)
    self.assertAllEqual([[2, 2], [2, 2], [2, 2]], gradient)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2951')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_v2_test.py: 92-136
</a>
<div class="mid" id="frag2951" style="display:none"><pre>
  def test_feature_column_dense_features_gradient(self):
    sparse_input = tf.SparseTensor(
        indices=((0, 0), (1, 0), (2, 0)),
        values=(0, 1, 2),
        dense_shape=(3, 3))

    # Create feature columns (categorical and embedding).
    categorical_column = tf.feature_column.categorical_column_with_identity(
        key='a', num_buckets=3)
    embedding_dimension = 2

    def _embedding_column_initializer(shape, dtype, partition_info=None):
      del shape  # unused
      del dtype  # unused
      del partition_info  # unused
      embedding_values = (
          (1, 0),  # id 0
          (0, 1),  # id 1
          (1, 1))  # id 2
      return embedding_values

    embedding_column = tf.feature_column.embedding_column(
        categorical_column,
        dimension=embedding_dimension,
        initializer=_embedding_column_initializer)

    dense_features = df.DenseFeatures([embedding_column])
    features = {'a': sparse_input}

    def scale_matrix():
      matrix = dense_features(features)
      return 2 * matrix

    # Sanity check: Verify that scale_matrix returns the correct output.
    self.assertAllEqual([[2, 0], [0, 2], [2, 2]], scale_matrix())

    # Check that the returned gradient is correct.
    grad_function = backprop.implicit_grad(scale_matrix)
    grads_and_vars = grad_function()
    indexed_slice = grads_and_vars[0][0]
    gradient = grads_and_vars[0][0].values

    self.assertAllEqual([0, 1, 2], indexed_slice.indices)
    self.assertAllEqual([[2, 2], [2, 2], [2, 2]], gradient)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2949')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_v2_test.py: 47-90
</a>
<div class="mid" id="frag2949" style="display:none"><pre>
  def test_reuses_variables(self):
    sparse_input = tf.SparseTensor(
        indices=((0, 0), (1, 0), (2, 0)),
        values=(0, 1, 2),
        dense_shape=(3, 3))

    # Create feature columns (categorical and embedding).
    categorical_column = tf.feature_column.categorical_column_with_identity(
        key='a', num_buckets=3)
    embedding_dimension = 2

    def _embedding_column_initializer(shape, dtype, partition_info=None):
      del shape  # unused
      del dtype  # unused
      del partition_info  # unused
      embedding_values = (
          (1, 0),  # id 0
          (0, 1),  # id 1
          (1, 1))  # id 2
      return embedding_values

    embedding_column = tf.feature_column.embedding_column(
        categorical_column,
        dimension=embedding_dimension,
        initializer=_embedding_column_initializer)

    dense_features = df.DenseFeatures([embedding_column])
    features = {'a': sparse_input}

    inputs = dense_features(features)
    variables = dense_features.variables

    # Sanity check: test that the inputs are correct.
    self.assertAllEqual([[1, 0], [0, 1], [1, 1]], inputs)

    # Check that only one variable was created.
    self.assertEqual(1, len(variables))

    # Check that invoking dense_features on the same features does not create
    # additional variables
    _ = dense_features(features)
    self.assertEqual(1, len(variables))
    self.assertIs(variables[0], dense_features.variables[0])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 118:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2914')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 266-283
</a>
<div class="mid" id="frag2914" style="display:none"><pre>
  def test_compute_output_shape(self):
    price1 = tf.feature_column.numeric_column('price1', shape=2)
    price2 = tf.feature_column.numeric_column('price2', shape=4)
    with tf.Graph().as_default():
      features = {
          'price1': [[1., 2.], [5., 6.]],
          'price2': [[3., 4., 5., 6.], [7., 8., 9., 10.]]
      }
      dense_features = df.DenseFeatures([price1, price2])
      self.assertEqual((None, 6), dense_features.compute_output_shape((None,)))
      net = dense_features(features)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 2., 3., 4., 5., 6.], [5., 6., 7., 8., 9., 10.]],
                          self.evaluate(net))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2965')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_v2_test.py: 249-266
</a>
<div class="mid" id="frag2965" style="display:none"><pre>
  def test_compute_output_shape(self):
    price1 = tf.feature_column.numeric_column('price1', shape=2)
    price2 = tf.feature_column.numeric_column('price2', shape=4)
    with tf.Graph().as_default():
      features = {
          'price1': [[1., 2.], [5., 6.]],
          'price2': [[3., 4., 5., 6.], [7., 8., 9., 10.]]
      }
      dense_features = df.DenseFeatures([price1, price2])
      self.assertEqual((None, 6), dense_features.compute_output_shape((None,)))
      net = dense_features(features)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 2., 3., 4., 5., 6.], [5., 6., 7., 8., 9., 10.]],
                          self.evaluate(net))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 119:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2918')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 316-332
</a>
<div class="mid" id="frag2918" style="display:none"><pre>
  def test_cols_to_output_tensors(self):
    price1 = tf.feature_column.numeric_column('price1', shape=2)
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      cols_dict = {}
      features = {'price1': [[1., 2.], [5., 6.]], 'price2': [[3.], [4.]]}
      dense_features = df.DenseFeatures([price1, price2])
      net = dense_features(features, cols_dict)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 2.], [5., 6.]],
                          self.evaluate(cols_dict[price1]))
      self.assertAllClose([[3.], [4.]], self.evaluate(cols_dict[price2]))
      self.assertAllClose([[1., 2., 3.], [5., 6., 4.]], self.evaluate(net))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2969')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_v2_test.py: 299-315
</a>
<div class="mid" id="frag2969" style="display:none"><pre>
  def test_cols_to_output_tensors(self):
    price1 = tf.feature_column.numeric_column('price1', shape=2)
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      cols_dict = {}
      features = {'price1': [[1., 2.], [5., 6.]], 'price2': [[3.], [4.]]}
      dense_features = df.DenseFeatures([price1, price2])
      net = dense_features(features, cols_dict)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 2.], [5., 6.]],
                          self.evaluate(cols_dict[price1]))
      self.assertAllClose([[3.], [4.]], self.evaluate(cols_dict[price2]))
      self.assertAllClose([[1., 2., 3.], [5., 6., 4.]], self.evaluate(net))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 120:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2919')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 333-349
</a>
<div class="mid" id="frag2919" style="display:none"><pre>
  def test_column_order(self):
    price_a = tf.feature_column.numeric_column('price_a')
    price_b = tf.feature_column.numeric_column('price_b')
    with tf.Graph().as_default():
      features = {
          'price_a': [[1.]],
          'price_b': [[3.]],
      }
      net1 = df.DenseFeatures([price_a, price_b])(features)
      net2 = df.DenseFeatures([price_b, price_a])(features)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 3.]], self.evaluate(net1))
      self.assertAllClose([[1., 3.]], self.evaluate(net2))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2970')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_v2_test.py: 316-332
</a>
<div class="mid" id="frag2970" style="display:none"><pre>
  def test_column_order(self):
    price_a = tf.feature_column.numeric_column('price_a')
    price_b = tf.feature_column.numeric_column('price_b')
    with tf.Graph().as_default():
      features = {
          'price_a': [[1.]],
          'price_b': [[3.]],
      }
      net1 = df.DenseFeatures([price_a, price_b])(features)
      net2 = df.DenseFeatures([price_b, price_a])(features)

      self.evaluate(tf.compat.v1.global_variables_initializer())
      self.evaluate(tf.compat.v1.tables_initializer())

      self.assertAllClose([[1., 3.]], self.evaluate(net1))
      self.assertAllClose([[1., 3.]], self.evaluate(net2))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 121:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2921')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 361-373
</a>
<div class="mid" id="frag2921" style="display:none"><pre>
  def test_static_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': [[1.], [5.], [7.]],  # batchsize = 3
          'price2': [[3.], [4.]]  # batchsize = 2
      }
      with self.assertRaisesRegex(
          ValueError,
          r'Batch size \(first dimension\) of each feature must be same.'):  # pylint: disable=anomalous-backslash-in-string
        df.DenseFeatures([price1, price2])(features)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2972')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_v2_test.py: 344-356
</a>
<div class="mid" id="frag2972" style="display:none"><pre>
  def test_static_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': [[1.], [5.], [7.]],  # batchsize = 3
          'price2': [[3.], [4.]]  # batchsize = 2
      }
      with self.assertRaisesRegex(
          ValueError,
          r'Batch size \(first dimension\) of each feature must be same.'):  # pylint: disable=anomalous-backslash-in-string
        df.DenseFeatures([price1, price2])(features)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 122:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2922')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 374-388
</a>
<div class="mid" id="frag2922" style="display:none"><pre>
  def test_subset_of_static_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    price3 = tf.feature_column.numeric_column('price3')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 3
          'price2': [[3.], [4.]],  # batchsize = 2
          'price3': [[3.], [4.], [5.]]  # batchsize = 3
      }
      with self.assertRaisesRegex(
          ValueError,
          r'Batch size \(first dimension\) of each feature must be same.'):  # pylint: disable=anomalous-backslash-in-string
        df.DenseFeatures([price1, price2, price3])(features)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2973')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_v2_test.py: 357-371
</a>
<div class="mid" id="frag2973" style="display:none"><pre>
  def test_subset_of_static_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    price3 = tf.feature_column.numeric_column('price3')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 3
          'price2': [[3.], [4.]],  # batchsize = 2
          'price3': [[3.], [4.], [5.]]  # batchsize = 3
      }
      with self.assertRaisesRegex(
          ValueError,
          r'Batch size \(first dimension\) of each feature must be same.'):  # pylint: disable=anomalous-backslash-in-string
        df.DenseFeatures([price1, price2, price3])(features)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 123:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2923')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 389-402
</a>
<div class="mid" id="frag2923" style="display:none"><pre>
  def test_runtime_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 3
          'price2': [[3.], [4.]]  # batchsize = 2
      }
      net = df.DenseFeatures([price1, price2])(features)
      with _initialized_session() as sess:
        with self.assertRaisesRegex(tf.errors.OpError,
                                    'Dimensions of inputs should match'):
          sess.run(net, feed_dict={features['price1']: [[1.], [5.], [7.]]})

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2974')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_v2_test.py: 372-385
</a>
<div class="mid" id="frag2974" style="display:none"><pre>
  def test_runtime_batch_size_mismatch(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 3
          'price2': [[3.], [4.]]  # batchsize = 2
      }
      net = df.DenseFeatures([price1, price2])(features)
      with _initialized_session() as sess:
        with self.assertRaisesRegex(tf.errors.OpError,
                                    'Dimensions of inputs should match'):
          sess.run(net, feed_dict={features['price1']: [[1.], [5.], [7.]]})

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 124:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2924')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 403-419
</a>
<div class="mid" id="frag2924" style="display:none"><pre>
  def test_runtime_batch_size_matches(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 2
          'price2': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 2
      }
      net = df.DenseFeatures([price1, price2])(features)
      with _initialized_session() as sess:
        sess.run(
            net,
            feed_dict={
                features['price1']: [[1.], [5.]],
                features['price2']: [[1.], [5.]],
            })

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2975')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_v2_test.py: 386-402
</a>
<div class="mid" id="frag2975" style="display:none"><pre>
  def test_runtime_batch_size_matches(self):
    price1 = tf.feature_column.numeric_column('price1')
    price2 = tf.feature_column.numeric_column('price2')
    with tf.Graph().as_default():
      features = {
          'price1': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 2
          'price2': tf.compat.v1.placeholder(dtype=tf.int64),  # batchsize = 2
      }
      net = df.DenseFeatures([price1, price2])(features)
      with _initialized_session() as sess:
        sess.run(
            net,
            feed_dict={
                features['price1']: [[1.], [5.]],
                features['price2']: [[1.], [5.]],
            })

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 125:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2925')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 420-443
</a>
<div class="mid" id="frag2925" style="display:none"><pre>
  def test_multiple_layers_with_same_embedding_column(self):
    some_sparse_column = tf.feature_column.categorical_column_with_hash_bucket(
        'sparse_feature', hash_bucket_size=5)
    some_embedding_column = tf.feature_column.embedding_column(
        some_sparse_column, dimension=10)

    with tf.Graph().as_default():
      features = {
          'sparse_feature': [['a'], ['x']],
      }
      all_cols = [some_embedding_column]
      df.DenseFeatures(all_cols)(features)
      df.DenseFeatures(all_cols)(features)
      # Make sure that 2 variables get created in this case.
      self.assertEqual(2,
                       len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))
      expected_var_names = [
          'dense_features/sparse_feature_embedding/embedding_weights:0',
          'dense_features_1/sparse_feature_embedding/embedding_weights:0'
      ]
      self.assertItemsEqual(
          expected_var_names,
          [v.name for v in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2976')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_v2_test.py: 403-426
</a>
<div class="mid" id="frag2976" style="display:none"><pre>
  def test_multiple_layers_with_same_embedding_column(self):
    some_sparse_column = tf.feature_column.categorical_column_with_hash_bucket(
        'sparse_feature', hash_bucket_size=5)
    some_embedding_column = tf.feature_column.embedding_column(
        some_sparse_column, dimension=10)

    with tf.Graph().as_default():
      features = {
          'sparse_feature': [['a'], ['x']],
      }
      all_cols = [some_embedding_column]
      df.DenseFeatures(all_cols)(features)
      df.DenseFeatures(all_cols)(features)
      # Make sure that 2 variables get created in this case.
      self.assertEqual(2,
                       len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))
      expected_var_names = [
          'dense_features/sparse_feature_embedding/embedding_weights:0',
          'dense_features_1/sparse_feature_embedding/embedding_weights:0'
      ]
      self.assertItemsEqual(
          expected_var_names,
          [v.name for v in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 126:</b> &nbsp; 2 fragments, nominal size 30 lines, similarity 96%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2926')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 445-477
</a>
<div class="mid" id="frag2926" style="display:none"><pre>
  def test_multiple_layers_with_same_shared_embedding_column(self):
    categorical_column_a = tf.feature_column.categorical_column_with_identity(
        key='aaa', num_buckets=3)
    categorical_column_b = tf.feature_column.categorical_column_with_identity(
        key='bbb', num_buckets=3)
    embedding_dimension = 2
    embedding_column_b, embedding_column_a = tf.feature_column.shared_embeddings(
        [categorical_column_b, categorical_column_a],
        dimension=embedding_dimension)

    with tf.Graph().as_default():
      features = {
          'aaa':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(0, 1, 0),
                  dense_shape=(2, 2)),
          'bbb':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(1, 2, 1),
                  dense_shape=(2, 2)),
      }
      all_cols = [embedding_column_a, embedding_column_b]
      df.DenseFeatures(all_cols)(features)
      df.DenseFeatures(all_cols)(features)
      # Make sure that only 1 variable gets created in this case.
      self.assertEqual(1,
                       len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))
      self.assertItemsEqual(
          ['aaa_bbb_shared_embedding:0'],
          [v.name for v in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2977')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_v2_test.py: 427-460
</a>
<div class="mid" id="frag2977" style="display:none"><pre>
  def test_multiple_layers_with_same_shared_embedding_column(self):
    categorical_column_a = tf.feature_column.categorical_column_with_identity(
        key='aaa', num_buckets=3)
    categorical_column_b = tf.feature_column.categorical_column_with_identity(
        key='bbb', num_buckets=3)
    embedding_dimension = 2

    # feature_column.shared_embeddings is not supported in eager.
    with tf.Graph().as_default():
      embedding_column_b, embedding_column_a = tf.feature_column.shared_embeddings(
          [categorical_column_b, categorical_column_a],
          dimension=embedding_dimension)
      features = {
          'aaa':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(0, 1, 0),
                  dense_shape=(2, 2)),
          'bbb':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(1, 2, 1),
                  dense_shape=(2, 2)),
      }
      all_cols = [embedding_column_a, embedding_column_b]
      df.DenseFeatures(all_cols)(features)
      df.DenseFeatures(all_cols)(features)
      # Make sure that only 1 variable gets created in this case.
      self.assertEqual(1,
                       len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))
      self.assertItemsEqual(
          ['aaa_bbb_shared_embedding:0'],
          [v.name for v in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 127:</b> &nbsp; 2 fragments, nominal size 45 lines, similarity 97%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2927')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 479-529
</a>
<div class="mid" id="frag2927" style="display:none"><pre>
  def test_multiple_layers_with_same_shared_embedding_column_diff_graphs(self):
    categorical_column_a = tf.feature_column.categorical_column_with_identity(
        key='aaa', num_buckets=3)
    categorical_column_b = tf.feature_column.categorical_column_with_identity(
        key='bbb', num_buckets=3)
    embedding_dimension = 2
    embedding_column_b, embedding_column_a = tf.feature_column.shared_embeddings(
        [categorical_column_b, categorical_column_a],
        dimension=embedding_dimension)
    all_cols = [embedding_column_a, embedding_column_b]

    with tf.Graph().as_default():
      features = {
          'aaa':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(0, 1, 0),
                  dense_shape=(2, 2)),
          'bbb':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(1, 2, 1),
                  dense_shape=(2, 2)),
      }
      df.DenseFeatures(all_cols)(features)
      # Make sure that only 1 variable gets created in this case.
      self.assertEqual(1,
                       len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))

    with tf.Graph().as_default():
      features1 = {
          'aaa':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(0, 1, 0),
                  dense_shape=(2, 2)),
          'bbb':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(1, 2, 1),
                  dense_shape=(2, 2)),
      }

      df.DenseFeatures(all_cols)(features1)
      # Make sure that only 1 variable gets created in this case.
      self.assertEqual(1,
                       len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))
      self.assertItemsEqual(
          ['aaa_bbb_shared_embedding:0'],
          [v.name for v in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2978')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_v2_test.py: 461-512
</a>
<div class="mid" id="frag2978" style="display:none"><pre>
  def test_multiple_layers_with_same_shared_embedding_column_diff_graphs(self):
    categorical_column_a = tf.feature_column.categorical_column_with_identity(
        key='aaa', num_buckets=3)
    categorical_column_b = tf.feature_column.categorical_column_with_identity(
        key='bbb', num_buckets=3)
    embedding_dimension = 2

    # feature_column.shared_embeddings is not supported in eager.
    with tf.Graph().as_default():
      embedding_column_b, embedding_column_a = tf.feature_column.shared_embeddings(
          [categorical_column_b, categorical_column_a],
          dimension=embedding_dimension)
      all_cols = [embedding_column_a, embedding_column_b]
      features = {
          'aaa':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(0, 1, 0),
                  dense_shape=(2, 2)),
          'bbb':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(1, 2, 1),
                  dense_shape=(2, 2)),
      }
      df.DenseFeatures(all_cols)(features)
      # Make sure that only 1 variable gets created in this case.
      self.assertEqual(1,
                       len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))

    with tf.Graph().as_default():
      features1 = {
          'aaa':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(0, 1, 0),
                  dense_shape=(2, 2)),
          'bbb':
              tf.SparseTensor(
                  indices=((0, 0), (1, 0), (1, 1)),
                  values=(1, 2, 1),
                  dense_shape=(2, 2)),
      }

      df.DenseFeatures(all_cols)(features1)
      # Make sure that only 1 variable gets created in this case.
      self.assertEqual(1,
                       len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)))
      self.assertItemsEqual(
          ['aaa_bbb_shared_embedding:0'],
          [v.name for v in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 128:</b> &nbsp; 2 fragments, nominal size 41 lines, similarity 97%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2928')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 531-586
</a>
<div class="mid" id="frag2928" style="display:none"><pre>
  def test_with_1d_sparse_tensor(self):
    embedding_values = (
        (1., 2., 3., 4., 5.),  # id 0
        (6., 7., 8., 9., 10.),  # id 1
        (11., 12., 13., 14., 15.)  # id 2
    )

    def _initializer(shape, dtype, partition_info=None):
      del shape, dtype, partition_info
      return embedding_values

    # price has 1 dimension in dense_features
    price = tf.feature_column.numeric_column('price')

    # one_hot_body_style has 3 dims in dense_features.
    body_style = tf.feature_column.categorical_column_with_vocabulary_list(
        'body-style', vocabulary_list=['hardtop', 'wagon', 'sedan'])
    one_hot_body_style = tf.feature_column.indicator_column(body_style)

    # embedded_body_style has 5 dims in dense_features.
    country = tf.feature_column.categorical_column_with_vocabulary_list(
        'country', vocabulary_list=['US', 'JP', 'CA'])
    embedded_country = tf.feature_column.embedding_column(
        country, dimension=5, initializer=_initializer)

    # Provides 1-dim tensor and dense tensor.
    features = {
        'price':
            tf.constant([
                11.,
                12.,
            ]),
        'body-style':
            tf.SparseTensor(
                indices=((0,), (1,)),
                values=('sedan', 'hardtop'),
                dense_shape=(2,)),
        # This is dense tensor for the categorical_column.
        'country':
            tf.constant(['CA', 'US']),
    }
    self.assertEqual(1, features['price'].shape.ndims)
    self.assertEqual(1, features['body-style'].dense_shape.get_shape()[0])
    self.assertEqual(1, features['country'].shape.ndims)

    net = df.DenseFeatures([price, one_hot_body_style, embedded_country])(
        features)
    self.assertEqual(1 + 3 + 5, net.shape[1])
    with _initialized_session() as sess:

      # Each row is formed by concatenating `embedded_body_style`,
      # `one_hot_body_style`, and `price` in order.
      self.assertAllEqual([[0., 0., 1., 11., 12., 13., 14., 15., 11.],
                           [1., 0., 0., 1., 2., 3., 4., 5., 12.]],
                          sess.run(net))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2979')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_v2_test.py: 513-569
</a>
<div class="mid" id="frag2979" style="display:none"><pre>
  def test_with_1d_sparse_tensor(self):
    embedding_values = (
        (1., 2., 3., 4., 5.),  # id 0
        (6., 7., 8., 9., 10.),  # id 1
        (11., 12., 13., 14., 15.)  # id 2
    )

    def _initializer(shape, dtype, partition_info=None):
      del shape, dtype, partition_info
      return embedding_values

    # price has 1 dimension in dense_features
    price = tf.feature_column.numeric_column('price')

    # one_hot_body_style has 3 dims in dense_features.
    body_style = tf.feature_column.categorical_column_with_vocabulary_list(
        'body-style', vocabulary_list=['hardtop', 'wagon', 'sedan'])
    one_hot_body_style = tf.feature_column.indicator_column(body_style)

    # embedded_body_style has 5 dims in dense_features.
    country = tf.feature_column.categorical_column_with_vocabulary_list(
        'country', vocabulary_list=['US', 'JP', 'CA'])
    embedded_country = tf.feature_column.embedding_column(
        country, dimension=5, initializer=_initializer)

    with tf.Graph().as_default():
      # Provides 1-dim tensor and dense tensor.
      features = {
          'price':
              tf.constant([
                  11.,
                  12.,
              ]),
          'body-style':
              tf.SparseTensor(
                  indices=((0,), (1,)),
                  values=('sedan', 'hardtop'),
                  dense_shape=(2,)),
          # This is dense tensor for the categorical_column.
          'country':
              tf.constant(['CA', 'US']),
      }
      self.assertEqual(1, features['price'].shape.ndims)
      self.assertEqual(1, features['body-style'].dense_shape.get_shape()[0])
      self.assertEqual(1, features['country'].shape.ndims)

      net = df.DenseFeatures([price, one_hot_body_style, embedded_country])(
          features)
      self.assertEqual(1 + 3 + 5, net.shape[1])
      with _initialized_session() as sess:

        # Each row is formed by concatenating `embedded_body_style`,
        # `one_hot_body_style`, and `price` in order.
        self.assertAllEqual([[0., 0., 1., 11., 12., 13., 14., 15., 11.],
                             [1., 0., 0., 1., 2., 3., 4., 5., 12.]],
                            sess.run(net))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 129:</b> &nbsp; 2 fragments, nominal size 41 lines, similarity 97%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2930')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 588-645
</a>
<div class="mid" id="frag2930" style="display:none"><pre>
  def test_with_1d_unknown_shape_sparse_tensor(self):
    embedding_values = (
        (1., 2.),  # id 0
        (6., 7.),  # id 1
        (11., 12.)  # id 2
    )

    def _initializer(shape, dtype, partition_info=None):
      del shape, dtype, partition_info
      return embedding_values

    # price has 1 dimension in dense_features
    price = tf.feature_column.numeric_column('price')

    # one_hot_body_style has 3 dims in dense_features.
    body_style = tf.feature_column.categorical_column_with_vocabulary_list(
        'body-style', vocabulary_list=['hardtop', 'wagon', 'sedan'])
    one_hot_body_style = tf.feature_column.indicator_column(body_style)

    # embedded_body_style has 5 dims in dense_features.
    country = tf.feature_column.categorical_column_with_vocabulary_list(
        'country', vocabulary_list=['US', 'JP', 'CA'])
    embedded_country = tf.feature_column.embedding_column(
        country, dimension=2, initializer=_initializer)

    # Provides 1-dim tensor and dense tensor.
    features = {
        'price': tf.compat.v1.placeholder(tf.float32),
        'body-style': tf.compat.v1.sparse_placeholder(tf.string),
        # This is dense tensor for the categorical_column.
        'country': tf.compat.v1.placeholder(tf.string),
    }
    self.assertIsNone(features['price'].shape.ndims)
    self.assertIsNone(features['body-style'].get_shape().ndims)
    self.assertIsNone(features['country'].shape.ndims)

    price_data = np.array([11., 12.])
    body_style_data = tf.compat.v1.SparseTensorValue(
        indices=((0,), (1,)), values=('sedan', 'hardtop'), dense_shape=(2,))
    country_data = np.array([['US'], ['CA']])

    net = df.DenseFeatures([price, one_hot_body_style, embedded_country])(
        features)
    self.assertEqual(1 + 3 + 2, net.shape[1])
    with _initialized_session() as sess:

      # Each row is formed by concatenating `embedded_body_style`,
      # `one_hot_body_style`, and `price` in order.
      self.assertAllEqual(
          [[0., 0., 1., 1., 2., 11.], [1., 0., 0., 11., 12., 12.]],
          sess.run(
              net,
              feed_dict={
                  features['price']: price_data,
                  features['body-style']: body_style_data,
                  features['country']: country_data
              }))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2981')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_v2_test.py: 570-628
</a>
<div class="mid" id="frag2981" style="display:none"><pre>
  def test_with_1d_unknown_shape_sparse_tensor(self):
    embedding_values = (
        (1., 2.),  # id 0
        (6., 7.),  # id 1
        (11., 12.)  # id 2
    )

    def _initializer(shape, dtype, partition_info=None):
      del shape, dtype, partition_info
      return embedding_values

    # price has 1 dimension in dense_features
    price = tf.feature_column.numeric_column('price')

    # one_hot_body_style has 3 dims in dense_features.
    body_style = tf.feature_column.categorical_column_with_vocabulary_list(
        'body-style', vocabulary_list=['hardtop', 'wagon', 'sedan'])
    one_hot_body_style = tf.feature_column.indicator_column(body_style)

    # embedded_body_style has 5 dims in dense_features.
    country = tf.feature_column.categorical_column_with_vocabulary_list(
        'country', vocabulary_list=['US', 'JP', 'CA'])
    embedded_country = tf.feature_column.embedding_column(
        country, dimension=2, initializer=_initializer)

    # Provides 1-dim tensor and dense tensor.
    with tf.Graph().as_default():
      features = {
          'price': tf.compat.v1.placeholder(tf.float32),
          'body-style': tf.compat.v1.sparse_placeholder(tf.string),
          # This is dense tensor for the categorical_column.
          'country': tf.compat.v1.placeholder(tf.string),
      }
      self.assertIsNone(features['price'].shape.ndims)
      self.assertIsNone(features['body-style'].get_shape().ndims)
      self.assertIsNone(features['country'].shape.ndims)

      price_data = np.array([11., 12.])
      body_style_data = tf.compat.v1.SparseTensorValue(
          indices=((0,), (1,)), values=('sedan', 'hardtop'), dense_shape=(2,))
      country_data = np.array([['US'], ['CA']])

      net = df.DenseFeatures([price, one_hot_body_style, embedded_country])(
          features)
      self.assertEqual(1 + 3 + 2, net.shape[1])
      with _initialized_session() as sess:

        # Each row is formed by concatenating `embedded_body_style`,
        # `one_hot_body_style`, and `price` in order.
        self.assertAllEqual(
            [[0., 0., 1., 1., 2., 11.], [1., 0., 0., 11., 12., 12.]],
            sess.run(
                net,
                feed_dict={
                    features['price']: price_data,
                    features['body-style']: body_style_data,
                    features['country']: country_data
                }))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 130:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag2932')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_test.py: 647-669
</a>
<div class="mid" id="frag2932" style="display:none"><pre>
  def test_with_rank_0_feature(self):
    # price has 1 dimension in dense_features
    price = tf.feature_column.numeric_column('price')
    features = {
        'price': tf.constant(0),
    }
    self.assertEqual(0, features['price'].shape.ndims)

    # Static rank 0 should fail
    with self.assertRaisesRegex(ValueError, 'Feature .* cannot have rank 0'):
      df.DenseFeatures([price])(features)

    # Dynamic rank 0 should fail
    features = {
        'price': tf.compat.v1.placeholder(tf.float32),
    }
    net = df.DenseFeatures([price])(features)
    self.assertEqual(1, net.shape[1])
    with _initialized_session() as sess:
      with self.assertRaisesOpError('Feature .* cannot have rank 0'):
        sess.run(net, feed_dict={features['price']: np.array(1)})


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag2983')" href="javascript:;">
keras-2.6.0/keras/feature_column/dense_features_v2_test.py: 629-652
</a>
<div class="mid" id="frag2983" style="display:none"><pre>
  def test_with_rank_0_feature(self):
    # price has 1 dimension in dense_features
    price = tf.feature_column.numeric_column('price')
    features = {
        'price': tf.constant(0),
    }
    self.assertEqual(0, features['price'].shape.ndims)

    # Static rank 0 should fail
    with self.assertRaisesRegex(ValueError, 'Feature .* cannot have rank 0'):
      df.DenseFeatures([price])(features)

    with tf.Graph().as_default():
      # Dynamic rank 0 should fail
      features = {
          'price': tf.compat.v1.placeholder(tf.float32),
      }
      net = df.DenseFeatures([price])(features)
      self.assertEqual(1, net.shape[1])
      with _initialized_session() as sess:
        with self.assertRaisesOpError('Feature .* cannot have rank 0'):
          sess.run(net, feed_dict={features['price']: np.array(1)})


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 131:</b> &nbsp; 11 fragments, nominal size 31 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3012')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 307-349
</a>
<div class="mid" id="frag3012" style="display:none"><pre>
  def test3DInputAxis1(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=1, epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())

      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 4, 1))
      np_beta = np.reshape(np_beta, (1, 4, 1))

      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 2))
      std = np.std(np_inputs, axis=(0, 2))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3014')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 391-432
</a>
<div class="mid" id="frag3014" style="display:none"><pre>
  def test4DInputAxis1(self):
    if tf.test.is_gpu_available(cuda_only=True):
      epsilon = 1e-3
      bn = normalization_layers.BatchNormalization(
          axis=1, epsilon=epsilon, momentum=0.9)
      inputs = tf.Variable(
          np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)
      training = tf.compat.v1.placeholder(dtype='bool')
      outputs = bn.apply(inputs, training=training)

      with self.session() as sess:
        # Test training with placeholder learning phase.
        self.evaluate(tf.compat.v1.global_variables_initializer())
        np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
        np_gamma = np.reshape(np_gamma, (1, 4, 1, 1))
        np_beta = np.reshape(np_beta, (1, 4, 1, 1))
        for _ in range(100):
          np_output, _, _ = sess.run(
              [outputs] + bn.updates, feed_dict={training: True})
          # Verify that the axis is normalized during training.
          normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
          self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
          self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

        # Verify that the statistics are updated during training.
        moving_mean, moving_var = self.evaluate(
            [bn.moving_mean, bn.moving_variance])
        np_inputs = self.evaluate(inputs)
        mean = np.mean(np_inputs, axis=(0, 2, 3))
        std = np.std(np_inputs, axis=(0, 2, 3))
        variance = np.square(std)
        self.assertAllClose(mean, moving_mean, atol=1e-2)
        self.assertAllClose(variance, moving_var, atol=1e-2)

        # Test inference with placeholder learning phase.
        np_output = sess.run(outputs, feed_dict={training: False})

        # Verify that the axis is normalized during inference.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3041')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 1342-1382
</a>
<div class="mid" id="frag3041" style="display:none"><pre>
  def test5DInputMultiAxis123(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=[1, 2, 3], epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 3, 4, 4, 3)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())

      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])

      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 4), keepdims=True)
      std = np.std(np_inputs, axis=(0, 4), keepdims=True)
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3018')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 556-597
</a>
<div class="mid" id="frag3018" style="display:none"><pre>
  def test4DInputAxis1Fused(self):
    if tf.test.is_gpu_available(cuda_only=True):
      epsilon = 1e-3
      bn = normalization_layers.BatchNormalization(
          axis=1, epsilon=epsilon, momentum=0.9, fused=True)
      inputs = tf.Variable(
          np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)
      training = tf.compat.v1.placeholder(dtype='bool')
      outputs = bn.apply(inputs, training=training)

      with self.cached_session() as sess:
        # Test training with placeholder learning phase.
        self.evaluate(tf.compat.v1.global_variables_initializer())
        np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
        np_gamma = np.reshape(np_gamma, (1, 4, 1, 1))
        np_beta = np.reshape(np_beta, (1, 4, 1, 1))
        for _ in range(100):
          np_output, _, _ = sess.run(
              [outputs] + bn.updates, feed_dict={training: True})
          # Verify that the axis is normalized during training.
          normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
          self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
          self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

        # Verify that the statistics are updated during training.
        moving_mean, moving_var = self.evaluate(
            [bn.moving_mean, bn.moving_variance])
        np_inputs = self.evaluate(inputs)
        mean = np.mean(np_inputs, axis=(0, 2, 3))
        std = np.std(np_inputs, axis=(0, 2, 3))
        variance = np.square(std)
        self.assertAllClose(mean, moving_mean, atol=1e-2)
        self.assertAllClose(variance, moving_var, atol=1e-2)

        # Test inference with placeholder learning phase.
        np_output = sess.run(outputs, feed_dict={training: False})

        # Verify that the axis is normalized during inference.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3040')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 1301-1341
</a>
<div class="mid" id="frag3040" style="display:none"><pre>
  def test3DInputMultiAxis12(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=[1, 2], epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())

      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])

      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=0, keepdims=True)
      std = np.std(np_inputs, axis=0, keepdims=True)
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3015')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 433-473
</a>
<div class="mid" id="frag3015" style="display:none"><pre>
  def test4DInputAxis2(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=2, epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())
      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 1, 3, 1))
      np_beta = np.reshape(np_beta, (1, 1, 3, 1))
      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 1, 3))
      std = np.std(np_inputs, axis=(0, 1, 3))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3019')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 598-639
</a>
<div class="mid" id="frag3019" style="display:none"><pre>
  def testNegativeAxis(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=-1, epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())
      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 1, 1, 6))
      np_beta = np.reshape(np_beta, (1, 1, 1, 6))
      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})

        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 1, 2))
      std = np.std(np_inputs, axis=(0, 1, 2))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3017')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 515-555
</a>
<div class="mid" id="frag3017" style="display:none"><pre>
  def test4DInputAxis3Fused(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=3, epsilon=epsilon, momentum=0.9, fused=True)
    inputs = tf.Variable(
        np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())
      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 1, 1, 6))
      np_beta = np.reshape(np_beta, (1, 1, 1, 6))
      for _ in range(100):
        np_output, _, _ = sess.run(
            [outputs] + bn.updates, feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 1, 2))
      std = np.std(np_inputs, axis=(0, 1, 2))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3016')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 474-514
</a>
<div class="mid" id="frag3016" style="display:none"><pre>
  def test4DInputAxis3(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=3, epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())
      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 1, 1, 6))
      np_beta = np.reshape(np_beta, (1, 1, 1, 6))
      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 1, 2))
      std = np.std(np_inputs, axis=(0, 1, 2))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3013')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 350-390
</a>
<div class="mid" id="frag3013" style="display:none"><pre>
  def test3DInputAxis2(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=2, epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3)) + 100, dtype=tf.float32)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())
      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 1, 3))
      np_beta = np.reshape(np_beta, (1, 1, 3))
      for _ in range(100):
        np_output, _, _ = sess.run([outputs] + bn.updates,
                                   feed_dict={training: True})
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 1))
      std = np.std(np_inputs, axis=(0, 1))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = sess.run(outputs, feed_dict={training: False})

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3020')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 640-679
</a>
<div class="mid" id="frag3020" style="display:none"><pre>
  def testBooleanLearningPhase(self):
    epsilon = 1e-3
    bn = normalization_layers.BatchNormalization(
        axis=-1, epsilon=epsilon, momentum=0.9)
    inputs = tf.Variable(
        np.random.random((5, 4, 3, 6)) + 100, dtype=tf.float32)
    outputs_training = bn.apply(inputs, training=True)
    outputs_infer = bn.apply(inputs, training=False)

    with self.cached_session() as sess:
      # Test training with placeholder learning phase.
      self.evaluate(tf.compat.v1.global_variables_initializer())
      np_gamma, np_beta = self.evaluate([bn.gamma, bn.beta])
      np_gamma = np.reshape(np_gamma, (1, 1, 1, 6))
      np_beta = np.reshape(np_beta, (1, 1, 1, 6))
      for _ in range(100):
        np_output, _, _ = sess.run([outputs_training] + bn.updates)
        # Verify that the axis is normalized during training.
        normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
        self.assertAlmostEqual(np.mean(normed_np_output), 0., places=2)
        self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

      # Verify that the statistics are updated during training.
      moving_mean, moving_var = self.evaluate(
          [bn.moving_mean, bn.moving_variance])
      np_inputs = self.evaluate(inputs)
      mean = np.mean(np_inputs, axis=(0, 1, 2))
      std = np.std(np_inputs, axis=(0, 1, 2))
      variance = np.square(std)
      self.assertAllClose(mean, moving_mean, atol=1e-2)
      self.assertAllClose(variance, moving_var, atol=1e-2)

      # Test inference with placeholder learning phase.
      np_output = self.evaluate(outputs_infer)

      # Verify that the axis is normalized during inference.
      normed_np_output = ((np_output - epsilon) * np_gamma) + np_beta
      self.assertAlmostEqual(np.mean(normed_np_output), 0., places=1)
      self.assertAlmostEqual(np.std(normed_np_output), 1., places=1)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 132:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3024')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 810-824
</a>
<div class="mid" id="frag3024" style="display:none"><pre>
  def testNoCenter(self):
    bn = normalization_layers.BatchNormalization(axis=1, center=False)
    inputs = tf.random.uniform((5, 4, 3), seed=1)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    # Verify shape.
    self.assertListEqual(outputs.get_shape().as_list(), [5, 4, 3])

    # Verify layer attributes.
    self.assertEqual(len(bn.updates), 2)
    self.assertEqual(len(bn.variables), 3)
    self.assertEqual(len(bn.trainable_variables), 1)
    self.assertEqual(len(bn.non_trainable_variables), 2)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3025')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 825-839
</a>
<div class="mid" id="frag3025" style="display:none"><pre>
  def testNoScale(self):
    bn = normalization_layers.BatchNormalization(axis=1, scale=False)
    inputs = tf.random.uniform((5, 4, 3), seed=1)
    training = tf.compat.v1.placeholder(dtype='bool')
    outputs = bn.apply(inputs, training=training)

    # Verify shape.
    self.assertListEqual(outputs.get_shape().as_list(), [5, 4, 3])

    # Verify layer attributes.
    self.assertEqual(len(bn.updates), 2)
    self.assertEqual(len(bn.variables), 3)
    self.assertEqual(len(bn.trainable_variables), 1)
    self.assertEqual(len(bn.non_trainable_variables), 2)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 133:</b> &nbsp; 3 fragments, nominal size 49 lines, similarity 79%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3028')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 865-918
</a>
<div class="mid" id="frag3028" style="display:none"><pre>
  def testRenorm(self):
    shape = (4, 3)
    xt = tf.compat.v1.placeholder(tf.float32, shape)
    momentum = 0.99
    renorm_momentum = 0.8
    rmax = 1.1
    rmin = 0.9
    dmax = 0.1
    gamma = 2.
    beta = 3.
    epsilon = 0.001
    bn = normalization_layers.BatchNormalization(
        axis=1,
        gamma_initializer=tf.compat.v1.constant_initializer(gamma),
        beta_initializer=tf.compat.v1.constant_initializer(beta),
        epsilon=epsilon,
        momentum=momentum,
        renorm=True,
        renorm_clipping={'rmax': rmax, 'rmin': rmin, 'dmax': dmax},
        renorm_momentum=renorm_momentum)
    training = tf.compat.v1.placeholder(tf.bool)
    yt = bn.apply(xt, training=training)

    moving_mean = 0.
    moving_stddev = 1.
    renorm_mean = 0.
    renorm_stddev = 1.
    with self.session() as sess:
      self.evaluate(tf.compat.v1.global_variables_initializer())
      for _ in range(5):
        x = np.random.random(shape)

        mean = x.mean(0)
        variance = x.var(0)
        stddev = np.sqrt(variance + epsilon)
        r = (stddev / renorm_stddev).clip(rmin, rmax)
        d = ((mean - renorm_mean) / renorm_stddev).clip(-dmax, dmax)
        y_train = ((x - mean) / stddev * r + d) * gamma + beta
        renorm_mean += (mean - renorm_mean) * (1. - renorm_momentum)
        renorm_stddev += (stddev - renorm_stddev) * (1. - renorm_momentum)
        moving_mean += (mean - moving_mean) * (1. - momentum)
        moving_stddev += (stddev - moving_stddev) * (1. - momentum)

        y_test = ((x - moving_mean) /
                  (moving_stddev * moving_stddev)**0.5 * gamma) + beta

        yt_val_train, _, _ = sess.run([yt] + bn.updates,
                                      feed_dict={xt: x, training: True})
        yt_val_test, _, _ = sess.run([yt] + bn.updates,
                                     feed_dict={xt: x, training: False})

        self.assertAllClose(y_train, yt_val_train, atol=1e-5)
        self.assertAllClose(y_test, yt_val_test, atol=1e-5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3029')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 919-974
</a>
<div class="mid" id="frag3029" style="display:none"><pre>
  def testRenormNoClippingSameMomentumGivesSameTestTrain(self):
    shape = (4, 3)
    xt = tf.compat.v1.placeholder(tf.float32, shape)
    momentum = 0.9
    renorm_momentum = 0.9
    gamma = 2.
    beta = 3.
    epsilon = 0.001
    bn = normalization_layers.BatchNormalization(
        axis=1,
        gamma_initializer=tf.compat.v1.constant_initializer(gamma),
        beta_initializer=tf.compat.v1.constant_initializer(beta),
        epsilon=epsilon,
        momentum=momentum,
        renorm=True,
        renorm_clipping=None,
        renorm_momentum=momentum)
    training = tf.compat.v1.placeholder(tf.bool)
    yt = bn.apply(xt, training=training)
    moving_mean = 0.
    moving_stddev = 1.
    renorm_mean = 0.
    renorm_stddev = 1.
    with self.session() as sess:
      self.evaluate(tf.compat.v1.global_variables_initializer())
      for step in range(6):
        x = np.random.random(shape)

        mean = x.mean(0)
        variance = x.var(0)
        stddev = np.sqrt(variance + epsilon)
        r = (stddev / renorm_stddev)
        d = ((mean - renorm_mean) / renorm_stddev)
        y_test = ((x - moving_mean) /
                  (moving_stddev * moving_stddev)**0.5 * gamma) + beta
        y_train = ((x - mean) / stddev * r + d) * gamma + beta
        renorm_mean += (mean - renorm_mean) * (1. - renorm_momentum)
        renorm_stddev += (stddev - renorm_stddev) * (1. - renorm_momentum)
        moving_mean += (mean - moving_mean) * (1. - momentum)
        moving_stddev += (stddev - moving_stddev) * (1. - momentum)

        # Compute test values first, before the train mode updates the moving
        # averages.
        yt_val_test, _, _ = sess.run([yt] + bn.updates,
                                     feed_dict={xt: x, training: False})
        yt_val_train, _, _ = sess.run([yt] + bn.updates,
                                      feed_dict={xt: x, training: True})

        # Due to initialization inconsistencies, values may not be identical
        # on the first iteration (but shouldn't be different by much more than
        # epsilon). After the first iteration they should be identical.
        atol = epsilon * 1.5 if step == 0 else 1e-5
        self.assertAllClose(y_train, yt_val_train, atol=atol)
        self.assertAllClose(y_test, yt_val_test, atol=atol)
        self.assertAllClose(yt_val_train, yt_val_test, atol=atol)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3031')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 1019-1076
</a>
<div class="mid" id="frag3031" style="display:none"><pre>
  def testRenormWithAdjustment(self):
    shape = (4, 3)
    xt = tf.compat.v1.placeholder(tf.float32, shape)
    momentum = 0.99
    renorm_momentum = 0.8
    rmax = 1.1
    rmin = 0.9
    dmax = 0.1
    gamma = 2.
    beta = 3.
    epsilon = 0.001
    adjust_scale = tf.random.uniform(shape[-1:], 0.5, 1.5)
    adjust_bias = tf.random.uniform(shape[-1:], -.2, .2)
    bn = normalization_layers.BatchNormalization(
        axis=1,
        gamma_initializer=tf.compat.v1.constant_initializer(gamma),
        beta_initializer=tf.compat.v1.constant_initializer(beta),
        epsilon=epsilon,
        momentum=momentum,
        renorm=True,
        renorm_clipping={'rmax': rmax, 'rmin': rmin, 'dmax': dmax},
        renorm_momentum=renorm_momentum,
        adjustment=lambda _: (adjust_scale, adjust_bias))
    training = tf.compat.v1.placeholder(tf.bool)
    yt = bn.apply(xt, training=training)

    moving_mean = 0.
    moving_stddev = 1.
    renorm_mean = 0.
    renorm_stddev = 1.
    with self.session() as sess:
      self.evaluate(tf.compat.v1.global_variables_initializer())
      for _ in range(5):
        x = np.random.random(shape)
        yt_val_train, adj_scale_val, adj_bias_val = sess.run(
            [yt, adjust_scale, adjust_bias] + bn.updates,
            feed_dict={xt: x, training: True})[:3]
        yt_val_test = sess.run([yt] + bn.updates,
                               feed_dict={xt: x, training: False})[0]

        mean = x.mean(0)
        variance = x.var(0)
        stddev = np.sqrt(variance + epsilon)
        r = (stddev / renorm_stddev).clip(rmin, rmax)
        d = ((mean - renorm_mean) / renorm_stddev).clip(-dmax, dmax)
        y_train = (((x - mean) / stddev * r + d) * adj_scale_val +
                   adj_bias_val) * gamma + beta
        renorm_mean += (mean - renorm_mean) * (1. - renorm_momentum)
        renorm_stddev += (stddev - renorm_stddev) * (1. - renorm_momentum)
        moving_mean += (mean - moving_mean) * (1. - momentum)
        moving_stddev += (stddev - moving_stddev) * (1. - momentum)

        y_test = ((x - moving_mean) /
                  (moving_stddev * moving_stddev)**0.5 * gamma) + beta

        self.assertAllClose(y_train, yt_val_train, atol=1e-5)
        self.assertAllClose(y_test, yt_val_test, atol=1e-5)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 134:</b> &nbsp; 4 fragments, nominal size 43 lines, similarity 79%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3036')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 1125-1177
</a>
<div class="mid" id="frag3036" style="display:none"><pre>
  def testGhostBN2Dims(self):
    shape = [6, 2]
    virtual_batch_size = 3
    beta = 2.
    gamma = 3.
    momentum = 0.8
    epsilon = 1e-3
    moving_means = np.zeros([2, 2], dtype=np.float32)
    moving_vars = np.ones([2, 2], dtype=np.float32)

    inp = tf.compat.v1.placeholder(tf.float32, shape)
    is_training = tf.compat.v1.placeholder(tf.bool)
    bn = normalization_layers.BatchNormalization(
        momentum=momentum,
        epsilon=epsilon,
        beta_initializer=tf.compat.v1.constant_initializer(beta),
        gamma_initializer=tf.compat.v1.constant_initializer(gamma),
        virtual_batch_size=virtual_batch_size)
    out = bn.apply(inp, training=is_training)
    ghost_shape = ([virtual_batch_size,
                    shape[0] // virtual_batch_size,
                    shape[1]])

    with self.session() as sess:
      self.evaluate(tf.compat.v1.global_variables_initializer())
      for _ in range(5):
        x = np.random.random(shape)

        sub_batched = np.reshape(x, ghost_shape)
        means = np.mean(sub_batched, axis=0, keepdims=True)
        variances = np.var(sub_batched, axis=0, keepdims=True)

        avg_means = np.mean(means, axis=1, keepdims=True)
        avg_variances = np.mean(variances, axis=1, keepdims=True)

        moving_means = moving_means * momentum + avg_means * (1. - momentum)
        moving_vars = moving_vars * momentum + avg_variances * (1. - momentum)

        y_train = ((sub_batched - means) /
                   (variances + epsilon) ** 0.5 * gamma) + beta
        y_test = ((sub_batched - moving_means) /
                  (moving_vars + epsilon) ** 0.5 * gamma) + beta

        y_train = np.reshape(y_train, shape)
        y_test = np.reshape(y_test, shape)

        y_val_train, _, _ = sess.run([out] + bn.updates,
                                     feed_dict={inp: x, is_training: True})
        y_val_test = sess.run(out, feed_dict={inp: x, is_training: False})

        self.assertAllClose(y_train, y_val_train, atol=1e-5)
        self.assertAllClose(y_test, y_val_test, atol=1e-5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3038')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 1231-1284
</a>
<div class="mid" id="frag3038" style="display:none"><pre>
  def testGhostBN4DimsAxis1(self):
    shape = [6, 3, 10, 10]
    virtual_batch_size = 2
    beta = 2.
    gamma = 3.
    momentum = 0.8
    epsilon = 1e-3
    moving_means = np.zeros([1, 1, 3, 1, 1], dtype=np.float32)
    moving_vars = np.ones([1, 1, 3, 1, 1], dtype=np.float32)

    inp = tf.compat.v1.placeholder(tf.float32, shape)
    is_training = tf.compat.v1.placeholder(tf.bool)
    bn = normalization_layers.BatchNormalization(
        axis=1,
        momentum=momentum,
        epsilon=epsilon,
        beta_initializer=tf.compat.v1.constant_initializer(beta),
        gamma_initializer=tf.compat.v1.constant_initializer(gamma),
        virtual_batch_size=virtual_batch_size,
        fused=False)      # NCHW is unsupported by CPU fused batch norm
    out = bn.apply(inp, training=is_training)
    ghost_shape = ([virtual_batch_size, shape[0] // virtual_batch_size] +
                   shape[1:])

    with self.session() as sess:
      self.evaluate(tf.compat.v1.global_variables_initializer())
      for _ in range(5):
        x = np.random.random(shape)

        sub_batched = np.reshape(x, ghost_shape)
        means = np.mean(sub_batched, axis=(0, 3, 4), keepdims=True)
        variances = np.var(sub_batched, axis=(0, 3, 4), keepdims=True)

        avg_means = np.mean(means, axis=1, keepdims=True)
        avg_variances = np.mean(variances, axis=1, keepdims=True)

        moving_means = moving_means * momentum + avg_means * (1. - momentum)
        moving_vars = moving_vars * momentum + avg_variances * (1. - momentum)

        y_train = ((sub_batched - means) /
                   (variances + epsilon) ** 0.5 * gamma) + beta
        y_test = ((sub_batched - moving_means) /
                  (moving_vars + epsilon) ** 0.5 * gamma) + beta

        y_train = np.reshape(y_train, shape)
        y_test = np.reshape(y_test, shape)

        y_val_train, _, _ = sess.run([out] + bn.updates,
                                     feed_dict={inp: x, is_training: True})
        y_val_test = sess.run(out, feed_dict={inp: x, is_training: False})

        self.assertAllClose(y_train, y_val_train, atol=1e-2)
        self.assertAllClose(y_test, y_val_test, atol=1e-2)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3037')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 1178-1230
</a>
<div class="mid" id="frag3037" style="display:none"><pre>
  def testGhostBN4DimsAxis3(self):
    shape = [6, 10, 10, 3]
    virtual_batch_size = 2
    beta = 2.
    gamma = 3.
    momentum = 0.8
    epsilon = 1e-3
    moving_means = np.zeros([1, 1, 1, 1, 3], dtype=np.float32)
    moving_vars = np.ones([1, 1, 1, 1, 3], dtype=np.float32)

    inp = tf.compat.v1.placeholder(tf.float32, shape)
    is_training = tf.compat.v1.placeholder(tf.bool)
    bn = normalization_layers.BatchNormalization(
        axis=3,
        momentum=momentum,
        epsilon=epsilon,
        beta_initializer=tf.compat.v1.constant_initializer(beta),
        gamma_initializer=tf.compat.v1.constant_initializer(gamma),
        virtual_batch_size=virtual_batch_size)
    out = bn.apply(inp, training=is_training)
    ghost_shape = ([virtual_batch_size, shape[0] // virtual_batch_size] +
                   shape[1:])

    with self.session() as sess:
      self.evaluate(tf.compat.v1.global_variables_initializer())
      for _ in range(5):
        x = np.random.random(shape)

        sub_batched = np.reshape(x, ghost_shape)
        means = np.mean(sub_batched, axis=(0, 2, 3), keepdims=True)
        variances = np.var(sub_batched, axis=(0, 2, 3), keepdims=True)

        avg_means = np.mean(means, axis=1, keepdims=True)
        avg_variances = np.mean(variances, axis=1, keepdims=True)

        moving_means = moving_means * momentum + avg_means * (1. - momentum)
        moving_vars = moving_vars * momentum + avg_variances * (1. - momentum)

        y_train = ((sub_batched - means) /
                   (variances + epsilon) ** 0.5 * gamma) + beta
        y_test = ((sub_batched - moving_means) /
                  (moving_vars + epsilon) ** 0.5 * gamma) + beta

        y_train = np.reshape(y_train, shape)
        y_test = np.reshape(y_test, shape)

        y_val_train, _, _ = sess.run([out] + bn.updates,
                                     feed_dict={inp: x, is_training: True})
        y_val_test = sess.run(out, feed_dict={inp: x, is_training: False})

        self.assertAllClose(y_train, y_val_train, atol=1e-2)
        self.assertAllClose(y_test, y_val_test, atol=1e-2)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3042')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization_test.py: 1383-1437
</a>
<div class="mid" id="frag3042" style="display:none"><pre>
  def testGhostBN5DimsMultiAxis14(self):
    shape = [6, 3, 10, 10, 4]
    virtual_batch_size = 3
    beta = 2.
    gamma = 3.
    momentum = 0.8
    epsilon = 1e-3
    moving_means = np.zeros([1, 1, 3, 1, 1, 4], dtype=np.float32)
    moving_vars = np.ones([1, 1, 3, 1, 1, 4], dtype=np.float32)

    inp = tf.compat.v1.placeholder(tf.float32, shape)
    is_training = tf.compat.v1.placeholder(tf.bool)
    bn = normalization_layers.BatchNormalization(
        axis=[1, 4],
        momentum=momentum,
        epsilon=epsilon,
        beta_initializer=tf.compat.v1.constant_initializer(beta),
        gamma_initializer=tf.compat.v1.constant_initializer(gamma),
        virtual_batch_size=virtual_batch_size,
        fused=False)
    out = bn.apply(inp, training=is_training)
    ghost_shape = ([virtual_batch_size, shape[0] // virtual_batch_size] +
                   shape[1:])

    with self.session() as sess:
      self.evaluate(tf.compat.v1.global_variables_initializer())
      for _ in range(5):
        x = np.random.random(shape)

        sub_batched = np.reshape(x, ghost_shape)
        means = np.mean(sub_batched, axis=(0, 3, 4), keepdims=True)
        variances = np.var(sub_batched, axis=(0, 3, 4), keepdims=True)

        avg_means = np.mean(means, axis=1, keepdims=True)
        avg_variances = np.mean(variances, axis=1, keepdims=True)

        moving_means = moving_means * momentum + avg_means * (1. - momentum)
        moving_vars = moving_vars * momentum + avg_variances * (1. - momentum)

        y_train = ((sub_batched - means) /
                   (variances + epsilon) ** 0.5 * gamma) + beta
        y_test = ((sub_batched - moving_means) /
                  (moving_vars + epsilon) ** 0.5 * gamma) + beta

        y_train = np.reshape(y_train, shape)
        y_test = np.reshape(y_test, shape)

        y_val_train, _, _ = sess.run([out] + bn.updates,
                                     feed_dict={inp: x, is_training: True})
        y_val_test = sess.run(out, feed_dict={inp: x, is_training: False})

        self.assertAllClose(y_train, y_val_train, atol=1e-2)
        self.assertAllClose(y_test, y_val_test, atol=1e-2)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 135:</b> &nbsp; 30 fragments, nominal size 47 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3043')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization.py: 179-225
</a>
<div class="mid" id="frag3043" style="display:none"><pre>
  def __init__(self,
               axis=-1,
               momentum=0.99,
               epsilon=1e-3,
               center=True,
               scale=True,
               beta_initializer=tf.compat.v1.zeros_initializer(),
               gamma_initializer=tf.compat.v1.ones_initializer(),
               moving_mean_initializer=tf.compat.v1.zeros_initializer(),
               moving_variance_initializer=tf.compat.v1.ones_initializer(),
               beta_regularizer=None,
               gamma_regularizer=None,
               beta_constraint=None,
               gamma_constraint=None,
               renorm=False,
               renorm_clipping=None,
               renorm_momentum=0.99,
               fused=None,
               trainable=True,
               virtual_batch_size=None,
               adjustment=None,
               name=None,
               **kwargs):
    super(BatchNormalization, self).__init__(
        axis=axis,
        momentum=momentum,
        epsilon=epsilon,
        center=center,
        scale=scale,
        beta_initializer=beta_initializer,
        gamma_initializer=gamma_initializer,
        moving_mean_initializer=moving_mean_initializer,
        moving_variance_initializer=moving_variance_initializer,
        beta_regularizer=beta_regularizer,
        gamma_regularizer=gamma_regularizer,
        beta_constraint=beta_constraint,
        gamma_constraint=gamma_constraint,
        renorm=renorm,
        renorm_clipping=renorm_clipping,
        renorm_momentum=renorm_momentum,
        fused=fused,
        trainable=trainable,
        virtual_batch_size=virtual_batch_size,
        adjustment=adjustment,
        name=name,
        **kwargs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3091')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/convolutional.py: 1120-1273
</a>
<div class="mid" id="frag3091" style="display:none"><pre>
def separable_conv1d(inputs,
                     filters,
                     kernel_size,
                     strides=1,
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=1,
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 1D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A single integer specifying the spatial
      dimensions of the filters.
    strides: A single integer specifying the strides
      of the convolution.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: A single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.SeparableConv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn('`tf.layers.separable_conv1d` is deprecated and '
                'will be removed in a future version. '
                'Please Use `tf.keras.layers.SeparableConv1D` instead.')
  layer = SeparableConv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3045')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/normalization.py: 232-458
</a>
<div class="mid" id="frag3045" style="display:none"><pre>
def batch_normalization(inputs,
                        axis=-1,
                        momentum=0.99,
                        epsilon=1e-3,
                        center=True,
                        scale=True,
                        beta_initializer=tf.compat.v1.zeros_initializer(),
                        gamma_initializer=tf.compat.v1.ones_initializer(),
                        moving_mean_initializer=tf.compat.v1.zeros_initializer(),
                        moving_variance_initializer=tf.compat.v1.ones_initializer(),
                        beta_regularizer=None,
                        gamma_regularizer=None,
                        beta_constraint=None,
                        gamma_constraint=None,
                        training=False,
                        trainable=True,
                        name=None,
                        reuse=None,
                        renorm=False,
                        renorm_clipping=None,
                        renorm_momentum=0.99,
                        fused=None,
                        virtual_batch_size=None,
                        adjustment=None):
  """Functional interface for the batch normalization layer from_config(Ioffe et al., 2015).

  Note: when training, the moving_mean and moving_variance need to be updated.
  By default the update ops are placed in `tf.GraphKeys.UPDATE_OPS`, so they
  need to be executed alongside the `train_op`. Also, be sure to add any
  batch_normalization ops before getting the update_ops collection. Otherwise,
  update_ops will be empty, and training/inference will not work properly. For
  example:

  ```python
    x_norm = tf.compat.v1.layers.batch_normalization(x, training=training)

    # ...

    update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)
    train_op = optimizer.minimize(loss)
    train_op = tf.group([train_op, update_ops])
  ```

  Args:
    inputs: Tensor input.
    axis: An `int`, the axis that should be normalized (typically the features
      axis). For instance, after a `Convolution2D` layer with
      `data_format="channels_first"`, set `axis=1` in `BatchNormalization`.
    momentum: Momentum for the moving average.
    epsilon: Small float added to variance to avoid dividing by zero.
    center: If True, add offset of `beta` to normalized tensor. If False, `beta`
      is ignored.
    scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the
      next layer is linear (also e.g. `nn.relu`), this can be disabled since the
      scaling can be done by the next layer.
    beta_initializer: Initializer for the beta weight.
    gamma_initializer: Initializer for the gamma weight.
    moving_mean_initializer: Initializer for the moving mean.
    moving_variance_initializer: Initializer for the moving variance.
    beta_regularizer: Optional regularizer for the beta weight.
    gamma_regularizer: Optional regularizer for the gamma weight.
    beta_constraint: An optional projection function to be applied to the `beta`
      weight after being updated by an `Optimizer` (e.g. used to implement norm
      constraints or value constraints for layer weights). The function must
      take as input the unprojected variable and must return the projected
      variable (which must have the same shape). Constraints are not safe to use
      when doing asynchronous distributed training.
    gamma_constraint: An optional projection function to be applied to the
      `gamma` weight after being updated by an `Optimizer`.
    training: Either a Python boolean, or a TensorFlow boolean scalar tensor
      (e.g. a placeholder). Whether to return the output in training mode
      (normalized with statistics of the current batch) or in inference mode
      (normalized with moving statistics). **NOTE**: make sure to set this
        parameter correctly, or else your training/inference will not work
        properly.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).
    name: String, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer by the same
      name.
    renorm: Whether to use Batch Renormalization (Ioffe, 2017). This adds extra
      variables during training. The inference is the same for either value of
      this parameter.
    renorm_clipping: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to
      scalar `Tensors` used to clip the renorm correction. The correction `(r,
      d)` is used as `corrected_value = normalized_value * r + d`, with `r`
      clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,
      dmax are set to inf, 0, inf, respectively.
    renorm_momentum: Momentum used to update the moving means and standard
      deviations with renorm. Unlike `momentum`, this affects training and
      should be neither too small (which would add noise) nor too large (which
      would give stale estimates). Note that `momentum` is still applied to get
      the means and variances for inference.
    fused: if `None` or `True`, use a faster, fused implementation if possible.
      If `False`, use the system recommended implementation.
    virtual_batch_size: An `int`. By default, `virtual_batch_size` is `None`,
      which means batch normalization is performed across the whole batch. When
      `virtual_batch_size` is not `None`, instead perform "Ghost Batch
      Normalization", which creates virtual sub-batches which are each
      normalized separately (with shared gamma, beta, and moving statistics).
      Must divide the actual batch size during execution.
    adjustment: A function taking the `Tensor` containing the (dynamic) shape of
      the input tensor and returning a pair (scale, bias) to apply to the
      normalized values (before gamma and beta), only during training. For
      example, if axis==-1,
        `adjustment = lambda shape: (
          tf.random.uniform(shape[-1:], 0.93, 1.07),
          tf.random.uniform(shape[-1:], -0.1, 0.1))` will scale the normalized
            value by up to 7% up or down, then shift the result by up to 0.1
            (with independent scaling and bias for each feature but shared
            across all examples), and finally apply gamma and/or beta. If
            `None`, no adjustment is applied. Cannot be specified if
            virtual_batch_size is specified.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.

  References:
    Batch Normalization - Accelerating Deep Network Training by Reducing
    Internal Covariate Shift:
      [Ioffe et al., 2015](http://proceedings.mlr.press/v37/ioffe15.html)
      ([pdf](http://proceedings.mlr.press/v37/ioffe15.pdf))
    Batch Renormalization - Towards Reducing Minibatch Dependence in
    Batch-Normalized Models:
      [Ioffe,
      2017](http://papers.nips.cc/paper/6790-batch-renormalization-towards-reducing-minibatch-dependence-in-batch-normalized-models)
      ([pdf](http://papers.nips.cc/paper/6790-batch-renormalization-towards-reducing-minibatch-dependence-in-batch-normalized-models.pdf))

  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.BatchNormalization`.

  The batch updating pattern with
  `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used in
  native TF2. Consult the `tf.keras.layers.BatchNormalization` documentation
  for further information.

  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   x_norm = tf.compat.v1.layers.batch_normalization(x)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input(shape=(28, 28, 1),)
   y = tf.keras.layers.BatchNormalization()(x)
   model = tf.keras.Model(x, y)
  ```
  #### How to Map Arguments

  TF1 Arg Name              | TF2 Arg Name              | Note
  :------------------------ | :------------------------ | :---------------
  `name`                    | `name`                    | Layer base class
  `trainable`               | `trainable`               | Layer base class
  `axis`                    | `axis`                    | -
  `momentum`                | `momentum`                | -
  `epsilon`                 | `epsilon`                 | -
  `center`                  | `center`                  | -
  `scale`                   | `scale`                   | -
  `beta_initializer`        | `beta_initializer`        | -
  `gamma_initializer`       | `gamma_initializer`       | -
  `moving_mean_initializer` | `moving_mean_initializer` | -
  `beta_regularizer`        | `beta_regularizer'        | -
  `gamma_regularizer`       | `gamma_regularizer'       | -
  `beta_constraint`         | `beta_constraint'         | -
  `gamma_constraint`        | `gamma_constraint'        | -
  `renorm`                  | Not supported             | -
  `renorm_clipping`         | Not supported             | -
  `renorm_momentum`         | Not supported             | -
  `fused`                   | Not supported             | -
  `virtual_batch_size`      | Not supported             | -
  `adjustment`              | Not supported             | -

  @end_compatibility
  """
  warnings.warn(
      '`tf.layers.batch_normalization` is deprecated and '
      'will be removed in a future version. '
      'Please use `tf.keras.layers.BatchNormalization` instead. '
      'In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` '
      'should not be used (consult the `tf.keras.layers.BatchNormalization` '
      'documentation).')
  layer = BatchNormalization(
      axis=axis,
      momentum=momentum,
      epsilon=epsilon,
      center=center,
      scale=scale,
      beta_initializer=beta_initializer,
      gamma_initializer=gamma_initializer,
      moving_mean_initializer=moving_mean_initializer,
      moving_variance_initializer=moving_variance_initializer,
      beta_regularizer=beta_regularizer,
      gamma_regularizer=gamma_regularizer,
      beta_constraint=beta_constraint,
      gamma_constraint=gamma_constraint,
      renorm=renorm,
      renorm_clipping=renorm_clipping,
      renorm_momentum=renorm_momentum,
      fused=fused,
      trainable=trainable,
      virtual_batch_size=virtual_batch_size,
      adjustment=adjustment,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs, training=training)


# Aliases

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3092')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/convolutional.py: 1276-1434
</a>
<div class="mid" id="frag3092" style="display:none"><pre>
def separable_conv2d(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     dilation_rate=(1, 1),
                     depth_multiplier=1,
                     activation=None,
                     use_bias=True,
                     depthwise_initializer=None,
                     pointwise_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     depthwise_regularizer=None,
                     pointwise_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     depthwise_constraint=None,
                     pointwise_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for the depthwise separable 2D convolution layer.

  This layer performs a depthwise convolution that acts separately on
  channels, followed by a pointwise convolution that mixes channels.
  If `use_bias` is True and a bias initializer is provided,
  it adds a bias vector to the output.
  It then optionally applies an activation function to produce the final output.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any `stride` value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    depth_multiplier: The number of depthwise convolution output channels for
      each input channel. The total number of depthwise convolution output
      channels will be equal to `num_filters_in * depth_multiplier`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    depthwise_initializer: An initializer for the depthwise convolution kernel.
    pointwise_initializer: An initializer for the pointwise convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    depthwise_regularizer: Optional regularizer for the depthwise
      convolution kernel.
    pointwise_regularizer: Optional regularizer for the pointwise
      convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    depthwise_constraint: Optional projection function to be applied to the
        depthwise kernel after being updated by an `Optimizer` (e.g. used for
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    pointwise_constraint: Optional projection function to be applied to the
        pointwise kernel after being updated by an `Optimizer`.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.SeparableConv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.separable_conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.SeparableConv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn('`tf.layers.separable_conv2d` is deprecated and '
                'will be removed in a future version. '
                'Please Use `tf.keras.layers.SeparableConv2D` instead.')
  layer = SeparableConv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      depth_multiplier=depth_multiplier,
      activation=activation,
      use_bias=use_bias,
      depthwise_initializer=depthwise_initializer,
      pointwise_initializer=pointwise_initializer,
      bias_initializer=bias_initializer,
      depthwise_regularizer=depthwise_regularizer,
      pointwise_regularizer=pointwise_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      depthwise_constraint=depthwise_constraint,
      pointwise_constraint=pointwise_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3090')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/convolutional.py: 1071-1117
</a>
<div class="mid" id="frag3090" style="display:none"><pre>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1),
               depth_multiplier=1,
               activation=None,
               use_bias=True,
               depthwise_initializer=None,
               pointwise_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               depthwise_regularizer=None,
               pointwise_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               depthwise_constraint=None,
               pointwise_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(SeparableConv2D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        depth_multiplier=depth_multiplier,
        activation=activation,
        use_bias=use_bias,
        depthwise_initializer=depthwise_initializer,
        pointwise_initializer=pointwise_initializer,
        bias_initializer=bias_initializer,
        depthwise_regularizer=depthwise_regularizer,
        pointwise_regularizer=pointwise_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        depthwise_constraint=depthwise_constraint,
        pointwise_constraint=pointwise_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3089')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/convolutional.py: 928-974
</a>
<div class="mid" id="frag3089" style="display:none"><pre>
  def __init__(self, filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format='channels_last',
               dilation_rate=1,
               depth_multiplier=1,
               activation=None,
               use_bias=True,
               depthwise_initializer=None,
               pointwise_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               depthwise_regularizer=None,
               pointwise_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               depthwise_constraint=None,
               pointwise_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(SeparableConv1D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        depth_multiplier=depth_multiplier,
        activation=activation,
        use_bias=use_bias,
        depthwise_initializer=depthwise_initializer,
        pointwise_initializer=pointwise_initializer,
        bias_initializer=bias_initializer,
        depthwise_regularizer=depthwise_regularizer,
        pointwise_regularizer=pointwise_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        depthwise_constraint=depthwise_constraint,
        pointwise_constraint=pointwise_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3086')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/convolutional.py: 418-561
</a>
<div class="mid" id="frag3086" style="display:none"><pre>
def conv2d(inputs,
           filters,
           kernel_size,
           strides=(1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 2D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 2 integers, specifying the
      height and width of the 2D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 2 integers,
      specifying the strides of the convolution along the height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.

    dilation_rate: An integer or tuple/list of 2 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.Conv2D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn('`tf.layers.conv2d` is deprecated and '
                'will be removed in a future version. '
                'Please Use `tf.keras.layers.Conv2D` instead.')
  layer = Conv2D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3096')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/convolutional.py: 1800-1927
</a>
<div class="mid" id="frag3096" style="display:none"><pre>
def conv3d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 3D convolution layer.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 3 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 3 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.Conv3DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn('`tf.layers.conv3d_transpose` is deprecated and '
                'will be removed in a future version. '
                'Please Use `tf.keras.layers.Conv3DTranspose` instead.')
  layer = Conv3DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


# Aliases

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3094')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/convolutional.py: 1553-1684
</a>
<div class="mid" id="frag3094" style="display:none"><pre>
def conv2d_transpose(inputs,
                     filters,
                     kernel_size,
                     strides=(1, 1),
                     padding='valid',
                     data_format='channels_last',
                     activation=None,
                     use_bias=True,
                     kernel_initializer=None,
                     bias_initializer=tf.compat.v1.zeros_initializer(),
                     kernel_regularizer=None,
                     bias_regularizer=None,
                     activity_regularizer=None,
                     kernel_constraint=None,
                     bias_constraint=None,
                     trainable=True,
                     name=None,
                     reuse=None):
  """Functional interface for transposed 2D convolution layer.

  The need for transposed convolutions generally arises
  from the desire to use a transformation going in the opposite direction
  of a normal convolution, i.e., from something that has the shape of the
  output of some convolution to something that has the shape of its input
  while maintaining a connectivity pattern that is compatible with
  said convolution.

  Args:
    inputs: Input tensor.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: A tuple or list of 2 positive integers specifying the spatial
      dimensions of the filters. Can be a single integer to specify the same
      value for all spatial dimensions.
    strides: A tuple or list of 2 positive integers specifying the strides
      of the convolution. Can be a single integer to specify the same value for
      all spatial dimensions.
    padding: one of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, height, width, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, height, width)`.
    activation: Activation function. Set it to `None` to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If `None`, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.Conv2DTranspose`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv2d_transpose(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv2DTranspose(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn('`tf.layers.conv2d_transpose` is deprecated and '
                'will be removed in a future version. '
                'Please Use `tf.keras.layers.Conv2DTranspose` instead.')
  layer = Conv2DTranspose(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3084')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/convolutional.py: 152-288
</a>
<div class="mid" id="frag3084" style="display:none"><pre>
def conv1d(inputs,
           filters,
           kernel_size,
           strides=1,
           padding='valid',
           data_format='channels_last',
           dilation_rate=1,
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for 1D convolution layer (e.g. temporal convolution).

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of a single integer, specifying the
      length of the 1D convolution window.
    strides: An integer or tuple/list of a single integer,
      specifying the stride length of the convolution.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    dilation_rate: An integer or tuple/list of a single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any `strides` value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.Conv1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv1d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv1D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn('`tf.layers.conv1d` is deprecated and '
                'will be removed in a future version. '
                'Please Use `tf.keras.layers.Conv1D` instead.')
  layer = Conv1D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3830')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_recurrent.py: 1069-1127
</a>
<div class="mid" id="frag3830" style="display:none"><pre>
  def __init__(self,
               filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format=None,
               dilation_rate=1,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               dropout=0.0,
               recurrent_dropout=0.0,
               **kwargs):
    super(ConvLSTM1D, self).__init__(
        rank=1,
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        **kwargs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3832')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_recurrent.py: 1401-1457
</a>
<div class="mid" id="frag3832" style="display:none"><pre>
  def __init__(self,
               filters,
               kernel_size,
               strides=(1, 1, 1),
               padding='valid',
               data_format=None,
               dilation_rate=(1, 1, 1),
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               dropout=0.0,
               recurrent_dropout=0.0,
               **kwargs):
    super(ConvLSTM3D, self).__init__(
        rank=3,
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        **kwargs)
</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3831')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_recurrent.py: 1235-1293
</a>
<div class="mid" id="frag3831" style="display:none"><pre>
  def __init__(self,
               filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format=None,
               dilation_rate=(1, 1),
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               dropout=0.0,
               recurrent_dropout=0.0,
               **kwargs):
    super(ConvLSTM2D, self).__init__(
        rank=2,
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        **kwargs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3088')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/convolutional.py: 692-836
</a>
<div class="mid" id="frag3088" style="display:none"><pre>
def conv3d(inputs,
           filters,
           kernel_size,
           strides=(1, 1, 1),
           padding='valid',
           data_format='channels_last',
           dilation_rate=(1, 1, 1),
           activation=None,
           use_bias=True,
           kernel_initializer=None,
           bias_initializer=tf.compat.v1.zeros_initializer(),
           kernel_regularizer=None,
           bias_regularizer=None,
           activity_regularizer=None,
           kernel_constraint=None,
           bias_constraint=None,
           trainable=True,
           name=None,
           reuse=None):
  """Functional interface for the 3D convolution layer.

  This layer creates a convolution kernel that is convolved
  (actually cross-correlated) with the layer input to produce a tensor of
  outputs. If `use_bias` is True (and a `bias_initializer` is provided),
  a bias vector is created and added to the outputs. Finally, if
  `activation` is not `None`, it is applied to the outputs as well.

  Args:
    inputs: Tensor input.
    filters: Integer, the dimensionality of the output space (i.e. the number
      of filters in the convolution).
    kernel_size: An integer or tuple/list of 3 integers, specifying the
      depth, height and width of the 3D convolution window.
      Can be a single integer to specify the same value for
      all spatial dimensions.
    strides: An integer or tuple/list of 3 integers,
      specifying the strides of the convolution along the depth,
      height and width.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"` or `"same"` (case-insensitive).
      `"valid"` means no padding. `"same"` results in padding evenly to
      the left/right or up/down of the input such that output has the same
      height/width dimension as the input.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, depth, height, width, channels)` while `channels_first`
      corresponds to inputs with shape
      `(batch, channels, depth, height, width)`.
    dilation_rate: An integer or tuple/list of 3 integers, specifying
      the dilation rate to use for dilated convolution.
      Can be a single integer to specify the same value for
      all spatial dimensions.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any stride value != 1.
    activation: Activation function. Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: An initializer for the convolution kernel.
    bias_initializer: An initializer for the bias vector. If None, the default
      initializer will be used.
    kernel_regularizer: Optional regularizer for the convolution kernel.
    bias_regularizer: Optional regularizer for the bias vector.
    activity_regularizer: Optional regularizer function for the output.
    kernel_constraint: Optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: Optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: A string, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.Conv3D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.conv3d(x, filters=3, kernel_size=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.Conv3D(filters=3, kernels_size=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn('`tf.layers.conv3d` is deprecated and '
                'will be removed in a future version. '
                'Please Use `tf.keras.layers.Conv3D` instead.')
  layer = Conv3D(
      filters=filters,
      kernel_size=kernel_size,
      strides=strides,
      padding=padding,
      data_format=data_format,
      dilation_rate=dilation_rate,
      activation=activation,
      use_bias=use_bias,
      kernel_initializer=kernel_initializer,
      bias_initializer=bias_initializer,
      kernel_regularizer=kernel_regularizer,
      bias_regularizer=bias_regularizer,
      activity_regularizer=activity_regularizer,
      kernel_constraint=kernel_constraint,
      bias_constraint=bias_constraint,
      trainable=trainable,
      name=name,
      _reuse=reuse,
      _scope=name)
  return layer.apply(inputs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3093')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/convolutional.py: 1514-1550
</a>
<div class="mid" id="frag3093" style="display:none"><pre>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format='channels_last',
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv2DTranspose, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3095')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/convolutional.py: 1760-1797
</a>
<div class="mid" id="frag3095" style="display:none"><pre>
  def __init__(self,
               filters,
               kernel_size,
               strides=(1, 1, 1),
               padding='valid',
               data_format='channels_last',
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv3DTranspose, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name,
        **kwargs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3087')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/convolutional.py: 652-689
</a>
<div class="mid" id="frag3087" style="display:none"><pre>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1, 1),
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv3D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3085')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/convolutional.py: 378-415
</a>
<div class="mid" id="frag3085" style="display:none"><pre>
  def __init__(self, filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format='channels_last',
               dilation_rate=(1, 1),
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv2D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3083')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/convolutional.py: 112-149
</a>
<div class="mid" id="frag3083" style="display:none"><pre>
  def __init__(self, filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format='channels_last',
               dilation_rate=1,
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Conv1D, self).__init__(
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        bias_constraint=bias_constraint,
        trainable=trainable,
        name=name, **kwargs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4161')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent.py: 2552-2593
</a>
<div class="mid" id="frag4161" style="display:none"><pre>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    warnings.warn('`tf.keras.experimental.PeepholeLSTMCell` is deprecated '
                  'and will be removed in a future version. '
                  'Please use tensorflow_addons.rnn.PeepholeLSTMCell '
                  'instead.')
    super(PeepholeLSTMCell, self).__init__(
        units=units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 1),
        **kwargs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4575')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent_v2.py: 332-403
</a>
<div class="mid" id="frag4575" style="display:none"><pre>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               unroll=False,
               time_major=False,
               reset_after=True,
               **kwargs):
    # return_runtime is a flag for testing, which shows the real backend
    # implementation chosen by grappler in graph mode.
    self._return_runtime = kwargs.pop('return_runtime', False)

    super(GRU, self).__init__(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 2),
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        unroll=unroll,
        time_major=time_major,
        reset_after=reset_after,
        **kwargs)
    # GPU kernel uses following setting by default and not configurable.
    self._could_use_gpu_kernel = (
        self.activation in (activations.tanh, tf.tanh) and
        self.recurrent_activation in (activations.sigmoid, tf.sigmoid) and
        recurrent_dropout == 0 and not unroll and use_bias and
        reset_after and tf.compat.v1.executing_eagerly_outside_functions())
    if tf.config.list_logical_devices('GPU'):
      # Only show the message when there is GPU available, user will not care
      # about the cuDNN if there isn't any GPU.
      if self._could_use_gpu_kernel:
        logging.debug(_CUDNN_AVAILABLE_MSG % self.name)
      else:
        logging.warning(_CUDNN_NOT_AVAILABLE_MSG % self.name)

    if _use_new_code():
      self._defun_wrapper = _DefunWrapper(time_major, go_backwards, 'gru')

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3805')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_recurrent.py: 773-835
</a>
<div class="mid" id="frag3805" style="display:none"><pre>
  def __init__(self,
               rank,
               filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format=None,
               dilation_rate=1,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               dropout=0.0,
               recurrent_dropout=0.0,
               **kwargs):
    cell = ConvLSTMCell(
        rank=rank,
        filters=filters,
        kernel_size=kernel_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        dilation_rate=dilation_rate,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        dtype=kwargs.get('dtype'))
    super(ConvLSTM, self).__init__(
        rank,
        cell,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        **kwargs)
    self.activity_regularizer = regularizers.get(activity_regularizer)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4165')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent.py: 2726-2791
</a>
<div class="mid" id="frag4165" style="display:none"><pre>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               unroll=False,
               **kwargs):
    implementation = kwargs.pop('implementation', 1)
    if implementation == 0:
      logging.warning('`implementation=0` has been deprecated, '
                      'and now defaults to `implementation=1`.'
                      'Please update your layer call.')
    if 'enable_caching_device' in kwargs:
      cell_kwargs = {'enable_caching_device':
                     kwargs.pop('enable_caching_device')}
    else:
      cell_kwargs = {}
    cell = LSTMCell(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        unit_forget_bias=unit_forget_bias,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=implementation,
        dtype=kwargs.get('dtype'),
        trainable=kwargs.get('trainable', True),
        **cell_kwargs)
    super(LSTM, self).__init__(
        cell,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        unroll=unroll,
        **kwargs)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.input_spec = [InputSpec(ndim=3)]

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4586')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent_v2.py: 903-941
</a>
<div class="mid" id="frag4586" style="display:none"><pre>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    super(LSTMCell, self).__init__(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 2),
        **kwargs)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3046')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/core.py: 116-143
</a>
<div class="mid" id="frag3046" style="display:none"><pre>
  def __init__(self, units,
               activation=None,
               use_bias=True,
               kernel_initializer=None,
               bias_initializer=tf.compat.v1.zeros_initializer(),
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               trainable=True,
               name=None,
               **kwargs):
    super(Dense, self).__init__(units=units,
                                activation=activation,
                                use_bias=use_bias,
                                kernel_initializer=kernel_initializer,
                                bias_initializer=bias_initializer,
                                kernel_regularizer=kernel_regularizer,
                                bias_regularizer=bias_regularizer,
                                activity_regularizer=activity_regularizer,
                                kernel_constraint=kernel_constraint,
                                bias_constraint=bias_constraint,
                                trainable=trainable,
                                name=name,
                                **kwargs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4574')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent_v2.py: 164-202
</a>
<div class="mid" id="frag4574" style="display:none"><pre>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               reset_after=True,
               **kwargs):
    super(GRUCell, self).__init__(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 2),
        reset_after=reset_after,
        **kwargs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4587')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent_v2.py: 1056-1130
</a>
<div class="mid" id="frag4587" style="display:none"><pre>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               time_major=False,
               unroll=False,
               **kwargs):
    # return_runtime is a flag for testing, which shows the real backend
    # implementation chosen by grappler in graph mode.
    self.return_runtime = kwargs.pop('return_runtime', False)

    super(LSTM, self).__init__(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        unit_forget_bias=unit_forget_bias,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        activity_regularizer=activity_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=kwargs.pop('implementation', 2),
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        time_major=time_major,
        unroll=unroll,
        **kwargs)

    self.state_spec = [
        InputSpec(shape=(None, dim)) for dim in (self.units, self.units)
    ]
    self._could_use_gpu_kernel = (
        self.activation in (activations.tanh, tf.tanh) and
        self.recurrent_activation in (activations.sigmoid, tf.sigmoid) and
        recurrent_dropout == 0 and not unroll and use_bias and
        tf.compat.v1.executing_eagerly_outside_functions())
    if tf.config.list_logical_devices('GPU'):
      # Only show the message when there is GPU available, user will not care
      # about the cuDNN if there isn't any GPU.
      if self._could_use_gpu_kernel:
        logging.debug(_CUDNN_AVAILABLE_MSG % self.name)
      else:
        logging.warning(_CUDNN_NOT_AVAILABLE_MSG % self.name)

    if _use_new_code():
      self._defun_wrapper = _DefunWrapper(time_major, go_backwards, 'lstm')

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4132')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent.py: 2053-2118
</a>
<div class="mid" id="frag4132" style="display:none"><pre>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               unroll=False,
               reset_after=False,
               **kwargs):
    implementation = kwargs.pop('implementation', 1)
    if implementation == 0:
      logging.warning('`implementation=0` has been deprecated, '
                      'and now defaults to `implementation=1`.'
                      'Please update your layer call.')
    if 'enable_caching_device' in kwargs:
      cell_kwargs = {'enable_caching_device':
                     kwargs.pop('enable_caching_device')}
    else:
      cell_kwargs = {}
    cell = GRUCell(
        units,
        activation=activation,
        recurrent_activation=recurrent_activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        implementation=implementation,
        reset_after=reset_after,
        dtype=kwargs.get('dtype'),
        trainable=kwargs.get('trainable', True),
        **cell_kwargs)
    super(GRU, self).__init__(
        cell,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        unroll=unroll,
        **kwargs)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.input_spec = [InputSpec(ndim=3)]

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4109')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent.py: 1516-1576
</a>
<div class="mid" id="frag4109" style="display:none"><pre>
  def __init__(self,
               units,
               activation='tanh',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               unroll=False,
               **kwargs):
    if 'implementation' in kwargs:
      kwargs.pop('implementation')
      logging.warning('The `implementation` argument '
                      'in `SimpleRNN` has been deprecated. '
                      'Please remove it from your layer call.')
    if 'enable_caching_device' in kwargs:
      cell_kwargs = {'enable_caching_device':
                     kwargs.pop('enable_caching_device')}
    else:
      cell_kwargs = {}
    cell = SimpleRNNCell(
        units,
        activation=activation,
        use_bias=use_bias,
        kernel_initializer=kernel_initializer,
        recurrent_initializer=recurrent_initializer,
        bias_initializer=bias_initializer,
        kernel_regularizer=kernel_regularizer,
        recurrent_regularizer=recurrent_regularizer,
        bias_regularizer=bias_regularizer,
        kernel_constraint=kernel_constraint,
        recurrent_constraint=recurrent_constraint,
        bias_constraint=bias_constraint,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        dtype=kwargs.get('dtype'),
        trainable=kwargs.get('trainable', True),
        **cell_kwargs)
    super(SimpleRNN, self).__init__(
        cell,
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        unroll=unroll,
        **kwargs)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.input_spec = [InputSpec(ndim=3)]

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3047')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/core.py: 146-255
</a>
<div class="mid" id="frag3047" style="display:none"><pre>
def dense(
    inputs, units,
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    reuse=None):
  """Functional interface for the densely-connected layer.

  This layer implements the operation:
  `outputs = activation(inputs * kernel + bias)`
  where `activation` is the activation function passed as the `activation`
  argument (if not `None`), `kernel` is a weights matrix created by the layer,
  and `bias` is a bias vector created by the layer
  (only if `use_bias` is `True`).

  Args:
    inputs: Tensor input.
    units: Integer or Long, dimensionality of the output space.
    activation: Activation function (callable). Set it to None to maintain a
      linear activation.
    use_bias: Boolean, whether the layer uses a bias.
    kernel_initializer: Initializer function for the weight matrix.
      If `None` (default), weights are initialized using the default
      initializer used by `tf.compat.v1.get_variable`.
    bias_initializer: Initializer function for the bias.
    kernel_regularizer: Regularizer function for the weight matrix.
    bias_regularizer: Regularizer function for the bias.
    activity_regularizer: Regularizer function for the output.
    kernel_constraint: An optional projection function to be applied to the
        kernel after being updated by an `Optimizer` (e.g. used to implement
        norm constraints or value constraints for layer weights). The function
        must take as input the unprojected variable and must return the
        projected variable (which must have the same shape). Constraints are
        not safe to use when doing asynchronous distributed training.
    bias_constraint: An optional projection function to be applied to the
        bias after being updated by an `Optimizer`.
    trainable: Boolean, if `True` also add variables to the graph collection
      `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).
    name: String, the name of the layer.
    reuse: Boolean, whether to reuse the weights of a previous layer
      by the same name.

  Returns:
    Output tensor the same shape as `inputs` except the last dimension is of
    size `units`.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.Dense`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.dense(x, units=3)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28,))
   y = tf.keras.layers.Dense(units=3)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility

  """
  warnings.warn('`tf.layers.dense` is deprecated and '
                'will be removed in a future version. '
                'Please use `tf.keras.layers.Dense` instead.')
  layer = Dense(units,
                activation=activation,
                use_bias=use_bias,
                kernel_initializer=kernel_initializer,
                bias_initializer=bias_initializer,
                kernel_regularizer=kernel_regularizer,
                bias_regularizer=bias_regularizer,
                activity_regularizer=activity_regularizer,
                kernel_constraint=kernel_constraint,
                bias_constraint=bias_constraint,
                trainable=trainable,
                name=name,
                _scope=name,
                _reuse=reuse)
  return layer.apply(inputs)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 136:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3071')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/pooling.py: 76-89
</a>
<div class="mid" id="frag3071" style="display:none"><pre>
  def __init__(self, pool_size, strides,
               padding='valid', data_format='channels_last',
               name=None, **kwargs):
    if strides is None:
      raise ValueError('Argument `strides` must not be None.')
    super(AveragePooling1D, self).__init__(
        pool_size=pool_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        name=name,
        **kwargs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3073')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/pooling.py: 208-221
</a>
<div class="mid" id="frag3073" style="display:none"><pre>
  def __init__(self, pool_size, strides,
               padding='valid', data_format='channels_last',
               name=None, **kwargs):
    if strides is None:
      raise ValueError('Argument `strides` must not be None.')
    super(MaxPooling1D, self).__init__(
        pool_size=pool_size,
        strides=strides,
        padding=padding,
        data_format=data_format,
        name=name,
        **kwargs)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 137:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3072')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/pooling.py: 92-160
</a>
<div class="mid" id="frag3072" style="display:none"><pre>
def average_pooling1d(inputs, pool_size, strides,
                      padding='valid', data_format='channels_last',
                      name=None):
  """Average Pooling layer for 1D inputs.

  Args:
    inputs: The tensor over which to pool. Must have rank 3.
    pool_size: An integer or tuple/list of a single integer,
      representing the size of the pooling window.
    strides: An integer or tuple/list of a single integer, specifying the
      strides of the pooling operation.
    padding: A string. The padding method, either 'valid' or 'same'.
      Case-insensitive.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    name: A string, the name of the layer.

  Returns:
    The output tensor, of rank 3.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.AveragePooling1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.average_pooling1d(x, pool_size=2, strides=2)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.AveragePooling1D(pool_size=2, strides=2)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn('`tf.layers.average_pooling1d` is deprecated and '
                'will be removed in a future version. '
                'Please use `tf.keras.layers.AveragePooling1D` instead.')
  layer = AveragePooling1D(pool_size=pool_size,
                           strides=strides,
                           padding=padding,
                           data_format=data_format,
                           name=name)
  return layer.apply(inputs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3074')" href="javascript:;">
keras-2.6.0/keras/legacy_tf_layers/pooling.py: 224-292
</a>
<div class="mid" id="frag3074" style="display:none"><pre>
def max_pooling1d(inputs, pool_size, strides,
                  padding='valid', data_format='channels_last',
                  name=None):
  """Max Pooling layer for 1D inputs.

  Args:
    inputs: The tensor over which to pool. Must have rank 3.
    pool_size: An integer or tuple/list of a single integer,
      representing the size of the pooling window.
    strides: An integer or tuple/list of a single integer, specifying the
      strides of the pooling operation.
    padding: A string. The padding method, either 'valid' or 'same'.
      Case-insensitive.
    data_format: A string, one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, length, channels)` while `channels_first` corresponds to
      inputs with shape `(batch, channels, length)`.
    name: A string, the name of the layer.

  Returns:
    The output tensor, of rank 3.

  Raises:
    ValueError: if eager execution is enabled.


  @compatibility(TF2)
  This API is not compatible with eager execution or `tf.function`.

  Please refer to [tf.layers section of the migration guide]
  (https://www.tensorflow.org/guide/migrate#models_based_on_tflayers)
  to migrate a TensorFlow v1 model to Keras. The corresponding TensorFlow v2
  layer is `tf.keras.layers.MaxPooling1D`.


  #### Structural Mapping to Native TF2

  None of the supported arguments have changed name.

  Before:

  ```python
   y = tf.compat.v1.layers.max_pooling1d(x, pool_size=2, strides=2)
  ```

  After:

  To migrate code using TF1 functional layers use the [Keras Functional API]
  (https://www.tensorflow.org/guide/keras/functional):

  ```python
   x = tf.keras.Input((28, 28, 1))
   y = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2)(x)
   model = tf.keras.Model(x, y)
  ```
  @end_compatibility
  """
  warnings.warn('`tf.layers.max_pooling1d` is deprecated and '
                'will be removed in a future version. '
                'Please use `tf.keras.layers.MaxPooling1D` instead.')
  layer = MaxPooling1D(pool_size=pool_size,
                       strides=strides,
                       padding=padding,
                       data_format=data_format,
                       name=name)
  return layer.apply(inputs)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 138:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3129')" href="javascript:;">
keras-2.6.0/keras/testing_utils.py: 651-675
</a>
<div class="mid" id="frag3129" style="display:none"><pre>
  def call(self, inputs, **kwargs):
    if self._shared_input_branch:
      for layer in self._shared_input_branch:
        inputs = layer(inputs)
      a = inputs
      b = inputs
    elif isinstance(inputs, dict):
      a = inputs['input_1']
      b = inputs['input_2']
    else:
      a, b = inputs

    for layer in self._branch_a:
      a = layer(a)
    for layer in self._branch_b:
      b = layer(b)
    outs = [a, b]

    if self._shared_output_branch:
      for layer in self._shared_output_branch:
        outs = layer(outs)

    return outs


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3132')" href="javascript:;">
keras-2.6.0/keras/testing_utils.py: 702-723
</a>
<div class="mid" id="frag3132" style="display:none"><pre>
  def call(self, inputs, **kwargs):
    if self._shared_input_branch:
      for layer in self._shared_input_branch:
        inputs = layer(inputs)
      a = inputs
      b = inputs
    else:
      a, b = inputs

    for layer in self._branch_a:
      a = layer(a)
    for layer in self._branch_b:
      b = layer(b)
    outs = a, b

    if self._shared_output_branch:
      for layer in self._shared_output_branch:
        outs = layer(outs)

    return outs


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 139:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3164')" href="javascript:;">
keras-2.6.0/keras/optimizers_test.py: 160-186
</a>
<div class="mid" id="frag3164" style="display:none"><pre>
  def test_tf_optimizer(self):
    if tf.executing_eagerly():
      self.skipTest(
          'v1 optimizer does not run in eager mode')
    optimizer = optimizer_v1.TFOptimizer(AdamOptimizer(0.01))
    model = keras.models.Sequential()
    model.add(keras.layers.Dense(
        2, input_shape=(3,), kernel_constraint=keras.constraints.MaxNorm(1)))
    # This is possible
    model.compile(
        loss='mean_squared_error',
        optimizer=optimizer,
        run_eagerly=testing_utils.should_run_eagerly())
    keras.backend.track_tf_optimizer(optimizer)
    model.fit(np.random.random((5, 3)),
              np.random.random((5, 2)),
              epochs=1,
              batch_size=5,
              verbose=0)
    # not supported
    with self.assertRaises(NotImplementedError):
      _ = optimizer.weights
    with self.assertRaises(NotImplementedError):
      optimizer.get_config()
    with self.assertRaises(NotImplementedError):
      optimizer.from_config(None)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3166')" href="javascript:;">
keras-2.6.0/keras/optimizers_test.py: 203-225
</a>
<div class="mid" id="frag3166" style="display:none"><pre>
  def test_tf_optimizer_iterations(self):
    if tf.executing_eagerly():
      self.skipTest(
          'v1 optimizer does not run in eager mode')
    with self.cached_session():
      optimizer = optimizer_v1.TFOptimizer(AdamOptimizer(0.01))
      model = keras.models.Sequential()
      model.add(keras.layers.Dense(
          2, input_shape=(3,), kernel_constraint=keras.constraints.MaxNorm(1)))
      model.compile(
          loss='mean_squared_error',
          optimizer=optimizer,
          run_eagerly=testing_utils.should_run_eagerly())
      keras.backend.track_tf_optimizer(optimizer)
      self.assertEqual(keras.backend.get_value(model.optimizer.iterations), 0)

      model.fit(np.random.random((55, 3)),
                np.random.random((55, 2)),
                epochs=1,
                batch_size=5,
                verbose=0)
      self.assertEqual(keras.backend.get_value(model.optimizer.iterations), 11)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 140:</b> &nbsp; 32 fragments, nominal size 14 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3209')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/text_classification_transformer_benchmark_test.py: 68-84
</a>
<div class="mid" id="frag3209" style="display:none"><pre>
  def benchmark_text_classification_bs_128(self):
    """Measure performance with batch_size=128."""
    batch_size = 128
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'transformer', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3238')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/antirectifier_benchmark_test.py: 107-129
</a>
<div class="mid" id="frag3238" style="display:none"><pre>
  def benchmark_antirectifier_bs_512_gpu_2(self):
    """Measure performance with batch_size=512, gpu=2 and

    distribution_strategy=`mirrored`.
    """
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy="mirrored",
        optimizer="rmsprop",
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"])

    metadata = benchmark_util.get_keras_examples_metadata(
        "antirectifier", batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3247')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/cifar10_cnn_benchmark_test.py: 106-122
</a>
<div class="mid" id="frag3247" style="display:none"><pre>
  def benchmark_cnn_cifar10_bs_1024(self):
    """Measure performance with batch_size=1024."""
    batch_size = 1024
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('cnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3246')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/cifar10_cnn_benchmark_test.py: 89-105
</a>
<div class="mid" id="frag3246" style="display:none"><pre>
  def benchmark_cnn_cifar10_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('cnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3245')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/cifar10_cnn_benchmark_test.py: 72-88
</a>
<div class="mid" id="frag3245" style="display:none"><pre>
  def benchmark_cnn_cifar10_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('cnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3237')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/antirectifier_benchmark_test.py: 90-106
</a>
<div class="mid" id="frag3237" style="display:none"><pre>
  def benchmark_antirectifier_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer="rmsprop",
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"])

    metadata = benchmark_util.get_keras_examples_metadata(
        "antirectifier", batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3262')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/reuters_mlp_benchmark_test.py: 63-79
</a>
<div class="mid" id="frag3262" style="display:none"><pre>
  def benchmark_mlp_reuters_bs_128(self):
    """Measure performance with batch_size=128."""
    batch_size = 128
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('mlp', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3236')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/antirectifier_benchmark_test.py: 73-89
</a>
<div class="mid" id="frag3236" style="display:none"><pre>
  def benchmark_antirectifier_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer="rmsprop",
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"])

    metadata = benchmark_util.get_keras_examples_metadata(
        "antirectifier", batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3263')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/reuters_mlp_benchmark_test.py: 80-96
</a>
<div class="mid" id="frag3263" style="display:none"><pre>
  def benchmark_mlp_reuters_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('mlp', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3264')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/reuters_mlp_benchmark_test.py: 97-113
</a>
<div class="mid" id="frag3264" style="display:none"><pre>
  def benchmark_mlp_reuters_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('mlp', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3232')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_hierarchical_rnn_benchmark_test.py: 115-137
</a>
<div class="mid" id="frag3232" style="display:none"><pre>
  def benchmark_hrnn_mnist_bs_1024_gpu_2(self):
    """Measure performance with batch_size=1024, gpu=2 and

    distribution_strategy='mirrored'
    """
    batch_size = 1024
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'hierarchical_rnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3265')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/reuters_mlp_benchmark_test.py: 114-136
</a>
<div class="mid" id="frag3265" style="display:none"><pre>
  def benchmark_mlp_reuters_bs_512_gpu_2(self):
    """Measure performance with batch_size=512, gpu=2 and

    distribution_strategy='mirrored'
    """
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('mlp', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3225')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_irnn_benchmark_test.py: 96-111
</a>
<div class="mid" id="frag3225" style="display:none"><pre>
  def benchmark_irnn_mnist_bs_1024(self):
    """Measure performance with batch_size=1024."""
    batch_size = 1024
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('irnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3224')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_irnn_benchmark_test.py: 80-95
</a>
<div class="mid" id="frag3224" style="display:none"><pre>
  def benchmark_irnn_mnist_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('irnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3235')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/antirectifier_benchmark_test.py: 56-72
</a>
<div class="mid" id="frag3235" style="display:none"><pre>
  def benchmark_antirectifier_bs_128(self):
    """Measure performance with batch_size=128."""
    batch_size = 128
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer="rmsprop",
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["sparse_categorical_accuracy"])

    metadata = benchmark_util.get_keras_examples_metadata(
        "antirectifier", batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3223')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_irnn_benchmark_test.py: 64-79
</a>
<div class="mid" id="frag3223" style="display:none"><pre>
  def benchmark_irnn_mnist_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('irnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3231')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_hierarchical_rnn_benchmark_test.py: 98-114
</a>
<div class="mid" id="frag3231" style="display:none"><pre>
  def benchmark_hrnn_mnist_bs_1024(self):
    """Measure performance with batch_size=1024."""
    batch_size = 1024
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'hierarchical_rnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3276')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/bidirectional_lstm_benchmark_test.py: 92-108
</a>
<div class="mid" id="frag3276" style="display:none"><pre>
  def benchmark_bidirect_lstm_imdb_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'bidirectional_lstm', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3230')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_hierarchical_rnn_benchmark_test.py: 81-97
</a>
<div class="mid" id="frag3230" style="display:none"><pre>
  def benchmark_hrnn_mnist_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'hierarchical_rnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3229')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_hierarchical_rnn_benchmark_test.py: 64-80
</a>
<div class="mid" id="frag3229" style="display:none"><pre>
  def benchmark_hrnn_mnist_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        optimizer='rmsprop',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'hierarchical_rnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3268')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_benchmark_test.py: 63-79
</a>
<div class="mid" id="frag3268" style="display:none"><pre>
  def benchmark_conv_mnist_bs_128(self):
    """Measure performance with batch_size=128."""
    batch_size = 128
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('conv', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3269')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_benchmark_test.py: 80-96
</a>
<div class="mid" id="frag3269" style="display:none"><pre>
  def benchmark_conv_mnist_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('conv', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3270')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_benchmark_test.py: 97-113
</a>
<div class="mid" id="frag3270" style="display:none"><pre>
  def benchmark_conv_mnist_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('conv', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3277')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/bidirectional_lstm_benchmark_test.py: 109-131
</a>
<div class="mid" id="frag3277" style="display:none"><pre>
  def benchmark_bidirect_lstm_imdb_bs_512_gpu_2(self):
    """Measure performance with batch_size=512, gpu=2 and

    distribution_strategy=`mirrored`.
    """
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'bidirectional_lstm', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3274')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/bidirectional_lstm_benchmark_test.py: 58-74
</a>
<div class="mid" id="frag3274" style="display:none"><pre>
  def benchmark_bidirect_lstm_imdb_bs_128(self):
    """Measure performance with batch_size=128."""
    batch_size = 128
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'bidirectional_lstm', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3212')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/text_classification_transformer_benchmark_test.py: 119-141
</a>
<div class="mid" id="frag3212" style="display:none"><pre>
  def benchmark_text_classification_bs_512_gpu_2(self):
    """Measure performance with batch_size=512, gpu=1 and

    distribution_strategy='mirrored'
    """
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'transformer', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3248')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/cifar10_cnn_benchmark_test.py: 123-145
</a>
<div class="mid" id="frag3248" style="display:none"><pre>
  def benchmark_cnn_cifar10_bs_1024_gpu_2(self):
    """Measure performance with batch_size=1024, gpu=2 and

    distribution_strategy=`mirrored`.
    """
    batch_size = 1024
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        epochs=self.epochs,
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('cnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3275')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/bidirectional_lstm_benchmark_test.py: 75-91
</a>
<div class="mid" id="frag3275" style="display:none"><pre>
  def benchmark_bidirect_lstm_imdb_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'bidirectional_lstm', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3226')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_irnn_benchmark_test.py: 112-133
</a>
<div class="mid" id="frag3226" style="display:none"><pre>
  def benchmark_irnn_mnist_bs_1024_gpu_2(self):
    """Measure performance with batch_size=1024, gpu=2 and

    distribution_strategy='mirrored'
    """
    batch_size = 1024
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('irnn', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3271')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_benchmark_test.py: 114-136
</a>
<div class="mid" id="frag3271" style="display:none"><pre>
  def benchmark_conv_mnist_bs_512_gpu_2(self):
    """Measure performance with batch_size=512, gpu=2 and

    distribution_strategy='mirrored'
    """
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.x_train,
        y=self.y_train,
        batch_size=batch_size,
        num_gpus=2,
        distribution_strategy='mirrored',
        epochs=self.epochs,
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata('conv', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3211')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/text_classification_transformer_benchmark_test.py: 102-118
</a>
<div class="mid" id="frag3211" style="display:none"><pre>
  def benchmark_text_classification_bs_512(self):
    """Measure performance with batch_size=512."""
    batch_size = 512
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'transformer', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3210')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/text_classification_transformer_benchmark_test.py: 85-101
</a>
<div class="mid" id="frag3210" style="display:none"><pre>
  def benchmark_text_classification_bs_256(self):
    """Measure performance with batch_size=256."""
    batch_size = 256
    metrics, wall_time, extras = benchmark_util.measure_performance(
        self._build_model,
        x=self.imdb_x,
        y=self.imdb_y,
        batch_size=batch_size,
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])

    metadata = benchmark_util.get_keras_examples_metadata(
        'transformer', batch_size)
    extras.update(metadata)
    self.report_benchmark(wall_time=wall_time, metrics=metrics, extras=extras)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 141:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3250')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_custom_training_benchmark_test.py: 45-59
</a>
<div class="mid" id="frag3250" style="display:none"><pre>
  def _build_model(self):
    """Model from https://keras.io/examples/vision/mnist_convnet/."""
    model = tf.keras.Sequential([
        tf.keras.Input(shape=self.input_shape),
        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(self.num_classes, activation='softmax'),
    ])

    return model

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3267')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_benchmark_test.py: 40-62
</a>
<div class="mid" id="frag3267" style="display:none"><pre>
  def _build_model(self):
    """Model from https://keras.io/examples/vision/mnist_convnet/."""
    model = tf.keras.Sequential([
        tf.keras.Input(shape=self.input_shape),
        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(self.num_classes, activation='softmax'),
    ])
    return model

  # In each benchmark test, the required arguments for the
  # method `measure_performance` include:
  #   x: Input data, it could be Numpy or loaded from tfds.
  #   y: Target data. If `x` is a dataset or generator instance,
  #      `y` should not be specified.
  #   loss: Loss function for model.
  #   optimizer: Optimizer for model.
  #   Check more details in `measure_performance()` method of
  #   benchmark_util.
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 142:</b> &nbsp; 3 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3256')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_custom_training_benchmark_test.py: 266-287
</a>
<div class="mid" id="frag3256" style="display:none"><pre>
    return metrics, wall_time

  def benchmark_custom_training_mnist_bs_128(self):
    """Measure performance with batch_size=128 and run_iters=5."""
    batch_size = 128
    run_iters = 5
    train_dataset = self.train_dataset.shuffle(
        buffer_size=1024).batch(batch_size)

    # Instantiate a loss function.
    loss_fn = tf.keras.losses.CategoricalCrossentropy(
        reduction=tf.keras.losses.Reduction.NONE)
    # Instantiate an optimizer to train the model.
    optimizer = tf.keras.optimizers.Adam()
    model = self._build_model()

    metrics, wall_time = self.measure_performance(model, train_dataset, loss_fn,
                                                  optimizer, batch_size,
                                                  run_iters, self.epochs)
    extras = benchmark_util.get_keras_examples_metadata('conv', batch_size,
                                                        '.keras.ctl_graph')
    self.report_benchmark(
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3257')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_custom_training_benchmark_test.py: 288-309
</a>
<div class="mid" id="frag3257" style="display:none"><pre>
        iters=run_iters, wall_time=wall_time, metrics=metrics, extras=extras)

  def benchmark_custom_training_mnist_bs_256(self):
    """Measure performance with batch_size=256 and run_iters=5."""
    batch_size = 256
    run_iters = 5
    train_dataset = self.train_dataset.shuffle(
        buffer_size=1024).batch(batch_size)

    # Instantiate a loss function.
    loss_fn = tf.keras.losses.CategoricalCrossentropy(
        reduction=tf.keras.losses.Reduction.NONE)
    # Instantiate an optimizer to train the model.
    optimizer = tf.keras.optimizers.Adam()
    model = self._build_model()

    metrics, wall_time = self.measure_performance(model, train_dataset, loss_fn,
                                                  optimizer, batch_size,
                                                  run_iters, self.epochs)
    extras = benchmark_util.get_keras_examples_metadata('conv', batch_size,
                                                        '.keras.ctl_graph')
    self.report_benchmark(
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3258')" href="javascript:;">
keras-2.6.0/keras/benchmarks/keras_examples_benchmarks/mnist_conv_custom_training_benchmark_test.py: 310-331
</a>
<div class="mid" id="frag3258" style="display:none"><pre>
        iters=run_iters, wall_time=wall_time, metrics=metrics, extras=extras)

  def benchmark_custom_training_mnist_bs_512(self):
    """Measure performance with batch_size=512 and run_iters=10."""
    batch_size = 512
    run_iters = 5
    train_dataset = self.train_dataset.shuffle(
        buffer_size=1024).batch(batch_size)

    # Instantiate a loss function.
    loss_fn = tf.keras.losses.CategoricalCrossentropy(
        reduction=tf.keras.losses.Reduction.NONE)
    # Instantiate an optimizer to train the model.
    optimizer = tf.keras.optimizers.Adam()
    model = self._build_model()

    metrics, wall_time = self.measure_performance(model, train_dataset, loss_fn,
                                                  optimizer, batch_size,
                                                  run_iters, self.epochs)
    extras = benchmark_util.get_keras_examples_metadata('conv', batch_size,
                                                        '.keras.ctl_graph')
    self.report_benchmark(
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 143:</b> &nbsp; 8 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3298')" href="javascript:;">
keras-2.6.0/keras/benchmarks/saved_model_benchmarks/densenet_benchmark_test.py: 27-41
</a>
<div class="mid" id="frag3298" style="display:none"><pre>
  def benchmark_save_and_load_densenet_201(self):
    app = tf.keras.applications.DenseNet201
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3305')" href="javascript:;">
keras-2.6.0/keras/benchmarks/saved_model_benchmarks/vgg_benchmark_test.py: 27-42
</a>
<div class="mid" id="frag3305" style="display:none"><pre>
  def benchmark_save_and_load_vgg19(self):
    app = tf.keras.applications.VGG19
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3300')" href="javascript:;">
keras-2.6.0/keras/benchmarks/saved_model_benchmarks/efficientnet_benchmark_test.py: 27-41
</a>
<div class="mid" id="frag3300" style="display:none"><pre>
  def benchmark_save_and_load_efficient_net_b7(self):
    app = tf.keras.applications.EfficientNetB7
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3299')" href="javascript:;">
keras-2.6.0/keras/benchmarks/saved_model_benchmarks/xception_benchmark_test.py: 27-42
</a>
<div class="mid" id="frag3299" style="display:none"><pre>
  def benchmark_save_and_load_xception(self):
    app = tf.keras.applications.Xception
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3306')" href="javascript:;">
keras-2.6.0/keras/benchmarks/saved_model_benchmarks/inception_resnet_v2_benchmark_test.py: 27-42
</a>
<div class="mid" id="frag3306" style="display:none"><pre>
  def benchmark_save_and_load_inception_resnet_v2(self):
    app = tf.keras.applications.InceptionResNetV2
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3301')" href="javascript:;">
keras-2.6.0/keras/benchmarks/saved_model_benchmarks/nasnet_large_benchmark_test.py: 27-41
</a>
<div class="mid" id="frag3301" style="display:none"><pre>
  def benchmark_save_and_load_nasnet_large(self):
    app = tf.keras.applications.NASNetLarge
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3302')" href="javascript:;">
keras-2.6.0/keras/benchmarks/saved_model_benchmarks/resnet152_v2_benchmark_test.py: 27-42
</a>
<div class="mid" id="frag3302" style="display:none"><pre>
  def benchmark_save_and_load_resnet152_v2(self):
    app = tf.keras.applications.ResNet152V2
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3304')" href="javascript:;">
keras-2.6.0/keras/benchmarks/saved_model_benchmarks/mobilenet_benchmark_test.py: 27-41
</a>
<div class="mid" id="frag3304" style="display:none"><pre>
  def benchmark_save_and_load_mobilenet_v2(self):
    app = tf.keras.applications.MobileNetV2
    save_result, load_result = (
        saved_model_benchmark_util.save_and_load_benchmark(app))

    self.report_benchmark(
        iters=save_result['iters'],
        wall_time=save_result['wall_time'],
        name=save_result['name'])

    self.report_benchmark(
        iters=load_result['iters'],
        wall_time=load_result['wall_time'],
        name=load_result['name'])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 144:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3318')" href="javascript:;">
keras-2.6.0/keras/benchmarks/model_components_benchmarks_test.py: 149-161
</a>
<div class="mid" id="frag3318" style="display:none"><pre>
  def _benchmark_keras_model_fit(self, model, run_eagerly=False):
    data = tf.random.uniform((10, 10), minval=-1, maxval=1)
    labels = tf.random.uniform((10, 10), minval=-1, maxval=1)
    dataset = tf.data.Dataset.from_tensors((data, labels)).repeat()
    model.compile(
        "sgd",
        loss="mse", run_eagerly=run_eagerly)
    func = lambda: model.fit(dataset, epochs=1, steps_per_epoch=1000, verbose=0)
    # First call is more expensive (creates variables etc.), discount that.
    model.fit(dataset, epochs=1, steps_per_epoch=1, verbose=0)

    self._run(func, 1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3319')" href="javascript:;">
keras-2.6.0/keras/benchmarks/model_components_benchmarks_test.py: 162-174
</a>
<div class="mid" id="frag3319" style="display:none"><pre>
  def _benchmark_keras_model_evaluate(self, model, run_eagerly=False):
    data = tf.random.uniform((10, 10), minval=-1, maxval=1)
    labels = tf.random.uniform((10, 10), minval=-1, maxval=1)
    dataset = tf.data.Dataset.from_tensors((data, labels)).repeat()
    model.compile(
        "sgd",
        loss="mse", run_eagerly=run_eagerly)
    func = lambda: model.evaluate(dataset, steps=1000, verbose=0)
    # First call is more expensive (creates variables etc.), discount that.
    model.evaluate(dataset, steps=1, verbose=0)

    self._run(func, 1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 145:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3345')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 46-59
</a>
<div class="mid" id="frag3345" style="display:none"><pre>
  def test_unweighted(self):
    fp_obj = metrics.FalsePositives()
    self.evaluate(tf.compat.v1.variables_initializer(fp_obj.variables))

    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))

    update_op = fp_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = fp_obj.result()
    self.assertAllClose(7., result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3361')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 262-275
</a>
<div class="mid" id="frag3361" style="display:none"><pre>
  def test_unweighted(self):
    tp_obj = metrics.TruePositives()
    self.evaluate(tf.compat.v1.variables_initializer(tp_obj.variables))

    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))

    update_op = tp_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = tp_obj.result()
    self.assertAllClose(7., result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3351')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 126-139
</a>
<div class="mid" id="frag3351" style="display:none"><pre>
  def test_unweighted(self):
    fn_obj = metrics.FalseNegatives()
    self.evaluate(tf.compat.v1.variables_initializer(fn_obj.variables))

    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))

    update_op = fn_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = fn_obj.result()
    self.assertAllClose(3., result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3356')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 194-207
</a>
<div class="mid" id="frag3356" style="display:none"><pre>
  def test_unweighted(self):
    tn_obj = metrics.TrueNegatives()
    self.evaluate(tf.compat.v1.variables_initializer(tn_obj.variables))

    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))

    update_op = tn_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = tn_obj.result()
    self.assertAllClose(3., result)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 146:</b> &nbsp; 4 fragments, nominal size 10 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3346')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 60-70
</a>
<div class="mid" id="frag3346" style="display:none"><pre>
  def test_weighted(self):
    fp_obj = metrics.FalsePositives()
    self.evaluate(tf.compat.v1.variables_initializer(fp_obj.variables))
    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))
    sample_weight = tf.constant((1., 1.5, 2., 2.5))
    result = fp_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose(14., self.evaluate(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3352')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 140-150
</a>
<div class="mid" id="frag3352" style="display:none"><pre>
  def test_weighted(self):
    fn_obj = metrics.FalseNegatives()
    self.evaluate(tf.compat.v1.variables_initializer(fn_obj.variables))
    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))
    sample_weight = tf.constant((1., 1.5, 2., 2.5))
    result = fn_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose(5., self.evaluate(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3362')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 276-286
</a>
<div class="mid" id="frag3362" style="display:none"><pre>
  def test_weighted(self):
    tp_obj = metrics.TruePositives()
    self.evaluate(tf.compat.v1.variables_initializer(tp_obj.variables))
    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))
    sample_weight = tf.constant((1., 1.5, 2., 2.5))
    result = tp_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose(12., self.evaluate(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3357')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 208-218
</a>
<div class="mid" id="frag3357" style="display:none"><pre>
  def test_weighted(self):
    tn_obj = metrics.TrueNegatives()
    self.evaluate(tf.compat.v1.variables_initializer(tn_obj.variables))
    y_true = tf.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),
                                   (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))
    y_pred = tf.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),
                                   (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))
    sample_weight = tf.constant((1., 1.5, 2., 2.5))
    result = tn_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose(4., self.evaluate(result))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 147:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3347')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 71-84
</a>
<div class="mid" id="frag3347" style="display:none"><pre>
  def test_unweighted_with_thresholds(self):
    fp_obj = metrics.FalsePositives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(fp_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))

    update_op = fp_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = fp_obj.result()
    self.assertAllClose([7., 4., 2.], result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3353')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 151-164
</a>
<div class="mid" id="frag3353" style="display:none"><pre>
  def test_unweighted_with_thresholds(self):
    fn_obj = metrics.FalseNegatives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(fn_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))

    update_op = fn_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = fn_obj.result()
    self.assertAllClose([1., 4., 6.], result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3363')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 287-300
</a>
<div class="mid" id="frag3363" style="display:none"><pre>
  def test_unweighted_with_thresholds(self):
    tp_obj = metrics.TruePositives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(tp_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))

    update_op = tp_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = tp_obj.result()
    self.assertAllClose([6., 3., 1.], result)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3358')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 219-232
</a>
<div class="mid" id="frag3358" style="display:none"><pre>
  def test_unweighted_with_thresholds(self):
    tn_obj = metrics.TrueNegatives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(tn_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))

    update_op = tn_obj.update_state(y_true, y_pred)
    self.evaluate(update_op)
    result = tn_obj.result()
    self.assertAllClose([2., 5., 7.], result)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 148:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3348')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 85-98
</a>
<div class="mid" id="frag3348" style="display:none"><pre>
  def test_weighted_with_thresholds(self):
    fp_obj = metrics.FalsePositives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(fp_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))
    sample_weight = ((1.0, 2.0, 3.0, 5.0), (7.0, 11.0, 13.0, 17.0),
                     (19.0, 23.0, 29.0, 31.0), (5.0, 15.0, 10.0, 0))

    result = fp_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose([125., 42., 12.], self.evaluate(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3359')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 233-246
</a>
<div class="mid" id="frag3359" style="display:none"><pre>
  def test_weighted_with_thresholds(self):
    tn_obj = metrics.TrueNegatives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(tn_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))
    sample_weight = ((0.0, 2.0, 3.0, 5.0),)

    result = tn_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose([5., 15., 23.], self.evaluate(result))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3354')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 165-178
</a>
<div class="mid" id="frag3354" style="display:none"><pre>
  def test_weighted_with_thresholds(self):
    fn_obj = metrics.FalseNegatives(thresholds=[0.15, 0.5, 0.85])
    self.evaluate(tf.compat.v1.variables_initializer(fn_obj.variables))

    y_pred = tf.constant(((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),
                                   (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3)))
    y_true = tf.constant(((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),
                                   (1, 1, 1, 1)))
    sample_weight = ((3.0,), (5.0,), (7.0,), (4.0,))

    result = fn_obj(y_true, y_pred, sample_weight=sample_weight)
    self.assertAllClose([4., 16., 23.], self.evaluate(result))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 149:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3365')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 317-335
</a>
<div class="mid" id="frag3365" style="display:none"><pre>
  def test_config(self):
    p_obj = metrics.Precision(
        name='my_precision', thresholds=[0.4, 0.9], top_k=15, class_id=12)
    self.assertEqual(p_obj.name, 'my_precision')
    self.assertLen(p_obj.variables, 2)
    self.assertEqual([v.name for v in p_obj.variables],
                     ['true_positives:0', 'false_positives:0'])
    self.assertEqual(p_obj.thresholds, [0.4, 0.9])
    self.assertEqual(p_obj.top_k, 15)
    self.assertEqual(p_obj.class_id, 12)

    # Check save and restore config
    p_obj2 = metrics.Precision.from_config(p_obj.get_config())
    self.assertEqual(p_obj2.name, 'my_precision')
    self.assertLen(p_obj2.variables, 2)
    self.assertEqual(p_obj2.thresholds, [0.4, 0.9])
    self.assertEqual(p_obj2.top_k, 15)
    self.assertEqual(p_obj2.class_id, 12)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3379')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 522-540
</a>
<div class="mid" id="frag3379" style="display:none"><pre>
  def test_config(self):
    r_obj = metrics.Recall(
        name='my_recall', thresholds=[0.4, 0.9], top_k=15, class_id=12)
    self.assertEqual(r_obj.name, 'my_recall')
    self.assertLen(r_obj.variables, 2)
    self.assertEqual([v.name for v in r_obj.variables],
                     ['true_positives:0', 'false_negatives:0'])
    self.assertEqual(r_obj.thresholds, [0.4, 0.9])
    self.assertEqual(r_obj.top_k, 15)
    self.assertEqual(r_obj.class_id, 12)

    # Check save and restore config
    r_obj2 = metrics.Recall.from_config(r_obj.get_config())
    self.assertEqual(r_obj2.name, 'my_recall')
    self.assertLen(r_obj2.variables, 2)
    self.assertEqual(r_obj2.thresholds, [0.4, 0.9])
    self.assertEqual(r_obj2.top_k, 15)
    self.assertEqual(r_obj2.class_id, 12)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 150:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3366')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 336-352
</a>
<div class="mid" id="frag3366" style="display:none"><pre>
  def test_value_is_idempotent(self):
    p_obj = metrics.Precision(thresholds=[0.3, 0.72])
    y_pred = tf.random.uniform(shape=(10, 3))
    y_true = tf.random.uniform(shape=(10, 3))
    update_op = p_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_precision = self.evaluate(p_obj.result())
    for _ in range(10):
      self.assertArrayNear(initial_precision, self.evaluate(p_obj.result()),
                           1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3380')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 541-556
</a>
<div class="mid" id="frag3380" style="display:none"><pre>
  def test_value_is_idempotent(self):
    r_obj = metrics.Recall(thresholds=[0.3, 0.72])
    y_pred = tf.random.uniform(shape=(10, 3))
    y_true = tf.random.uniform(shape=(10, 3))
    update_op = r_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_recall = self.evaluate(r_obj.result())
    for _ in range(10):
      self.assertArrayNear(initial_recall, self.evaluate(r_obj.result()), 1e-3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 151:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3369')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 370-383
</a>
<div class="mid" id="frag3369" style="display:none"><pre>
  def test_weighted(self):
    p_obj = metrics.Precision()
    y_pred = tf.constant([[1, 0, 1, 0], [1, 0, 1, 0]])
    y_true = tf.constant([[0, 1, 1, 0], [1, 0, 0, 1]])
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))
    result = p_obj(
        y_true,
        y_pred,
        sample_weight=tf.constant([[1, 2, 3, 4], [4, 3, 2, 1]]))
    weighted_tp = 3.0 + 4.0
    weighted_positives = (1.0 + 3.0) + (4.0 + 2.0)
    expected_precision = weighted_tp / weighted_positives
    self.assertAlmostEqual(expected_precision, self.evaluate(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3383')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 574-587
</a>
<div class="mid" id="frag3383" style="display:none"><pre>
  def test_weighted(self):
    r_obj = metrics.Recall()
    y_pred = tf.constant([[1, 0, 1, 0], [0, 1, 0, 1]])
    y_true = tf.constant([[0, 1, 1, 0], [1, 0, 0, 1]])
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))
    result = r_obj(
        y_true,
        y_pred,
        sample_weight=tf.constant([[1, 2, 3, 4], [4, 3, 2, 1]]))
    weighted_tp = 3.0 + 1.0
    weighted_t = (2.0 + 3.0) + (4.0 + 1.0)
    expected_recall = weighted_tp / weighted_t
    self.assertAlmostEqual(expected_recall, self.evaluate(result))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 152:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3372')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 400-415
</a>
<div class="mid" id="frag3372" style="display:none"><pre>
  def test_weighted_with_threshold(self):
    p_obj = metrics.Precision(thresholds=[0.5, 1.])
    y_true = tf.constant([[0, 1], [1, 0]], shape=(2, 2))
    y_pred = tf.constant([[1, 0], [0.6, 0]],
                                  shape=(2, 2),
                                  dtype=tf.float32)
    weights = tf.constant([[4, 0], [3, 1]],
                                   shape=(2, 2),
                                   dtype=tf.float32)
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))
    result = p_obj(y_true, y_pred, sample_weight=weights)
    weighted_tp = 0 + 3.
    weighted_positives = (0 + 3.) + (4. + 0.)
    expected_precision = weighted_tp / weighted_positives
    self.assertArrayNear([expected_precision, 0], self.evaluate(result), 1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3386')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 604-619
</a>
<div class="mid" id="frag3386" style="display:none"><pre>
  def test_weighted_with_threshold(self):
    r_obj = metrics.Recall(thresholds=[0.5, 1.])
    y_true = tf.constant([[0, 1], [1, 0]], shape=(2, 2))
    y_pred = tf.constant([[1, 0], [0.6, 0]],
                                  shape=(2, 2),
                                  dtype=tf.float32)
    weights = tf.constant([[1, 4], [3, 2]],
                                   shape=(2, 2),
                                   dtype=tf.float32)
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))
    result = r_obj(y_true, y_pred, sample_weight=weights)
    weighted_tp = 0 + 3.
    weighted_positives = (0 + 3.) + (4. + 0.)
    expected_recall = weighted_tp / weighted_positives
    self.assertArrayNear([expected_recall, 0], self.evaluate(result), 1e-3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 153:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3373')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 416-435
</a>
<div class="mid" id="frag3373" style="display:none"><pre>
  def test_multiple_updates(self):
    p_obj = metrics.Precision(thresholds=[0.5, 1.])
    y_true = tf.constant([[0, 1], [1, 0]], shape=(2, 2))
    y_pred = tf.constant([[1, 0], [0.6, 0]],
                                  shape=(2, 2),
                                  dtype=tf.float32)
    weights = tf.constant([[4, 0], [3, 1]],
                                   shape=(2, 2),
                                   dtype=tf.float32)
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))
    update_op = p_obj.update_state(y_true, y_pred, sample_weight=weights)
    for _ in range(2):
      self.evaluate(update_op)

    weighted_tp = (0 + 3.) + (0 + 3.)
    weighted_positives = ((0 + 3.) + (4. + 0.)) + ((0 + 3.) + (4. + 0.))
    expected_precision = weighted_tp / weighted_positives
    self.assertArrayNear([expected_precision, 0], self.evaluate(p_obj.result()),
                         1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3387')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 620-639
</a>
<div class="mid" id="frag3387" style="display:none"><pre>
  def test_multiple_updates(self):
    r_obj = metrics.Recall(thresholds=[0.5, 1.])
    y_true = tf.constant([[0, 1], [1, 0]], shape=(2, 2))
    y_pred = tf.constant([[1, 0], [0.6, 0]],
                                  shape=(2, 2),
                                  dtype=tf.float32)
    weights = tf.constant([[1, 4], [3, 2]],
                                   shape=(2, 2),
                                   dtype=tf.float32)
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))
    update_op = r_obj.update_state(y_true, y_pred, sample_weight=weights)
    for _ in range(2):
      self.evaluate(update_op)

    weighted_tp = (0 + 3.) + (0 + 3.)
    weighted_positives = ((0 + 3.) + (4. + 0.)) + ((0 + 3.) + (4. + 0.))
    expected_recall = weighted_tp / weighted_positives
    self.assertArrayNear([expected_recall, 0], self.evaluate(r_obj.result()),
                         1e-3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 154:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3375')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 444-463
</a>
<div class="mid" id="frag3375" style="display:none"><pre>
  def test_weighted_top_k(self):
    p_obj = metrics.Precision(top_k=3)
    y_pred1 = tf.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))
    y_true1 = tf.constant([0, 1, 1, 0, 1], shape=(1, 5))
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))
    self.evaluate(
        p_obj(
            y_true1,
            y_pred1,
            sample_weight=tf.constant([[1, 4, 2, 3, 5]])))

    y_pred2 = tf.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))
    y_true2 = tf.constant([1, 0, 1, 1, 1], shape=(1, 5))
    result = p_obj(y_true2, y_pred2, sample_weight=tf.constant(3))

    tp = (2 + 5) + (3 + 3)
    predicted_positives = (1 + 2 + 5) + (3 + 3 + 3)
    expected_precision = tp / predicted_positives
    self.assertAlmostEqual(expected_precision, self.evaluate(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3389')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 648-667
</a>
<div class="mid" id="frag3389" style="display:none"><pre>
  def test_weighted_top_k(self):
    r_obj = metrics.Recall(top_k=3)
    y_pred1 = tf.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))
    y_true1 = tf.constant([0, 1, 1, 0, 1], shape=(1, 5))
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))
    self.evaluate(
        r_obj(
            y_true1,
            y_pred1,
            sample_weight=tf.constant([[1, 4, 2, 3, 5]])))

    y_pred2 = tf.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))
    y_true2 = tf.constant([1, 0, 1, 1, 1], shape=(1, 5))
    result = r_obj(y_true2, y_pred2, sample_weight=tf.constant(3))

    tp = (2 + 5) + (3 + 3)
    positives = (4 + 2 + 5) + (3 + 3 + 3 + 3)
    expected_recall = tp / positives
    self.assertAlmostEqual(expected_recall, self.evaluate(result))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 155:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3376')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 464-488
</a>
<div class="mid" id="frag3376" style="display:none"><pre>
  def test_unweighted_class_id(self):
    p_obj = metrics.Precision(class_id=2)
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))

    y_pred = tf.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))
    y_true = tf.constant([0, 1, 1, 0, 0], shape=(1, 5))
    result = p_obj(y_true, y_pred)
    self.assertAlmostEqual(1, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(p_obj.true_positives))
    self.assertAlmostEqual(0, self.evaluate(p_obj.false_positives))

    y_pred = tf.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))
    y_true = tf.constant([0, 1, 1, 0, 0], shape=(1, 5))
    result = p_obj(y_true, y_pred)
    self.assertAlmostEqual(1, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(p_obj.true_positives))
    self.assertAlmostEqual(0, self.evaluate(p_obj.false_positives))

    y_pred = tf.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))
    y_true = tf.constant([0, 1, 0, 0, 0], shape=(1, 5))
    result = p_obj(y_true, y_pred)
    self.assertAlmostEqual(0.5, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(p_obj.true_positives))
    self.assertAlmostEqual(1, self.evaluate(p_obj.false_positives))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3390')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 668-692
</a>
<div class="mid" id="frag3390" style="display:none"><pre>
  def test_unweighted_class_id(self):
    r_obj = metrics.Recall(class_id=2)
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))

    y_pred = tf.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))
    y_true = tf.constant([0, 1, 1, 0, 0], shape=(1, 5))
    result = r_obj(y_true, y_pred)
    self.assertAlmostEqual(1, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(r_obj.true_positives))
    self.assertAlmostEqual(0, self.evaluate(r_obj.false_negatives))

    y_pred = tf.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))
    y_true = tf.constant([0, 1, 1, 0, 0], shape=(1, 5))
    result = r_obj(y_true, y_pred)
    self.assertAlmostEqual(0.5, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(r_obj.true_positives))
    self.assertAlmostEqual(1, self.evaluate(r_obj.false_negatives))

    y_pred = tf.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))
    y_true = tf.constant([0, 1, 0, 0, 0], shape=(1, 5))
    result = r_obj(y_true, y_pred)
    self.assertAlmostEqual(0.5, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(r_obj.true_positives))
    self.assertAlmostEqual(1, self.evaluate(r_obj.false_negatives))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 156:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3377')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 489-506
</a>
<div class="mid" id="frag3377" style="display:none"><pre>
  def test_unweighted_top_k_and_class_id(self):
    p_obj = metrics.Precision(class_id=2, top_k=2)
    self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))

    y_pred = tf.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))
    y_true = tf.constant([0, 1, 1, 0, 0], shape=(1, 5))
    result = p_obj(y_true, y_pred)
    self.assertAlmostEqual(1, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(p_obj.true_positives))
    self.assertAlmostEqual(0, self.evaluate(p_obj.false_positives))

    y_pred = tf.constant([1, 1, 0.9, 1, 1], shape=(1, 5))
    y_true = tf.constant([0, 1, 1, 0, 0], shape=(1, 5))
    result = p_obj(y_true, y_pred)
    self.assertAlmostEqual(1, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(p_obj.true_positives))
    self.assertAlmostEqual(0, self.evaluate(p_obj.false_positives))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3391')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 693-710
</a>
<div class="mid" id="frag3391" style="display:none"><pre>
  def test_unweighted_top_k_and_class_id(self):
    r_obj = metrics.Recall(class_id=2, top_k=2)
    self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))

    y_pred = tf.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))
    y_true = tf.constant([0, 1, 1, 0, 0], shape=(1, 5))
    result = r_obj(y_true, y_pred)
    self.assertAlmostEqual(1, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(r_obj.true_positives))
    self.assertAlmostEqual(0, self.evaluate(r_obj.false_negatives))

    y_pred = tf.constant([1, 1, 0.9, 1, 1], shape=(1, 5))
    y_true = tf.constant([0, 1, 1, 0, 0], shape=(1, 5))
    result = r_obj(y_true, y_pred)
    self.assertAlmostEqual(0.5, self.evaluate(result))
    self.assertAlmostEqual(1, self.evaluate(r_obj.true_positives))
    self.assertAlmostEqual(1, self.evaluate(r_obj.false_negatives))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 157:</b> &nbsp; 4 fragments, nominal size 16 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3393')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 726-745
</a>
<div class="mid" id="frag3393" style="display:none"><pre>
  def test_config(self):
    s_obj = metrics.SensitivityAtSpecificity(
        0.4,
        num_thresholds=100,
        class_id=12,
        name='sensitivity_at_specificity_1')
    self.assertEqual(s_obj.name, 'sensitivity_at_specificity_1')
    self.assertLen(s_obj.variables, 4)
    self.assertEqual(s_obj.specificity, 0.4)
    self.assertEqual(s_obj.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

    # Check save and restore config
    s_obj2 = metrics.SensitivityAtSpecificity.from_config(s_obj.get_config())
    self.assertEqual(s_obj2.name, 'sensitivity_at_specificity_1')
    self.assertLen(s_obj2.variables, 4)
    self.assertEqual(s_obj2.specificity, 0.4)
    self.assertEqual(s_obj2.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3420')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1064-1080
</a>
<div class="mid" id="frag3420" style="display:none"><pre>
  def test_config(self):
    s_obj = metrics.RecallAtPrecision(
        0.4, num_thresholds=100, class_id=12, name='recall_at_precision_1')
    self.assertEqual(s_obj.name, 'recall_at_precision_1')
    self.assertLen(s_obj.variables, 4)
    self.assertEqual(s_obj.precision, 0.4)
    self.assertEqual(s_obj.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

    # Check save and restore config
    s_obj2 = metrics.RecallAtPrecision.from_config(s_obj.get_config())
    self.assertEqual(s_obj2.name, 'recall_at_precision_1')
    self.assertLen(s_obj2.variables, 4)
    self.assertEqual(s_obj2.precision, 0.4)
    self.assertEqual(s_obj2.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3411')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 951-967
</a>
<div class="mid" id="frag3411" style="display:none"><pre>
  def test_config(self):
    s_obj = metrics.PrecisionAtRecall(
        0.4, num_thresholds=100, class_id=12, name='precision_at_recall_1')
    self.assertEqual(s_obj.name, 'precision_at_recall_1')
    self.assertLen(s_obj.variables, 4)
    self.assertEqual(s_obj.recall, 0.4)
    self.assertEqual(s_obj.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

    # Check save and restore config
    s_obj2 = metrics.PrecisionAtRecall.from_config(s_obj.get_config())
    self.assertEqual(s_obj2.name, 'precision_at_recall_1')
    self.assertLen(s_obj2.variables, 4)
    self.assertEqual(s_obj2.recall, 0.4)
    self.assertEqual(s_obj2.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3402')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 839-858
</a>
<div class="mid" id="frag3402" style="display:none"><pre>
  def test_config(self):
    s_obj = metrics.SpecificityAtSensitivity(
        0.4,
        num_thresholds=100,
        class_id=12,
        name='specificity_at_sensitivity_1')
    self.assertEqual(s_obj.name, 'specificity_at_sensitivity_1')
    self.assertLen(s_obj.variables, 4)
    self.assertEqual(s_obj.sensitivity, 0.4)
    self.assertEqual(s_obj.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

    # Check save and restore config
    s_obj2 = metrics.SpecificityAtSensitivity.from_config(s_obj.get_config())
    self.assertEqual(s_obj2.name, 'specificity_at_sensitivity_1')
    self.assertLen(s_obj2.variables, 4)
    self.assertEqual(s_obj2.sensitivity, 0.4)
    self.assertEqual(s_obj2.num_thresholds, 100)
    self.assertEqual(s_obj.class_id, 12)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 158:</b> &nbsp; 4 fragments, nominal size 18 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3394')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 746-768
</a>
<div class="mid" id="frag3394" style="display:none"><pre>
  def test_value_is_idempotent(self):
    s_obj = metrics.SensitivityAtSpecificity(0.7)
    y_pred = tf.random.uniform((10, 3),
                                       maxval=1,
                                       dtype=tf.float32,
                                       seed=1)
    y_true = tf.random.uniform((10, 3),
                                       maxval=2,
                                       dtype=tf.int64,
                                       seed=1)
    update_op = s_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_sensitivity = self.evaluate(s_obj.result())
    for _ in range(10):
      self.assertAlmostEqual(initial_sensitivity, self.evaluate(s_obj.result()),
                             1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3403')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 859-881
</a>
<div class="mid" id="frag3403" style="display:none"><pre>
  def test_value_is_idempotent(self):
    s_obj = metrics.SpecificityAtSensitivity(0.7)
    y_pred = tf.random.uniform((10, 3),
                                       maxval=1,
                                       dtype=tf.float32,
                                       seed=1)
    y_true = tf.random.uniform((10, 3),
                                       maxval=2,
                                       dtype=tf.int64,
                                       seed=1)
    update_op = s_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_specificity = self.evaluate(s_obj.result())
    for _ in range(10):
      self.assertAlmostEqual(initial_specificity, self.evaluate(s_obj.result()),
                             1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3421')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1081-1103
</a>
<div class="mid" id="frag3421" style="display:none"><pre>
  def test_value_is_idempotent(self):
    s_obj = metrics.RecallAtPrecision(0.7)
    y_pred = tf.random.uniform((10, 3),
                                       maxval=1,
                                       dtype=tf.float32,
                                       seed=1)
    y_true = tf.random.uniform((10, 3),
                                       maxval=2,
                                       dtype=tf.int64,
                                       seed=1)
    update_op = s_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_recall = self.evaluate(s_obj.result())
    for _ in range(10):
      self.assertAlmostEqual(initial_recall, self.evaluate(s_obj.result()),
                             1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3412')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 968-990
</a>
<div class="mid" id="frag3412" style="display:none"><pre>
  def test_value_is_idempotent(self):
    s_obj = metrics.PrecisionAtRecall(0.7)
    y_pred = tf.random.uniform((10, 3),
                                       maxval=1,
                                       dtype=tf.float32,
                                       seed=1)
    y_true = tf.random.uniform((10, 3),
                                       maxval=2,
                                       dtype=tf.int64,
                                       seed=1)
    update_op = s_obj.update_state(y_true, y_pred)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))

    # Run several updates.
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_precision = self.evaluate(s_obj.result())
    for _ in range(10):
      self.assertAlmostEqual(initial_precision, self.evaluate(s_obj.result()),
                             1e-3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 159:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3399')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 813-825
</a>
<div class="mid" id="frag3399" style="display:none"><pre>
  def test_weighted(self, label_dtype):
    s_obj = metrics.SensitivityAtSpecificity(0.4)
    pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]
    label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
    weight_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

    y_pred = tf.constant(pred_values, dtype=tf.float32)
    y_true = tf.cast(label_values, dtype=label_dtype)
    weights = tf.constant(weight_values)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))
    result = s_obj(y_true, y_pred, sample_weight=weights)
    self.assertAlmostEqual(0.675, self.evaluate(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3408')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 925-937
</a>
<div class="mid" id="frag3408" style="display:none"><pre>
  def test_weighted(self, label_dtype):
    s_obj = metrics.SpecificityAtSensitivity(0.4)
    pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]
    label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
    weight_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

    y_pred = tf.constant(pred_values, dtype=tf.float32)
    y_true = tf.cast(label_values, dtype=label_dtype)
    weights = tf.constant(weight_values)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))
    result = s_obj(y_true, y_pred, sample_weight=weights)
    self.assertAlmostEqual(0.4, self.evaluate(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3426')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1159-1170
</a>
<div class="mid" id="frag3426" style="display:none"><pre>
  def test_weighted(self, label_dtype):
    s_obj = metrics.RecallAtPrecision(0.75)
    pred_values = [0.1, 0.2, 0.3, 0.5, 0.6, 0.9, 0.9]
    label_values = [0, 1, 0, 0, 0, 1, 1]
    weight_values = [1, 2, 1, 2, 1, 2, 1]
    y_pred = tf.constant(pred_values, dtype=tf.float32)
    y_true = tf.cast(label_values, dtype=label_dtype)
    weights = tf.constant(weight_values)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))
    result = s_obj(y_true, y_pred, sample_weight=weights)
    self.assertAlmostEqual(0.6, self.evaluate(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3417')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1037-1050
</a>
<div class="mid" id="frag3417" style="display:none"><pre>
  def test_weighted(self, label_dtype):
    s_obj = metrics.PrecisionAtRecall(7.0/8)
    pred_values = [0.0, 0.1, 0.2, 0.5, 0.6, 0.2, 0.5, 0.6, 0.8, 0.9]
    label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
    weight_values = [2, 1, 2, 1, 2, 1, 2, 2, 1, 2]

    y_pred = tf.constant(pred_values, dtype=tf.float32)
    y_true = tf.cast(label_values, dtype=label_dtype)
    weights = tf.constant(weight_values)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))
    result = s_obj(y_true, y_pred, sample_weight=weights)
    # For 0.0 &lt; decision threshold &lt; 0.2.
    self.assertAlmostEqual(0.7, self.evaluate(result))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 160:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3423')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1113-1127
</a>
<div class="mid" id="frag3423" style="display:none"><pre>
  def test_unweighted_high_precision(self):
    s_obj = metrics.RecallAtPrecision(0.75)
    pred_values = [
        0.05, 0.1, 0.2, 0.3, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.9, 0.95
    ]
    label_values = [0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1]
    # precisions: [1/2, 6/11, 1/2, 5/9, 5/8, 5/7, 2/3, 3/5, 3/5, 2/3, 1/2, 1].
    # recalls:    [1,   1,    5/6, 5/6, 5/6, 5/6, 2/3, 1/2, 1/2, 1/3, 1/6, 1/6].
    y_pred = tf.constant(pred_values, dtype=tf.float32)
    y_true = tf.constant(label_values)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))
    result = s_obj(y_true, y_pred)
    # The precision 0.75 can be reached at thresholds 0.4&lt;=t&lt;0.45.
    self.assertAlmostEqual(0.5, self.evaluate(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3424')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1128-1142
</a>
<div class="mid" id="frag3424" style="display:none"><pre>
  def test_unweighted_low_precision(self):
    s_obj = metrics.RecallAtPrecision(2.0 / 3)
    pred_values = [
        0.05, 0.1, 0.2, 0.3, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.9, 0.95
    ]
    label_values = [0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1]
    # precisions: [1/2, 6/11, 1/2, 5/9, 5/8, 5/7, 2/3, 3/5, 3/5, 2/3, 1/2, 1].
    # recalls:    [1,   1,    5/6, 5/6, 5/6, 5/6, 2/3, 1/2, 1/2, 1/3, 1/6, 1/6].
    y_pred = tf.constant(pred_values, dtype=tf.float32)
    y_true = tf.constant(label_values)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))
    result = s_obj(y_true, y_pred)
    # The precision 5/7 can be reached at thresholds 00.3&lt;=t&lt;0.35.
    self.assertAlmostEqual(5. / 6, self.evaluate(result))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3425')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1143-1157
</a>
<div class="mid" id="frag3425" style="display:none"><pre>
  def test_unweighted_class_id(self):
    s_obj = metrics.RecallAtPrecision(2.0 / 3, class_id=2)
    pred_values = [
        0.05, 0.1, 0.2, 0.3, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.9, 0.95
    ]
    label_values = [0, 2, 0, 0, 0, 2, 2, 0, 2, 2, 0, 2]
    # precisions: [1/2, 6/11, 1/2, 5/9, 5/8, 5/7, 2/3, 3/5, 3/5, 2/3, 1/2, 1].
    # recalls:    [1,   1,    5/6, 5/6, 5/6, 5/6, 2/3, 1/2, 1/2, 1/3, 1/6, 1/6].
    y_pred = tf.transpose([pred_values] * 3)
    y_true = tf.one_hot(label_values, depth=3)
    self.evaluate(tf.compat.v1.variables_initializer(s_obj.variables))
    result = s_obj(y_true, y_pred)
    # The precision 5/7 can be reached at thresholds 00.3&lt;=t&lt;0.35.
    self.assertAlmostEqual(5. / 6, self.evaluate(result))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 161:</b> &nbsp; 2 fragments, nominal size 29 lines, similarity 86%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3431')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1225-1256
</a>
<div class="mid" id="frag3431" style="display:none"><pre>
  def test_config(self):
    self.setup()
    auc_obj = metrics.AUC(
        num_thresholds=100,
        curve='PR',
        summation_method='majoring',
        name='auc_1')
    auc_obj.update_state(self.y_true, self.y_pred)
    self.assertEqual(auc_obj.name, 'auc_1')
    self.assertLen(auc_obj.variables, 4)
    self.assertEqual(auc_obj.num_thresholds, 100)
    self.assertEqual(auc_obj.curve, metrics_utils.AUCCurve.PR)
    self.assertEqual(auc_obj.summation_method,
                     metrics_utils.AUCSummationMethod.MAJORING)
    old_config = auc_obj.get_config()
    self.assertNotIn('thresholds', old_config)
    self.assertDictEqual(old_config, json.loads(json.dumps(old_config)))

    # Check save and restore config.
    auc_obj2 = metrics.AUC.from_config(auc_obj.get_config())
    auc_obj2.update_state(self.y_true, self.y_pred)
    self.assertEqual(auc_obj2.name, 'auc_1')
    self.assertLen(auc_obj2.variables, 4)
    self.assertEqual(auc_obj2.num_thresholds, 100)
    self.assertEqual(auc_obj2.curve, metrics_utils.AUCCurve.PR)
    self.assertEqual(auc_obj2.summation_method,
                     metrics_utils.AUCSummationMethod.MAJORING)
    new_config = auc_obj2.get_config()
    self.assertNotIn('thresholds', new_config)
    self.assertDictEqual(old_config, new_config)
    self.assertAllClose(auc_obj.thresholds, auc_obj2.thresholds)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3432')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1257-1288
</a>
<div class="mid" id="frag3432" style="display:none"><pre>
  def test_config_manual_thresholds(self):
    self.setup()
    auc_obj = metrics.AUC(
        num_thresholds=None,
        curve='PR',
        summation_method='majoring',
        name='auc_1',
        thresholds=[0.3, 0.5])
    auc_obj.update_state(self.y_true, self.y_pred)
    self.assertEqual(auc_obj.name, 'auc_1')
    self.assertLen(auc_obj.variables, 4)
    self.assertEqual(auc_obj.num_thresholds, 4)
    self.assertAllClose(auc_obj.thresholds, [0.0, 0.3, 0.5, 1.0])
    self.assertEqual(auc_obj.curve, metrics_utils.AUCCurve.PR)
    self.assertEqual(auc_obj.summation_method,
                     metrics_utils.AUCSummationMethod.MAJORING)
    old_config = auc_obj.get_config()
    self.assertDictEqual(old_config, json.loads(json.dumps(old_config)))

    # Check save and restore config.
    auc_obj2 = metrics.AUC.from_config(auc_obj.get_config())
    auc_obj2.update_state(self.y_true, self.y_pred)
    self.assertEqual(auc_obj2.name, 'auc_1')
    self.assertLen(auc_obj2.variables, 4)
    self.assertEqual(auc_obj2.num_thresholds, 4)
    self.assertEqual(auc_obj2.curve, metrics_utils.AUCCurve.PR)
    self.assertEqual(auc_obj2.summation_method,
                     metrics_utils.AUCSummationMethod.MAJORING)
    new_config = auc_obj2.get_config()
    self.assertDictEqual(old_config, new_config)
    self.assertAllClose(auc_obj.thresholds, auc_obj2.thresholds)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 162:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3433')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1289-1303
</a>
<div class="mid" id="frag3433" style="display:none"><pre>
  def test_value_is_idempotent(self):
    self.setup()
    auc_obj = metrics.AUC(num_thresholds=3)
    self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))

    # Run several updates.
    update_op = auc_obj.update_state(self.y_true, self.y_pred)
    for _ in range(10):
      self.evaluate(update_op)

    # Then verify idempotency.
    initial_auc = self.evaluate(auc_obj.result())
    for _ in range(10):
      self.assertAllClose(initial_auc, self.evaluate(auc_obj.result()), 1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3449')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1542-1557
</a>
<div class="mid" id="frag3449" style="display:none"><pre>
  def test_value_is_idempotent(self):
    with self.test_session():
      self.setup()
      auc_obj = metrics.AUC(num_thresholds=5, multi_label=True)
      self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))

      # Run several updates.
      update_op = auc_obj.update_state(self.y_true_good, self.y_pred)
      for _ in range(10):
        self.evaluate(update_op)

      # Then verify idempotency.
      initial_auc = self.evaluate(auc_obj.result())
      for _ in range(10):
        self.assertAllClose(initial_auc, self.evaluate(auc_obj.result()), 1e-3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 163:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3441')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1400-1416
</a>
<div class="mid" id="frag3441" style="display:none"><pre>
  def test_weighted_pr_majoring(self):
    self.setup()
    auc_obj = metrics.AUC(
        num_thresholds=self.num_thresholds,
        curve='PR',
        summation_method='majoring')
    self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))
    result = auc_obj(self.y_true, self.y_pred, sample_weight=self.sample_weight)

    # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]
    # precision = [7/(7+3), 4/4, 0] = [0.7, 1, 0]
    # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]
    # heights = [max(0.7, 1), max(1, 0)] = [1, 1]
    # widths = [(1 - 0.571), (0.571 - 0)] = [0.429, 0.571]
    expected_result = (1 * 0.429 + 1 * 0.571)
    self.assertAllClose(self.evaluate(result), expected_result, 1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3442')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1417-1433
</a>
<div class="mid" id="frag3442" style="display:none"><pre>
  def test_weighted_pr_minoring(self):
    self.setup()
    auc_obj = metrics.AUC(
        num_thresholds=self.num_thresholds,
        curve='PR',
        summation_method='minoring')
    self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))
    result = auc_obj(self.y_true, self.y_pred, sample_weight=self.sample_weight)

    # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]
    # precision = [7/(7+3), 4/4, 0] = [0.7, 1, 0]
    # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]
    # heights = [min(0.7, 1), min(1, 0)] = [0.7, 0]
    # widths = [(1 - 0.571), (0.571 - 0)] = [0.429, 0.571]
    expected_result = (0.7 * 0.429 + 0 * 0.571)
    self.assertAllClose(self.evaluate(result), expected_result, 1e-3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 164:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3453')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1586-1600
</a>
<div class="mid" id="frag3453" style="display:none"><pre>
  def test_unweighted_from_logits(self):
    with self.test_session():
      self.setup()
      auc_obj = metrics.AUC(
          num_thresholds=self.num_thresholds,
          multi_label=True,
          from_logits=True)
      self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))
      result = auc_obj(self.y_true_good, self.y_pred_logits)

      # tpr = [[1, 1, 0.5, 0.5, 0], [1, 1, 0, 0, 0]]
      # fpr = [[1, 0.5, 0, 0, 0], [1, 0, 0, 0, 0]]
      expected_result = (0.875 + 1.0) / 2.0
      self.assertAllClose(self.evaluate(result), expected_result, 1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3460')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1687-1711
</a>
<div class="mid" id="frag3460" style="display:none"><pre>
  def test_manual_thresholds(self):
    with self.test_session():
      self.setup()
      # Verify that when specified, thresholds are used instead of
      # num_thresholds.
      auc_obj = metrics.AUC(num_thresholds=2, thresholds=[0.5],
                            multi_label=True)
      self.assertEqual(auc_obj.num_thresholds, 3)
      self.assertAllClose(auc_obj.thresholds, [0.0, 0.5, 1.0])
      self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))
      result = auc_obj(self.y_true_good, self.y_pred)

      # tp = [[2, 1, 0], [2, 0, 0]]
      # fp = [2, 0, 0], [2, 0, 0]]
      # fn = [[0, 1, 2], [0, 2, 2]]
      # tn = [[0, 2, 2], [0, 2, 2]]

      # tpr = [[1, 0.5, 0], [1, 0, 0]]
      # fpr = [[1, 0, 0], [1, 0, 0]]

      # auc by slice = [0.75, 0.5]
      expected_result = (0.75 + 0.5) / 2.0

      self.assertAllClose(self.evaluate(result), expected_result, 1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3456')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1625-1639
</a>
<div class="mid" id="frag3456" style="display:none"><pre>
  def test_label_weights(self):
    with self.test_session():
      self.setup()
      auc_obj = metrics.AUC(
          num_thresholds=self.num_thresholds,
          multi_label=True,
          label_weights=[0.75, 0.25])
      self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))
      result = auc_obj(self.y_true_good, self.y_pred)

      # tpr = [[1, 1, 0.5, 0.5, 0], [1, 1, 0, 0, 0]]
      # fpr = [[1, 0.5, 0, 0, 0], [1, 0, 0, 0, 0]]
      expected_result = (0.875 * 0.75 + 1.0 * 0.25) / (0.75 + 0.25)
      self.assertAllClose(self.evaluate(result), expected_result, 1e-3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3457')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1640-1653
</a>
<div class="mid" id="frag3457" style="display:none"><pre>
  def test_label_weights_flat(self):
    self.setup()
    auc_obj = metrics.AUC(
        num_thresholds=self.num_thresholds,
        multi_label=False,
        label_weights=[0.75, 0.25])
    self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))
    result = auc_obj(self.y_true_good, self.y_pred)

    # tpr = [1, 1, 0.375, 0.375, 0]
    # fpr = [1, 0.375, 0, 0, 0]
    expected_result = 1.0 - ((1.0 - 0.375) * 0.375 / 2.0)
    self.assertAllClose(self.evaluate(result), expected_result, 1e-2)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 165:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3469')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1839-1863
</a>
<div class="mid" id="frag3469" style="display:none"><pre>
  def test_even_thresholds_correctness(self, metric_cls):
    with tf.compat.forward_compatibility_horizon(2021, 6, 9):
      # make sure the old approach and new approach produce same result
      # for evenly distributed thresholds
      y_true = np.random.randint(2, size=(10,))
      y_pred = np.random.rand(10)

      even_thresholds = [0.0, 0.25, 0.5, 0.75, 1.0]
      if metric_cls == metrics.AUC:
        even_thresholds = even_thresholds[1:-1]
      metric_obj = metric_cls(thresholds=even_thresholds)
      metric_obj.update_state(y_true, y_pred)
      result1 = metric_obj.result()

      metric_obj2 = metric_cls(thresholds=even_thresholds)
      # Force to use the old approach
      metric_obj2._thresholds_distributed_evenly = False
      metric_obj2.update_state(y_true, y_pred)
      result2 = metric_obj2.result()

      self.assertAllClose(result1, result2)
      # Check all the variables are the same, eg tp, tn, fp, fn
      for v1, v2 in zip(metric_obj.variables, metric_obj2.variables):
        self.assertAllClose(v1, v2)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3470')" href="javascript:;">
keras-2.6.0/keras/metrics_confusion_matrix_test.py: 1869-1889
</a>
<div class="mid" id="frag3470" style="display:none"><pre>
  def test_even_thresholds_correctness_2(self, metric_cls):
    with tf.compat.forward_compatibility_horizon(2021, 6, 9):
      y_true = np.random.randint(2, size=(10,))
      y_pred = np.random.rand(10)

      metric_obj = metric_cls(0.5)
      metric_obj.update_state(y_true, y_pred)
      result1 = metric_obj.result()

      metric_obj2 = metric_cls(0.5)
      # Force to use the old approach
      metric_obj2._thresholds_distributed_evenly = False
      metric_obj2.update_state(y_true, y_pred)
      result2 = metric_obj2.result()

      self.assertAllClose(result1, result2)
      # Check all the variables are the same, eg tp, tn, fp, fn
      for v1, v2 in zip(metric_obj.variables, metric_obj2.variables):
        self.assertAllClose(v1, v2)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 166:</b> &nbsp; 2 fragments, nominal size 32 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3487')" href="javascript:;">
keras-2.6.0/keras/preprocessing/image_test.py: 99-139
</a>
<div class="mid" id="frag3487" style="display:none"><pre>
  def test_image_data_generator(self):
    if PIL is None:
      return  # Skip test if PIL is not available.

    for test_images in _generate_test_images():
      img_list = []
      for im in test_images:
        img_list.append(preprocessing_image.img_to_array(im)[None, ...])

      images = np.vstack(img_list)
      generator = preprocessing_image.ImageDataGenerator(
          featurewise_center=True,
          samplewise_center=True,
          featurewise_std_normalization=True,
          samplewise_std_normalization=True,
          zca_whitening=True,
          rotation_range=90.,
          width_shift_range=0.1,
          height_shift_range=0.1,
          shear_range=0.5,
          zoom_range=0.2,
          channel_shift_range=0.,
          brightness_range=(1, 5),
          fill_mode='nearest',
          cval=0.5,
          horizontal_flip=True,
          vertical_flip=True)
      # Basic test before fit
      x = np.random.random((32, 10, 10, 3))
      generator.flow(x)

      # Fit
      generator.fit(images, augment=True)

      for x, _ in generator.flow(
          images,
          np.arange(images.shape[0]),
          shuffle=True):
        self.assertEqual(x.shape[1:], images.shape[1:])
        break

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3498')" href="javascript:;">
keras-2.6.0/keras/preprocessing/image_test.py: 397-431
</a>
<div class="mid" id="frag3498" style="display:none"><pre>
  def test_batch_standardize(self):
    if PIL is None:
      return  # Skip test if PIL is not available.

    # ImageDataGenerator.standardize should work on batches
    for test_images in _generate_test_images():
      img_list = []
      for im in test_images:
        img_list.append(preprocessing_image.img_to_array(im)[None, ...])

      images = np.vstack(img_list)
      generator = preprocessing_image.ImageDataGenerator(
          featurewise_center=True,
          samplewise_center=True,
          featurewise_std_normalization=True,
          samplewise_std_normalization=True,
          zca_whitening=True,
          rotation_range=90.,
          width_shift_range=0.1,
          height_shift_range=0.1,
          shear_range=0.5,
          zoom_range=0.2,
          channel_shift_range=0.,
          brightness_range=(1, 5),
          fill_mode='nearest',
          cval=0.5,
          horizontal_flip=True,
          vertical_flip=True)
      generator.fit(images, augment=True)

      transformed = np.copy(images)
      for i, im in enumerate(transformed):
        transformed[i] = generator.random_transform(im)
      transformed = generator.standardize(transformed)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 167:</b> &nbsp; 2 fragments, nominal size 28 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3510')" href="javascript:;">
keras-2.6.0/keras/preprocessing/text_dataset_test.py: 90-119
</a>
<div class="mid" id="frag3510" style="display:none"><pre>
  def test_text_dataset_from_directory_binary(self):
    directory = self._prepare_directory(num_classes=2)
    dataset = text_dataset.text_dataset_from_directory(
        directory, batch_size=8, label_mode='int', max_length=10)
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8,))
    self.assertEqual(batch[0].dtype.name, 'string')
    self.assertEqual(len(batch[0].numpy()[0]), 10)  # Test max_length
    self.assertEqual(batch[1].shape, (8,))
    self.assertEqual(batch[1].dtype.name, 'int32')

    dataset = text_dataset.text_dataset_from_directory(
        directory, batch_size=8, label_mode='binary')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8,))
    self.assertEqual(batch[0].dtype.name, 'string')
    self.assertEqual(batch[1].shape, (8, 1))
    self.assertEqual(batch[1].dtype.name, 'float32')

    dataset = text_dataset.text_dataset_from_directory(
        directory, batch_size=8, label_mode='categorical')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8,))
    self.assertEqual(batch[0].dtype.name, 'string')
    self.assertEqual(batch[1].shape, (8, 2))
    self.assertEqual(batch[1].dtype.name, 'float32')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3532')" href="javascript:;">
keras-2.6.0/keras/preprocessing/image_dataset_test.py: 115-146
</a>
<div class="mid" id="frag3532" style="display:none"><pre>
  def test_image_dataset_from_directory_binary(self):
    if PIL is None:
      return  # Skip test if PIL is not available.

    directory = self._prepare_directory(num_classes=2)
    dataset = image_dataset.image_dataset_from_directory(
        directory, batch_size=8, image_size=(18, 18), label_mode='int')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8, 18, 18, 3))
    self.assertEqual(batch[0].dtype.name, 'float32')
    self.assertEqual(batch[1].shape, (8,))
    self.assertEqual(batch[1].dtype.name, 'int32')

    dataset = image_dataset.image_dataset_from_directory(
        directory, batch_size=8, image_size=(18, 18), label_mode='binary')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8, 18, 18, 3))
    self.assertEqual(batch[0].dtype.name, 'float32')
    self.assertEqual(batch[1].shape, (8, 1))
    self.assertEqual(batch[1].dtype.name, 'float32')

    dataset = image_dataset.image_dataset_from_directory(
        directory, batch_size=8, image_size=(18, 18), label_mode='categorical')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8, 18, 18, 3))
    self.assertEqual(batch[0].dtype.name, 'float32')
    self.assertEqual(batch[1].shape, (8, 2))
    self.assertEqual(batch[1].dtype.name, 'float32')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 168:</b> &nbsp; 2 fragments, nominal size 30 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3512')" href="javascript:;">
keras-2.6.0/keras/preprocessing/text_dataset_test.py: 129-162
</a>
<div class="mid" id="frag3512" style="display:none"><pre>
  def test_text_dataset_from_directory_multiclass(self):
    directory = self._prepare_directory(num_classes=4, count=15)

    dataset = text_dataset.text_dataset_from_directory(
        directory, batch_size=8, label_mode=None)
    batch = next(iter(dataset))
    self.assertEqual(batch.shape, (8,))

    dataset = text_dataset.text_dataset_from_directory(
        directory, batch_size=8, label_mode=None)
    sample_count = 0
    iterator = iter(dataset)
    for batch in dataset:
      sample_count += next(iterator).shape[0]
    self.assertEqual(sample_count, 15)

    dataset = text_dataset.text_dataset_from_directory(
        directory, batch_size=8, label_mode='int')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8,))
    self.assertEqual(batch[0].dtype.name, 'string')
    self.assertEqual(batch[1].shape, (8,))
    self.assertEqual(batch[1].dtype.name, 'int32')

    dataset = text_dataset.text_dataset_from_directory(
        directory, batch_size=8, label_mode='categorical')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8,))
    self.assertEqual(batch[0].dtype.name, 'string')
    self.assertEqual(batch[1].shape, (8, 4))
    self.assertEqual(batch[1].dtype.name, 'float32')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3536')" href="javascript:;">
keras-2.6.0/keras/preprocessing/image_dataset_test.py: 175-211
</a>
<div class="mid" id="frag3536" style="display:none"><pre>
  def test_image_dataset_from_directory_multiclass(self):
    if PIL is None:
      return  # Skip test if PIL is not available.

    directory = self._prepare_directory(num_classes=4, count=15)

    dataset = image_dataset.image_dataset_from_directory(
        directory, batch_size=8, image_size=(18, 18), label_mode=None)
    batch = next(iter(dataset))
    self.assertEqual(batch.shape, (8, 18, 18, 3))

    dataset = image_dataset.image_dataset_from_directory(
        directory, batch_size=8, image_size=(18, 18), label_mode=None)
    sample_count = 0
    iterator = iter(dataset)
    for batch in dataset:
      sample_count += next(iterator).shape[0]
    self.assertEqual(sample_count, 15)

    dataset = image_dataset.image_dataset_from_directory(
        directory, batch_size=8, image_size=(18, 18), label_mode='int')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8, 18, 18, 3))
    self.assertEqual(batch[0].dtype.name, 'float32')
    self.assertEqual(batch[1].shape, (8,))
    self.assertEqual(batch[1].dtype.name, 'int32')

    dataset = image_dataset.image_dataset_from_directory(
        directory, batch_size=8, image_size=(18, 18), label_mode='categorical')
    batch = next(iter(dataset))
    self.assertLen(batch, 2)
    self.assertEqual(batch[0].shape, (8, 18, 18, 3))
    self.assertEqual(batch[0].dtype.name, 'float32')
    self.assertEqual(batch[1].shape, (8, 4))
    self.assertEqual(batch[1].dtype.name, 'float32')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 169:</b> &nbsp; 2 fragments, nominal size 42 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3517')" href="javascript:;">
keras-2.6.0/keras/preprocessing/text_dataset_test.py: 201-251
</a>
<div class="mid" id="frag3517" style="display:none"><pre>
  def test_text_dataset_from_directory_errors(self):
    directory = self._prepare_directory(num_classes=3, count=5)

    with self.assertRaisesRegex(ValueError, '`labels` argument should be'):
      _ = text_dataset.text_dataset_from_directory(
          directory, labels='other')

    with self.assertRaisesRegex(ValueError, '`label_mode` argument must be'):
      _ = text_dataset.text_dataset_from_directory(
          directory, label_mode='other')

    with self.assertRaisesRegex(
        ValueError, 'only pass `class_names` if the labels are inferred'):
      _ = text_dataset.text_dataset_from_directory(
          directory, labels=[0, 0, 1, 1, 1],
          class_names=['class_0', 'class_1', 'class_2'])

    with self.assertRaisesRegex(
        ValueError,
        'Expected the lengths of `labels` to match the number of files'):
      _ = text_dataset.text_dataset_from_directory(
          directory, labels=[0, 0, 1, 1])

    with self.assertRaisesRegex(
        ValueError, '`class_names` passed did not match'):
      _ = text_dataset.text_dataset_from_directory(
          directory, class_names=['class_0', 'class_2'])

    with self.assertRaisesRegex(ValueError, 'there must exactly 2 classes'):
      _ = text_dataset.text_dataset_from_directory(
          directory, label_mode='binary')

    with self.assertRaisesRegex(ValueError,
                                '`validation_split` must be between 0 and 1'):
      _ = text_dataset.text_dataset_from_directory(
          directory, validation_split=2)

    with self.assertRaisesRegex(ValueError,
                                '`subset` must be either "training" or'):
      _ = text_dataset.text_dataset_from_directory(
          directory, validation_split=0.2, subset='other')

    with self.assertRaisesRegex(ValueError, '`validation_split` must be set'):
      _ = text_dataset.text_dataset_from_directory(
          directory, validation_split=0, subset='training')

    with self.assertRaisesRegex(ValueError, 'must provide a `seed`'):
      _ = text_dataset.text_dataset_from_directory(
          directory, validation_split=0.2, subset='training')


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3543')" href="javascript:;">
keras-2.6.0/keras/preprocessing/image_dataset_test.py: 292-349
</a>
<div class="mid" id="frag3543" style="display:none"><pre>
  def test_image_dataset_from_directory_errors(self):
    if PIL is None:
      return  # Skip test if PIL is not available.

    directory = self._prepare_directory(num_classes=3, count=5)

    with self.assertRaisesRegex(ValueError, '`labels` argument should be'):
      _ = image_dataset.image_dataset_from_directory(
          directory, labels='other')

    with self.assertRaisesRegex(ValueError, '`label_mode` argument must be'):
      _ = image_dataset.image_dataset_from_directory(
          directory, label_mode='other')

    with self.assertRaisesRegex(ValueError, '`color_mode` must be one of'):
      _ = image_dataset.image_dataset_from_directory(
          directory, color_mode='other')

    with self.assertRaisesRegex(
        ValueError, 'only pass `class_names` if the labels are inferred'):
      _ = image_dataset.image_dataset_from_directory(
          directory, labels=[0, 0, 1, 1, 1],
          class_names=['class_0', 'class_1', 'class_2'])

    with self.assertRaisesRegex(
        ValueError,
        'Expected the lengths of `labels` to match the number of files'):
      _ = image_dataset.image_dataset_from_directory(
          directory, labels=[0, 0, 1, 1])

    with self.assertRaisesRegex(
        ValueError, '`class_names` passed did not match'):
      _ = image_dataset.image_dataset_from_directory(
          directory, class_names=['class_0', 'class_2'])

    with self.assertRaisesRegex(ValueError, 'there must exactly 2 classes'):
      _ = image_dataset.image_dataset_from_directory(
          directory, label_mode='binary')

    with self.assertRaisesRegex(ValueError,
                                '`validation_split` must be between 0 and 1'):
      _ = image_dataset.image_dataset_from_directory(
          directory, validation_split=2)

    with self.assertRaisesRegex(ValueError,
                                '`subset` must be either "training" or'):
      _ = image_dataset.image_dataset_from_directory(
          directory, validation_split=0.2, subset='other')

    with self.assertRaisesRegex(ValueError, '`validation_split` must be set'):
      _ = image_dataset.image_dataset_from_directory(
          directory, validation_split=0, subset='training')

    with self.assertRaisesRegex(ValueError, 'must provide a `seed`'):
      _ = image_dataset.image_dataset_from_directory(
          directory, validation_split=0.2, subset='training')


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 170:</b> &nbsp; 3 fragments, nominal size 16 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3554')" href="javascript:;">
keras-2.6.0/keras/preprocessing/timeseries_test.py: 25-45
</a>
<div class="mid" id="frag3554" style="display:none"><pre>
  def test_basics(self):
    # Test ordering, targets, sequence length, batch size
    data = np.arange(100)
    targets = data * 2
    dataset = timeseries.timeseries_dataset_from_array(
        data, targets, sequence_length=9, batch_size=5)
    # Expect 19 batches
    for i, batch in enumerate(dataset):
      self.assertLen(batch, 2)
      inputs, targets = batch
      if i &lt; 18:
        self.assertEqual(inputs.shape, (5, 9))
      if i == 18:
        # Last batch: size 2
        self.assertEqual(inputs.shape, (2, 9))
      # Check target values
      self.assertAllClose(targets, inputs[:, 0] * 2)
      for j in range(min(5, len(inputs))):
        # Check each sample in the batch
        self.assertAllClose(inputs[j], np.arange(i * 5 + j, i * 5 + j + 9))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3558')" href="javascript:;">
keras-2.6.0/keras/preprocessing/timeseries_test.py: 100-120
</a>
<div class="mid" id="frag3558" style="display:none"><pre>
  def test_sampling_rate(self):
    data = np.arange(100)
    targets = data * 2
    dataset = timeseries.timeseries_dataset_from_array(
        data, targets, sequence_length=9, batch_size=5, sampling_rate=2)
    for i, batch in enumerate(dataset):
      self.assertLen(batch, 2)
      inputs, targets = batch
      if i &lt; 16:
        self.assertEqual(inputs.shape, (5, 9))
      if i == 16:
        # Last batch: size 3
        self.assertEqual(inputs.shape, (3, 9))
      # Check target values
      self.assertAllClose(inputs[:, 0] * 2, targets)
      for j in range(min(5, len(inputs))):
        # Check each sample in the batch
        start_index = i * 5 + j
        end_index = start_index + 9 * 2
        self.assertAllClose(inputs[j], np.arange(start_index, end_index, 2))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3559')" href="javascript:;">
keras-2.6.0/keras/preprocessing/timeseries_test.py: 121-142
</a>
<div class="mid" id="frag3559" style="display:none"><pre>
  def test_sequence_stride(self):
    data = np.arange(100)
    targets = data * 2
    dataset = timeseries.timeseries_dataset_from_array(
        data, targets, sequence_length=9, batch_size=5, sequence_stride=3)
    for i, batch in enumerate(dataset):
      self.assertLen(batch, 2)
      inputs, targets = batch
      if i &lt; 6:
        self.assertEqual(inputs.shape, (5, 9))
      if i == 6:
        # Last batch: size 1
        self.assertEqual(inputs.shape, (1, 9))
      # Check target values
      self.assertAllClose(inputs[:, 0] * 2, targets)
      for j in range(min(5, len(inputs))):
        # Check each sample in the batch
        start_index = i * 5 * 3 + j * 3
        end_index = start_index + 9
        self.assertAllClose(inputs[j],
                            np.arange(start_index, end_index))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 171:</b> &nbsp; 3 fragments, nominal size 24 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3581')" href="javascript:;">
keras-2.6.0/keras/premade/linear_test.py: 142-164
</a>
<div class="mid" id="frag3581" style="display:none"><pre>
  def test_linear_model_with_feature_column(self):
    vocab_list = ['alpha', 'beta', 'gamma']
    vocab_val = [0.4, 0.6, 0.9]
    data = np.random.choice(vocab_list, size=256)
    y = np.zeros_like(data, dtype=np.float32)
    for vocab, val in zip(vocab_list, vocab_val):
      indices = np.where(data == vocab)
      y[indices] = val + np.random.uniform(
          low=-0.01, high=0.01, size=indices[0].shape)
    cat_column = tf.feature_column.categorical_column_with_vocabulary_list(
        key='symbol', vocabulary_list=vocab_list)
    ind_column = tf.feature_column.indicator_column(cat_column)
    dense_feature_layer = dense_features_v2.DenseFeatures([ind_column])
    linear_model = linear.LinearModel(
        use_bias=False, kernel_initializer='zeros')
    combined = sequential.Sequential([dense_feature_layer, linear_model])
    opt = gradient_descent.SGD(learning_rate=0.1)
    combined.compile(opt, 'mse', [])
    combined.fit(x={'symbol': data}, y=y, batch_size=32, epochs=10)
    self.assertAllClose([[0.4], [0.6], [0.9]],
                        combined.layers[1].dense_layers[0].kernel.numpy(),
                        atol=0.01)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3595')" href="javascript:;">
keras-2.6.0/keras/premade/wide_deep_test.py: 185-213
</a>
<div class="mid" id="frag3595" style="display:none"><pre>
  def test_wide_deep_model_with_single_feature_column(self):
    vocab_list = ['alpha', 'beta', 'gamma']
    vocab_val = [0.4, 0.6, 0.9]
    data = np.random.choice(vocab_list, size=256)
    y = np.zeros_like(data, dtype=np.float32)
    for vocab, val in zip(vocab_list, vocab_val):
      indices = np.where(data == vocab)
      y[indices] = val + np.random.uniform(
          low=-0.01, high=0.01, size=indices[0].shape)
    cat_column = tf.feature_column.categorical_column_with_vocabulary_list(
        key='symbol', vocabulary_list=vocab_list)
    ind_column = tf.feature_column.indicator_column(cat_column)
    dense_feature_layer = dense_features_v2.DenseFeatures([ind_column])
    linear_model = linear.LinearModel(
        use_bias=False, kernel_initializer='zeros')
    dnn_model = sequential.Sequential([core.Dense(units=1)])
    wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)
    combined = sequential.Sequential([dense_feature_layer, wide_deep_model])
    opt = gradient_descent.SGD(learning_rate=0.1)
    combined.compile(
        opt,
        'mse', [],
        run_eagerly=testing_utils.should_run_eagerly())
    combined.fit(x={'symbol': data}, y=y, batch_size=32, epochs=10)

  # This test is an example for cases where linear and dnn model accepts
  # same raw input but different transformed inputs, i.e,. the raw input is
  # categorical, and linear model accepts one hot encoding, while dnn model
  # accepts embedding encoding.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3596')" href="javascript:;">
keras-2.6.0/keras/premade/wide_deep_test.py: 214-242
</a>
<div class="mid" id="frag3596" style="display:none"><pre>
  def test_wide_deep_model_with_two_feature_columns(self):
    vocab_list = ['alpha', 'beta', 'gamma']
    vocab_val = [0.4, 0.6, 0.9]
    data = np.random.choice(vocab_list, size=256)
    y = np.zeros_like(data, dtype=np.float32)
    for vocab, val in zip(vocab_list, vocab_val):
      indices = np.where(data == vocab)
      y[indices] = val + np.random.uniform(
          low=-0.01, high=0.01, size=indices[0].shape)
    cat_column = tf.feature_column.categorical_column_with_vocabulary_list(
        key='symbol', vocabulary_list=vocab_list)
    ind_column = tf.feature_column.indicator_column(cat_column)
    emb_column = tf.feature_column.embedding_column(cat_column, dimension=5)
    linear_feature_layer = dense_features_v2.DenseFeatures([ind_column])
    linear_model = linear.LinearModel(
        use_bias=False, kernel_initializer='zeros')
    combined_linear = sequential.Sequential(
        [linear_feature_layer, linear_model])
    dnn_model = sequential.Sequential([core.Dense(units=1)])
    dnn_feature_layer = dense_features_v2.DenseFeatures([emb_column])
    combined_dnn = sequential.Sequential([dnn_feature_layer, dnn_model])
    wide_deep_model = wide_deep.WideDeepModel(combined_linear, combined_dnn)
    opt = gradient_descent.SGD(learning_rate=0.1)
    wide_deep_model.compile(
        opt,
        'mse', [],
        run_eagerly=testing_utils.should_run_eagerly())
    wide_deep_model.fit(x={'symbol': data}, y=y, batch_size=32, epochs=10)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 172:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3588')" href="javascript:;">
keras-2.6.0/keras/premade/wide_deep_test.py: 35-50
</a>
<div class="mid" id="frag3588" style="display:none"><pre>
  def test_wide_deep_model(self):
    linear_model = linear.LinearModel(units=1)
    dnn_model = sequential.Sequential([core.Dense(units=1, input_dim=3)])
    wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)
    linear_inp = np.random.uniform(low=-5, high=5, size=(64, 2))
    dnn_inp = np.random.uniform(low=-5, high=5, size=(64, 3))
    inputs = [linear_inp, dnn_inp]
    output = .3 * linear_inp[:, 0] + .2 * dnn_inp[:, 1]
    wide_deep_model.compile(
        optimizer=['sgd', 'adam'],
        loss='mse',
        metrics=[],
        run_eagerly=testing_utils.should_run_eagerly())
    wide_deep_model.fit(inputs, output, epochs=5)
    self.assertTrue(wide_deep_model.built)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3590')" href="javascript:;">
keras-2.6.0/keras/premade/wide_deep_test.py: 77-89
</a>
<div class="mid" id="frag3590" style="display:none"><pre>
  def test_wide_deep_model_with_single_input(self):
    linear_model = linear.LinearModel(units=1)
    dnn_model = sequential.Sequential([core.Dense(units=1, input_dim=3)])
    wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)
    inputs = np.random.uniform(low=-5, high=5, size=(64, 3))
    output = .3 * inputs[:, 0]
    wide_deep_model.compile(
        optimizer=['sgd', 'adam'],
        loss='mse',
        metrics=[],
        run_eagerly=testing_utils.should_run_eagerly())
    wide_deep_model.fit(inputs, output, epochs=5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3592')" href="javascript:;">
keras-2.6.0/keras/premade/wide_deep_test.py: 114-129
</a>
<div class="mid" id="frag3592" style="display:none"><pre>
  def test_wide_deep_model_with_single_optimizer(self):
    linear_model = linear.LinearModel(units=1)
    dnn_model = sequential.Sequential([core.Dense(units=1, input_dim=3)])
    wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)
    linear_inp = np.random.uniform(low=-5, high=5, size=(64, 2))
    dnn_inp = np.random.uniform(low=-5, high=5, size=(64, 3))
    inputs = [linear_inp, dnn_inp]
    output = .3 * linear_inp[:, 0] + .2 * dnn_inp[:, 1]
    wide_deep_model.compile(
        optimizer='sgd',
        loss='mse',
        metrics=[],
        run_eagerly=testing_utils.should_run_eagerly())
    wide_deep_model.fit(inputs, output, epochs=5)
    self.assertTrue(wide_deep_model.built)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 173:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3602')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 74-88
</a>
<div class="mid" id="frag3602" style="display:none"><pre>
  def test_static_shape_inference_LSTM(self):
    # Github issue: 15165
    timesteps = 3
    embedding_dim = 4
    units = 2

    model = keras.models.Sequential()
    inputs = keras.layers.Dense(
        embedding_dim, input_shape=(timesteps, embedding_dim))
    model.add(inputs)
    layer = rnn.LSTM(units, return_sequences=True)
    model.add(layer)
    outputs = model.layers[-1].output
    self.assertEqual(outputs.shape.as_list(), [None, timesteps, units])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3860')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 60-74
</a>
<div class="mid" id="frag3860" style="display:none"><pre>
  def test_static_shape_inference_LSTM(self):
    # Github issue: 15165
    timesteps = 3
    embedding_dim = 4
    units = 2

    model = keras.models.Sequential()
    inputs = keras.layers.Dense(embedding_dim,
                                input_shape=(timesteps, embedding_dim))
    model.add(inputs)
    layer = keras.layers.LSTM(units, return_sequences=True)
    model.add(layer)
    outputs = model.layers[-1].output
    self.assertEqual(outputs.shape.as_list(), [None, timesteps, units])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 174:</b> &nbsp; 5 fragments, nominal size 12 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3603')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 89-101
</a>
<div class="mid" id="frag3603" style="display:none"><pre>
  def test_dynamic_behavior_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer = rnn.LSTM(units, input_shape=(None, embedding_dim))
    model = keras.models.Sequential()
    model.add(layer)
    model.compile(tf.compat.v1.train.GradientDescentOptimizer(0.001), 'mse')
    x = np.random.random((num_samples, timesteps, embedding_dim))
    y = np.random.random((num_samples, units))
    model.train_on_batch(x, y)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3881')" href="javascript:;">
keras-2.6.0/keras/layers/simplernn_test.py: 57-69
</a>
<div class="mid" id="frag3881" style="display:none"><pre>
  def test_dynamic_behavior_SimpleRNN(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer = keras.layers.SimpleRNN(units, input_shape=(None, embedding_dim))
    model = keras.models.Sequential()
    model.add(layer)
    model.compile('rmsprop', 'mse')
    x = np.random.random((num_samples, timesteps, embedding_dim))
    y = np.random.random((num_samples, units))
    model.train_on_batch(x, y)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4199')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 103-115
</a>
<div class="mid" id="frag4199" style="display:none"><pre>
  def test_dynamic_behavior_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer = rnn.GRU(units, input_shape=(None, embedding_dim))
    model = keras.models.Sequential()
    model.add(layer)
    model.compile(tf.compat.v1.train.GradientDescentOptimizer(0.001), 'mse')
    x = np.random.random((num_samples, timesteps, embedding_dim))
    y = np.random.random((num_samples, units))
    model.train_on_batch(x, y)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4475')" href="javascript:;">
keras-2.6.0/keras/layers/gru_test.py: 62-77
</a>
<div class="mid" id="frag4475" style="display:none"><pre>
  def test_dynamic_behavior_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer = keras.layers.GRU(units, input_shape=(None, embedding_dim))
    model = keras.models.Sequential()
    model.add(layer)
    model.compile(
        'rmsprop',
        'mse',
        run_eagerly=testing_utils.should_run_eagerly())
    x = np.random.random((num_samples, timesteps, embedding_dim))
    y = np.random.random((num_samples, units))
    model.train_on_batch(x, y)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3861')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 75-91
</a>
<div class="mid" id="frag3861" style="display:none"><pre>
  def test_dynamic_behavior_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer = keras.layers.LSTM(units, input_shape=(None, embedding_dim))
    model = keras.models.Sequential()
    model.add(layer)
    model.compile(
        'rmsprop',
        'mse',
        run_eagerly=testing_utils.should_run_eagerly())

    x = np.random.random((num_samples, timesteps, embedding_dim))
    y = np.random.random((num_samples, units))
    model.train_on_batch(x, y)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 175:</b> &nbsp; 4 fragments, nominal size 11 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3604')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 102-113
</a>
<div class="mid" id="frag3604" style="display:none"><pre>
  def test_stacking_LSTM(self):
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(rnn.LSTM(10, return_sequences=True, unroll=False))
    model.add(rnn.LSTM(5, return_sequences=True, unroll=False))
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3615')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 405-417
</a>
<div class="mid" id="frag3615" style="display:none"><pre>
  def test_masking_with_stacking_LSTM(self):
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(keras.layers.Masking(input_shape=(3, 4)))
    model.add(rnn.LSTM(10, return_sequences=True, unroll=False))
    model.add(rnn.LSTM(5, return_sequences=True, unroll=False))
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4200')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 116-127
</a>
<div class="mid" id="frag4200" style="display:none"><pre>
  def test_stacking_GRU(self):
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(rnn.GRU(10, return_sequences=True, unroll=False))
    model.add(rnn.GRU(5, return_sequences=True, unroll=False))
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4209')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 325-337
</a>
<div class="mid" id="frag4209" style="display:none"><pre>
  def test_masking_with_stacking_GRU(self):
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(keras.layers.Masking(input_shape=(3, 4)))
    model.add(rnn.GRU(10, return_sequences=True, unroll=False))
    model.add(rnn.GRU(5, return_sequences=True, unroll=False))
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 176:</b> &nbsp; 7 fragments, nominal size 23 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3606')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 121-151
</a>
<div class="mid" id="frag3606" style="display:none"><pre>
  def test_specify_initial_state_keras_tensor(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    # Test with Keras tensor
    inputs = keras.Input((timesteps, embedding_dim))
    initial_state = [keras.Input((units,)) for _ in range(num_states)]
    layer = rnn.LSTM(units)
    if len(initial_state) == 1:
      output = layer(inputs, initial_state=initial_state[0])
    else:
      output = layer(inputs, initial_state=initial_state)
    self.assertTrue(
        any(initial_state[0] is t
            for t in layer._inbound_nodes[0].input_tensors))

    model = keras.models.Model([inputs] + initial_state, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    initial_state = [
        np.random.random((num_samples, units)) for _ in range(num_states)
    ]
    targets = np.random.random((num_samples, units))
    model.train_on_batch([inputs] + initial_state, targets)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3609')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 214-238
</a>
<div class="mid" id="frag3609" style="display:none"><pre>
  def test_specify_state_with_masking(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    inputs = keras.Input((timesteps, embedding_dim))
    _ = keras.layers.Masking()(inputs)
    initial_state = [keras.Input((units,)) for _ in range(num_states)]
    output = rnn.LSTM(units)(
        inputs, initial_state=initial_state)

    model = keras.models.Model([inputs] + initial_state, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    initial_state = [
        np.random.random((num_samples, units)) for _ in range(num_states)
    ]
    targets = np.random.random((num_samples, units))
    model.train_on_batch([inputs] + initial_state, targets)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3870')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 187-217
</a>
<div class="mid" id="frag3870" style="display:none"><pre>
  def test_specify_initial_state_keras_tensor(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    # Test with Keras tensor
    inputs = keras.Input((timesteps, embedding_dim))
    initial_state = [keras.Input((units,)) for _ in range(num_states)]
    layer = keras.layers.LSTM(units)
    if len(initial_state) == 1:
      output = layer(inputs, initial_state=initial_state[0])
    else:
      output = layer(inputs, initial_state=initial_state)
    self.assertTrue(
        any(initial_state[0] is t
            for t in layer._inbound_nodes[0].input_tensors))

    model = keras.models.Model([inputs] + initial_state, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.AdamOptimizer(),
        run_eagerly=testing_utils.should_run_eagerly())

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    initial_state = [np.random.random((num_samples, units))
                     for _ in range(num_states)]
    targets = np.random.random((num_samples, units))
    model.train_on_batch([inputs] + initial_state, targets)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4491')" href="javascript:;">
keras-2.6.0/keras/layers/cudnn_recurrent_test.py: 124-155
</a>
<div class="mid" id="frag4491" style="display:none"><pre>
  def test_specify_initial_state_keras_tensor(self, layer_class):
    input_size = 10
    timesteps = 6
    units = 2
    num_samples = 32
    num_states = 2 if layer_class is keras.layers.CuDNNLSTM else 1

    inputs = keras.Input((timesteps, input_size))
    initial_state = [keras.Input((units,)) for _ in range(num_states)]
    layer = layer_class(units)
    if len(initial_state) == 1:
      output = layer(inputs, initial_state=initial_state[0])
    else:
      output = layer(inputs, initial_state=initial_state)
    self.assertTrue(
        any(initial_state[0] is t
            for t in layer._inbound_nodes[0].input_tensors))

    model = keras.models.Model([inputs] + initial_state, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer=RMSprop(learning_rate=0.001),
        run_eagerly=testing_utils.should_run_eagerly())

    inputs = np.random.random((num_samples, timesteps, input_size))
    initial_state = [
        np.random.random((num_samples, units)) for _ in range(num_states)
    ]
    targets = np.random.random((num_samples, units))
    model.fit([inputs] + initial_state, targets)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3612')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 278-308
</a>
<div class="mid" id="frag3612" style="display:none"><pre>
  def test_initial_states_as_other_inputs(self):
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2
    num_states = 2
    layer_class = rnn.LSTM

    # Test with Keras tensor
    main_inputs = keras.Input((timesteps, embedding_dim))
    initial_state = [keras.Input((units,)) for _ in range(num_states)]
    inputs = [main_inputs] + initial_state

    layer = layer_class(units)
    output = layer(inputs)
    self.assertTrue(
        any(initial_state[0] is t
            for t in layer._inbound_nodes[0].input_tensors))

    model = keras.models.Model(inputs, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))

    main_inputs = np.random.random((num_samples, timesteps, embedding_dim))
    initial_state = [
        np.random.random((num_samples, units)) for _ in range(num_states)
    ]
    targets = np.random.random((num_samples, units))
    model.train_on_batch([main_inputs] + initial_state, targets)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3873')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 273-296
</a>
<div class="mid" id="frag3873" style="display:none"><pre>
  def test_specify_state_with_masking(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    inputs = keras.Input((timesteps, embedding_dim))
    _ = keras.layers.Masking()(inputs)
    initial_state = [keras.Input((units,)) for _ in range(num_states)]
    output = keras.layers.LSTM(units)(inputs, initial_state=initial_state)

    model = keras.models.Model([inputs] + initial_state, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer='rmsprop',
        run_eagerly=testing_utils.should_run_eagerly())

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    initial_state = [np.random.random((num_samples, units))
                     for _ in range(num_states)]
    targets = np.random.random((num_samples, units))
    model.train_on_batch([inputs] + initial_state, targets)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3876')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 331-361
</a>
<div class="mid" id="frag3876" style="display:none"><pre>
  def test_initial_states_as_other_inputs(self):
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2
    num_states = 2
    layer_class = keras.layers.LSTM

    # Test with Keras tensor
    main_inputs = keras.Input((timesteps, embedding_dim))
    initial_state = [keras.Input((units,)) for _ in range(num_states)]
    inputs = [main_inputs] + initial_state

    layer = layer_class(units)
    output = layer(inputs)
    self.assertTrue(
        any(initial_state[0] is t
            for t in layer._inbound_nodes[0].input_tensors))

    model = keras.models.Model(inputs, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.AdamOptimizer(),
        run_eagerly=testing_utils.should_run_eagerly())

    main_inputs = np.random.random((num_samples, timesteps, embedding_dim))
    initial_state = [np.random.random((num_samples, units))
                     for _ in range(num_states)]
    targets = np.random.random((num_samples, units))
    model.train_on_batch([main_inputs] + initial_state, targets)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 177:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3607')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 152-176
</a>
<div class="mid" id="frag3607" style="display:none"><pre>
  def test_specify_initial_state_non_keras_tensor(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    # Test with non-Keras tensor
    inputs = keras.Input((timesteps, embedding_dim))
    initial_state = [
        keras.backend.random_normal_variable((num_samples, units), 0, 1)
        for _ in range(num_states)
    ]
    layer = rnn.LSTM(units)
    output = layer(inputs, initial_state=initial_state)

    model = keras.models.Model(inputs, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01))

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    targets = np.random.random((num_samples, units))
    model.train_on_batch(inputs, targets)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3871')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 218-242
</a>
<div class="mid" id="frag3871" style="display:none"><pre>
  def test_specify_initial_state_non_keras_tensor(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    # Test with non-Keras tensor
    inputs = keras.Input((timesteps, embedding_dim))
    initial_state = [keras.backend.random_normal_variable(
        (num_samples, units), 0, 1)
                     for _ in range(num_states)]
    layer = keras.layers.LSTM(units)
    output = layer(inputs, initial_state=initial_state)

    model = keras.models.Model(inputs, output)
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.compat.v1.train.AdamOptimizer(),
        run_eagerly=testing_utils.should_run_eagerly())

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    targets = np.random.random((num_samples, units))
    model.train_on_batch(inputs, targets)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 178:</b> &nbsp; 2 fragments, nominal size 29 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3608')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 177-213
</a>
<div class="mid" id="frag3608" style="display:none"><pre>
  def test_reset_states_with_values(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    layer = rnn.LSTM(units, stateful=True)
    layer.build((num_samples, timesteps, embedding_dim))
    initial_weight_count = len(layer.weights)
    layer.reset_states()
    assert len(layer.states) == num_states
    assert layer.states[0] is not None
    self.assertAllClose(
        keras.backend.eval(layer.states[0]),
        np.zeros(keras.backend.int_shape(layer.states[0])),
        atol=1e-4)
    state_shapes = [keras.backend.int_shape(state) for state in layer.states]
    values = [np.ones(shape) for shape in state_shapes]
    if len(values) == 1:
      values = values[0]
    layer.reset_states(values)
    self.assertAllClose(
        keras.backend.eval(layer.states[0]),
        np.ones(keras.backend.int_shape(layer.states[0])),
        atol=1e-4)

    # Test with invalid data
    with self.assertRaises(ValueError):
      layer.reset_states([1] * (len(layer.states) + 1))

    self.assertEqual(initial_weight_count, len(layer.weights))
    # Variables in "states" shouldn't show up in .weights
    layer.states = tf.nest.map_structure(tf.Variable, values)
    layer.reset_states()
    self.assertEqual(initial_weight_count, len(layer.weights))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3872')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 243-272
</a>
<div class="mid" id="frag3872" style="display:none"><pre>
  def test_reset_states_with_values(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    layer = keras.layers.LSTM(units, stateful=True)
    layer.build((num_samples, timesteps, embedding_dim))
    layer.reset_states()
    assert len(layer.states) == num_states
    assert layer.states[0] is not None
    self.assertAllClose(
        keras.backend.eval(layer.states[0]),
        np.zeros(keras.backend.int_shape(layer.states[0])),
        atol=1e-4)
    state_shapes = [keras.backend.int_shape(state) for state in layer.states]
    values = [np.ones(shape) for shape in state_shapes]
    if len(values) == 1:
      values = values[0]
    layer.reset_states(values)
    self.assertAllClose(
        keras.backend.eval(layer.states[0]),
        np.ones(keras.backend.int_shape(layer.states[0])),
        atol=1e-4)

    # Test with invalid data
    with self.assertRaises(ValueError):
      layer.reset_states([1] * (len(layer.states) + 1))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 179:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3610')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 242-260
</a>
<div class="mid" id="frag3610" style="display:none"><pre>
  def test_return_state(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    inputs = keras.Input(batch_shape=(num_samples, timesteps, embedding_dim))
    masked = keras.layers.Masking()(inputs)
    layer = rnn.LSTM(units, return_state=True, stateful=True)
    outputs = layer(masked)
    state = outputs[1:]
    assert len(state) == num_states
    model = keras.models.Model(inputs, state[0])

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    state = model.predict(inputs)
    self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3874')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 297-314
</a>
<div class="mid" id="frag3874" style="display:none"><pre>
  def test_return_state(self):
    num_states = 2
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    inputs = keras.Input(batch_shape=(num_samples, timesteps, embedding_dim))
    layer = keras.layers.LSTM(units, return_state=True, stateful=True)
    outputs = layer(inputs)
    state = outputs[1:]
    assert len(state) == num_states
    model = keras.models.Model(inputs, state[0])

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    state = model.predict(inputs)
    self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 180:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3611')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 261-277
</a>
<div class="mid" id="frag3611" style="display:none"><pre>
  def test_state_reuse(self):
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    inputs = keras.Input(batch_shape=(num_samples, timesteps, embedding_dim))
    layer = rnn.LSTM(
        units, return_state=True, return_sequences=True)
    outputs = layer(inputs)
    output, state = outputs[0], outputs[1:]
    output = rnn.LSTM(units)(output, initial_state=state)
    model = keras.models.Model(inputs, output)

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    model.predict(inputs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3875')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 315-330
</a>
<div class="mid" id="frag3875" style="display:none"><pre>
  def test_state_reuse(self):
    timesteps = 3
    embedding_dim = 4
    units = 3
    num_samples = 2

    inputs = keras.Input(batch_shape=(num_samples, timesteps, embedding_dim))
    layer = keras.layers.LSTM(units, return_state=True, return_sequences=True)
    outputs = layer(inputs)
    output, state = outputs[0], outputs[1:]
    output = keras.layers.LSTM(units)(output, initial_state=state)
    model = keras.models.Model(inputs, output)

    inputs = np.random.random((num_samples, timesteps, embedding_dim))
    outputs = model.predict(inputs)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 181:</b> &nbsp; 2 fragments, nominal size 38 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3613')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 313-355
</a>
<div class="mid" id="frag3613" style="display:none"><pre>
  def test_lstm_v2_feature_parity_with_canonical_lstm(self):
    input_shape = 10
    rnn_state_size = 8
    timestep = 4
    batch = 20

    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=batch,
        test_samples=0,
        input_shape=(timestep, input_shape),
        num_classes=rnn_state_size,
        random_seed=87654321)
    y_train = np_utils.to_categorical(y_train, rnn_state_size)
    # For the last batch item of the test data, we filter out the last
    # timestep to simulate the variable length sequence and masking test.
    x_train[-2:, -1, :] = 0.0
    y_train[-2:] = 0

    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)
    masked_input = keras.layers.Masking()(inputs)
    lstm_layer = rnn_v1.LSTM(rnn_state_size,
                             recurrent_activation='sigmoid')
    output = lstm_layer(masked_input)
    lstm_model = keras.models.Model(inputs, output)
    weights = lstm_model.get_weights()
    y_1 = lstm_model.predict(x_train)
    lstm_model.compile('rmsprop', 'mse')
    lstm_model.fit(x_train, y_train)
    y_2 = lstm_model.predict(x_train)

    with testing_utils.device(should_use_gpu=True):
      cudnn_layer = rnn.LSTM(rnn_state_size)
      cudnn_model = keras.models.Model(inputs, cudnn_layer(masked_input))
    cudnn_model.set_weights(weights)
    y_3 = cudnn_model.predict(x_train)
    cudnn_model.compile('rmsprop', 'mse')
    cudnn_model.fit(x_train, y_train)
    y_4 = cudnn_model.predict(x_train)

    self.assertAllClose(y_1, y_3, rtol=1e-5, atol=2e-5)
    self.assertAllClose(y_2, y_4, rtol=1e-5, atol=2e-5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4202')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 139-184
</a>
<div class="mid" id="frag4202" style="display:none"><pre>
  def test_gru_v2_feature_parity_with_canonical_gru(self):
    input_shape = 10
    rnn_state_size = 8
    timestep = 4
    batch = 20

    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=batch,
        test_samples=0,
        input_shape=(timestep, input_shape),
        num_classes=rnn_state_size,
        random_seed=87654321)
    y_train = np_utils.to_categorical(y_train, rnn_state_size)
    # For the last batch item of the test data, we filter out the last
    # timestep to simulate the variable length sequence and masking test.
    x_train[-2:, -1, :] = 0.0
    y_train[-2:] = 0

    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)
    masked_input = keras.layers.Masking()(inputs)
    gru_layer = rnn_v1.GRU(rnn_state_size,
                           recurrent_activation='sigmoid',
                           reset_after=True)
    output = gru_layer(masked_input)
    gru_model = keras.models.Model(inputs, output)
    weights = gru_model.get_weights()
    y_1 = gru_model.predict(x_train)
    gru_model.compile('rmsprop', 'mse')
    gru_model.fit(x_train, y_train)
    y_2 = gru_model.predict(x_train)

    with testing_utils.device(should_use_gpu=True):
      cudnn_layer = rnn.GRU(rnn_state_size,
                            recurrent_activation='sigmoid',
                            reset_after=True)
      cudnn_model = keras.models.Model(inputs, cudnn_layer(masked_input))
    cudnn_model.set_weights(weights)
    y_3 = cudnn_model.predict(x_train)
    cudnn_model.compile('rmsprop', 'mse')
    cudnn_model.fit(x_train, y_train)
    y_4 = cudnn_model.predict(x_train)

    self.assertAllClose(y_1, y_3, rtol=2e-5, atol=2e-5)
    self.assertAllClose(y_2, y_4, rtol=2e-5, atol=2e-5)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 182:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3617')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 433-450
</a>
<div class="mid" id="frag3617" style="display:none"><pre>
    def build_model(layer_cls):
      inputs = keras.layers.Input(
          shape=[timestep, input_shape], dtype=tf.float32)
      layer = layer_cls(rnn_state_size,
                        recurrent_activation='sigmoid',
                        time_major=time_major,
                        return_sequences=True,
                        go_backwards=go_backwards)
      if time_major:
        converted_input = keras.layers.Lambda(
            lambda t: tf.transpose(t, [1, 0, 2]))(inputs)
        outputs = layer(converted_input)
        outputs = keras.layers.Lambda(
            lambda t: tf.transpose(t, [1, 0, 2]))(outputs)
      else:
        outputs = layer(inputs)
      return keras.models.Model(inputs, outputs)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4207')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 278-296
</a>
<div class="mid" id="frag4207" style="display:none"><pre>
    def build_model(layer_cls):
      inputs = keras.layers.Input(
          shape=[timestep, input_shape], dtype=tf.float32)
      layer = layer_cls(rnn_state_size,
                        recurrent_activation='sigmoid',
                        time_major=time_major,
                        return_sequences=True,
                        go_backwards=go_backwards,
                        reset_after=True)
      if time_major:
        converted_input = keras.layers.Lambda(
            lambda t: tf.transpose(t, [1, 0, 2]))(inputs)
        outputs = layer(converted_input)
        outputs = keras.layers.Lambda(
            lambda t: tf.transpose(t, [1, 0, 2]))(outputs)
      else:
        outputs = layer(inputs)
      return keras.models.Model(inputs, outputs)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 183:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3618')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 493-525
</a>
<div class="mid" id="frag3618" style="display:none"><pre>
  def test_lstm_model_save_load(self, use_bias, bias_initializer):
    temp_dir = self.get_temp_dir()
    self.addCleanup(shutil.rmtree, temp_dir)
    h5_path = os.path.join(temp_dir, 'test.h5')

    batch = 10
    timestep = 3
    input_dim = 5
    units = 2

    x = np.random.random((batch, timestep, input_dim))

    def build_model():
      inputs = keras.layers.Input(
          shape=[timestep, input_dim], dtype=tf.float32)
      layer = rnn.LSTM(
          units,
          use_bias=use_bias,
          bias_initializer=bias_initializer)
      output = layer(inputs)
      return keras.models.Model(inputs, output), layer

    model, layer = build_model()
    y_ref = model.predict(x)
    model.save_weights(h5_path)

    cloned_model, new_layer = build_model()
    cloned_model.load_weights(h5_path)
    y = cloned_model.predict(x)

    self.assertAllClose(y, y_ref)
    self.assertAllClose(layer.get_weights(), new_layer.get_weights())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4203')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 191-223
</a>
<div class="mid" id="frag4203" style="display:none"><pre>
  def test_gru_v2_model_save_load(self, use_bias, bias_initializer):
    temp_dir = self.get_temp_dir()
    self.addCleanup(shutil.rmtree, temp_dir)
    h5_path = os.path.join(temp_dir, 'test.h5')

    batch = 10
    timestep = 3
    input_dim = 5
    units = 2

    x = np.random.random((batch, timestep, input_dim))

    def build_model():
      inputs = keras.layers.Input(
          shape=[timestep, input_dim], dtype=tf.float32)
      layer = rnn.GRU(
          units,
          use_bias=use_bias,
          bias_initializer=bias_initializer)
      output = layer(inputs)
      return keras.models.Model(inputs, output), layer

    model, layer = build_model()
    y_ref = model.predict(x)
    model.save_weights(h5_path)

    cloned_model, new_layer = build_model()
    cloned_model.load_weights(h5_path)
    y = cloned_model.predict(x)

    self.assertAllClose(y, y_ref)
    self.assertAllClose(layer.get_weights(), new_layer.get_weights())

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 184:</b> &nbsp; 2 fragments, nominal size 29 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3620')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 526-563
</a>
<div class="mid" id="frag3620" style="display:none"><pre>
  def test_lstm_output_on_multiple_kernel(self):
    input_shape = 10
    rnn_state_size = 8
    timestep = 4
    batch = 100

    x_train = np.random.random((batch, timestep, input_shape))

    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)
    with testing_utils.device(should_use_gpu=False):
      layer = rnn.LSTM(rnn_state_size)
      output = layer(inputs)
      cpu_model = keras.models.Model(inputs, output)
      weights = cpu_model.get_weights()
    y_1 = cpu_model.predict(x_train)

    with testing_utils.device(should_use_gpu=True):
      layer = rnn.LSTM(rnn_state_size)
      output = layer(inputs)
      gpu_model = keras.models.Model(inputs, output)
      gpu_model.set_weights(weights)
    y_2 = gpu_model.predict(x_train)

    # Note that CuDNN uses 'sigmoid' as activation, so the LSTM V2 uses
    # 'sigmoid' as default. Construct the canonical LSTM with sigmoid to achieve
    # the same output.
    with testing_utils.device(should_use_gpu=True):
      layer = rnn_v1.LSTM(rnn_state_size, recurrent_activation='sigmoid')
      output = layer(inputs)
      canonical_model = keras.models.Model(inputs, output)
      # Remove the extra cudnn bias since canonical lstm will not use it.
      canonical_model.set_weights(weights[:3])
    y_3 = canonical_model.predict(x_train)

    self.assertAllClose(y_1, y_2)
    self.assertAllClose(y_2, y_3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4205')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 224-262
</a>
<div class="mid" id="frag4205" style="display:none"><pre>
  def test_gru_v2_output_on_multiple_kernel(self):
    input_shape = 10
    rnn_state_size = 8
    timestep = 4
    batch = 100

    x_train = np.random.random((batch, timestep, input_shape))

    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)
    with testing_utils.device(should_use_gpu=False):
      layer = rnn.GRU(rnn_state_size)
      output = layer(inputs)
      cpu_model = keras.models.Model(inputs, output)
      weights = cpu_model.get_weights()
      y_1 = cpu_model.predict(x_train)

    with testing_utils.device(should_use_gpu=True):
      layer = rnn.GRU(rnn_state_size)
      output = layer(inputs)
      gpu_model = keras.models.Model(inputs, output)
      gpu_model.set_weights(weights)
      y_2 = gpu_model.predict(x_train)

    # Note that CuDNN uses 'sigmoid' as activation, so the GRU V2 uses
    # 'sigmoid' as default. Construct the canonical GRU with sigmoid to achieve
    # the same output.
    with testing_utils.device(should_use_gpu=True):
      layer = rnn_v1.GRU(rnn_state_size,
                         recurrent_activation='sigmoid',
                         reset_after=True)
      output = layer(inputs)
      canonical_model = keras.models.Model(inputs, output)
      canonical_model.set_weights(weights)
      y_3 = canonical_model.predict(x_train)

    self.assertAllClose(y_1, y_2, rtol=1e-5, atol=1e-5)
    self.assertAllClose(y_2, y_3, rtol=1e-5, atol=1e-5)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 185:</b> &nbsp; 24 fragments, nominal size 11 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3621')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 564-576
</a>
<div class="mid" id="frag3621" style="display:none"><pre>
  def test_return_sequences_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        rnn.LSTM,
        kwargs={
            'units': units,
            'return_sequences': True
        },
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3883')" href="javascript:;">
keras-2.6.0/keras/layers/simplernn_test.py: 82-93
</a>
<div class="mid" id="frag3883" style="display:none"><pre>
  def test_implementation_mode_SimpleRNN(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    for mode in [0, 1, 2]:
      testing_utils.layer_test(
          keras.layers.SimpleRNN,
          kwargs={'units': units,
                  'implementation': mode},
          input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3858')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 32-42
</a>
<div class="mid" id="frag3858" style="display:none"><pre>
  def test_return_sequences_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.LSTM,
        kwargs={'units': units,
                'return_sequences': True},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3879')" href="javascript:;">
keras-2.6.0/keras/layers/simplernn_test.py: 32-42
</a>
<div class="mid" id="frag3879" style="display:none"><pre>
  def test_return_sequences_SimpleRNN(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.SimpleRNN,
        kwargs={'units': units,
                'return_sequences': True},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4210')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 338-348
</a>
<div class="mid" id="frag4210" style="display:none"><pre>
  def test_return_sequences_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        rnn.GRU,
        kwargs={'units': units,
                'return_sequences': True},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4213')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 384-395
</a>
<div class="mid" id="frag4213" style="display:none"><pre>
  def test_dropout_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        rnn.GRU,
        kwargs={'units': units,
                'dropout': 0.1,
                'recurrent_dropout': 0.1},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4215')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 416-426
</a>
<div class="mid" id="frag4215" style="display:none"><pre>
  def test_implementation_mode_GRU(self, implementation_mode):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        rnn.GRU,
        kwargs={'units': units,
                'implementation': implementation_mode},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4473')" href="javascript:;">
keras-2.6.0/keras/layers/gru_test.py: 34-44
</a>
<div class="mid" id="frag4473" style="display:none"><pre>
  def test_return_sequences_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.GRU,
        kwargs={'units': units,
                'return_sequences': True},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3626')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 715-728
</a>
<div class="mid" id="frag3626" style="display:none"><pre>
  def test_dropout_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        rnn.LSTM,
        kwargs={
            'units': units,
            'dropout': 0.1,
            'recurrent_dropout': 0.1
        },
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4488')" href="javascript:;">
keras-2.6.0/keras/layers/cudnn_recurrent_test.py: 57-67
</a>
<div class="mid" id="frag4488" style="display:none"><pre>
  def test_cudnn_rnn_go_backward(self, layer_class, go_backwards):
    input_size = 10
    timesteps = 6
    units = 2
    num_samples = 32
    testing_utils.layer_test(
        layer_class,
        kwargs={'units': units,
                'go_backwards': go_backwards},
        input_shape=(num_samples, timesteps, input_size))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4478')" href="javascript:;">
keras-2.6.0/keras/layers/gru_test.py: 96-106
</a>
<div class="mid" id="frag4478" style="display:none"><pre>
  def test_implementation_mode_GRU(self, implementation_mode):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.GRU,
        kwargs={'units': units,
                'implementation': implementation_mode},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3864')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 110-120
</a>
<div class="mid" id="frag3864" style="display:none"><pre>
  def test_implementation_mode_LSTM(self, implementation_mode):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.LSTM,
        kwargs={'units': units,
                'implementation': implementation_mode},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4476')" href="javascript:;">
keras-2.6.0/keras/layers/gru_test.py: 78-89
</a>
<div class="mid" id="frag4476" style="display:none"><pre>
  def test_dropout_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.GRU,
        kwargs={'units': units,
                'dropout': 0.1,
                'recurrent_dropout': 0.1},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3882')" href="javascript:;">
keras-2.6.0/keras/layers/simplernn_test.py: 70-81
</a>
<div class="mid" id="frag3882" style="display:none"><pre>
  def test_dropout_SimpleRNN(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.SimpleRNN,
        kwargs={'units': units,
                'dropout': 0.1,
                'recurrent_dropout': 0.1},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4487')" href="javascript:;">
keras-2.6.0/keras/layers/cudnn_recurrent_test.py: 41-51
</a>
<div class="mid" id="frag4487" style="display:none"><pre>
  def test_cudnn_rnn_return_sequence(self, layer_class, return_sequences):
    input_size = 10
    timesteps = 6
    units = 2
    num_samples = 32
    testing_utils.layer_test(
        layer_class,
        kwargs={'units': units,
                'return_sequences': return_sequences},
        input_shape=(num_samples, timesteps, input_size))

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3862')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 92-103
</a>
<div class="mid" id="frag3862" style="display:none"><pre>
  def test_dropout_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.LSTM,
        kwargs={'units': units,
                'dropout': 0.1,
                'recurrent_dropout': 0.1},
        input_shape=(num_samples, timesteps, embedding_dim))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4030')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_transpose_test.py: 30-41
</a>
<div class="mid" id="frag4030" style="display:none"><pre>
  def _run_test(self, kwargs):
    num_samples = 2
    stack_size = 3
    num_row = 7
    num_col = 6

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.Conv2DTranspose,
          kwargs=kwargs,
          input_shape=(num_samples, num_row, num_col, stack_size))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4555')" href="javascript:;">
keras-2.6.0/keras/layers/separable_convolutional_test.py: 98-109
</a>
<div class="mid" id="frag4555" style="display:none"><pre>
  def _run_test(self, kwargs):
    num_samples = 2
    stack_size = 3
    num_row = 7
    num_col = 6

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.SeparableConv2D,
          kwargs=kwargs,
          input_shape=(num_samples, num_row, num_col, stack_size))

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4035')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_transpose_test.py: 123-135
</a>
<div class="mid" id="frag4035" style="display:none"><pre>
  def _run_test(self, kwargs):
    num_samples = 2
    stack_size = 3
    num_row = 7
    num_col = 6
    depth = 5

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.Conv3DTranspose,
          kwargs=kwargs,
          input_shape=(num_samples, depth, num_row, num_col, stack_size))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3666')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_test.py: 487-498
</a>
<div class="mid" id="frag3666" style="display:none"><pre>
  def _run_test(self, kwargs, expected_output_shape):
    num_samples = 2
    stack_size = 3
    num_col = 6

    with testing_utils.use_gpu():
      testing_utils.layer_test(
          keras.layers.Conv1DTranspose,
          kwargs=kwargs,
          input_shape=(num_samples, num_col, stack_size),
          expected_output_shape=expected_output_shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3643')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_test.py: 31-42
</a>
<div class="mid" id="frag3643" style="display:none"><pre>
  def _run_test(self, kwargs, expected_output_shape):
    num_samples = 2
    stack_size = 3
    length = 7

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.Conv1D,
          kwargs=kwargs,
          input_shape=(num_samples, length, stack_size),
          expected_output_shape=expected_output_shape)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3685')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_test.py: 1140-1152
</a>
<div class="mid" id="frag3685" style="display:none"><pre>
  def _run_test(self, kwargs, expected_output_shape=None):
    num_samples = 2
    stack_size = 3
    num_row = 7
    num_col = 6

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.DepthwiseConv2D,
          kwargs=kwargs,
          input_shape=(num_samples, num_row, num_col, stack_size),
          expected_output_shape=expected_output_shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3668')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_test.py: 520-533
</a>
<div class="mid" id="frag3668" style="display:none"><pre>
  def _run_test(self, kwargs, expected_output_shape):
    num_samples = 2
    stack_size = 3
    num_row = 7
    num_col = 6
    depth = 5

    with testing_utils.use_gpu():
      testing_utils.layer_test(
          keras.layers.Conv3DTranspose,
          kwargs=kwargs,
          input_shape=(num_samples, depth, num_row, num_col, stack_size),
          expected_output_shape=expected_output_shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3657')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_test.py: 300-314
</a>
<div class="mid" id="frag3657" style="display:none"><pre>
  def _run_test(self, kwargs, expected_output_shape, validate_training=True):
    num_samples = 2
    stack_size = 3
    num_row = 7
    num_col = 6
    depth = 5

    with self.cached_session():
      testing_utils.layer_test(
          keras.layers.Conv3D,
          kwargs=kwargs,
          input_shape=(num_samples, depth, num_row, num_col, stack_size),
          expected_output_shape=expected_output_shape,
          validate_training=validate_training)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 186:</b> &nbsp; 5 fragments, nominal size 13 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3622')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 581-595
</a>
<div class="mid" id="frag3622" style="display:none"><pre>
  def test_float64_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        rnn.LSTM,
        kwargs={
            'units': units,
            'return_sequences': True,
            'dtype': 'float64'
        },
        input_shape=(num_samples, timesteps, embedding_dim),
        input_dtype='float64')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4474')" href="javascript:;">
keras-2.6.0/keras/layers/gru_test.py: 49-61
</a>
<div class="mid" id="frag4474" style="display:none"><pre>
  def test_float64_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.GRU,
        kwargs={'units': units,
                'return_sequences': True,
                'dtype': 'float64'},
        input_shape=(num_samples, timesteps, embedding_dim),
        input_dtype='float64')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3880')" href="javascript:;">
keras-2.6.0/keras/layers/simplernn_test.py: 44-56
</a>
<div class="mid" id="frag3880" style="display:none"><pre>
  def test_float64_SimpleRNN(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.SimpleRNN,
        kwargs={'units': units,
                'return_sequences': True,
                'dtype': 'float64'},
        input_shape=(num_samples, timesteps, embedding_dim),
        input_dtype='float64')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3859')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 47-59
</a>
<div class="mid" id="frag3859" style="display:none"><pre>
  def test_float64_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        keras.layers.LSTM,
        kwargs={'units': units,
                'return_sequences': True,
                'dtype': 'float64'},
        input_shape=(num_samples, timesteps, embedding_dim),
        input_dtype='float64')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4211')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 353-365
</a>
<div class="mid" id="frag4211" style="display:none"><pre>
  def test_float64_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    testing_utils.layer_test(
        rnn.GRU,
        kwargs={'units': units,
                'return_sequences': True,
                'dtype': 'float64'},
        input_shape=(num_samples, timesteps, embedding_dim),
        input_dtype='float64')

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 187:</b> &nbsp; 5 fragments, nominal size 20 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3623')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 596-616
</a>
<div class="mid" id="frag3623" style="display:none"><pre>
  def test_regularizers_LSTM(self):
    embedding_dim = 4
    layer_class = rnn.LSTM
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_regularizer=keras.regularizers.l1(0.01),
        recurrent_regularizer=keras.regularizers.l1(0.01),
        bias_regularizer='l2',
        activity_regularizer='l1')
    layer.build((None, None, 2))
    self.assertEqual(len(layer.losses), 3)
    x = keras.backend.variable(np.ones((2, 3, 2)))
    layer(x)
    if tf.executing_eagerly():
      self.assertEqual(len(layer.losses), 4)
    else:
      self.assertEqual(len(layer.get_losses_for(x)), 1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3877')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 362-382
</a>
<div class="mid" id="frag3877" style="display:none"><pre>
  def test_regularizers_LSTM(self):
    embedding_dim = 4
    layer_class = keras.layers.LSTM
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_regularizer=keras.regularizers.l1(0.01),
        recurrent_regularizer=keras.regularizers.l1(0.01),
        bias_regularizer='l2',
        activity_regularizer='l1')
    layer.build((None, None, 2))
    self.assertEqual(len(layer.losses), 3)
    x = keras.backend.variable(np.ones((2, 3, 2)))
    layer(x)
    if tf.executing_eagerly():
      self.assertEqual(len(layer.losses), 4)
    else:
      self.assertEqual(len(layer.get_losses_for(x)), 1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4486')" href="javascript:;">
keras-2.6.0/keras/layers/gru_test.py: 261-283
</a>
<div class="mid" id="frag4486" style="display:none"><pre>
  def test_regularizers_GRU(self):
    embedding_dim = 4
    layer_class = keras.layers.GRU
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_regularizer=keras.regularizers.l1(0.01),
        recurrent_regularizer=keras.regularizers.l1(0.01),
        bias_regularizer='l2',
        activity_regularizer='l1')
    layer.build((None, None, 2))
    self.assertEqual(len(layer.losses), 3)

    x = keras.backend.variable(np.ones((2, 3, 2)))
    layer(x)
    if tf.executing_eagerly():
      self.assertEqual(len(layer.losses), 4)
    else:
      self.assertEqual(len(layer.get_losses_for(x)), 1)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4216')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 427-448
</a>
<div class="mid" id="frag4216" style="display:none"><pre>
  def test_regularizers_GRU(self):
    embedding_dim = 4
    layer_class = rnn.GRU
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_regularizer=keras.regularizers.l1(0.01),
        recurrent_regularizer=keras.regularizers.l1(0.01),
        bias_regularizer='l2',
        activity_regularizer='l1')
    layer.build((None, None, 2))
    self.assertEqual(len(layer.losses), 3)

    x = keras.backend.variable(np.ones((2, 3, 2)))
    layer(x)
    if tf.executing_eagerly():
      self.assertEqual(len(layer.losses), 4)
    else:
      self.assertEqual(len(layer.get_losses_for(x)), 1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3888')" href="javascript:;">
keras-2.6.0/keras/layers/simplernn_test.py: 137-158
</a>
<div class="mid" id="frag3888" style="display:none"><pre>
  def test_regularizers_SimpleRNN(self):
    embedding_dim = 4
    layer_class = keras.layers.SimpleRNN
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_regularizer=keras.regularizers.l1(0.01),
        recurrent_regularizer=keras.regularizers.l1(0.01),
        bias_regularizer='l2',
        activity_regularizer='l1')
    layer.build((None, None, 2))
    self.assertLen(layer.losses, 3)

    x = keras.backend.variable(np.ones((2, 3, 2)))
    layer(x)
    if tf.executing_eagerly():
      self.assertLen(layer.losses, 4)
    else:
      self.assertLen(layer.get_losses_for(x), 1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 188:</b> &nbsp; 5 fragments, nominal size 50 lines, similarity 79%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3624')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 620-692
</a>
<div class="mid" id="frag3624" style="display:none"><pre>
  def test_statefulness_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer_class = rnn.LSTM
    model = keras.models.Sequential()
    model.add(
        keras.layers.Embedding(
            4,
            embedding_dim,
            mask_zero=True,
            input_length=timesteps,
            batch_input_shape=(num_samples, timesteps)))
    layer = layer_class(
        units, return_sequences=False, stateful=True, weights=None)
    model.add(layer)
    model.compile(
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),
        loss='mse',
        run_eagerly=testing_utils.should_run_eagerly())
    out1 = model.predict(np.ones((num_samples, timesteps)))
    self.assertEqual(out1.shape, (num_samples, units))

    # train once so that the states change
    model.train_on_batch(
        np.ones((num_samples, timesteps)), np.ones((num_samples, units)))
    out2 = model.predict(np.ones((num_samples, timesteps)))

    # if the state is not reset, output should be different
    self.assertNotEqual(out1.max(), out2.max())

    # check that output changes after states are reset
    # (even though the model itself didn't change)
    layer.reset_states()
    out3 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out2.max(), out3.max())

    # check that container-level reset_states() works
    model.reset_states()
    out4 = model.predict(np.ones((num_samples, timesteps)))
    self.assertAllClose(out3, out4, atol=1e-5)

    # check that the call to `predict` updated the states
    out5 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out4.max(), out5.max())

    # Check masking
    layer.reset_states()

    left_padded_input = np.ones((num_samples, timesteps))
    left_padded_input[0, :1] = 0
    left_padded_input[1, :2] = 0
    out6 = model.predict(left_padded_input)

    layer.reset_states()

    right_padded_input = np.ones((num_samples, timesteps))
    right_padded_input[0, -1:] = 0
    right_padded_input[1, -2:] = 0
    out7 = model.predict(right_padded_input)

    layer.reset_states()

    mix_padded_input = np.ones((num_samples, timesteps))
    mix_padded_input[0, 1] = 0
    mix_padded_input[1, 0] = 0
    mix_padded_input[1, 2] = 0
    out8 = model.predict(mix_padded_input)

    self.assertAllClose(out7, out6, atol=1e-5)
    self.assertAllClose(out8, out7, atol=1e-5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3878')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 386-450
</a>
<div class="mid" id="frag3878" style="display:none"><pre>
  def test_statefulness_LSTM(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer_class = keras.layers.LSTM
    model = keras.models.Sequential()
    model.add(
        keras.layers.Embedding(
            4,
            embedding_dim,
            mask_zero=True,
            input_length=timesteps,
            batch_input_shape=(num_samples, timesteps)))
    layer = layer_class(
        units, return_sequences=False, stateful=True, weights=None)
    model.add(layer)
    model.compile(
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),
        loss='mse',
        run_eagerly=testing_utils.should_run_eagerly())
    out1 = model.predict(np.ones((num_samples, timesteps)))
    self.assertEqual(out1.shape, (num_samples, units))

    # train once so that the states change
    model.train_on_batch(
        np.ones((num_samples, timesteps)), np.ones((num_samples, units)))
    out2 = model.predict(np.ones((num_samples, timesteps)))

    # if the state is not reset, output should be different
    self.assertNotEqual(out1.max(), out2.max())

    # check that output changes after states are reset
    # (even though the model itself didn't change)
    layer.reset_states()
    out3 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out2.max(), out3.max())

    # check that container-level reset_states() works
    model.reset_states()
    out4 = model.predict(np.ones((num_samples, timesteps)))
    self.assertAllClose(out3, out4, atol=1e-5)

    # check that the call to `predict` updated the states
    out5 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out4.max(), out5.max())

    # Check masking
    layer.reset_states()

    left_padded_input = np.ones((num_samples, timesteps))
    left_padded_input[0, :1] = 0
    left_padded_input[1, :2] = 0
    out6 = model.predict(left_padded_input)

    layer.reset_states()

    right_padded_input = np.ones((num_samples, timesteps))
    right_padded_input[0, -1:] = 0
    right_padded_input[1, -2:] = 0
    out7 = model.predict(right_padded_input)

    self.assertAllClose(out7, out6, atol=1e-5)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4481')" href="javascript:;">
keras-2.6.0/keras/layers/gru_test.py: 152-216
</a>
<div class="mid" id="frag4481" style="display:none"><pre>
  def test_statefulness_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer_class = keras.layers.GRU

    model = keras.models.Sequential()
    model.add(
        keras.layers.Embedding(
            4,
            embedding_dim,
            mask_zero=True,
            input_length=timesteps,
            batch_input_shape=(num_samples, timesteps)))
    layer = layer_class(
        units, return_sequences=False, stateful=True, weights=None)
    model.add(layer)
    model.compile(
        optimizer='sgd',
        loss='mse',
        run_eagerly=testing_utils.should_run_eagerly())
    out1 = model.predict(np.ones((num_samples, timesteps)))
    self.assertEqual(out1.shape, (num_samples, units))

    # train once so that the states change
    model.train_on_batch(
        np.ones((num_samples, timesteps)), np.ones((num_samples, units)))
    out2 = model.predict(np.ones((num_samples, timesteps)))

    # if the state is not reset, output should be different
    self.assertNotEqual(out1.max(), out2.max())

    # check that output changes after states are reset
    # (even though the model itself didn't change)
    layer.reset_states()
    out3 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out2.max(), out3.max())

    # check that container-level reset_states() works
    model.reset_states()
    out4 = model.predict(np.ones((num_samples, timesteps)))
    np.testing.assert_allclose(out3, out4, atol=1e-5)

    # check that the call to `predict` updated the states
    out5 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out4.max(), out5.max())

    # Check masking
    layer.reset_states()

    left_padded_input = np.ones((num_samples, timesteps))
    left_padded_input[0, :1] = 0
    left_padded_input[1, :2] = 0
    out6 = model.predict(left_padded_input)

    layer.reset_states()

    right_padded_input = np.ones((num_samples, timesteps))
    right_padded_input[0, -1:] = 0
    right_padded_input[1, -2:] = 0
    out7 = model.predict(right_padded_input)

    np.testing.assert_allclose(out7, out6, atol=1e-5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4217')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 452-524
</a>
<div class="mid" id="frag4217" style="display:none"><pre>
  def test_statefulness_GRU(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer_class = rnn.GRU
    model = keras.models.Sequential()
    model.add(
        keras.layers.Embedding(
            4,
            embedding_dim,
            mask_zero=True,
            input_length=timesteps,
            batch_input_shape=(num_samples, timesteps)))
    layer = layer_class(
        units, return_sequences=False, stateful=True, weights=None)
    model.add(layer)
    model.compile(
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),
        loss='mse',
        run_eagerly=testing_utils.should_run_eagerly())
    out1 = model.predict(np.ones((num_samples, timesteps)))
    self.assertEqual(out1.shape, (num_samples, units))

    # train once so that the states change
    model.train_on_batch(
        np.ones((num_samples, timesteps)), np.ones((num_samples, units)))
    out2 = model.predict(np.ones((num_samples, timesteps)))

    # if the state is not reset, output should be different
    self.assertNotEqual(out1.max(), out2.max())

    # check that output changes after states are reset
    # (even though the model itself didn't change)
    layer.reset_states()
    out3 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out2.max(), out3.max())

    # check that container-level reset_states() works
    model.reset_states()
    out4 = model.predict(np.ones((num_samples, timesteps)))
    np.testing.assert_allclose(out3, out4, atol=1e-5)

    # check that the call to `predict` updated the states
    out5 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out4.max(), out5.max())

    # Check masking
    layer.reset_states()

    left_padded_input = np.ones((num_samples, timesteps))
    left_padded_input[0, :1] = 0
    left_padded_input[1, :2] = 0
    out6 = model.predict(left_padded_input)

    layer.reset_states()

    right_padded_input = np.ones((num_samples, timesteps))
    right_padded_input[0, -1:] = 0
    right_padded_input[1, -2:] = 0
    out7 = model.predict(right_padded_input)

    layer.reset_states()

    mix_padded_input = np.ones((num_samples, timesteps))
    mix_padded_input[0, 1] = 0
    mix_padded_input[1, 0] = 0
    mix_padded_input[1, 2] = 0
    out8 = model.predict(mix_padded_input)

    self.assertAllClose(out7, out6, atol=1e-5)
    self.assertAllClose(out8, out7, atol=1e-5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3889')" href="javascript:;">
keras-2.6.0/keras/layers/simplernn_test.py: 159-222
</a>
<div class="mid" id="frag3889" style="display:none"><pre>
  def test_statefulness_SimpleRNN(self):
    num_samples = 2
    timesteps = 3
    embedding_dim = 4
    units = 2
    layer_class = keras.layers.SimpleRNN
    model = keras.models.Sequential()
    model.add(
        keras.layers.Embedding(
            4,
            embedding_dim,
            mask_zero=True,
            input_length=timesteps,
            batch_input_shape=(num_samples, timesteps)))
    layer = layer_class(
        units, return_sequences=False, stateful=True, weights=None)
    model.add(layer)
    model.compile(
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),
        loss='mse',
        run_eagerly=testing_utils.should_run_eagerly())
    out1 = model.predict(np.ones((num_samples, timesteps)))
    self.assertEqual(out1.shape, (num_samples, units))

    # train once so that the states change
    model.train_on_batch(
        np.ones((num_samples, timesteps)), np.ones((num_samples, units)))
    out2 = model.predict(np.ones((num_samples, timesteps)))

    # if the state is not reset, output should be different
    self.assertNotEqual(out1.max(), out2.max())

    # check that output changes after states are reset
    # (even though the model itself didn't change)
    layer.reset_states()
    out3 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out2.max(), out3.max())

    # check that container-level reset_states() works
    model.reset_states()
    out4 = model.predict(np.ones((num_samples, timesteps)))
    np.testing.assert_allclose(out3, out4, atol=1e-5)

    # check that the call to `predict` updated the states
    out5 = model.predict(np.ones((num_samples, timesteps)))
    self.assertNotEqual(out4.max(), out5.max())

    # Check masking
    layer.reset_states()

    left_padded_input = np.ones((num_samples, timesteps))
    left_padded_input[0, :1] = 0
    left_padded_input[1, :2] = 0
    out6 = model.predict(left_padded_input)

    layer.reset_states()

    right_padded_input = np.ones((num_samples, timesteps))
    right_padded_input[0, -1:] = 0
    right_padded_input[1, -2:] = 0
    out7 = model.predict(right_padded_input)

    np.testing.assert_allclose(out7, out6, atol=1e-5)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 189:</b> &nbsp; 3 fragments, nominal size 19 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3625')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 693-714
</a>
<div class="mid" id="frag3625" style="display:none"><pre>
  def test_stateful_LSTM_training(self):
    # See b/123587692 for more context.
    vocab_size = 20
    embedding_dim = 10
    batch_size = 8
    timestep = 12
    units = 5
    x = np.random.randint(0, vocab_size, size=(batch_size, timestep))
    y = np.random.randint(0, vocab_size, size=(batch_size, timestep))

    model = keras.Sequential([
        keras.layers.Embedding(vocab_size, embedding_dim,
                               batch_input_shape=[batch_size, timestep]),
        rnn.LSTM(units, return_sequences=True, stateful=True),
        keras.layers.Dense(vocab_size)
    ])
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        run_eagerly=testing_utils.should_run_eagerly())
    model.fit(x, y, epochs=1, shuffle=False)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4356')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent_v2_test.py: 37-62
</a>
<div class="mid" id="frag4356" style="display:none"><pre>
  def test_device_placement(self, layer):
    if not tf.test.is_gpu_available():
      self.skipTest('Need GPU for testing.')
    vocab_size = 20
    embedding_dim = 10
    batch_size = 8
    timestep = 12
    units = 5
    x = np.random.randint(0, vocab_size, size=(batch_size, timestep))
    y = np.random.randint(0, vocab_size, size=(batch_size, timestep))

    # Test when GPU is available but not used, the graph should be properly
    # created with CPU ops.
    with testing_utils.device(should_use_gpu=False):
      model = keras.Sequential([
          keras.layers.Embedding(vocab_size, embedding_dim,
                                 batch_input_shape=[batch_size, timestep]),
          layer(units, return_sequences=True, stateful=True),
          keras.layers.Dense(vocab_size)
      ])
      model.compile(
          optimizer='adam',
          loss='sparse_categorical_crossentropy',
          run_eagerly=testing_utils.should_run_eagerly())
      model.fit(x, y, epochs=1, shuffle=False)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4218')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 525-546
</a>
<div class="mid" id="frag4218" style="display:none"><pre>
  def test_stateful_GRU_training(self):
    # See b/123587692 for more context.
    vocab_size = 20
    embedding_dim = 10
    batch_size = 8
    timestep = 12
    units = 5
    x = np.random.randint(0, vocab_size, size=(batch_size, timestep))
    y = np.random.randint(0, vocab_size, size=(batch_size, timestep))

    model = keras.Sequential([
        keras.layers.Embedding(vocab_size, embedding_dim,
                               batch_input_shape=[batch_size, timestep]),
        rnn.GRU(units, return_sequences=True, stateful=True),
        keras.layers.Dense(vocab_size)
    ])
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        run_eagerly=testing_utils.should_run_eagerly())
    model.fit(x, y, epochs=1, shuffle=False)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 190:</b> &nbsp; 2 fragments, nominal size 18 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3628')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 756-779
</a>
<div class="mid" id="frag3628" style="display:none"><pre>
  def test_explicit_device_with_go_backward_and_mask(self):
    batch_size = 8
    timestep = 7
    masksteps = 5
    units = 4

    inputs = np.random.randn(batch_size, timestep, units).astype(np.float32)
    mask = np.ones((batch_size, timestep)).astype(np.bool)
    mask[:, masksteps:] = 0

    # Test for V1 behavior.
    lstm_v1 = rnn_v1.LSTM(units, return_sequences=True, go_backwards=True)
    with testing_utils.device(should_use_gpu=True):
      outputs_masked_v1 = lstm_v1(inputs, mask=tf.constant(mask))
      outputs_trimmed_v1 = lstm_v1(inputs[:, :masksteps])
    self.assertAllClose(outputs_masked_v1[:, -masksteps:], outputs_trimmed_v1)

    # Test for V2 behavior.
    lstm = rnn.LSTM(units, return_sequences=True, go_backwards=True)
    with testing_utils.device(should_use_gpu=True):
      outputs_masked = lstm(inputs, mask=tf.constant(mask))
      outputs_trimmed = lstm(inputs[:, :masksteps])
    self.assertAllClose(outputs_masked[:, -masksteps:], outputs_trimmed)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4219')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 551-574
</a>
<div class="mid" id="frag4219" style="display:none"><pre>
  def test_explicit_device_with_go_backward_and_mask(self):
    batch_size = 8
    timestep = 7
    masksteps = 5
    units = 4

    inputs = np.random.randn(batch_size, timestep, units).astype(np.float32)
    mask = np.ones((batch_size, timestep)).astype(np.bool)
    mask[:, masksteps:] = 0

    # Test for V1 behavior.
    lstm_v1 = rnn_v1.GRU(units, return_sequences=True, go_backwards=True)
    with testing_utils.device(should_use_gpu=True):
      outputs_masked_v1 = lstm_v1(inputs, mask=tf.constant(mask))
      outputs_trimmed_v1 = lstm_v1(inputs[:, :masksteps])
    self.assertAllClose(outputs_masked_v1[:, -masksteps:], outputs_trimmed_v1)

    # Test for V2 behavior.
    lstm = rnn.GRU(units, return_sequences=True, go_backwards=True)
    with testing_utils.device(should_use_gpu=True):
      outputs_masked = lstm(inputs, mask=tf.constant(mask))
      outputs_trimmed = lstm(inputs[:, :masksteps])
    self.assertAllClose(outputs_masked[:, -masksteps:], outputs_trimmed)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 191:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3629')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 781-797
</a>
<div class="mid" id="frag3629" style="display:none"><pre>
  def test_v1_session_behavior(self):
    with tf.compat.v1.get_default_graph().as_default():
      # See b/139132348 for more details.
      x = np.random.uniform(size=(100, 4, 8))
      y = np.random.uniform(size=(100, 1))
      dataset = tf.data.Dataset.from_tensor_slices(
          (x, y)).shuffle(100).batch(32)

      inp = keras.layers.Input(shape=(4, 8))
      layer = rnn.LSTM(1)(inp)
      layer = keras.layers.Dense(1)(layer)

      model = keras.models.Model(inp, layer)

      model.compile(loss='mse', optimizer='sgd')
      model.fit(dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4220')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 576-592
</a>
<div class="mid" id="frag4220" style="display:none"><pre>
  def test_v1_session_behavior(self):
    with tf.compat.v1.get_default_graph().as_default():
      # See b/139132348 for more details.
      x = np.random.uniform(size=(100, 4, 8))
      y = np.random.uniform(size=(100, 1))
      dataset = tf.data.Dataset.from_tensor_slices(
          (x, y)).shuffle(100).batch(32)

      inp = keras.layers.Input(shape=(4, 8))
      layer = rnn.GRU(1)(inp)
      layer = keras.layers.Dense(1)(layer)

      model = keras.models.Model(inp, layer)

      model.compile(loss='mse', optimizer='sgd')
      model.fit(dataset)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 192:</b> &nbsp; 2 fragments, nominal size 23 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3630')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 798-826
</a>
<div class="mid" id="frag3630" style="display:none"><pre>
  def test_with_fully_masked_inputs(self):
    num_samples = 8
    timestep = 5
    embedding_dim = 4
    vocab_size = 20
    units = 2

    inputs = np.random.randint(0, vocab_size, size=(num_samples, timestep))
    # Set the first inputs to be fully zero.
    inputs[0, :] = 0.0

    model = keras.models.Sequential()
    model.add(
        keras.layers.Embedding(
            vocab_size,
            embedding_dim,
            mask_zero=True,
            input_length=timestep,
            batch_input_shape=(num_samples, timestep)))
    layer = rnn.LSTM(units)
    model.add(layer)
    model.compile(
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),
        loss='mse',
        run_eagerly=testing_utils.should_run_eagerly())
    # Make sure it doesn't crash with cudnn kernel.
    model.predict(inputs)

  # TODO (b/169895267): test with xla_gpu is disabled.
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4221')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 593-621
</a>
<div class="mid" id="frag4221" style="display:none"><pre>
  def test_with_fully_masked_inputs(self):
    num_samples = 8
    timestep = 5
    embedding_dim = 4
    vocab_size = 20
    units = 2

    inputs = np.random.randint(0, vocab_size, size=(num_samples, timestep))
    # Set the first inputs to be fully zero.
    inputs[0, :] = 0.0

    model = keras.models.Sequential()
    model.add(
        keras.layers.Embedding(
            vocab_size,
            embedding_dim,
            mask_zero=True,
            input_length=timestep,
            batch_input_shape=(num_samples, timestep)))
    layer = rnn.GRU(units)
    model.add(layer)
    model.compile(
        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),
        loss='mse',
        run_eagerly=testing_utils.should_run_eagerly())
    # Make sure it doesn't crash with cudnn kernel.
    model.predict(inputs)

  # TODO (b/169895267): test with xla_gpu is disabled.
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 193:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3631')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 827-851
</a>
<div class="mid" id="frag3631" style="display:none"><pre>
  def test_deepcopy(self):
    if not tf.executing_eagerly():
      self.skipTest('v2-only test')
    original_layer = rnn.LSTM(5)
    copied_layer = copy.deepcopy(original_layer)
    self.assertEqual(copied_layer.units, 5)
    self.assertEqual(original_layer.get_config(), original_layer.get_config())

    # Copy layer before layer call on inputs without weight initialization.
    inputs = np.random.normal(size=[32, 10, 8]).astype(np.float32)
    original_layer = rnn.LSTM(4)
    copied_layer = copy.deepcopy(original_layer)
    outputs = original_layer(inputs)
    copied_outputs = copied_layer(inputs)
    self.assertNotAllClose(
        self.evaluate(outputs), self.evaluate(copied_outputs))

    # Copy layer after layer call on inputs with weight initialization.
    original_layer = rnn.LSTM(4)
    outputs = original_layer(inputs)
    copied_layer = copy.deepcopy(original_layer)
    copied_outputs = copied_layer(inputs)
    self.assertAllClose(self.evaluate(outputs), self.evaluate(copied_outputs))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4222')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 622-646
</a>
<div class="mid" id="frag4222" style="display:none"><pre>
  def test_deepcopy(self):
    if not tf.executing_eagerly():
      self.skipTest('v2-only test')
    original_layer = rnn.GRU(5)
    copied_layer = copy.deepcopy(original_layer)
    self.assertEqual(copied_layer.units, 5)
    self.assertEqual(original_layer.get_config(), original_layer.get_config())

    # Copy layer before layer call on inputs without weight initialization.
    inputs = np.random.normal(size=[32, 10, 8]).astype(np.float32)
    original_layer = rnn.GRU(4)
    copied_layer = copy.deepcopy(original_layer)
    outputs = original_layer(inputs)
    copied_outputs = copied_layer(inputs)
    self.assertNotAllClose(
        self.evaluate(outputs), self.evaluate(copied_outputs))

    # Copy layer after layer call on inputs with weight initialization.
    original_layer = rnn.GRU(4)
    outputs = original_layer(inputs)
    copied_layer = copy.deepcopy(original_layer)
    copied_outputs = copied_layer(inputs)
    self.assertAllClose(self.evaluate(outputs), self.evaluate(copied_outputs))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 194:</b> &nbsp; 2 fragments, nominal size 22 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3632')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 862-889
</a>
<div class="mid" id="frag3632" style="display:none"><pre>
  def _test_runtime_with_model(self, model):

    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=self.batch,
        test_samples=0,
        input_shape=(self.timestep, self.input_shape),
        num_classes=self.output_shape)
    y_train = np_utils.to_categorical(y_train, self.output_shape)

    model.compile(
        optimizer='sgd',
        loss=['categorical_crossentropy', None],
        run_eagerly=testing_utils.should_run_eagerly())

    existing_loss = 0
    for _ in range(self.epoch):
      history = model.fit(x_train, y_train)
      loss_value = history.history['loss'][0]

      self.assertNotEqual(existing_loss, loss_value)
      existing_loss = loss_value

    _, runtime_value = model.predict(x_train)
    if tf.test.is_gpu_available():
      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)
    else:
      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4224')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 686-711
</a>
<div class="mid" id="frag4224" style="display:none"><pre>
  def _test_runtime_with_model(self, model):
    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=self.batch,
        test_samples=0,
        input_shape=(self.timestep, self.input_shape),
        num_classes=self.output_shape)
    y_train = np_utils.to_categorical(y_train, self.output_shape)

    model.compile(
        optimizer='sgd',
        loss=['categorical_crossentropy', None])

    existing_loss = 0
    for _ in range(self.epoch):
      history = model.fit(x_train, y_train)
      loss_value = history.history['loss'][0]

      self.assertNotEqual(existing_loss, loss_value)
      existing_loss = loss_value

    _, runtime_value = model.predict(x_train)
    if tf.test.is_gpu_available():
      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)
    else:
      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 195:</b> &nbsp; 2 fragments, nominal size 36 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3634')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 910-963
</a>
<div class="mid" id="frag3634" style="display:none"><pre>
  def test_LSTM_runtime_with_mask(self):
    # Masking will affect which backend is selected based on whether the mask
    # is strictly right padded.
    layer = rnn.LSTM(self.rnn_state_size, return_runtime=True)

    inputs = keras.layers.Input(
        shape=[self.timestep, self.input_shape], dtype=tf.float32)
    masked_inputs = keras.layers.Masking()(inputs)

    outputs, runtime = layer(masked_inputs)
    # Expand the runtime so that it is a 1D tensor instead of scalar.
    # TF model does not work with scalar model output, specially during
    # aggregation.
    runtime = keras.layers.Lambda(
        lambda x: tf.expand_dims(x, axis=-1))(runtime)
    model = keras.models.Model(inputs=inputs, outputs=[outputs, runtime])

    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=self.batch,
        test_samples=0,
        input_shape=(self.timestep, self.input_shape),
        num_classes=self.output_shape)
    y_train = np_utils.to_categorical(y_train, self.output_shape)

    model.compile(
        optimizer='sgd',
        loss=['categorical_crossentropy', None],
        run_eagerly=testing_utils.should_run_eagerly())

    model.fit(x_train, y_train)

    # Verify unpadded data.
    _, runtime_value = model.predict(x_train)
    if tf.test.is_gpu_available():
      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)
    else:
      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

    # Update x/y to be right padded by setting the last timestep to 0
    x_train[:, -1, :] = 0
    y_train[:, -1] = 0
    _, runtime_value = model.predict(x_train)
    if tf.test.is_gpu_available():
      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)
    else:
      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

    # Further update x/y to be mix padded (masks in the middle), and verify
    # only cpu kernel can be selected.
    x_train[:, -3, :] = 0
    y_train[:, -3] = 0
    _, runtime_value = model.predict(x_train)
    self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4226')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 732-785
</a>
<div class="mid" id="frag4226" style="display:none"><pre>
  def test_GRU_runtime_with_mask(self):
    # Masking will affect which backend is selected based on whether the mask
    # is strictly right padded.
    layer = rnn.GRU(self.rnn_state_size, return_runtime=True)

    inputs = keras.layers.Input(
        shape=[self.timestep, self.input_shape], dtype=tf.float32)
    masked_inputs = keras.layers.Masking()(inputs)

    outputs, runtime = layer(masked_inputs)
    # Expand the runtime so that it is a 1D tensor instead of scalar.
    # TF model does not work with scalar model output, specially during
    # aggregation.
    runtime = keras.layers.Lambda(
        lambda x: tf.expand_dims(x, axis=-1))(runtime)
    model = keras.models.Model(inputs=inputs, outputs=[outputs, runtime])

    (x_train, y_train), _ = testing_utils.get_test_data(
        train_samples=self.batch,
        test_samples=0,
        input_shape=(self.timestep, self.input_shape),
        num_classes=self.output_shape)
    y_train = np_utils.to_categorical(y_train, self.output_shape)

    model.compile(
        optimizer='sgd',
        loss=['categorical_crossentropy', None],
        run_eagerly=testing_utils.should_run_eagerly())

    model.fit(x_train, y_train)

    # Verify unpadded data.
    _, runtime_value = model.predict(x_train)
    if tf.test.is_gpu_available():
      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)
    else:
      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

    # Update x/y to be right padded by setting the last timestep to 0
    x_train[:, -1, :] = 0
    y_train[:, -1] = 0
    _, runtime_value = model.predict(x_train)
    if tf.test.is_gpu_available():
      self.assertEqual(runtime_value[0], rnn._RUNTIME_GPU)
    else:
      self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

    # Further update x/y to be mix padded (masks in the middle), and verify
    # only cpu kernel can be selected.
    x_train[:, -3, :] = 0
    y_train[:, -3] = 0
    _, runtime_value = model.predict(x_train)
    self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 196:</b> &nbsp; 2 fragments, nominal size 16 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3635')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 965-992
</a>
<div class="mid" id="frag3635" style="display:none"><pre>
  def test_LSTM_runtime_with_cond(self):
    # This test is to demonstrate the graph rewrite of grappler plugin under
    # the condition that the function returns different number of internal
    # states.
    layer = rnn.LSTM(self.rnn_state_size, return_runtime=True)

    inputs = keras.layers.Input(
        shape=[self.timestep, self.input_shape], dtype=tf.float32)

    zeros = tf.zeros([self.batch, self.output_shape])
    dummy_runtime = rnn._runtime(rnn._RUNTIME_UNKNOWN)
    a = tf.constant(0)
    b = tf.constant(1)
    # Will always run the lstm layer.
    outputs, runtime = tf.cond(
        tf.less(a, b),
        lambda: layer(inputs),
        lambda: (zeros, dummy_runtime))

    # Expand the runtime so that it is a 1D tensor instead of scalar.
    # TF model does not work with scalar model output, specially during
    # aggregation.
    runtime = keras.layers.Lambda(
        lambda x: tf.expand_dims(x, axis=-1))(runtime)
    model = keras.models.Model(inputs=inputs, outputs=[outputs, runtime])
    self._test_runtime_with_model(model)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4227')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 787-814
</a>
<div class="mid" id="frag4227" style="display:none"><pre>
  def test_GRU_runtime_with_cond(self):
    # This test is to demonstrate the graph rewrite of grappler plugin under
    # the condition that the function returns different number of internal
    # states.
    layer = rnn.GRU(self.rnn_state_size, return_runtime=True)

    inputs = keras.layers.Input(
        shape=[self.timestep, self.input_shape], dtype=tf.float32)

    zeros = tf.zeros([self.batch, self.output_shape])
    dummy_runtime = rnn._runtime(rnn._RUNTIME_UNKNOWN)
    a = tf.constant(0)
    b = tf.constant(1)
    # Will always run the GRU layer.
    outputs, runtime = tf.cond(
        tf.less(a, b),
        lambda: layer(inputs),
        lambda: (zeros, dummy_runtime))

    # Expand the runtime so that it is a 1D tensor instead of scalar.
    # TF model does not work with scalar model output, specially during
    # aggregation.
    runtime = keras.layers.Lambda(
        lambda x: tf.expand_dims(x, axis=-1))(runtime)
    model = keras.models.Model(inputs=inputs, outputs=[outputs, runtime])
    self._test_runtime_with_model(model)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 197:</b> &nbsp; 3 fragments, nominal size 16 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3637')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 1007-1026
</a>
<div class="mid" id="frag3637" style="display:none"><pre>
  def _time_performance_run_cudnn_lstm(self, test_config, x_train, y_train):
    # Get the performance number for standard Cudnn LSTM
    input_shape = test_config['input_shape']
    rnn_state_size = test_config['rnn_state_size']
    timestep = test_config['timestep']

    cudnn_lstm_layer = keras.layers.CuDNNLSTM(rnn_state_size)
    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)

    outputs = cudnn_lstm_layer(inputs)
    model = keras.models.Model(inputs, outputs)
    model.compile('sgd', 'mse')

    sec_per_epoch = self._measure_performance(
        test_config, model, x_train, y_train)
    logging.info('Average performance for %s per epoch is: %s',
                 'CuDNN LSTM', sec_per_epoch)
    return sec_per_epoch

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3639')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 1048-1068
</a>
<div class="mid" id="frag3639" style="display:none"><pre>
  def _time_performance_run_normal_lstm(
      self, test_config, x_train, y_train):
    # Get performance number for standard LSTM on GPU.
    input_shape = test_config['input_shape']
    rnn_state_size = test_config['rnn_state_size']
    timestep = test_config['timestep']

    layer = rnn_v1.LSTM(rnn_state_size)
    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)

    outputs = layer(inputs)
    model = keras.models.Model(inputs, outputs)
    model.compile('sgd', 'mse')

    sec_per_epoch = self._measure_performance(
        test_config, model, x_train, y_train)
    logging.info('Average performance for %s per epoch is: %s',
                 'Normal LSTM', sec_per_epoch)
    return sec_per_epoch

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3638')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_v2_test.py: 1027-1047
</a>
<div class="mid" id="frag3638" style="display:none"><pre>
  def _time_performance_run_unifed_lstm_gpu(
      self, test_config, x_train, y_train):
    # Get performance number for lstm_v2 with grappler swap the impl
    input_shape = test_config['input_shape']
    rnn_state_size = test_config['rnn_state_size']
    timestep = test_config['timestep']

    layer = rnn.LSTM(rnn_state_size)
    inputs = keras.layers.Input(
        shape=[timestep, input_shape], dtype=tf.float32)

    outputs = layer(inputs)
    model = keras.models.Model(inputs, outputs)
    model.compile('sgd', 'mse')

    sec_per_epoch = self._measure_performance(
        test_config, model, x_train, y_train)
    logging.info('Average performance for %s per epoch is: %s',
                 'LSTM V2', sec_per_epoch)
    return sec_per_epoch

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 198:</b> &nbsp; 7 fragments, nominal size 15 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3646')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_test.py: 96-112
</a>
<div class="mid" id="frag3646" style="display:none"><pre>
  def test_conv1d_regularizers(self):
    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv1D(**kwargs)
      layer.build((None, 5, 2))
      self.assertEqual(len(layer.losses), 2)
      layer(keras.backend.variable(np.ones((1, 5, 2))))
      self.assertEqual(len(layer.losses), 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4557')" href="javascript:;">
keras-2.6.0/keras/layers/separable_convolutional_test.py: 127-144
</a>
<div class="mid" id="frag4557" style="display:none"><pre>
  def test_separable_conv2d_regularizers(self):
    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'depthwise_regularizer': 'l2',
        'pointwise_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.SeparableConv2D(**kwargs)
      layer.build((None, 5, 5, 2))
      self.assertEqual(len(layer.losses), 3)
      layer(keras.backend.variable(np.ones((1, 5, 5, 2))))
      self.assertEqual(len(layer.losses), 4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4553')" href="javascript:;">
keras-2.6.0/keras/layers/separable_convolutional_test.py: 55-72
</a>
<div class="mid" id="frag4553" style="display:none"><pre>
  def test_separable_conv1d_regularizers(self):
    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'depthwise_regularizer': 'l2',
        'pointwise_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.SeparableConv1D(**kwargs)
      layer.build((None, 5, 2))
      self.assertEqual(len(layer.losses), 3)
      layer(keras.backend.variable(np.ones((1, 5, 2))))
      self.assertEqual(len(layer.losses), 4)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4037')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_transpose_test.py: 152-168
</a>
<div class="mid" id="frag4037" style="display:none"><pre>
  def test_conv3d_transpose_regularizers(self):
    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv3DTranspose(**kwargs)
      layer.build((None, 5, 5, 5, 2))
      self.assertEqual(len(layer.losses), 2)
      layer(keras.backend.variable(np.ones((1, 5, 5, 5, 2))))
      self.assertEqual(len(layer.losses), 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4032')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_transpose_test.py: 57-73
</a>
<div class="mid" id="frag4032" style="display:none"><pre>
  def test_conv2d_transpose_regularizers(self):
    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv2DTranspose(**kwargs)
      layer.build((None, 5, 5, 2))
      self.assertEqual(len(layer.losses), 2)
      layer(keras.backend.variable(np.ones((1, 5, 5, 2))))
      self.assertEqual(len(layer.losses), 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3660')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_test.py: 371-388
</a>
<div class="mid" id="frag3660" style="display:none"><pre>
  def test_conv3d_regularizers(self):
    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv3D(**kwargs)
      layer.build((None, 5, 5, 5, 2))
      self.assertEqual(len(layer.losses), 2)
      self.assertEqual(len(layer.losses), 2)
      layer(keras.backend.variable(np.ones((1, 5, 5, 5, 2))))
      self.assertEqual(len(layer.losses), 3)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3654')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_test.py: 256-272
</a>
<div class="mid" id="frag3654" style="display:none"><pre>
  def test_conv2d_regularizers(self):
    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv2D(**kwargs)
      layer.build((None, 5, 5, 2))
      self.assertEqual(len(layer.losses), 2)
      layer(keras.backend.variable(np.ones((1, 5, 5, 2))))
      self.assertEqual(len(layer.losses), 3)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 199:</b> &nbsp; 7 fragments, nominal size 15 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3647')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_test.py: 113-130
</a>
<div class="mid" id="frag3647" style="display:none"><pre>
  def test_conv1d_constraints(self):
    k_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_constraint': k_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv1D(**kwargs)
      layer.build((None, 5, 2))
      self.assertEqual(layer.kernel.constraint, k_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4554')" href="javascript:;">
keras-2.6.0/keras/layers/separable_convolutional_test.py: 73-94
</a>
<div class="mid" id="frag4554" style="display:none"><pre>
  def test_separable_conv1d_constraints(self):
    d_constraint = lambda x: x
    p_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'pointwise_constraint': p_constraint,
        'depthwise_constraint': d_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.SeparableConv1D(**kwargs)
      layer.build((None, 5, 2))
      self.assertEqual(layer.depthwise_kernel.constraint, d_constraint)
      self.assertEqual(layer.pointwise_kernel.constraint, p_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3661')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_test.py: 389-406
</a>
<div class="mid" id="frag3661" style="display:none"><pre>
  def test_conv3d_constraints(self):
    k_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_constraint': k_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv3D(**kwargs)
      layer.build((None, 5, 5, 5, 2))
      self.assertEqual(layer.kernel.constraint, k_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4558')" href="javascript:;">
keras-2.6.0/keras/layers/separable_convolutional_test.py: 145-164
</a>
<div class="mid" id="frag4558" style="display:none"><pre>
  def test_separable_conv2d_constraints(self):
    d_constraint = lambda x: x
    p_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'pointwise_constraint': p_constraint,
        'depthwise_constraint': d_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.SeparableConv2D(**kwargs)
      layer.build((None, 5, 5, 2))
      self.assertEqual(layer.depthwise_kernel.constraint, d_constraint)
      self.assertEqual(layer.pointwise_kernel.constraint, p_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)
</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4038')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_transpose_test.py: 169-186
</a>
<div class="mid" id="frag4038" style="display:none"><pre>
  def test_conv3d_transpose_constraints(self):
    k_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_constraint': k_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv3DTranspose(**kwargs)
      layer.build((None, 5, 5, 5, 2))
      self.assertEqual(layer.kernel.constraint, k_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4033')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_transpose_test.py: 74-91
</a>
<div class="mid" id="frag4033" style="display:none"><pre>
  def test_conv2d_transpose_constraints(self):
    k_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_constraint': k_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv2DTranspose(**kwargs)
      layer.build((None, 5, 5, 2))
      self.assertEqual(layer.kernel.constraint, k_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3655')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_test.py: 273-290
</a>
<div class="mid" id="frag3655" style="display:none"><pre>
  def test_conv2d_constraints(self):
    k_constraint = lambda x: x
    b_constraint = lambda x: x

    kwargs = {
        'filters': 3,
        'kernel_size': 3,
        'padding': 'valid',
        'kernel_constraint': k_constraint,
        'bias_constraint': b_constraint,
        'strides': 1
    }
    with self.cached_session():
      layer = keras.layers.Conv2D(**kwargs)
      layer.build((None, 5, 5, 2))
      self.assertEqual(layer.kernel.constraint, k_constraint)
      self.assertEqual(layer.bias.constraint, b_constraint)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 200:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3662')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_test.py: 407-431
</a>
<div class="mid" id="frag3662" style="display:none"><pre>
  def test_conv3d_dynamic_shape(self):
    input_data = np.random.random((1, 3, 3, 3, 3)).astype(np.float32)
    with self.cached_session():
      # Won't raise error here.
      testing_utils.layer_test(
          keras.layers.Conv3D,
          kwargs={
              'data_format': 'channels_last',
              'filters': 3,
              'kernel_size': 3
          },
          input_shape=(None, None, None, None, 3),
          input_data=input_data)
      if tf.test.is_gpu_available(cuda_only=True):
        testing_utils.layer_test(
            keras.layers.Conv3D,
            kwargs={
                'data_format': 'channels_first',
                'filters': 3,
                'kernel_size': 3
            },
            input_shape=(None, 3, None, None, None),
            input_data=input_data)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4039')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_transpose_test.py: 187-210
</a>
<div class="mid" id="frag4039" style="display:none"><pre>
  def test_conv3d_transpose_dynamic_shape(self):
    input_data = np.random.random((1, 3, 3, 3, 3)).astype(np.float32)
    with self.cached_session():
      # Won't raise error here.
      testing_utils.layer_test(
          keras.layers.Conv3DTranspose,
          kwargs={
              'data_format': 'channels_last',
              'filters': 3,
              'kernel_size': 3
          },
          input_shape=(None, None, None, None, 3),
          input_data=input_data)
      if tf.test.is_gpu_available(cuda_only=True):
        testing_utils.layer_test(
            keras.layers.Conv3DTranspose,
            kwargs={
                'data_format': 'channels_first',
                'filters': 3,
                'kernel_size': 3
            },
            input_shape=(None, 3, None, None, None),
            input_data=input_data)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 201:</b> &nbsp; 2 fragments, nominal size 45 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3679')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_test.py: 854-902
</a>
<div class="mid" id="frag3679" style="display:none"><pre>
  def test_upsampling_2d(self):
    num_samples = 2
    stack_size = 2
    input_num_row = 11
    input_num_col = 12

    for data_format in ['channels_first', 'channels_last']:
      if data_format == 'channels_first':
        inputs = np.random.rand(num_samples, stack_size, input_num_row,
                                input_num_col)
      else:
        inputs = np.random.rand(num_samples, input_num_row, input_num_col,
                                stack_size)

      # basic test
      with self.cached_session():
        testing_utils.layer_test(
            keras.layers.UpSampling2D,
            kwargs={'size': (2, 2),
                    'data_format': data_format},
            input_shape=inputs.shape)

        for length_row in [2]:
          for length_col in [2, 3]:
            layer = keras.layers.UpSampling2D(
                size=(length_row, length_col), data_format=data_format)
            layer.build(inputs.shape)
            output = layer(keras.backend.variable(inputs))
            if tf.executing_eagerly():
              np_output = output.numpy()
            else:
              np_output = keras.backend.eval(output)
            if data_format == 'channels_first':
              assert np_output.shape[2] == length_row * input_num_row
              assert np_output.shape[3] == length_col * input_num_col
            else:  # tf
              assert np_output.shape[1] == length_row * input_num_row
              assert np_output.shape[2] == length_col * input_num_col

            # compare with numpy
            if data_format == 'channels_first':
              expected_out = np.repeat(inputs, length_row, axis=2)
              expected_out = np.repeat(expected_out, length_col, axis=3)
            else:  # tf
              expected_out = np.repeat(inputs, length_row, axis=1)
              expected_out = np.repeat(expected_out, length_col, axis=2)

            np.testing.assert_allclose(np_output, expected_out)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3681')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_test.py: 938-994
</a>
<div class="mid" id="frag3681" style="display:none"><pre>
  def test_upsampling_3d(self):
    num_samples = 2
    stack_size = 2
    input_len_dim1 = 10
    input_len_dim2 = 11
    input_len_dim3 = 12

    for data_format in ['channels_first', 'channels_last']:
      if data_format == 'channels_first':
        inputs = np.random.rand(num_samples, stack_size, input_len_dim1,
                                input_len_dim2, input_len_dim3)
      else:
        inputs = np.random.rand(num_samples, input_len_dim1, input_len_dim2,
                                input_len_dim3, stack_size)

      # basic test
      with self.cached_session():
        testing_utils.layer_test(
            keras.layers.UpSampling3D,
            kwargs={'size': (2, 2, 2),
                    'data_format': data_format},
            input_shape=inputs.shape)

        for length_dim1 in [2, 3]:
          for length_dim2 in [2]:
            for length_dim3 in [3]:
              layer = keras.layers.UpSampling3D(
                  size=(length_dim1, length_dim2, length_dim3),
                  data_format=data_format)
              layer.build(inputs.shape)
              output = layer(keras.backend.variable(inputs))
              if tf.executing_eagerly():
                np_output = output.numpy()
              else:
                np_output = keras.backend.eval(output)
              if data_format == 'channels_first':
                assert np_output.shape[2] == length_dim1 * input_len_dim1
                assert np_output.shape[3] == length_dim2 * input_len_dim2
                assert np_output.shape[4] == length_dim3 * input_len_dim3
              else:  # tf
                assert np_output.shape[1] == length_dim1 * input_len_dim1
                assert np_output.shape[2] == length_dim2 * input_len_dim2
                assert np_output.shape[3] == length_dim3 * input_len_dim3

              # compare with numpy
              if data_format == 'channels_first':
                expected_out = np.repeat(inputs, length_dim1, axis=2)
                expected_out = np.repeat(expected_out, length_dim2, axis=3)
                expected_out = np.repeat(expected_out, length_dim3, axis=4)
              else:  # tf
                expected_out = np.repeat(inputs, length_dim1, axis=1)
                expected_out = np.repeat(expected_out, length_dim2, axis=2)
                expected_out = np.repeat(expected_out, length_dim3, axis=3)

              np.testing.assert_allclose(np_output, expected_out)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 202:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3729')" href="javascript:;">
keras-2.6.0/keras/layers/dense_attention_test.py: 185-206
</a>
<div class="mid" id="frag3729" style="display:none"><pre>
  def test_calculate_scores_multi_dim(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Key tensor of shape [1, 3, 4]
    k = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    attention_layer = dense_attention.Attention()
    attention_layer.build(input_shape=([1, 2, 4], [1, 3, 4]))
    actual = attention_layer._calculate_scores(query=q, key=k)

    # Expected tensor of shape [1, 2, 3].
    # expected000 = 1.*1.5+1.1*1.6+1.2*1.7+1.3*1.8 = 7.64
    # expected001 = 1.*2.5+1.1*2.6+1.2*2.7+1.3*2.8 = 12.24
    # expected002 = 1.*3.5+1.1*3.6+1.2*3.7+1.3*3.8 = 16.84
    # expected010 = 2.*1.5+2.1*1.6+2.2*1.7+2.3*1.8 = 14.24
    # expected011 = 2.*2.5+2.1*2.6+2.2*2.7+2.3*2.8 = 22.84
    # expected012 = 2.*3.5+2.1*3.6+2.2*3.7+2.3*3.8 = 31.44
    expected = np.array([[[7.64, 12.24, 16.84], [14.24, 22.84, 31.44]]],
                        dtype=np.float32)
    self.assertAllClose(expected, actual)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3751')" href="javascript:;">
keras-2.6.0/keras/layers/dense_attention_test.py: 536-561
</a>
<div class="mid" id="frag3751" style="display:none"><pre>
  def test_calculate_scores_multi_dim(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Key tensor of shape [1, 3, 4]
    k = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    attention_layer = dense_attention.AdditiveAttention()
    attention_layer.build(input_shape=([1, 2, 4], [1, 3, 4]))
    # Scale tensor of shape [4]
    attention_layer.scale = np.array([[[0.5, 0.6, 0.7, 0.8]]], dtype=np.float32)
    actual = attention_layer._calculate_scores(query=q, key=k)

    # pylint:disable=line-too-long
    # expected000 = 0.5*tanh(1.+1.5) + 0.6*tanh(1.1+1.6) + 0.7*tanh(1.2+1.7) + 0.8*tanh(1.3+1.8) = 2.58044532581
    # expected001 = 0.5*tanh(1.+2.5) + 0.6*tanh(1.1+2.6) + 0.7*tanh(1.2+2.7) + 0.8*tanh(1.3+2.8) = 2.59734317449
    # expected002 = 0.5*tanh(1.+3.5) + 0.6*tanh(1.1+3.6) + 0.7*tanh(1.2+3.7) + 0.8*tanh(1.3+3.8) = 2.59964024652
    # expected010 = 0.5*tanh(2.+1.5) + 0.6*tanh(2.1+1.6) + 0.7*tanh(2.2+1.7) + 0.8*tanh(2.3+1.8) = 2.59734317449
    # expected011 = 0.5*tanh(2.+2.5) + 0.6*tanh(2.1+2.6) + 0.7*tanh(2.2+2.7) + 0.8*tanh(2.3+2.8) = 2.59964024652
    # expected012 = 0.5*tanh(2.+3.5) + 0.6*tanh(2.1+3.6) + 0.7*tanh(2.2+3.7) + 0.8*tanh(2.3+3.8) = 2.59995130916
    # pylint:enable=line-too-long
    expected = np.array([[[2.58044532581, 2.59734317449, 2.59964024652],
                          [2.59734317449, 2.59964024652, 2.59995130916]]],
                        dtype=np.float32)
    self.assertAllClose(expected, actual)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 203:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3732')" href="javascript:;">
keras-2.6.0/keras/layers/dense_attention_test.py: 238-252
</a>
<div class="mid" id="frag3732" style="display:none"><pre>
  def test_shape(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 4]
    v = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.Attention()
    actual = attention_layer([q, v], mask=[None, v_mask])

    expected_shape = [1, 2, 4]
    self.assertAllEqual(expected_shape, tf.shape(actual))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3754')" href="javascript:;">
keras-2.6.0/keras/layers/dense_attention_test.py: 595-609
</a>
<div class="mid" id="frag3754" style="display:none"><pre>
  def test_shape_no_scale(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 4]
    v = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.AdditiveAttention(use_scale=False)
    actual = attention_layer([q, v], mask=[None, v_mask])

    expected_shape = [1, 2, 4]
    self.assertAllEqual(expected_shape, tf.shape(actual))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3753')" href="javascript:;">
keras-2.6.0/keras/layers/dense_attention_test.py: 580-594
</a>
<div class="mid" id="frag3753" style="display:none"><pre>
  def test_shape(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 4]
    v = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.AdditiveAttention()
    actual = attention_layer([q, v], mask=[None, v_mask])

    expected_shape = [1, 2, 4]
    self.assertAllEqual(expected_shape, tf.shape(actual))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 204:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3733')" href="javascript:;">
keras-2.6.0/keras/layers/dense_attention_test.py: 253-271
</a>
<div class="mid" id="frag3733" style="display:none"><pre>
  def test_shape_with_key(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 4]
    v = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Key tensor of shape [1, 3, 4]
    k = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.Attention()
    actual = attention_layer([q, v, k], mask=[None, v_mask])

    expected_shape = [1, 2, 4]
    self.assertAllEqual(expected_shape, tf.shape(actual))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3755')" href="javascript:;">
keras-2.6.0/keras/layers/dense_attention_test.py: 610-628
</a>
<div class="mid" id="frag3755" style="display:none"><pre>
  def test_shape_with_key(self):
    # Query tensor of shape [1, 2, 4]
    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 4]
    v = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Key tensor of shape [1, 3, 4]
    k = np.array(
        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],
        dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.AdditiveAttention()
    actual = attention_layer([q, v, k], mask=[None, v_mask])

    expected_shape = [1, 2, 4]
    self.assertAllEqual(expected_shape, tf.shape(actual))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 205:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3756')" href="javascript:;">
keras-2.6.0/keras/layers/dense_attention_test.py: 629-662
</a>
<div class="mid" id="frag3756" style="display:none"><pre>
  def test_multi_dim(self):
    # Query tensor of shape [1, 1, 1]
    q = np.array([[[1.1]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 1]
    v = np.array([[[1.6], [0.7], [-0.8]]], dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.AdditiveAttention()
    attention_layer.build(input_shape=([1, 1, 1], [1, 3, 1]))
    # Scale tensor of shape [1]
    attention_layer.scale = np.array([[[0.5]]], dtype=np.float32)
    actual = attention_layer([q, v], mask=[None, v_mask])

    # pylint:disable=line-too-long
    # Expected scores of shape [1, 1, 3]
    # scores = [[[0.5 * tanh(1.1 + 1.6), 0.5 * tanh(1.1 + 0.7), 0.5 * tanh(1.1 - 0.8)]]]
    #        = [[[0.49550372683, 0.47340300642, 0.14565630622]]]
    # Expected attention distribution = softmax(scores) with zeros in
    # positions where v_mask == False.
    # =&gt; attention_distribution000
    #      = exp(0.49550372683)/(exp(0.49550372683) + exp(0.47340300642))
    #      = 0.50552495521
    #    attention_distribution001
    #      = exp(0.47340300642)/(exp(0.49550372683) + exp(0.47340300642))
    #      = 0.49447504478
    #    attention_distribution002 = 0
    #
    # Expected tensor of shape [1, 1, 1].
    # expected000 = 0.50552495521 * 1.6 + 0.49447504478 * 0.7 - 0 * 0.8
    #             = 1.15497245968
    # pylint:enable=line-too-long
    expected = np.array([[[1.15497245968]]], dtype=np.float32)
    self.assertAllClose(expected, actual)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3757')" href="javascript:;">
keras-2.6.0/keras/layers/dense_attention_test.py: 663-698
</a>
<div class="mid" id="frag3757" style="display:none"><pre>
  def test_multi_dim_with_key(self):
    # Query tensor of shape [1, 1, 1]
    q = np.array([[[1.1]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 1]
    v = np.array([[[0.5], [0.8], [-0.3]]], dtype=np.float32)
    # Key tensor of shape [1, 3, 1]
    k = np.array([[[1.6], [0.7], [-0.8]]], dtype=np.float32)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.AdditiveAttention()
    attention_layer.build(input_shape=([1, 1, 1], [1, 3, 1]))
    # Scale tensor of shape [1]
    attention_layer.scale = np.array([[[0.5]]], dtype=np.float32)
    actual = attention_layer([q, v, k], mask=[None, v_mask])

    # pylint:disable=line-too-long
    # Expected scores of shape [1, 1, 3]
    # scores = [[[0.5 * tanh(1.1 + 1.6), 0.5 * tanh(1.1 + 0.7), 0.5 * tanh(1.1 - 0.8)]]]
    #        = [[[0.49550372683, 0.47340300642, 0.14565630622]]]
    # Expected attention distribution = softmax(scores) with zeros in
    # positions where v_mask == False.
    # =&gt; attention_distribution000
    #        = exp(0.49550372683)/(exp(0.49550372683) + exp(0.47340300642))
    #        = 0.50552495521
    #    attention_distribution001
    #        = exp(0.47340300642)/(exp(0.49550372683) + exp(0.47340300642))
    #        = 0.49447504478
    #    attention_distribution002 = 0
    #
    # Expected tensor of shape [1, 1, 1].
    # expected000 = 0.50552495521 * 0.5 + 0.49447504478 * 0.8 - 0 * 0.3
    #             = 0.64834251342
    # pylint:enable=line-too-long
    expected = np.array([[[0.64834251342]]], dtype=np.float32)
    self.assertAllClose(expected, actual)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3758')" href="javascript:;">
keras-2.6.0/keras/layers/dense_attention_test.py: 699-744
</a>
<div class="mid" id="frag3758" style="display:none"><pre>
  def test_multi_dim_with_query_mask(self):
    # Query tensor of shape [1, 2, 1]
    q = np.array([[[1.1], [-0.5]]], dtype=np.float32)
    # Value tensor of shape [1, 3, 1]
    v = np.array([[[1.6], [0.7], [-0.8]]], dtype=np.float32)
    # Query mask tensor of shape [1, 2]
    q_mask = np.array([[True, False]], dtype=np.bool_)
    # Value mask tensor of shape [1, 3]
    v_mask = np.array([[True, True, False]], dtype=np.bool_)
    attention_layer = dense_attention.AdditiveAttention()
    attention_layer.build(input_shape=([1, 1, 1], [1, 3, 1]))
    # Scale tensor of shape [1]
    attention_layer.scale = np.array([[[0.5]]], dtype=np.float32)
    actual = attention_layer([q, v], mask=[q_mask, v_mask])

    # pylint:disable=line-too-long
    # Expected scores of shape [1, 2, 3]
    # scores = [[[0.5 * tanh(1.1 + 1.6), 0.5 * tanh(1.1 + 0.7), 0.5 * tanh(1.1 - 0.8)],
    #            [0.5 * tanh(-0.5 + 1.6), 0.5 * tanh(-0.5 + 0.7), 0.5 * tanh(-0.5 - 0.8)]]]
    #        = [[[0.49550372683, 0.47340300642, 0.14565630622],
    #            [0.40024951088, 0.09868766011, -0.43086157965]]]
    # Expected attention distribution = softmax(scores) with zeros in
    # positions where v_mask == False.
    # =&gt; attention_distribution000
    #        = exp(0.49550372683)/(exp(0.49550372683) + exp(0.47340300642))
    #        = 0.50552495521
    #    attention_distribution001
    #        = exp(0.47340300642)/(exp(0.49550372683) + exp(0.47340300642))
    #        = 0.49447504478
    #    attention_distribution002 = 0
    # =&gt; attention_distribution010
    #        = exp(0.40024951088)/(exp(0.40024951088) + exp(0.09868766011))
    #        = 0.57482427975
    #    attention_distribution011
    #        = exp(0.09868766011)/(exp(0.40024951088) + exp(0.09868766011))
    #        = 0.42517572025
    #    attention_distribution012 = 0
    #
    # Expected tensor of shape [1, 2, 1] with zeros where  q_mask == False.
    # expected000 = 0.50552495521 * 1.6 + 0.49447504478 * 0.7 - 0 * 0.8
    #             = 1.15497245968
    # expected000 = 0
    # pylint:enable=line-too-long
    expected = np.array([[[1.15497245968], [0.]]], dtype=np.float32)
    self.assertAllClose(expected, actual)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 206:</b> &nbsp; 2 fragments, nominal size 37 lines, similarity 97%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3772')" href="javascript:;">
keras-2.6.0/keras/layers/local.py: 116-154
</a>
<div class="mid" id="frag3772" style="display:none"><pre>
  def __init__(self,
               filters,
               kernel_size,
               strides=1,
               padding='valid',
               data_format=None,
               activation=None,
               use_bias=True,
               kernel_initializer='glorot_uniform',
               bias_initializer='zeros',
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               implementation=1,
               **kwargs):
    super(LocallyConnected1D, self).__init__(**kwargs)
    self.filters = filters
    self.kernel_size = conv_utils.normalize_tuple(kernel_size, 1, 'kernel_size')
    self.strides = conv_utils.normalize_tuple(strides, 1, 'strides')
    self.padding = conv_utils.normalize_padding(padding)
    if self.padding != 'valid' and implementation == 1:
      raise ValueError('Invalid border mode for LocallyConnected1D '
                       '(only "valid" is supported if implementation is 1): ' +
                       padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.activation = activations.get(activation)
    self.use_bias = use_bias
    self.kernel_initializer = initializers.get(kernel_initializer)
    self.bias_initializer = initializers.get(bias_initializer)
    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.kernel_constraint = constraints.get(kernel_constraint)
    self.bias_constraint = constraints.get(bias_constraint)
    self.implementation = implementation
    self.input_spec = InputSpec(ndim=3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3778')" href="javascript:;">
keras-2.6.0/keras/layers/local.py: 423-461
</a>
<div class="mid" id="frag3778" style="display:none"><pre>
  def __init__(self,
               filters,
               kernel_size,
               strides=(1, 1),
               padding='valid',
               data_format=None,
               activation=None,
               use_bias=True,
               kernel_initializer='glorot_uniform',
               bias_initializer='zeros',
               kernel_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               bias_constraint=None,
               implementation=1,
               **kwargs):
    super(LocallyConnected2D, self).__init__(**kwargs)
    self.filters = filters
    self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, 'kernel_size')
    self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')
    self.padding = conv_utils.normalize_padding(padding)
    if self.padding != 'valid' and implementation == 1:
      raise ValueError('Invalid border mode for LocallyConnected2D '
                       '(only "valid" is supported if implementation is 1): ' +
                       padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.activation = activations.get(activation)
    self.use_bias = use_bias
    self.kernel_initializer = initializers.get(kernel_initializer)
    self.bias_initializer = initializers.get(bias_initializer)
    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.kernel_constraint = constraints.get(kernel_constraint)
    self.bias_constraint = constraints.get(bias_constraint)
    self.implementation = implementation
    self.input_spec = InputSpec(ndim=4)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 207:</b> &nbsp; 2 fragments, nominal size 78 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3774')" href="javascript:;">
keras-2.6.0/keras/layers/local.py: 160-249
</a>
<div class="mid" id="frag3774" style="display:none"><pre>
  def build(self, input_shape):
    if self.data_format == 'channels_first':
      input_dim, input_length = input_shape[1], input_shape[2]
    else:
      input_dim, input_length = input_shape[2], input_shape[1]

    if input_dim is None:
      raise ValueError(
          'Axis 2 of input should be fully-defined. '
          'Found shape:', input_shape)
    self.output_length = conv_utils.conv_output_length(input_length,
                                                       self.kernel_size[0],
                                                       self.padding,
                                                       self.strides[0])

    if self.implementation == 1:
      self.kernel_shape = (self.output_length, self.kernel_size[0] * input_dim,
                           self.filters)

      self.kernel = self.add_weight(
          shape=self.kernel_shape,
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

    elif self.implementation == 2:
      if self.data_format == 'channels_first':
        self.kernel_shape = (input_dim, input_length, self.filters,
                             self.output_length)
      else:
        self.kernel_shape = (input_length, input_dim, self.output_length,
                             self.filters)

      self.kernel = self.add_weight(
          shape=self.kernel_shape,
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

      self.kernel_mask = get_locallyconnected_mask(
          input_shape=(input_length,),
          kernel_shape=self.kernel_size,
          strides=self.strides,
          padding=self.padding,
          data_format=self.data_format,
      )

    elif self.implementation == 3:
      self.kernel_shape = (self.output_length * self.filters,
                           input_length * input_dim)

      self.kernel_idxs = sorted(
          conv_utils.conv_kernel_idxs(
              input_shape=(input_length,),
              kernel_shape=self.kernel_size,
              strides=self.strides,
              padding=self.padding,
              filters_in=input_dim,
              filters_out=self.filters,
              data_format=self.data_format))

      self.kernel = self.add_weight(
          shape=(len(self.kernel_idxs),),
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

    else:
      raise ValueError('Unrecognized implementation mode: %d.' %
                       self.implementation)

    if self.use_bias:
      self.bias = self.add_weight(
          shape=(self.output_length, self.filters),
          initializer=self.bias_initializer,
          name='bias',
          regularizer=self.bias_regularizer,
          constraint=self.bias_constraint)
    else:
      self.bias = None

    if self.data_format == 'channels_first':
      self.input_spec = InputSpec(ndim=3, axes={1: input_dim})
    else:
      self.input_spec = InputSpec(ndim=3, axes={-1: input_dim})
    self.built = True

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3780')" href="javascript:;">
keras-2.6.0/keras/layers/local.py: 467-559
</a>
<div class="mid" id="frag3780" style="display:none"><pre>
  def build(self, input_shape):
    if self.data_format == 'channels_last':
      input_row, input_col = input_shape[1:-1]
      input_filter = input_shape[3]
    else:
      input_row, input_col = input_shape[2:]
      input_filter = input_shape[1]
    if input_row is None or input_col is None:
      raise ValueError('The spatial dimensions of the inputs to '
                       ' a LocallyConnected2D layer '
                       'should be fully-defined, but layer received '
                       'the inputs shape ' + str(input_shape))
    output_row = conv_utils.conv_output_length(input_row, self.kernel_size[0],
                                               self.padding, self.strides[0])
    output_col = conv_utils.conv_output_length(input_col, self.kernel_size[1],
                                               self.padding, self.strides[1])
    self.output_row = output_row
    self.output_col = output_col

    if self.implementation == 1:
      self.kernel_shape = (output_row * output_col, self.kernel_size[0] *
                           self.kernel_size[1] * input_filter, self.filters)

      self.kernel = self.add_weight(
          shape=self.kernel_shape,
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

    elif self.implementation == 2:
      if self.data_format == 'channels_first':
        self.kernel_shape = (input_filter, input_row, input_col, self.filters,
                             self.output_row, self.output_col)
      else:
        self.kernel_shape = (input_row, input_col, input_filter,
                             self.output_row, self.output_col, self.filters)

      self.kernel = self.add_weight(
          shape=self.kernel_shape,
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

      self.kernel_mask = get_locallyconnected_mask(
          input_shape=(input_row, input_col),
          kernel_shape=self.kernel_size,
          strides=self.strides,
          padding=self.padding,
          data_format=self.data_format,
      )

    elif self.implementation == 3:
      self.kernel_shape = (self.output_row * self.output_col * self.filters,
                           input_row * input_col * input_filter)

      self.kernel_idxs = sorted(
          conv_utils.conv_kernel_idxs(
              input_shape=(input_row, input_col),
              kernel_shape=self.kernel_size,
              strides=self.strides,
              padding=self.padding,
              filters_in=input_filter,
              filters_out=self.filters,
              data_format=self.data_format))

      self.kernel = self.add_weight(
          shape=(len(self.kernel_idxs),),
          initializer=self.kernel_initializer,
          name='kernel',
          regularizer=self.kernel_regularizer,
          constraint=self.kernel_constraint)

    else:
      raise ValueError('Unrecognized implementation mode: %d.' %
                       self.implementation)

    if self.use_bias:
      self.bias = self.add_weight(
          shape=(output_row, output_col, self.filters),
          initializer=self.bias_initializer,
          name='bias',
          regularizer=self.bias_regularizer,
          constraint=self.bias_constraint)
    else:
      self.bias = None
    if self.data_format == 'channels_first':
      self.input_spec = InputSpec(ndim=4, axes={1: input_filter})
    else:
      self.input_spec = InputSpec(ndim=4, axes={-1: input_filter})
    self.built = True

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 208:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3776')" href="javascript:;">
keras-2.6.0/keras/layers/local.py: 265-289
</a>
<div class="mid" id="frag3776" style="display:none"><pre>
  def call(self, inputs):
    if self.implementation == 1:
      output = backend.local_conv(
          inputs, self.kernel, self.kernel_size, self.strides,
          (self.output_length,), self.data_format)

    elif self.implementation == 2:
      output = local_conv_matmul(inputs, self.kernel, self.kernel_mask,
                                 self.compute_output_shape(inputs.shape))

    elif self.implementation == 3:
      output = local_conv_sparse_matmul(inputs, self.kernel, self.kernel_idxs,
                                        self.kernel_shape,
                                        self.compute_output_shape(inputs.shape))

    else:
      raise ValueError('Unrecognized implementation mode: %d.' %
                       self.implementation)

    if self.use_bias:
      output = backend.bias_add(output, self.bias, data_format=self.data_format)

    output = self.activation(output)
    return output

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3782')" href="javascript:;">
keras-2.6.0/keras/layers/local.py: 579-604
</a>
<div class="mid" id="frag3782" style="display:none"><pre>
  def call(self, inputs):
    if self.implementation == 1:
      output = backend.local_conv(
          inputs, self.kernel, self.kernel_size, self.strides,
          (self.output_row, self.output_col),
          self.data_format)

    elif self.implementation == 2:
      output = local_conv_matmul(inputs, self.kernel, self.kernel_mask,
                                 self.compute_output_shape(inputs.shape))

    elif self.implementation == 3:
      output = local_conv_sparse_matmul(inputs, self.kernel, self.kernel_idxs,
                                        self.kernel_shape,
                                        self.compute_output_shape(inputs.shape))

    else:
      raise ValueError('Unrecognized implementation mode: %d.' %
                       self.implementation)

    if self.use_bias:
      output = backend.bias_add(output, self.bias, data_format=self.data_format)

    output = self.activation(output)
    return output

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 209:</b> &nbsp; 8 fragments, nominal size 36 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3777')" href="javascript:;">
keras-2.6.0/keras/layers/local.py: 290-326
</a>
<div class="mid" id="frag3777" style="display:none"><pre>
  def get_config(self):
    config = {
        'filters':
            self.filters,
        'kernel_size':
            self.kernel_size,
        'strides':
            self.strides,
        'padding':
            self.padding,
        'data_format':
            self.data_format,
        'activation':
            activations.serialize(self.activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'implementation':
            self.implementation
    }
    base_config = super(LocallyConnected1D, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4159')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent.py: 2474-2514
</a>
<div class="mid" id="frag4159" style="display:none"><pre>
  def get_config(self):
    config = {
        'units':
            self.units,
        'activation':
            activations.serialize(self.activation),
        'recurrent_activation':
            activations.serialize(self.recurrent_activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'unit_forget_bias':
            self.unit_forget_bias,
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'dropout':
            self.dropout,
        'recurrent_dropout':
            self.recurrent_dropout,
        'implementation':
            self.implementation
    }
    config.update(_config_for_enable_caching_device(self))
    base_config = super(LSTMCell, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4108')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent.py: 1392-1427
</a>
<div class="mid" id="frag4108" style="display:none"><pre>
  def get_config(self):
    config = {
        'units':
            self.units,
        'activation':
            activations.serialize(self.activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'dropout':
            self.dropout,
        'recurrent_dropout':
            self.recurrent_dropout
    }
    config.update(_config_for_enable_caching_device(self))
    base_config = super(SimpleRNNCell, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3804')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_recurrent.py: 645-693
</a>
<div class="mid" id="frag3804" style="display:none"><pre>
  def get_config(self):
    config = {
        'filters':
            self.filters,
        'kernel_size':
            self.kernel_size,
        'strides':
            self.strides,
        'padding':
            self.padding,
        'data_format':
            self.data_format,
        'dilation_rate':
            self.dilation_rate,
        'activation':
            activations.serialize(self.activation),
        'recurrent_activation':
            activations.serialize(self.recurrent_activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'unit_forget_bias':
            self.unit_forget_bias,
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'dropout':
            self.dropout,
        'recurrent_dropout':
            self.recurrent_dropout,
    }
    base_config = super(ConvLSTMCell, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4151')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent.py: 2191-2234
</a>
<div class="mid" id="frag4151" style="display:none"><pre>
  def get_config(self):
    config = {
        'units':
            self.units,
        'activation':
            activations.serialize(self.activation),
        'recurrent_activation':
            activations.serialize(self.recurrent_activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'dropout':
            self.dropout,
        'recurrent_dropout':
            self.recurrent_dropout,
        'implementation':
            self.implementation,
        'reset_after':
            self.reset_after
    }
    config.update(_config_for_enable_caching_device(self.cell))
    base_config = super(GRU, self).get_config()
    del base_config['cell']
    return dict(list(base_config.items()) + list(config.items()))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4184')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent.py: 2864-2907
</a>
<div class="mid" id="frag4184" style="display:none"><pre>
  def get_config(self):
    config = {
        'units':
            self.units,
        'activation':
            activations.serialize(self.activation),
        'recurrent_activation':
            activations.serialize(self.recurrent_activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'unit_forget_bias':
            self.unit_forget_bias,
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'dropout':
            self.dropout,
        'recurrent_dropout':
            self.recurrent_dropout,
        'implementation':
            self.implementation
    }
    config.update(_config_for_enable_caching_device(self.cell))
    base_config = super(LSTM, self).get_config()
    del base_config['cell']
    return dict(list(base_config.items()) + list(config.items()))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3783')" href="javascript:;">
keras-2.6.0/keras/layers/local.py: 605-641
</a>
<div class="mid" id="frag3783" style="display:none"><pre>
  def get_config(self):
    config = {
        'filters':
            self.filters,
        'kernel_size':
            self.kernel_size,
        'strides':
            self.strides,
        'padding':
            self.padding,
        'data_format':
            self.data_format,
        'activation':
            activations.serialize(self.activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'implementation':
            self.implementation
    }
    base_config = super(LocallyConnected2D, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4125')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent.py: 1637-1674
</a>
<div class="mid" id="frag4125" style="display:none"><pre>
  def get_config(self):
    config = {
        'units':
            self.units,
        'activation':
            activations.serialize(self.activation),
        'use_bias':
            self.use_bias,
        'kernel_initializer':
            initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer':
            initializers.serialize(self.bias_initializer),
        'kernel_regularizer':
            regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer':
            regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint':
            constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint':
            constraints.serialize(self.bias_constraint),
        'dropout':
            self.dropout,
        'recurrent_dropout':
            self.recurrent_dropout
    }
    base_config = super(SimpleRNN, self).get_config()
    config.update(_config_for_enable_caching_device(self.cell))
    del base_config['cell']
    return dict(list(base_config.items()) + list(config.items()))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 210:</b> &nbsp; 4 fragments, nominal size 18 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3865')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 121-139
</a>
<div class="mid" id="frag3865" style="display:none"><pre>
  def test_constraints_LSTM(self):
    embedding_dim = 4
    layer_class = keras.layers.LSTM
    k_constraint = keras.constraints.max_norm(0.01)
    r_constraint = keras.constraints.max_norm(0.01)
    b_constraint = keras.constraints.max_norm(0.01)
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_constraint=k_constraint,
        recurrent_constraint=r_constraint,
        bias_constraint=b_constraint)
    layer.build((None, None, embedding_dim))
    self.assertEqual(layer.cell.kernel.constraint, k_constraint)
    self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)
    self.assertEqual(layer.cell.bias.constraint, b_constraint)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4214')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 396-414
</a>
<div class="mid" id="frag4214" style="display:none"><pre>
  def test_constraints_GRU(self):
    embedding_dim = 4
    layer_class = rnn.GRU
    k_constraint = keras.constraints.max_norm(0.01)
    r_constraint = keras.constraints.max_norm(0.01)
    b_constraint = keras.constraints.max_norm(0.01)
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_constraint=k_constraint,
        recurrent_constraint=r_constraint,
        bias_constraint=b_constraint)
    layer.build((None, None, embedding_dim))
    self.assertEqual(layer.cell.kernel.constraint, k_constraint)
    self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)
    self.assertEqual(layer.cell.bias.constraint, b_constraint)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4483')" href="javascript:;">
keras-2.6.0/keras/layers/gru_test.py: 229-247
</a>
<div class="mid" id="frag4483" style="display:none"><pre>
  def test_constraints_GRU(self):
    embedding_dim = 4
    layer_class = keras.layers.GRU
    k_constraint = keras.constraints.max_norm(0.01)
    r_constraint = keras.constraints.max_norm(0.01)
    b_constraint = keras.constraints.max_norm(0.01)
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_constraint=k_constraint,
        recurrent_constraint=r_constraint,
        bias_constraint=b_constraint)
    layer.build((None, None, embedding_dim))
    self.assertEqual(layer.cell.kernel.constraint, k_constraint)
    self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)
    self.assertEqual(layer.cell.bias.constraint, b_constraint)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3884')" href="javascript:;">
keras-2.6.0/keras/layers/simplernn_test.py: 94-112
</a>
<div class="mid" id="frag3884" style="display:none"><pre>
  def test_constraints_SimpleRNN(self):
    embedding_dim = 4
    layer_class = keras.layers.SimpleRNN
    k_constraint = keras.constraints.max_norm(0.01)
    r_constraint = keras.constraints.max_norm(0.01)
    b_constraint = keras.constraints.max_norm(0.01)
    layer = layer_class(
        5,
        return_sequences=False,
        weights=None,
        input_shape=(None, embedding_dim),
        kernel_constraint=k_constraint,
        recurrent_constraint=r_constraint,
        bias_constraint=b_constraint)
    layer.build((None, None, embedding_dim))
    self.assertEqual(layer.cell.kernel.constraint, k_constraint)
    self.assertEqual(layer.cell.recurrent_kernel.constraint, r_constraint)
    self.assertEqual(layer.cell.bias.constraint, b_constraint)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 211:</b> &nbsp; 3 fragments, nominal size 13 lines, similarity 78%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3866')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 144-157
</a>
<div class="mid" id="frag3866" style="display:none"><pre>
  def test_with_masking_layer_LSTM(self, unroll):
    layer_class = keras.layers.LSTM
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(keras.layers.Masking(input_shape=(3, 4)))
    model.add(layer_class(units=5, return_sequences=True, unroll=unroll))
    model.compile(
        loss='categorical_crossentropy',
        optimizer='rmsprop',
        run_eagerly=testing_utils.should_run_eagerly())
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3867')" href="javascript:;">
keras-2.6.0/keras/layers/lstm_test.py: 159-173
</a>
<div class="mid" id="frag3867" style="display:none"><pre>
  def test_masking_with_stacking_LSTM(self, unroll):
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(keras.layers.Masking(input_shape=(3, 4)))
    lstm_cells = [keras.layers.LSTMCell(10), keras.layers.LSTMCell(5)]
    model.add(keras.layers.RNN(
        lstm_cells, return_sequences=True, unroll=unroll))
    model.compile(
        loss='categorical_crossentropy',
        optimizer='rmsprop',
        run_eagerly=testing_utils.should_run_eagerly())
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4480')" href="javascript:;">
keras-2.6.0/keras/layers/gru_test.py: 135-148
</a>
<div class="mid" id="frag4480" style="display:none"><pre>
  def test_with_masking_layer_GRU(self):
    layer_class = keras.layers.GRU
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(keras.layers.Masking(input_shape=(3, 4)))
    model.add(layer_class(units=5, return_sequences=True, unroll=False))
    model.compile(
        loss='categorical_crossentropy',
        optimizer='rmsprop',
        run_eagerly=testing_utils.should_run_eagerly())
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 212:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3885')" href="javascript:;">
keras-2.6.0/keras/layers/simplernn_test.py: 113-123
</a>
<div class="mid" id="frag3885" style="display:none"><pre>
  def test_with_masking_layer_SimpleRNN(self):
    layer_class = keras.layers.SimpleRNN
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(keras.layers.Masking(input_shape=(3, 4)))
    model.add(layer_class(units=5, return_sequences=True, unroll=False))
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4208')" href="javascript:;">
keras-2.6.0/keras/layers/gru_v2_test.py: 310-321
</a>
<div class="mid" id="frag4208" style="display:none"><pre>
  def test_with_masking_layer_GRU(self):
    layer_class = rnn.GRU
    inputs = np.random.random((2, 3, 4))
    targets = np.abs(np.random.random((2, 3, 5)))
    targets /= targets.sum(axis=-1, keepdims=True)
    model = keras.models.Sequential()
    model.add(keras.layers.Masking(input_shape=(3, 4)))
    model.add(layer_class(units=5, return_sequences=True, unroll=False))
    model.compile(loss='categorical_crossentropy',
                  optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.001))
    model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 213:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3948')" href="javascript:;">
keras-2.6.0/keras/layers/pooling_test.py: 64-82
</a>
<div class="mid" id="frag3948" style="display:none"><pre>
  def test_globalpooling_1d_with_ragged(self):
    ragged_data = tf.ragged.constant(
        [[[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]], [[1.0, 1.0], [2.0, 2.0]]],
        ragged_rank=1)
    dense_data = ragged_data.to_tensor()

    inputs = keras.Input(shape=(None, 2), dtype='float32', ragged=True)
    out = keras.layers.GlobalAveragePooling1D()(inputs)
    model = keras.models.Model(inputs=inputs, outputs=out)
    output_ragged = model.predict(ragged_data, steps=1)

    inputs = keras.Input(shape=(None, 2), dtype='float32')
    masking = keras.layers.Masking(mask_value=0., input_shape=(3, 2))(inputs)
    out = keras.layers.GlobalAveragePooling1D()(masking)
    model = keras.models.Model(inputs=inputs, outputs=out)
    output_dense = model.predict(dense_data, steps=1)

    self.assertAllEqual(output_ragged, output_dense)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3949')" href="javascript:;">
keras-2.6.0/keras/layers/pooling_test.py: 83-101
</a>
<div class="mid" id="frag3949" style="display:none"><pre>
  def test_globalpooling_2d_with_ragged(self):
    ragged_data = tf.ragged.constant(
        [[[[1.0], [1.0]], [[2.0], [2.0]], [[3.0], [3.0]]],
         [[[1.0], [1.0]], [[2.0], [2.0]]]],
        ragged_rank=1)
    dense_data = ragged_data.to_tensor()

    inputs = keras.Input(shape=(None, 2, 1), dtype='float32', ragged=True)
    out = keras.layers.GlobalMaxPooling2D()(inputs)
    model = keras.models.Model(inputs=inputs, outputs=out)
    output_ragged = model.predict(ragged_data, steps=1)

    inputs = keras.Input(shape=(None, 2, 1), dtype='float32')
    out = keras.layers.GlobalMaxPooling2D()(inputs)
    model = keras.models.Model(inputs=inputs, outputs=out)
    output_dense = model.predict(dense_data, steps=1)

    self.assertAllEqual(output_ragged, output_dense)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 214:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3951')" href="javascript:;">
keras-2.6.0/keras/layers/pooling_test.py: 117-134
</a>
<div class="mid" id="frag3951" style="display:none"><pre>
  def test_globalpooling_2d(self):
    testing_utils.layer_test(
        keras.layers.pooling.GlobalMaxPooling2D,
        kwargs={'data_format': 'channels_first'},
        input_shape=(3, 4, 5, 6))
    testing_utils.layer_test(
        keras.layers.pooling.GlobalMaxPooling2D,
        kwargs={'data_format': 'channels_last'},
        input_shape=(3, 5, 6, 4))
    testing_utils.layer_test(
        keras.layers.pooling.GlobalAveragePooling2D,
        kwargs={'data_format': 'channels_first'},
        input_shape=(3, 4, 5, 6))
    testing_utils.layer_test(
        keras.layers.pooling.GlobalAveragePooling2D,
        kwargs={'data_format': 'channels_last'},
        input_shape=(3, 5, 6, 4))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3952')" href="javascript:;">
keras-2.6.0/keras/layers/pooling_test.py: 135-152
</a>
<div class="mid" id="frag3952" style="display:none"><pre>
  def test_globalpooling_3d(self):
    testing_utils.layer_test(
        keras.layers.pooling.GlobalMaxPooling3D,
        kwargs={'data_format': 'channels_first'},
        input_shape=(3, 4, 3, 4, 3))
    testing_utils.layer_test(
        keras.layers.pooling.GlobalMaxPooling3D,
        kwargs={'data_format': 'channels_last'},
        input_shape=(3, 4, 3, 4, 3))
    testing_utils.layer_test(
        keras.layers.pooling.GlobalAveragePooling3D,
        kwargs={'data_format': 'channels_first'},
        input_shape=(3, 4, 3, 4, 3))
    testing_utils.layer_test(
        keras.layers.pooling.GlobalAveragePooling3D,
        kwargs={'data_format': 'channels_last'},
        input_shape=(3, 4, 3, 4, 3))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 215:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3959')" href="javascript:;">
keras-2.6.0/keras/layers/pooling_test.py: 285-304
</a>
<div class="mid" id="frag3959" style="display:none"><pre>
  def test_maxpooling_3d(self):
    pool_size = (3, 3, 3)
    testing_utils.layer_test(
        keras.layers.MaxPooling3D,
        kwargs={
            'strides': 2,
            'padding': 'valid',
            'pool_size': pool_size
        },
        input_shape=(3, 11, 12, 10, 4))
    testing_utils.layer_test(
        keras.layers.MaxPooling3D,
        kwargs={
            'strides': 3,
            'padding': 'valid',
            'data_format': 'channels_first',
            'pool_size': pool_size
        },
        input_shape=(3, 4, 11, 12, 10))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3960')" href="javascript:;">
keras-2.6.0/keras/layers/pooling_test.py: 305-325
</a>
<div class="mid" id="frag3960" style="display:none"><pre>
  def test_averagepooling_3d(self):
    pool_size = (3, 3, 3)
    testing_utils.layer_test(
        keras.layers.AveragePooling3D,
        kwargs={
            'strides': 2,
            'padding': 'valid',
            'pool_size': pool_size
        },
        input_shape=(3, 11, 12, 10, 4))
    testing_utils.layer_test(
        keras.layers.AveragePooling3D,
        kwargs={
            'strides': 3,
            'padding': 'valid',
            'data_format': 'channels_first',
            'pool_size': pool_size
        },
        input_shape=(3, 4, 11, 12, 10))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 216:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3961')" href="javascript:;">
keras-2.6.0/keras/layers/pooling_test.py: 329-343
</a>
<div class="mid" id="frag3961" style="display:none"><pre>
  def test_maxpooling_1d(self):
    for padding in ['valid', 'same']:
      for stride in [1, 2]:
        testing_utils.layer_test(
            keras.layers.MaxPooling1D,
            kwargs={
                'strides': stride,
                'padding': padding
            },
            input_shape=(3, 5, 4))
    testing_utils.layer_test(
        keras.layers.MaxPooling1D,
        kwargs={'data_format': 'channels_first'},
        input_shape=(3, 2, 6))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3962')" href="javascript:;">
keras-2.6.0/keras/layers/pooling_test.py: 344-360
</a>
<div class="mid" id="frag3962" style="display:none"><pre>
  def test_averagepooling_1d(self):
    for padding in ['valid', 'same']:
      for stride in [1, 2]:
        testing_utils.layer_test(
            keras.layers.AveragePooling1D,
            kwargs={
                'strides': stride,
                'padding': padding
            },
            input_shape=(3, 5, 4))

    testing_utils.layer_test(
        keras.layers.AveragePooling1D,
        kwargs={'data_format': 'channels_first'},
        input_shape=(3, 2, 6))


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 217:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3973')" href="javascript:;">
keras-2.6.0/keras/layers/tensorflow_op_layer_test.py: 123-143
</a>
<div class="mid" id="frag3973" style="display:none"><pre>
def _int32_manipulation_too_big_for_shape():
  # This test verifies that the Keras Functional API
  # won't crash when manipulating int32 tensors that are too large
  # to represent shapes.
  inputs = keras.Input(batch_size=2, shape=(10,))
  batch_size = tf.shape(inputs)[0]
  num_features = 3 * 1024 * 16
  x = tf.range(batch_size * num_features, dtype='int32')
  assert x.shape.as_list() == [inputs.shape[0] * num_features]
  x = tf.reshape(x, (batch_size, num_features))
  x = tf.cast(x, dtype='float32')
  outputs = keras.layers.Dense(10)(x)
  if tf.executing_eagerly():
    return keras.Model(inputs, outputs)
  else:
    # In V1 the op layer fails for some reason,
    # but we don't have access to the test case to call
    # self.skip_test in this util method
    return keras.Model(inputs, inputs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3974')" href="javascript:;">
keras-2.6.0/keras/layers/tensorflow_op_layer_test.py: 144-171
</a>
<div class="mid" id="frag3974" style="display:none"><pre>
def _int32_manipulation_at_max_shape_dims_limit():
  # This test verifies that the Keras Functional API
  # won't crash when manipulating int32 tensors that are at the limit
  # of the max tensor size Keras can try inferring values for.
  inputs = keras.Input(batch_size=2, shape=(10,))
  batch_size = tf.shape(inputs)[0]
  num_features = int(keras_tensor._MAX_TENSOR_RANK / int(inputs.shape[0]))
  x = tf.range(batch_size * num_features, dtype='int32')
  assert x.shape.as_list() == [keras_tensor._MAX_TENSOR_RANK]

  # Verify that a value was actually inferred for a tensor that *might*
  # represent the shape, bying checking that a value in
  # the range appears in the printed inferred value
  if tf.compat.v1.executing_eagerly_outside_functions():
    assert str(keras_tensor._MAX_TENSOR_RANK - 1) in str(x)

  x = tf.reshape(x, (batch_size, num_features))
  x = tf.cast(x, dtype='float32')
  outputs = keras.layers.Dense(10)(x)
  if tf.executing_eagerly():
    return keras.Model(inputs, outputs)
  else:
    # In V1 the op layer fails for some reason,
    # but we don't have access to the test case to call
    # self.skip_test in this util method
    return keras.Model(inputs, inputs)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 218:</b> &nbsp; 5 fragments, nominal size 29 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag3990')" href="javascript:;">
keras-2.6.0/keras/layers/tensorflow_op_layer_test.py: 391-427
</a>
<div class="mid" id="frag3990" style="display:none"><pre>
  def test_getitem_slice_with_step_only(self):
    if not tf.executing_eagerly():
      self.skipTest('Complex slicing like this fails in v1')
    inp = keras.Input(shape=(8,))
    slice_step = keras.Input(shape=(), dtype='int32')

    out = inp[..., ::slice_step[0]]
    model = keras.Model(
        inputs=[inp, slice_step],
        outputs=out)
    model.compile(
        adam.Adam(0.001),
        'mse',
        run_eagerly=testing_utils.should_run_eagerly())
    batch_size = 7
    step = 3
    x = tf.stack([
        tf.range(8) for _ in range(batch_size)])
    args = [x, tf.constant(step, shape=(batch_size,))]
    expected = tf.stack([
        tf.range(8)[::step] for _ in range(batch_size)])

    if tf.compat.v1.executing_eagerly_outside_functions():
      self.assertIn('tf.__operators__.getitem', (
          x.name for x in model.layers))
      self.assertNotIn('tf.strided_slice', (
          x.name for x in model.layers))
    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

    # Make sure it can be successfully saved and loaded
    config = model.get_config()
    model = keras.Model.from_config(config)

    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3993')" href="javascript:;">
keras-2.6.0/keras/layers/tensorflow_op_layer_test.py: 497-532
</a>
<div class="mid" id="frag3993" style="display:none"><pre>
  def test_getitem_slice_with_stop_only(self):
    if not tf.executing_eagerly():
      self.skipTest('Complex slicing like this fails in v1')
    inp = keras.Input(shape=(8,))
    slice_stop = keras.Input(shape=(), dtype='int32')

    out = inp[:slice_stop[0]]
    model = keras.Model(
        inputs=[inp, slice_stop],
        outputs=out)
    model.compile(
        adam.Adam(0.001),
        'mse',
        run_eagerly=testing_utils.should_run_eagerly())
    batch_size = 7
    stop = 6
    x = tf.stack([
        tf.range(8) for _ in range(batch_size)])
    args = [x, tf.constant(stop, shape=(batch_size,))]
    expected = x[:stop]

    if tf.compat.v1.executing_eagerly_outside_functions():
      self.assertIn('tf.__operators__.getitem', (
          x.name for x in model.layers))
      self.assertNotIn('tf.strided_slice', (
          x.name for x in model.layers))
    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

    # Make sure it can be successfully saved and loaded
    config = model.get_config()
    model = keras.Model.from_config(config)

    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3994')" href="javascript:;">
keras-2.6.0/keras/layers/tensorflow_op_layer_test.py: 533-569
</a>
<div class="mid" id="frag3994" style="display:none"><pre>
  def test_getitem_slice_with_stop_and_ellipsis_only(self):
    if not tf.executing_eagerly():
      self.skipTest('Complex slicing like this fails in v1')
    inp = keras.Input(shape=(8,))
    slice_stop = keras.Input(shape=(), dtype='int32')

    out = inp[..., :slice_stop[0]]
    model = keras.Model(
        inputs=[inp, slice_stop],
        outputs=out)
    model.compile(
        adam.Adam(0.001),
        'mse',
        run_eagerly=testing_utils.should_run_eagerly())
    batch_size = 7
    stop = 6
    x = tf.stack([
        tf.range(8) for _ in range(batch_size)])
    args = [x, tf.constant(stop, shape=(batch_size,))]
    expected = tf.stack([
        tf.range(8)[:stop] for _ in range(batch_size)])

    if tf.compat.v1.executing_eagerly_outside_functions():
      self.assertIn('tf.__operators__.getitem', (
          x.name for x in model.layers))
      self.assertNotIn('tf.strided_slice', (
          x.name for x in model.layers))
    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

    # Make sure it can be successfully saved and loaded
    config = model.get_config()
    model = keras.Model.from_config(config)

    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3991')" href="javascript:;">
keras-2.6.0/keras/layers/tensorflow_op_layer_test.py: 428-461
</a>
<div class="mid" id="frag3991" style="display:none"><pre>
  def test_getitem_slice_real_tensor(self):
    if not tf.executing_eagerly():
      self.skipTest('Complex slicing like this fails in v1')
    x = tf.range(10.0)
    slice_stop = keras.Input(shape=(), dtype='int32')

    out = x[:slice_stop[0]]
    model = keras.Model(
        inputs=slice_stop,
        outputs=out)
    model.compile(
        adam.Adam(0.001),
        'mse',
        run_eagerly=testing_utils.should_run_eagerly())
    batch_size = 7
    stop = 6
    args = tf.constant(stop, shape=(batch_size,))
    expected = x[:stop]

    if tf.compat.v1.executing_eagerly_outside_functions():
      self.assertIn('tf.__operators__.getitem', (
          x.name for x in model.layers))
      # TODO(b/161925288): Fix the dispatch triggering then uncomment:
      # self.assertNotIn('tf.strided_slice', (
      #     x.name for x in model.layers))
    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

    config = model.get_config()
    model = keras.Model.from_config(config)

    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag3992')" href="javascript:;">
keras-2.6.0/keras/layers/tensorflow_op_layer_test.py: 462-496
</a>
<div class="mid" id="frag3992" style="display:none"><pre>
  def test_getitem_index_real_tensor(self):
    if not tf.executing_eagerly():
      self.skipTest('Complex slicing like this fails in v1')
    x = tf.range(10.0)
    slice_stop = keras.Input(shape=(), dtype='int32')

    out = x[slice_stop[0]]
    model = keras.Model(
        inputs=slice_stop,
        outputs=out)
    model.compile(
        adam.Adam(0.001),
        'mse',
        run_eagerly=testing_utils.should_run_eagerly())
    batch_size = 7
    index = 6
    args = tf.constant(index, shape=(batch_size,))
    expected = x[index]

    if tf.compat.v1.executing_eagerly_outside_functions():
      self.assertIn('tf.__operators__.getitem', (
          x.name for x in model.layers))
      # TODO(b/161925288): Fix the bug then uncomment:
      # self.assertNotIn('tf.strided_slice', (
      #     x.name for x in model.layers))
    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

    # Make sure it can be successfully saved and loaded
    config = model.get_config()
    model = keras.Model.from_config(config)

    self.assertAllEqual(model(args), expected)
    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 219:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 94%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4015')" href="javascript:;">
keras-2.6.0/keras/layers/serialization_test.py: 99-116
</a>
<div class="mid" id="frag4015" style="display:none"><pre>
  def test_serialize_deserialize_batchnorm(self, batchnorm_layer):
    layer = batchnorm_layer(
        momentum=0.9, beta_initializer='zeros', gamma_regularizer='l2')
    config = keras.layers.serialize(layer)
    self.assertEqual(config['class_name'], 'BatchNormalization')
    new_layer = keras.layers.deserialize(config)
    self.assertEqual(new_layer.momentum, 0.9)
    if tf.__internal__.tf2.enabled():
      self.assertIsInstance(new_layer, batchnorm_v2.BatchNormalization)
      self.assertEqual(new_layer.beta_initializer.__class__,
                       keras.initializers.ZerosV2)
    else:
      self.assertIsInstance(new_layer, batchnorm_v1.BatchNormalization)
      self.assertEqual(new_layer.beta_initializer.__class__,
                       keras.initializers.Zeros)
    self.assertEqual(new_layer.gamma_regularizer.__class__,
                     keras.regularizers.L2)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4016')" href="javascript:;">
keras-2.6.0/keras/layers/serialization_test.py: 119-135
</a>
<div class="mid" id="frag4016" style="display:none"><pre>
  def test_deserialize_batchnorm_backwards_compatibility(self, batchnorm_layer):
    layer = batchnorm_layer(
        momentum=0.9, beta_initializer='zeros', gamma_regularizer='l2')
    config = keras.layers.serialize(layer)
    new_layer = keras.layers.deserialize(config)
    self.assertEqual(new_layer.momentum, 0.9)
    if tf.__internal__.tf2.enabled():
      self.assertIsInstance(new_layer, batchnorm_v2.BatchNormalization)
      self.assertEqual(new_layer.beta_initializer.__class__,
                       keras.initializers.ZerosV2)
    else:
      self.assertIsInstance(new_layer, batchnorm_v1.BatchNormalization)
      self.assertEqual(new_layer.beta_initializer.__class__,
                       keras.initializers.Zeros)
    self.assertEqual(new_layer.gamma_regularizer.__class__,
                     keras.regularizers.L2)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 220:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4017')" href="javascript:;">
keras-2.6.0/keras/layers/serialization_test.py: 137-149
</a>
<div class="mid" id="frag4017" style="display:none"><pre>
  def test_serialize_deserialize_lstm(self, layer):
    lstm = layer(5, return_sequences=True)
    config = keras.layers.serialize(lstm)
    self.assertEqual(config['class_name'], 'LSTM')
    new_layer = keras.layers.deserialize(config)
    self.assertEqual(new_layer.units, 5)
    self.assertEqual(new_layer.return_sequences, True)
    if tf.__internal__.tf2.enabled():
      self.assertIsInstance(new_layer, rnn_v2.LSTM)
    else:
      self.assertIsInstance(new_layer, rnn_v1.LSTM)
      self.assertNotIsInstance(new_layer, rnn_v2.LSTM)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4018')" href="javascript:;">
keras-2.6.0/keras/layers/serialization_test.py: 151-164
</a>
<div class="mid" id="frag4018" style="display:none"><pre>
  def test_serialize_deserialize_gru(self, layer):
    gru = layer(5, return_sequences=True)
    config = keras.layers.serialize(gru)
    self.assertEqual(config['class_name'], 'GRU')
    new_layer = keras.layers.deserialize(config)
    self.assertEqual(new_layer.units, 5)
    self.assertEqual(new_layer.return_sequences, True)
    if tf.__internal__.tf2.enabled():
      self.assertIsInstance(new_layer, rnn_v2.GRU)
    else:
      self.assertIsInstance(new_layer, rnn_v1.GRU)
      self.assertNotIsInstance(new_layer, rnn_v2.GRU)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 221:</b> &nbsp; 2 fragments, nominal size 26 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4043')" href="javascript:;">
keras-2.6.0/keras/layers/local_test.py: 86-114
</a>
<div class="mid" id="frag4043" style="display:none"><pre>
  def test_locallyconnected_1d(self, data_format, padding, implementation):
    with self.cached_session():
      num_samples = 2
      num_steps = 8
      input_dim = 5
      filter_length = 3
      filters = 4

      for strides in [1]:
        if padding == 'same' and strides != 1:
          continue
        kwargs = {
            'filters': filters,
            'kernel_size': filter_length,
            'padding': padding,
            'strides': strides,
            'data_format': data_format,
            'implementation': implementation
        }

        if padding == 'same' and implementation == 1:
          self.assertRaises(ValueError, keras.layers.LocallyConnected1D,
                            **kwargs)
        else:
          testing_utils.layer_test(
              keras.layers.LocallyConnected1D,
              kwargs=kwargs,
              input_shape=(num_samples, num_steps, input_dim))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4045')" href="javascript:;">
keras-2.6.0/keras/layers/local_test.py: 165-196
</a>
<div class="mid" id="frag4045" style="display:none"><pre>
  def test_locallyconnected_2d(self, data_format, padding, implementation):
    with self.cached_session():
      num_samples = 8
      filters = 3
      stack_size = 4
      num_row = 6
      num_col = 10

      for strides in [(1, 1), (2, 2)]:
        if padding == 'same' and strides != (1, 1):
          continue

        kwargs = {
            'filters': filters,
            'kernel_size': 3,
            'padding': padding,
            'kernel_regularizer': 'l2',
            'bias_regularizer': 'l2',
            'strides': strides,
            'data_format': data_format,
            'implementation': implementation
        }

        if padding == 'same' and implementation == 1:
          self.assertRaises(ValueError, keras.layers.LocallyConnected2D,
                            **kwargs)
        else:
          testing_utils.layer_test(
              keras.layers.LocallyConnected2D,
              kwargs=kwargs,
              input_shape=(num_samples, num_row, num_col, stack_size))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 222:</b> &nbsp; 2 fragments, nominal size 40 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4044')" href="javascript:;">
keras-2.6.0/keras/layers/local_test.py: 116-160
</a>
<div class="mid" id="frag4044" style="display:none"><pre>
  def test_locallyconnected_1d_regularization(self, data_format, padding,
                                              implementation):
    num_samples = 2
    num_steps = 8
    input_dim = 5
    filter_length = 3
    filters = 4
    kwargs = {
        'filters': filters,
        'kernel_size': filter_length,
        'kernel_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'data_format': data_format,
        'implementation': implementation,
        'padding': padding
    }

    if padding == 'same' and implementation == 1:
      self.assertRaises(ValueError, keras.layers.LocallyConnected1D, **kwargs)
    else:
      with self.cached_session():
        layer = keras.layers.LocallyConnected1D(**kwargs)
        layer.build((num_samples, num_steps, input_dim))
        self.assertEqual(len(layer.losses), 2)
        layer(
            keras.backend.variable(
                np.ones((num_samples, num_steps, input_dim))))
        self.assertEqual(len(layer.losses), 3)

      k_constraint = keras.constraints.max_norm(0.01)
      b_constraint = keras.constraints.max_norm(0.01)
      kwargs = {
          'filters': filters,
          'kernel_size': filter_length,
          'kernel_constraint': k_constraint,
          'bias_constraint': b_constraint,
      }
      with self.cached_session():
        layer = keras.layers.LocallyConnected1D(**kwargs)
        layer.build((num_samples, num_steps, input_dim))
        self.assertEqual(layer.kernel.constraint, k_constraint)
        self.assertEqual(layer.bias.constraint, b_constraint)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4047')" href="javascript:;">
keras-2.6.0/keras/layers/local_test.py: 223-267
</a>
<div class="mid" id="frag4047" style="display:none"><pre>
  def test_locallyconnected_2d_regularization(self, data_format, padding,
                                              implementation):
    num_samples = 2
    filters = 3
    stack_size = 4
    num_row = 6
    num_col = 7
    kwargs = {
        'filters': filters,
        'kernel_size': 3,
        'kernel_regularizer': 'l2',
        'bias_regularizer': 'l2',
        'activity_regularizer': 'l2',
        'implementation': implementation,
        'padding': padding,
        'data_format': data_format
    }

    if padding == 'same' and implementation == 1:
      self.assertRaises(ValueError, keras.layers.LocallyConnected2D, **kwargs)
    else:
      with self.cached_session():
        layer = keras.layers.LocallyConnected2D(**kwargs)
        layer.build((num_samples, num_row, num_col, stack_size))
        self.assertEqual(len(layer.losses), 2)
        layer(
            keras.backend.variable(
                np.ones((num_samples, num_row, num_col, stack_size))))
        self.assertEqual(len(layer.losses), 3)

      k_constraint = keras.constraints.max_norm(0.01)
      b_constraint = keras.constraints.max_norm(0.01)
      kwargs = {
          'filters': filters,
          'kernel_size': 3,
          'kernel_constraint': k_constraint,
          'bias_constraint': b_constraint,
      }
      with self.cached_session():
        layer = keras.layers.LocallyConnected2D(**kwargs)
        layer.build((num_samples, num_row, num_col, stack_size))
        self.assertEqual(layer.kernel.constraint, k_constraint)
        self.assertEqual(layer.bias.constraint, b_constraint)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 223:</b> &nbsp; 2 fragments, nominal size 65 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4048')" href="javascript:;">
keras-2.6.0/keras/layers/local_test.py: 278-363
</a>
<div class="mid" id="frag4048" style="display:none"><pre>
  def test_locallyconnected_implementation(self, width, data_format):
    with self.cached_session():
      num_samples = 4
      num_classes = 3
      num_epochs = 2

      np.random.seed(1)
      tf_test_util.random_seed.set_seed(1)
      targets = np.random.randint(0, num_classes, (num_samples,))

      height = 7
      filters = 2
      inputs = get_inputs(data_format, filters, height, num_samples, width)

      kernel_x = (3,)
      kernel_y = () if width == 1 else (2,)
      stride_x = (1,)
      stride_y = () if width == 1 else (3,)
      layers = 2

      kwargs = {
          'layers': layers,
          'filters': filters,
          'kernel_size': kernel_x + kernel_y,
          'strides': stride_x + stride_y,
          'data_format': data_format,
          'num_classes': num_classes
      }

      model_1 = get_model(implementation=1, **kwargs)
      model_2 = get_model(implementation=2, **kwargs)
      model_3 = get_model(implementation=3, **kwargs)

      # Build models.
      model_1.train_on_batch(inputs, targets)
      model_2.train_on_batch(inputs, targets)
      model_3.train_on_batch(inputs, targets)

      # Copy weights.
      copy_model_weights(model_from=model_2, model_to=model_1)
      copy_model_weights(model_from=model_2, model_to=model_3)

      # Compare outputs at initialization.
      out_1 = model_1(inputs)
      out_2 = model_2(inputs)
      out_3 = model_3(inputs)

      self.assertAllCloseAccordingToType(
          out_2, out_1, rtol=1e-5, atol=1e-5)
      self.assertAllCloseAccordingToType(
          out_2, out_3, rtol=1e-5, atol=1e-5)
      self.assertAllCloseAccordingToType(
          out_1, out_3, rtol=1e-5, atol=1e-5)

      # Train.
      model_1.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)
      model_2.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)
      model_3.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)

      # Compare outputs after a few training steps.
      out_1 = model_1(inputs)
      out_2 = model_2(inputs)
      out_3 = model_3(inputs)

      self.assertAllCloseAccordingToType(
          out_2, out_1, atol=2e-4)
      self.assertAllCloseAccordingToType(
          out_2, out_3, atol=2e-4)
      self.assertAllCloseAccordingToType(
          out_1, out_3, atol=2e-4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4049')" href="javascript:;">
keras-2.6.0/keras/layers/local_test.py: 382-456
</a>
<div class="mid" id="frag4049" style="display:none"><pre>
  def test_locallyconnected_save(self, width, data_format):
    with self.cached_session():
      num_samples = 4
      num_classes = 3
      num_epochs = 2

      np.random.seed(1)
      tf_test_util.random_seed.set_seed(1)
      targets = np.random.randint(0, num_classes, (num_samples,))

      height = 7
      filters = 2
      inputs = get_inputs(data_format, filters, height, num_samples, width)

      kernel_x = (3,)
      kernel_y = () if width == 1 else (2,)
      stride_x = (1,)
      stride_y = () if width == 1 else (3,)
      layers = 2

      kwargs = {
          'layers': layers,
          'filters': filters,
          'kernel_size': kernel_x + kernel_y,
          'strides': stride_x + stride_y,
          'data_format': data_format,
          'num_classes': num_classes
      }

      model_1 = get_model_saveable(implementation=1, **kwargs)
      model_2 = get_model_saveable(implementation=2, **kwargs)
      model_3 = get_model_saveable(implementation=3, **kwargs)

      # Train.
      model_1.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)
      model_2.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)
      model_3.fit(
          x=inputs,
          y=targets,
          epochs=num_epochs,
          batch_size=num_samples,
          shuffle=False)

      out_1_before = model_1(inputs)
      out_2_before = model_2(inputs)
      out_3_before = model_3(inputs)

      path_1 = os.path.join(self.get_temp_dir(), 'model_1_path')
      model_1.save(path_1)
      model_1 = keras.models.load_model(path_1, custom_objects={'xent': xent})
      path_2 = os.path.join(self.get_temp_dir(), 'model_2_path')
      model_2.save(path_2)
      model_2 = keras.models.load_model(path_2, custom_objects={'xent': xent})
      path_3 = os.path.join(self.get_temp_dir(), 'model_3_path')
      model_3.save(path_3)
      model_3 = keras.models.load_model(path_3, custom_objects={'xent': xent})

      out_1_after = model_1(inputs)
      out_2_after = model_2(inputs)
      out_3_after = model_3(inputs)

      self.assertAllCloseAccordingToType(out_1_before, out_1_after, atol=2e-4)
      self.assertAllCloseAccordingToType(out_2_before, out_2_after, atol=2e-4)
      self.assertAllCloseAccordingToType(out_3_before, out_3_after, atol=2e-4)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 224:</b> &nbsp; 2 fragments, nominal size 30 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4053')" href="javascript:;">
keras-2.6.0/keras/layers/local_test.py: 525-562
</a>
<div class="mid" id="frag4053" style="display:none"><pre>
def get_model(implementation,
              filters,
              kernel_size,
              strides,
              layers,
              num_classes,
              data_format):
  model = keras.Sequential()

  if len(kernel_size) == 1:
    lc_layer = keras.layers.LocallyConnected1D
  elif len(kernel_size) == 2:
    lc_layer = keras.layers.LocallyConnected2D
  else:
    raise NotImplementedError(kernel_size)

  for _ in range(layers):
    model.add(lc_layer(
        padding='valid',
        kernel_initializer=keras.initializers.random_normal(),
        bias_initializer=keras.initializers.random_normal(),
        filters=filters,
        strides=strides,
        kernel_size=kernel_size,
        activation=keras.activations.relu,
        data_format=data_format,
        implementation=implementation))

  model.add(keras.layers.Flatten())
  model.add(keras.layers.Dense(num_classes))
  model.compile(
      optimizer=RMSPropOptimizer(0.01),
      metrics=[keras.metrics.categorical_accuracy],
      loss=xent
  )
  return model


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4054')" href="javascript:;">
keras-2.6.0/keras/layers/local_test.py: 563-595
</a>
<div class="mid" id="frag4054" style="display:none"><pre>
def get_model_saveable(implementation, filters, kernel_size, strides, layers,
                       num_classes, data_format):
  model = keras.Sequential()

  if len(kernel_size) == 1:
    lc_layer = keras.layers.LocallyConnected1D
  elif len(kernel_size) == 2:
    lc_layer = keras.layers.LocallyConnected2D
  else:
    raise NotImplementedError(kernel_size)

  for _ in range(layers):
    model.add(
        lc_layer(
            padding='valid',
            kernel_initializer=keras.initializers.random_normal(),
            bias_initializer=keras.initializers.random_normal(),
            filters=filters,
            strides=strides,
            kernel_size=kernel_size,
            activation=keras.activations.relu,
            data_format=data_format,
            implementation=implementation))

  model.add(keras.layers.Flatten())
  model.add(keras.layers.Dense(num_classes))
  model.compile(
      optimizer=rmsprop.RMSProp(learning_rate=0.01),
      metrics=[keras.metrics.categorical_accuracy],
      loss=xent)
  return model


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 225:</b> &nbsp; 3 fragments, nominal size 44 lines, similarity 77%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4104')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent.py: 1292-1337
</a>
<div class="mid" id="frag4104" style="display:none"><pre>
  def __init__(self,
               units,
               activation='tanh',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    if units &lt; 0:
      raise ValueError(f'Received an invalid value for units, expected '
                       f'a positive integer, got {units}.')
    # By default use cached variable under v2 mode, see b/143699808.
    if tf.compat.v1.executing_eagerly_outside_functions():
      self._enable_caching_device = kwargs.pop('enable_caching_device', True)
    else:
      self._enable_caching_device = kwargs.pop('enable_caching_device', False)
    super(SimpleRNNCell, self).__init__(**kwargs)
    self.units = units
    self.activation = activations.get(activation)
    self.use_bias = use_bias

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

    self.dropout = min(1., max(0., dropout))
    self.recurrent_dropout = min(1., max(0., recurrent_dropout))
    self.state_size = self.units
    self.output_size = self.units

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4153')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent.py: 2294-2349
</a>
<div class="mid" id="frag4153" style="display:none"><pre>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               **kwargs):
    if units &lt; 0:
      raise ValueError(f'Received an invalid value for units, expected '
                       f'a positive integer, got {units}.')
    # By default use cached variable under v2 mode, see b/143699808.
    if tf.compat.v1.executing_eagerly_outside_functions():
      self._enable_caching_device = kwargs.pop('enable_caching_device', True)
    else:
      self._enable_caching_device = kwargs.pop('enable_caching_device', False)
    super(LSTMCell, self).__init__(**kwargs)
    self.units = units
    self.activation = activations.get(activation)
    self.recurrent_activation = activations.get(recurrent_activation)
    self.use_bias = use_bias

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)
    self.unit_forget_bias = unit_forget_bias

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

    self.dropout = min(1., max(0., dropout))
    self.recurrent_dropout = min(1., max(0., recurrent_dropout))
    implementation = kwargs.pop('implementation', 1)
    if self.recurrent_dropout != 0 and implementation != 1:
      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)
      self.implementation = 1
    else:
      self.implementation = implementation
    self.state_size = [self.units, self.units]
    self.output_size = self.units

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4127')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent.py: 1731-1787
</a>
<div class="mid" id="frag4127" style="display:none"><pre>
  def __init__(self,
               units,
               activation='tanh',
               recurrent_activation='hard_sigmoid',
               use_bias=True,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               dropout=0.,
               recurrent_dropout=0.,
               reset_after=False,
               **kwargs):
    if units &lt; 0:
      raise ValueError(f'Received an invalid value for units, expected '
                       f'a positive integer, got {units}.')
    # By default use cached variable under v2 mode, see b/143699808.
    if tf.compat.v1.executing_eagerly_outside_functions():
      self._enable_caching_device = kwargs.pop('enable_caching_device', True)
    else:
      self._enable_caching_device = kwargs.pop('enable_caching_device', False)
    super(GRUCell, self).__init__(**kwargs)
    self.units = units
    self.activation = activations.get(activation)
    self.recurrent_activation = activations.get(recurrent_activation)
    self.use_bias = use_bias

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

    self.dropout = min(1., max(0., dropout))
    self.recurrent_dropout = min(1., max(0., recurrent_dropout))

    implementation = kwargs.pop('implementation', 1)
    if self.recurrent_dropout != 0 and implementation != 1:
      logging.debug(RECURRENT_DROPOUT_WARNING_MSG)
      self.implementation = 1
    else:
      self.implementation = implementation
    self.reset_after = reset_after
    self.state_size = self.units
    self.output_size = self.units

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 226:</b> &nbsp; 2 fragments, nominal size 29 lines, similarity 74%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4105')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent.py: 1339-1366
</a>
<div class="mid" id="frag4105" style="display:none"><pre>
  def build(self, input_shape):
    default_caching_device = _caching_device(self)
    self.kernel = self.add_weight(
        shape=(input_shape[-1], self.units),
        name='kernel',
        initializer=self.kernel_initializer,
        regularizer=self.kernel_regularizer,
        constraint=self.kernel_constraint,
        caching_device=default_caching_device)
    self.recurrent_kernel = self.add_weight(
        shape=(self.units, self.units),
        name='recurrent_kernel',
        initializer=self.recurrent_initializer,
        regularizer=self.recurrent_regularizer,
        constraint=self.recurrent_constraint,
        caching_device=default_caching_device)
    if self.use_bias:
      self.bias = self.add_weight(
          shape=(self.units,),
          name='bias',
          initializer=self.bias_initializer,
          regularizer=self.bias_regularizer,
          constraint=self.bias_constraint,
          caching_device=default_caching_device)
    else:
      self.bias = None
    self.built = True

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4128')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent.py: 1789-1825
</a>
<div class="mid" id="frag4128" style="display:none"><pre>
  def build(self, input_shape):
    input_dim = input_shape[-1]
    default_caching_device = _caching_device(self)
    self.kernel = self.add_weight(
        shape=(input_dim, self.units * 3),
        name='kernel',
        initializer=self.kernel_initializer,
        regularizer=self.kernel_regularizer,
        constraint=self.kernel_constraint,
        caching_device=default_caching_device)
    self.recurrent_kernel = self.add_weight(
        shape=(self.units, self.units * 3),
        name='recurrent_kernel',
        initializer=self.recurrent_initializer,
        regularizer=self.recurrent_regularizer,
        constraint=self.recurrent_constraint,
        caching_device=default_caching_device)

    if self.use_bias:
      if not self.reset_after:
        bias_shape = (3 * self.units,)
      else:
        # separate biases for input and recurrent kernels
        # Note: the shape is intentionally different from CuDNNGRU biases
        # `(2 * 3 * self.units,)`, so that we can distinguish the classes
        # when loading and converting saved weights.
        bias_shape = (2, 3 * self.units)
      self.bias = self.add_weight(shape=bias_shape,
                                  name='bias',
                                  initializer=self.bias_initializer,
                                  regularizer=self.bias_regularizer,
                                  constraint=self.bias_constraint,
                                  caching_device=default_caching_device)
    else:
      self.bias = None
    self.built = True

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 227:</b> &nbsp; 3 fragments, nominal size 14 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4228')" href="javascript:;">
keras-2.6.0/keras/layers/pooling.py: 50-64
</a>
<div class="mid" id="frag4228" style="display:none"><pre>
  def __init__(self, pool_function, pool_size, strides,
               padding='valid', data_format='channels_last',
               name=None, **kwargs):
    super(Pooling1D, self).__init__(name=name, **kwargs)
    if data_format is None:
      data_format = backend.image_data_format()
    if strides is None:
      strides = pool_size
    self.pool_function = pool_function
    self.pool_size = conv_utils.normalize_tuple(pool_size, 1, 'pool_size')
    self.strides = conv_utils.normalize_tuple(strides, 1, 'strides')
    self.padding = conv_utils.normalize_padding(padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.input_spec = InputSpec(ndim=3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4234')" href="javascript:;">
keras-2.6.0/keras/layers/pooling.py: 333-347
</a>
<div class="mid" id="frag4234" style="display:none"><pre>
  def __init__(self, pool_function, pool_size, strides,
               padding='valid', data_format=None,
               name=None, **kwargs):
    super(Pooling2D, self).__init__(name=name, **kwargs)
    if data_format is None:
      data_format = backend.image_data_format()
    if strides is None:
      strides = pool_size
    self.pool_function = pool_function
    self.pool_size = conv_utils.normalize_tuple(pool_size, 2, 'pool_size')
    self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')
    self.padding = conv_utils.normalize_padding(padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.input_spec = InputSpec(ndim=4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4240')" href="javascript:;">
keras-2.6.0/keras/layers/pooling.py: 671-685
</a>
<div class="mid" id="frag4240" style="display:none"><pre>
  def __init__(self, pool_function, pool_size, strides,
               padding='valid', data_format='channels_last',
               name=None, **kwargs):
    super(Pooling3D, self).__init__(name=name, **kwargs)
    if data_format is None:
      data_format = backend.image_data_format()
    if strides is None:
      strides = pool_size
    self.pool_function = pool_function
    self.pool_size = conv_utils.normalize_tuple(pool_size, 3, 'pool_size')
    self.strides = conv_utils.normalize_tuple(strides, 3, 'strides')
    self.padding = conv_utils.normalize_padding(padding)
    self.data_format = conv_utils.normalize_data_format(data_format)
    self.input_spec = InputSpec(ndim=5)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 228:</b> &nbsp; 4 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4238')" href="javascript:;">
keras-2.6.0/keras/layers/pooling.py: 518-529
</a>
<div class="mid" id="frag4238" style="display:none"><pre>
  def __init__(self,
               pool_size=(2, 2),
               strides=None,
               padding='valid',
               data_format=None,
               **kwargs):
    super(MaxPooling2D, self).__init__(
        tf.compat.v1.nn.max_pool,
        pool_size=pool_size, strides=strides,
        padding=padding, data_format=data_format, **kwargs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4245')" href="javascript:;">
keras-2.6.0/keras/layers/pooling.py: 870-881
</a>
<div class="mid" id="frag4245" style="display:none"><pre>
  def __init__(self,
               pool_size=(2, 2, 2),
               strides=None,
               padding='valid',
               data_format=None,
               **kwargs):
    super(AveragePooling3D, self).__init__(
        tf.nn.avg_pool3d,
        pool_size=pool_size, strides=strides,
        padding=padding, data_format=data_format, **kwargs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4239')" href="javascript:;">
keras-2.6.0/keras/layers/pooling.py: 632-643
</a>
<div class="mid" id="frag4239" style="display:none"><pre>
  def __init__(self,
               pool_size=(2, 2),
               strides=None,
               padding='valid',
               data_format=None,
               **kwargs):
    super(AveragePooling2D, self).__init__(
        tf.nn.avg_pool,
        pool_size=pool_size, strides=strides,
        padding=padding, data_format=data_format, **kwargs)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4244')" href="javascript:;">
keras-2.6.0/keras/layers/pooling.py: 799-810
</a>
<div class="mid" id="frag4244" style="display:none"><pre>
  def __init__(self,
               pool_size=(2, 2, 2),
               strides=None,
               padding='valid',
               data_format=None,
               **kwargs):
    super(MaxPooling3D, self).__init__(
        tf.nn.max_pool3d,
        pool_size=pool_size, strides=strides,
        padding=padding, data_format=data_format, **kwargs)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 229:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4247')" href="javascript:;">
keras-2.6.0/keras/layers/pooling.py: 891-903
</a>
<div class="mid" id="frag4247" style="display:none"><pre>
  def compute_output_shape(self, input_shape):
    input_shape = tf.TensorShape(input_shape).as_list()
    if self.data_format == 'channels_first':
      if self.keepdims:
        return tf.TensorShape([input_shape[0], input_shape[1], 1])
      else:
        return tf.TensorShape([input_shape[0], input_shape[1]])
    else:
      if self.keepdims:
        return tf.TensorShape([input_shape[0], 1, input_shape[2]])
      else:
        return tf.TensorShape([input_shape[0], input_shape[2]])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4261')" href="javascript:;">
keras-2.6.0/keras/layers/pooling.py: 1196-1210
</a>
<div class="mid" id="frag4261" style="display:none"><pre>
  def compute_output_shape(self, input_shape):
    input_shape = tf.TensorShape(input_shape).as_list()
    if self.data_format == 'channels_last':
      if self.keepdims:
        return tf.TensorShape(
            [input_shape[0], 1, 1, 1, input_shape[4]])
      else:
        return tf.TensorShape([input_shape[0], input_shape[4]])
    else:
      if self.keepdims:
        return tf.TensorShape(
            [input_shape[0], input_shape[1], 1, 1, 1])
      else:
        return tf.TensorShape([input_shape[0], input_shape[1]])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4255')" href="javascript:;">
keras-2.6.0/keras/layers/pooling.py: 1058-1070
</a>
<div class="mid" id="frag4255" style="display:none"><pre>
  def compute_output_shape(self, input_shape):
    input_shape = tf.TensorShape(input_shape).as_list()
    if self.data_format == 'channels_last':
      if self.keepdims:
        return tf.TensorShape([input_shape[0], 1, 1, input_shape[3]])
      else:
        return tf.TensorShape([input_shape[0], input_shape[3]])
    else:
      if self.keepdims:
        return tf.TensorShape([input_shape[0], input_shape[1], 1, 1])
      else:
        return tf.TensorShape([input_shape[0], input_shape[1]])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 230:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4285')" href="javascript:;">
keras-2.6.0/keras/layers/subclassed_layers_test.py: 29-49
</a>
<div class="mid" id="frag4285" style="display:none"><pre>
  def test_simple_build_with_constant(self):

    class BuildConstantLayer(keras.layers.Layer):

      def build(self, input_shape):
        self.b = tf.convert_to_tensor(2.0)

      def call(self, inputs):
        return self.b * inputs

    layer = BuildConstantLayer()
    model = testing_utils.get_model_from_layers(
        [layer, keras.layers.Dense(1)], input_shape=(1,))

    x = tf.convert_to_tensor([[3.0]])
    self.assertEqual(
        tf_utils.is_symbolic_tensor(model(x)), not tf.executing_eagerly())
    self.assertEqual(
        tf_utils.is_symbolic_tensor(layer(x)), not tf.executing_eagerly())
    self.assertAllClose(keras.backend.get_value(layer(x)), [[6.0]])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4288')" href="javascript:;">
keras-2.6.0/keras/layers/subclassed_layers_test.py: 50-74
</a>
<div class="mid" id="frag4288" style="display:none"><pre>
  def test_build_with_derived_constant(self):

    class BuildDerivedConstantLayer(keras.layers.Layer):

      def build(self, input_shape):
        a = tf.convert_to_tensor(1.0)
        b = 2.0 * a
        self.variable = tf.Variable(b)
        self.constant = tf.convert_to_tensor(self.variable)

      def call(self, inputs):
        return self.variable * self.constant * inputs

    layer = BuildDerivedConstantLayer()
    model = testing_utils.get_model_from_layers(
        [layer, keras.layers.Dense(1)], input_shape=(1,))

    x = tf.convert_to_tensor([[3.0]])
    self.assertEqual(
        tf_utils.is_symbolic_tensor(model(x)), not tf.executing_eagerly())
    self.assertEqual(
        tf_utils.is_symbolic_tensor(layer(x)), not tf.executing_eagerly())
    self.assertAllClose(keras.backend.get_value(layer(x)), [[12.0]])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 231:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4378')" href="javascript:;">
keras-2.6.0/keras/layers/legacy_rnn/rnn_cell_wrapper_impl.py: 303-314
</a>
<div class="mid" id="frag4378" style="display:none"><pre>
  def from_config(cls, config, custom_objects=None):
    if "dropout_fn" in config:
      config = config.copy()
      dropout_state_filter = _parse_config_to_function(
          config, custom_objects, "dropout_fn", "dropout_fn_type",
          "dropout_fn_module")
      config.pop("dropout_fn")
      config["dropout_state_filter_visitor"] = dropout_state_filter
    return super(DropoutWrapperBase, cls).from_config(
        config, custom_objects=custom_objects)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4387')" href="javascript:;">
keras-2.6.0/keras/layers/legacy_rnn/rnn_cell_wrapper_impl.py: 391-402
</a>
<div class="mid" id="frag4387" style="display:none"><pre>
  def from_config(cls, config, custom_objects=None):
    if "residual_fn" in config:
      config = config.copy()
      residual_function = _parse_config_to_function(config, custom_objects,
                                                    "residual_fn",
                                                    "residual_fn_type",
                                                    "residual_fn_module")
      config["residual_fn"] = residual_function
    return super(ResidualWrapperBase, cls).from_config(
        config, custom_objects=custom_objects)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 232:</b> &nbsp; 3 fragments, nominal size 23 lines, similarity 74%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4415')" href="javascript:;">
keras-2.6.0/keras/layers/legacy_rnn/rnn_cell_impl.py: 408-436
</a>
<div class="mid" id="frag4415" style="display:none"><pre>
  def __init__(self,
               num_units,
               activation=None,
               reuse=None,
               name=None,
               dtype=None,
               **kwargs):
    warnings.warn("`tf.nn.rnn_cell.BasicRNNCell` is deprecated and will be "
                  "removed in a future version. This class "
                  "is equivalent as `tf.keras.layers.SimpleRNNCell`, "
                  "and will be replaced by that in Tensorflow 2.0.")
    super(BasicRNNCell, self).__init__(
        _reuse=reuse, name=name, dtype=dtype, **kwargs)
    _check_supported_dtypes(self.dtype)
    if tf.executing_eagerly() and tf.config.list_logical_devices("GPU"):
      logging.warning(
          "%s: Note that this cell is not optimized for performance. "
          "Please use tf.contrib.cudnn_rnn.CudnnRNNTanh for better "
          "performance on GPU.", self)

    # Inputs must be 2-dimensional.
    self.input_spec = input_spec.InputSpec(ndim=2)

    self._num_units = num_units
    if activation:
      self._activation = activations.get(activation)
    else:
      self._activation = tf.tanh

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4428')" href="javascript:;">
keras-2.6.0/keras/layers/legacy_rnn/rnn_cell_impl.py: 667-726
</a>
<div class="mid" id="frag4428" style="display:none"><pre>
  def __init__(self,
               num_units,
               forget_bias=1.0,
               state_is_tuple=True,
               activation=None,
               reuse=None,
               name=None,
               dtype=None,
               **kwargs):
    """Initialize the basic LSTM cell.

    Args:
      num_units: int, The number of units in the LSTM cell.
      forget_bias: float, The bias added to forget gates (see above). Must set
        to `0.0` manually when restoring from CudnnLSTM-trained checkpoints.
      state_is_tuple: If True, accepted and returned states are 2-tuples of the
        `c_state` and `m_state`.  If False, they are concatenated along the
        column axis.  The latter behavior will soon be deprecated.
      activation: Activation function of the inner states.  Default: `tanh`. It
        could also be string that is within Keras activation function names.
      reuse: (optional) Python boolean describing whether to reuse variables in
        an existing scope.  If not `True`, and the existing scope already has
        the given variables, an error is raised.
      name: String, the name of the layer. Layers with the same name will share
        weights, but to avoid mistakes we require reuse=True in such cases.
      dtype: Default dtype of the layer (default of `None` means use the type of
        the first input). Required when `build` is called before `call`.
      **kwargs: Dict, keyword named properties for common layer attributes, like
        `trainable` etc when constructing the cell from configs of get_config().
        When restoring from CudnnLSTM-trained checkpoints, must use
        `CudnnCompatibleLSTMCell` instead.
    """
    warnings.warn("`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be "
                  "removed in a future version. This class "
                  "is equivalent as `tf.keras.layers.LSTMCell`, "
                  "and will be replaced by that in Tensorflow 2.0.")
    super(BasicLSTMCell, self).__init__(
        _reuse=reuse, name=name, dtype=dtype, **kwargs)
    _check_supported_dtypes(self.dtype)
    if not state_is_tuple:
      logging.warning(
          "%s: Using a concatenated state is slower and will soon be "
          "deprecated.  Use state_is_tuple=True.", self)
    if tf.executing_eagerly() and tf.config.list_logical_devices("GPU"):
      logging.warning(
          "%s: Note that this cell is not optimized for performance. "
          "Please use tf.contrib.cudnn_rnn.CudnnLSTM for better "
          "performance on GPU.", self)

    # Inputs must be 2-dimensional.
    self.input_spec = input_spec.InputSpec(ndim=2)

    self._num_units = num_units
    self._forget_bias = forget_bias
    self._state_is_tuple = state_is_tuple
    if activation:
      self._activation = activations.get(activation)
    else:
      self._activation = tf.tanh

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4421')" href="javascript:;">
keras-2.6.0/keras/layers/legacy_rnn/rnn_cell_impl.py: 515-547
</a>
<div class="mid" id="frag4421" style="display:none"><pre>
  def __init__(self,
               num_units,
               activation=None,
               reuse=None,
               kernel_initializer=None,
               bias_initializer=None,
               name=None,
               dtype=None,
               **kwargs):
    warnings.warn("`tf.nn.rnn_cell.GRUCell` is deprecated and will be removed "
                  "in a future version. This class "
                  "is equivalent as `tf.keras.layers.GRUCell`, "
                  "and will be replaced by that in Tensorflow 2.0.")
    super(GRUCell, self).__init__(
        _reuse=reuse, name=name, dtype=dtype, **kwargs)
    _check_supported_dtypes(self.dtype)

    if tf.executing_eagerly() and tf.config.list_logical_devices("GPU"):
      logging.warning(
          "%s: Note that this cell is not optimized for performance. "
          "Please use tf.contrib.cudnn_rnn.CudnnGRU for better "
          "performance on GPU.", self)
    # Inputs must be 2-dimensional.
    self.input_spec = input_spec.InputSpec(ndim=2)

    self._num_units = num_units
    if activation:
      self._activation = activations.get(activation)
    else:
      self._activation = tf.tanh
    self._kernel_initializer = initializers.get(kernel_initializer)
    self._bias_initializer = initializers.get(bias_initializer)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 233:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4418')" href="javascript:;">
keras-2.6.0/keras/layers/legacy_rnn/rnn_cell_impl.py: 446-462
</a>
<div class="mid" id="frag4418" style="display:none"><pre>
  def build(self, inputs_shape):
    if inputs_shape[-1] is None:
      raise ValueError("Expected inputs.shape[-1] to be known, saw shape: %s" %
                       str(inputs_shape))
    _check_supported_dtypes(self.dtype)

    input_depth = inputs_shape[-1]
    self._kernel = self.add_variable(
        _WEIGHTS_VARIABLE_NAME,
        shape=[input_depth + self._num_units, self._num_units])
    self._bias = self.add_variable(
        _BIAS_VARIABLE_NAME,
        shape=[self._num_units],
        initializer=tf.compat.v1.zeros_initializer(dtype=self.dtype))

    self.built = True

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4431')" href="javascript:;">
keras-2.6.0/keras/layers/legacy_rnn/rnn_cell_impl.py: 737-753
</a>
<div class="mid" id="frag4431" style="display:none"><pre>
  def build(self, inputs_shape):
    if inputs_shape[-1] is None:
      raise ValueError("Expected inputs.shape[-1] to be known, saw shape: %s" %
                       str(inputs_shape))
    _check_supported_dtypes(self.dtype)
    input_depth = inputs_shape[-1]
    h_depth = self._num_units
    self._kernel = self.add_variable(
        _WEIGHTS_VARIABLE_NAME,
        shape=[input_depth + h_depth, 4 * self._num_units])
    self._bias = self.add_variable(
        _BIAS_VARIABLE_NAME,
        shape=[4 * self._num_units],
        initializer=tf.compat.v1.zeros_initializer(dtype=self.dtype))

    self.built = True

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 234:</b> &nbsp; 4 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4426')" href="javascript:;">
keras-2.6.0/keras/layers/legacy_rnn/rnn_cell_impl.py: 607-618
</a>
<div class="mid" id="frag4426" style="display:none"><pre>
  def get_config(self):
    config = {
        "num_units": self._num_units,
        "kernel_initializer": initializers.serialize(self._kernel_initializer),
        "bias_initializer": initializers.serialize(self._bias_initializer),
        "activation": activations.serialize(self._activation),
        "reuse": self._reuse,
    }
    base_config = super(GRUCell, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4433')" href="javascript:;">
keras-2.6.0/keras/layers/legacy_rnn/rnn_cell_impl.py: 802-813
</a>
<div class="mid" id="frag4433" style="display:none"><pre>
  def get_config(self):
    config = {
        "num_units": self._num_units,
        "forget_bias": self._forget_bias,
        "state_is_tuple": self._state_is_tuple,
        "activation": activations.serialize(self._activation),
        "reuse": self._reuse,
    }
    base_config = super(BasicLSTMCell, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4534')" href="javascript:;">
keras-2.6.0/keras/layers/cudnn_recurrent.py: 117-128
</a>
<div class="mid" id="frag4534" style="display:none"><pre>
  def get_config(self):
    config = {
        'return_sequences': self.return_sequences,
        'return_state': self.return_state,
        'go_backwards': self.go_backwards,
        'stateful': self.stateful,
        'time_major': self.time_major,
    }
    base_config = super(  # pylint: disable=bad-super-call
        RNN, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5109')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization.py: 365-380
</a>
<div class="mid" id="frag5109" style="display:none"><pre>
  def get_config(self):
    # This does not include the 'vocabulary' arg, since if the vocab was passed
    # at init time it's now stored in variable state - we don't need to
    # pull it off disk again.
    config = {
        "max_tokens": self._index_lookup_layer.max_tokens,
        "standardize": self._standardize,
        "split": self._split,
        "ngrams": self._ngrams_arg,
        "output_mode": self._output_mode,
        "output_sequence_length": self._output_sequence_length,
        "pad_to_max_tokens": self._index_lookup_layer.pad_to_max_tokens,
    }
    base_config = super(TextVectorization, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 235:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4515')" href="javascript:;">
keras-2.6.0/keras/layers/dense_attention.py: 319-331
</a>
<div class="mid" id="frag4515" style="display:none"><pre>
  def build(self, input_shape):
    """Creates scale variable if use_scale==True."""
    if self.use_scale:
      self.scale = self.add_weight(
          name='scale',
          shape=(),
          initializer='ones',
          dtype=self.dtype,
          trainable=True)
    else:
      self.scale = None
    super(Attention, self).build(input_shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4519')" href="javascript:;">
keras-2.6.0/keras/layers/dense_attention.py: 459-473
</a>
<div class="mid" id="frag4519" style="display:none"><pre>
  def build(self, input_shape):
    v_shape = tf.TensorShape(input_shape[1])
    dim = v_shape[-1]
    dim = tf.compat.dimension_value(dim)
    if self.use_scale:
      self.scale = self.add_weight(
          name='scale',
          shape=[dim],
          initializer='glorot_uniform',
          dtype=self.dtype,
          trainable=True)
    else:
      self.scale = None
    super(AdditiveAttention, self).build(input_shape)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 236:</b> &nbsp; 3 fragments, nominal size 41 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4524')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_recurrent_test.py: 34-82
</a>
<div class="mid" id="frag4524" style="display:none"><pre>
  def test_conv_lstm(self, data_format, return_sequences):
    num_row = 3
    filters = 3
    num_samples = 1
    input_channel = 2
    input_num_row = 5
    sequence_len = 2
    if data_format == 'channels_first':
      inputs = np.random.rand(num_samples, sequence_len, input_channel,
                              input_num_row)
    else:
      inputs = np.random.rand(num_samples, sequence_len, input_num_row,
                              input_channel)

    # test for return state:
    x = keras.Input(batch_shape=inputs.shape)
    kwargs = {
        'data_format': data_format,
        'return_sequences': return_sequences,
        'return_state': True,
        'stateful': True,
        'filters': filters,
        'kernel_size': num_row,
        'padding': 'valid',
    }
    layer = keras.layers.ConvLSTM1D(**kwargs)
    layer.build(inputs.shape)
    outputs = layer(x)
    _, states = outputs[0], outputs[1:]
    self.assertEqual(len(states), 2)
    model = keras.models.Model(x, states[0])

    state = model.predict(inputs)

    self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)

    # test for output shape:
    testing_utils.layer_test(
        keras.layers.ConvLSTM1D,
        kwargs={
            'data_format': data_format,
            'return_sequences': return_sequences,
            'filters': filters,
            'kernel_size': num_row,
            'padding': 'valid'
        },
        input_shape=inputs.shape)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4531')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_recurrent_test.py: 295-347
</a>
<div class="mid" id="frag4531" style="display:none"><pre>
  def test_conv_lstm(self, data_format, return_sequences):
    num_height = 3
    num_width = 3
    num_depth = 3
    filters = 3
    num_samples = 1
    input_channel = 2
    input_height = 5
    input_width = 5
    input_depth = 5
    sequence_len = 2
    if data_format == 'channels_first':
      inputs = np.random.rand(num_samples, sequence_len, input_channel,
                              input_height, input_width, input_depth)
    else:
      inputs = np.random.rand(num_samples, sequence_len, input_height,
                              input_width, input_depth, input_channel)

    # test for return state:
    x = keras.Input(batch_shape=inputs.shape)
    kwargs = {
        'data_format': data_format,
        'return_sequences': return_sequences,
        'return_state': True,
        'stateful': True,
        'filters': filters,
        'kernel_size': (num_height, num_width, num_depth),
        'padding': 'same'
    }
    layer = keras.layers.ConvLSTM3D(**kwargs)
    layer.build(inputs.shape)
    outputs = layer(x)
    _, states = outputs[0], outputs[1:]
    self.assertEqual(len(states), 2)
    model = keras.models.Model(x, states[0])

    state = model.predict(inputs)

    self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)

    # test for output shape:
    testing_utils.layer_test(
        keras.layers.ConvLSTM3D,
        kwargs={
            'data_format': data_format,
            'return_sequences': return_sequences,
            'filters': filters,
            'kernel_size': (num_height, num_width, num_depth),
            'padding': 'valid'
        },
        input_shape=inputs.shape)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4525')" href="javascript:;">
keras-2.6.0/keras/layers/convolutional_recurrent_test.py: 90-136
</a>
<div class="mid" id="frag4525" style="display:none"><pre>
  def test_conv_lstm(self, data_format, return_sequences):
    num_row = 3
    num_col = 3
    filters = 2
    num_samples = 1
    input_channel = 2
    input_num_row = 5
    input_num_col = 5
    sequence_len = 2
    if data_format == 'channels_first':
      inputs = np.random.rand(num_samples, sequence_len,
                              input_channel,
                              input_num_row, input_num_col)
    else:
      inputs = np.random.rand(num_samples, sequence_len,
                              input_num_row, input_num_col,
                              input_channel)

    # test for return state:
    x = keras.Input(batch_shape=inputs.shape)
    kwargs = {'data_format': data_format,
              'return_sequences': return_sequences,
              'return_state': True,
              'stateful': True,
              'filters': filters,
              'kernel_size': (num_row, num_col),
              'padding': 'valid'}
    layer = keras.layers.ConvLSTM2D(**kwargs)
    layer.build(inputs.shape)
    outputs = layer(x)
    _, states = outputs[0], outputs[1:]
    self.assertEqual(len(states), 2)
    model = keras.models.Model(x, states[0])
    state = model.predict(inputs)

    self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)

    # test for output shape:
    testing_utils.layer_test(
        keras.layers.ConvLSTM2D,
        kwargs={'data_format': data_format,
                'return_sequences': return_sequences,
                'filters': filters,
                'kernel_size': (num_row, num_col),
                'padding': 'valid'},
        input_shape=inputs.shape)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 237:</b> &nbsp; 2 fragments, nominal size 37 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4540')" href="javascript:;">
keras-2.6.0/keras/layers/cudnn_recurrent.py: 192-231
</a>
<div class="mid" id="frag4540" style="display:none"><pre>
  def __init__(self,
               units,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               **kwargs):
    self.units = units
    cell_spec = collections.namedtuple('cell', 'state_size')
    self._cell = cell_spec(state_size=self.units)
    super(CuDNNGRU, self).__init__(
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        **kwargs)

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)
    self.activity_regularizer = regularizers.get(activity_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4545')" href="javascript:;">
keras-2.6.0/keras/layers/cudnn_recurrent.py: 376-417
</a>
<div class="mid" id="frag4545" style="display:none"><pre>
  def __init__(self,
               units,
               kernel_initializer='glorot_uniform',
               recurrent_initializer='orthogonal',
               bias_initializer='zeros',
               unit_forget_bias=True,
               kernel_regularizer=None,
               recurrent_regularizer=None,
               bias_regularizer=None,
               activity_regularizer=None,
               kernel_constraint=None,
               recurrent_constraint=None,
               bias_constraint=None,
               return_sequences=False,
               return_state=False,
               go_backwards=False,
               stateful=False,
               **kwargs):
    self.units = units
    cell_spec = collections.namedtuple('cell', 'state_size')
    self._cell = cell_spec(state_size=(self.units, self.units))
    super(CuDNNLSTM, self).__init__(
        return_sequences=return_sequences,
        return_state=return_state,
        go_backwards=go_backwards,
        stateful=stateful,
        **kwargs)

    self.kernel_initializer = initializers.get(kernel_initializer)
    self.recurrent_initializer = initializers.get(recurrent_initializer)
    self.bias_initializer = initializers.get(bias_initializer)
    self.unit_forget_bias = unit_forget_bias

    self.kernel_regularizer = regularizers.get(kernel_regularizer)
    self.recurrent_regularizer = regularizers.get(recurrent_regularizer)
    self.bias_regularizer = regularizers.get(bias_regularizer)
    self.activity_regularizer = regularizers.get(activity_regularizer)

    self.kernel_constraint = constraints.get(kernel_constraint)
    self.recurrent_constraint = constraints.get(recurrent_constraint)
    self.bias_constraint = constraints.get(bias_constraint)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 238:</b> &nbsp; 2 fragments, nominal size 45 lines, similarity 74%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4543')" href="javascript:;">
keras-2.6.0/keras/layers/cudnn_recurrent.py: 265-311
</a>
<div class="mid" id="frag4543" style="display:none"><pre>
  def _process_batch(self, inputs, initial_state):
    if not self.time_major:
      inputs = tf.transpose(inputs, perm=(1, 0, 2))
    input_h = initial_state[0]
    input_h = tf.expand_dims(input_h, axis=0)

    params = recurrent_v2._canonical_to_params(    # pylint: disable=protected-access
        weights=[
            self.kernel[:, self.units:self.units * 2],
            self.kernel[:, :self.units],
            self.kernel[:, self.units * 2:],
            self.recurrent_kernel[:, self.units:self.units * 2],
            self.recurrent_kernel[:, :self.units],
            self.recurrent_kernel[:, self.units * 2:],
        ],
        biases=[
            self.bias[self.units:self.units * 2],
            self.bias[:self.units],
            self.bias[self.units * 2:self.units * 3],
            self.bias[self.units * 4:self.units * 5],
            self.bias[self.units * 3:self.units * 4],
            self.bias[self.units * 5:],
        ],
        shape=self._vector_shape)

    args = {
        'input': inputs,
        'input_h': input_h,
        'input_c': 0,
        'params': params,
        'is_training': True,
        'rnn_mode': 'gru',
    }

    outputs, h, _, _, _ = tf.raw_ops.CudnnRNNV2(**args)

    if self.stateful or self.return_state:
      h = h[0]
    if self.return_sequences:
      if self.time_major:
        output = outputs
      else:
        output = tf.transpose(outputs, perm=(1, 0, 2))
    else:
      output = outputs[-1]
    return output, [h]

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4549')" href="javascript:;">
keras-2.6.0/keras/layers/cudnn_recurrent.py: 461-513
</a>
<div class="mid" id="frag4549" style="display:none"><pre>
  def _process_batch(self, inputs, initial_state):
    if not self.time_major:
      inputs = tf.transpose(inputs, perm=(1, 0, 2))
    input_h = initial_state[0]
    input_c = initial_state[1]
    input_h = tf.expand_dims(input_h, axis=0)
    input_c = tf.expand_dims(input_c, axis=0)

    params = recurrent_v2._canonical_to_params(    # pylint: disable=protected-access
        weights=[
            self.kernel[:, :self.units],
            self.kernel[:, self.units:self.units * 2],
            self.kernel[:, self.units * 2:self.units * 3],
            self.kernel[:, self.units * 3:],
            self.recurrent_kernel[:, :self.units],
            self.recurrent_kernel[:, self.units:self.units * 2],
            self.recurrent_kernel[:, self.units * 2:self.units * 3],
            self.recurrent_kernel[:, self.units * 3:],
        ],
        biases=[
            self.bias[:self.units],
            self.bias[self.units:self.units * 2],
            self.bias[self.units * 2:self.units * 3],
            self.bias[self.units * 3:self.units * 4],
            self.bias[self.units * 4:self.units * 5],
            self.bias[self.units * 5:self.units * 6],
            self.bias[self.units * 6:self.units * 7],
            self.bias[self.units * 7:],
        ],
        shape=self._vector_shape)

    args = {
        'input': inputs,
        'input_h': input_h,
        'input_c': input_c,
        'params': params,
        'is_training': True,
    }

    outputs, h, c, _, _ = tf.raw_ops.CudnnRNNV2(**args)

    if self.stateful or self.return_state:
      h = h[0]
      c = c[0]
    if self.return_sequences:
      if self.time_major:
        output = outputs
      else:
        output = tf.transpose(outputs, perm=(1, 0, 2))
    else:
      output = outputs[-1]
    return output, [h, c]

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 239:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 95%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4544')" href="javascript:;">
keras-2.6.0/keras/layers/cudnn_recurrent.py: 312-333
</a>
<div class="mid" id="frag4544" style="display:none"><pre>
  def get_config(self):
    config = {
        'units': self.units,
        'kernel_initializer': initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer': initializers.serialize(self.bias_initializer),
        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer': regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint': constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint': constraints.serialize(self.bias_constraint)
    }
    base_config = super(CuDNNGRU, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4550')" href="javascript:;">
keras-2.6.0/keras/layers/cudnn_recurrent.py: 514-534
</a>
<div class="mid" id="frag4550" style="display:none"><pre>
  def get_config(self):
    config = {
        'units': self.units,
        'kernel_initializer': initializers.serialize(self.kernel_initializer),
        'recurrent_initializer':
            initializers.serialize(self.recurrent_initializer),
        'bias_initializer': initializers.serialize(self.bias_initializer),
        'unit_forget_bias': self.unit_forget_bias,
        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),
        'recurrent_regularizer':
            regularizers.serialize(self.recurrent_regularizer),
        'bias_regularizer': regularizers.serialize(self.bias_regularizer),
        'activity_regularizer':
            regularizers.serialize(self.activity_regularizer),
        'kernel_constraint': constraints.serialize(self.kernel_constraint),
        'recurrent_constraint':
            constraints.serialize(self.recurrent_constraint),
        'bias_constraint': constraints.serialize(self.bias_constraint)
    }
    base_config = super(CuDNNLSTM, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))
</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 240:</b> &nbsp; 2 fragments, nominal size 54 lines, similarity 79%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4581')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent_v2.py: 611-696
</a>
<div class="mid" id="frag4581" style="display:none"><pre>
def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,
            go_backwards, sequence_lengths):
  """GRU with CuDNN implementation which is only available for GPU."""
  if not time_major and mask is None:
    inputs = tf.transpose(inputs, perm=(1, 0, 2))
    seq_axis, batch_axis = (0, 1)
  else:
    seq_axis, batch_axis = (0, 1) if time_major else (1, 0)
  # For init_h, cuDNN expects one more dim of num_layers before or after batch
  # dim for time major or batch major inputs respectively
  init_h = tf.expand_dims(init_h, axis=seq_axis)

  weights = tf.split(kernel, 3, axis=1)
  weights += tf.split(recurrent_kernel, 3, axis=1)
  # Note that the bias was initialized as shape (2, 3 * units), flat it into
  # (6 * units)
  bias = tf.split(backend.flatten(bias), 6)

  if tf.sysconfig.get_build_info()['is_cuda_build']:
    # Note that the gate order for CuDNN is different from the canonical format.
    # canonical format is [z, r, h], whereas CuDNN is [r, z, h]. The swap need
    # to be done for kernel, recurrent_kernel, input_bias, recurrent_bias.
    # z is update gate weights.
    # r is reset gate weights.
    # h is output gate weights.
    weights[0], weights[1] = weights[1], weights[0]
    weights[3], weights[4] = weights[4], weights[3]
    bias[0], bias[1] = bias[1], bias[0]
    bias[3], bias[4] = bias[4], bias[3]

  params = _canonical_to_params(
      weights=weights,
      biases=bias,
      shape=tf.constant([-1]),
      transpose_weights=True)

  if mask is not None:
    sequence_lengths = calculate_sequence_by_mask(mask, time_major)

  if sequence_lengths is not None:
    if go_backwards:
      # Three reversals are required. E.g.,
      # normal input = [1, 2, 3, 0, 0]  # where 0 need to be masked
      # reversed_input_to_cudnn = [3, 2, 1, 0, 0]
      # output_from_cudnn = [6, 5, 4, 0, 0]
      # expected_output = [0, 0, 6, 5 ,4]
      inputs = tf.reverse_sequence(
          inputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)
    outputs, h, _, _, _ = tf.raw_ops.CudnnRNNV3(
        input=inputs,
        input_h=init_h,
        input_c=0,
        params=params,
        is_training=True,
        rnn_mode='gru',
        sequence_lengths=sequence_lengths,
        time_major=time_major)
    if go_backwards:
      outputs = tf.reverse_sequence(
          outputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)
      outputs = tf.reverse(outputs, axis=[seq_axis])
  else:
    if go_backwards:
      # Reverse axis 0 since the input is already convert to time major.
      inputs = tf.reverse(inputs, axis=[0])
    outputs, h, _, _ = tf.raw_ops.CudnnRNN(
        input=inputs, input_h=init_h, input_c=0, params=params,
        is_training=True, rnn_mode='gru')

  last_output = outputs[-1]
  if not time_major and mask is None:
    outputs = tf.transpose(outputs, perm=[1, 0, 2])
  h = tf.squeeze(h, axis=seq_axis)

  # In the case of variable length input, the cudnn kernel will fill zeros for
  # the output, whereas the default keras behavior is to bring over the previous
  # output for t-1, so that in the return_sequence=False case, user can quickly
  # get the final effect output instead just 0s at the last timestep.
  # In order to mimic the default keras behavior, we copy the final h state as
  # the last_output, since it is numerically same as the output.
  if mask is not None:
    last_output = h

  return last_output, outputs, h, _runtime(_RUNTIME_GPU)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4594')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent_v2.py: 1394-1518
</a>
<div class="mid" id="frag4594" style="display:none"><pre>
def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,
             time_major, go_backwards, sequence_lengths):
  """LSTM with either CuDNN or ROCm implementation which is only available for GPU.

  Note that currently only right padded data is supported, or the result will be
  polluted by the unmasked data which should be filtered.

  Args:
    inputs: Input tensor of LSTM layer.
    init_h: Initial state tensor for the cell output.
    init_c: Initial state tensor for the cell hidden state.
    kernel: Weights for cell kernel.
    recurrent_kernel: Weights for cell recurrent kernel.
    bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias
      is used in this case.
    mask: Boolean tensor for mask out the steps within sequence.
      An individual `True` entry indicates that the corresponding timestep
      should be utilized, while a `False` entry indicates that the corresponding
      timestep should be ignored.
    time_major: Boolean, whether the inputs are in the format of [time, batch,
      feature] or [batch, time, feature].
    go_backwards: Boolean (default False). If True, process the input sequence
      backwards and return the reversed sequence.
    sequence_lengths: The lengths of all sequences coming from a variable length
      input, such as ragged tensors. If the input has a fixed timestep size,
      this should be None.

  Returns:
    last_output: Output tensor for the last timestep, which has shape
      [batch, units].
    outputs: Output tensor for all timesteps, which has shape
      [batch, time, units].
    state_0: The cell output, which has same shape as init_h.
    state_1: The cell hidden state, which has same shape as init_c.
    runtime: Constant string tensor which indicate real runtime hardware. This
      value is for testing purpose and should not be used by user.
  """
  if not time_major and mask is None:
    inputs = tf.transpose(inputs, perm=(1, 0, 2))
    seq_axis, batch_axis = (0, 1)
  else:
    seq_axis, batch_axis = (0, 1) if time_major else (1, 0)
  # For init_h and init_c, cuDNN expects one more dim of num_layers before or
  # after batch dim for time major or batch major inputs respectively
  init_h = tf.expand_dims(init_h, axis=seq_axis)
  init_c = tf.expand_dims(init_c, axis=seq_axis)

  weights = tf.split(kernel, 4, axis=1)
  weights += tf.split(recurrent_kernel, 4, axis=1)
  # CuDNN has an extra set of bias for inputs, we disable them (setting to 0),
  # so that mathematically it is same as the canonical LSTM implementation.
  full_bias = tf.concat((tf.zeros_like(bias), bias), 0)

  if tf.sysconfig.get_build_info()['is_rocm_build']:
    # ROCm MIOpen's weight sequence for LSTM is different from both canonical
    # and Cudnn format
    # MIOpen: [i, f, o, c] Cudnn/Canonical: [i, f, c, o]
    # i is input gate weights.
    # f is forget gate weights.
    # o is output gate weights.
    # c is cell gate weights.
    weights = [weights[x] for x in (0, 1, 3, 2, 4, 5, 7, 6)]
    # full_bias is a tensor of shape (8*n,)
    full_bias = tf.split(full_bias, 8, axis=0)
    full_bias = [full_bias[x] for x in (0, 1, 3, 2, 4, 5, 7, 6)]

  params = _canonical_to_params(
      weights=weights,
      biases=tf.split(full_bias, 8),
      shape=tf.constant([-1]),
      transpose_weights=True)

  if mask is not None:
    sequence_lengths = calculate_sequence_by_mask(mask, time_major)

  if sequence_lengths is not None:
    if go_backwards:
      # Three reversals are required. E.g.,
      # normal input = [1, 2, 3, 0, 0]  # where 0 need to be masked
      # reversed_input_to_cudnn = [3, 2, 1, 0, 0]
      # output_from_cudnn = [6, 5, 4, 0, 0]
      # expected_output = [0, 0, 6, 5 ,4]
      inputs = tf.reverse_sequence(
          inputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)
    outputs, h, c, _, _ = tf.raw_ops.CudnnRNNV3(
        input=inputs,
        input_h=init_h,
        input_c=init_c,
        params=params,
        is_training=True,
        rnn_mode='lstm',
        sequence_lengths=sequence_lengths,
        time_major=time_major)
    if go_backwards:
      outputs = tf.reverse_sequence(
          outputs, sequence_lengths, seq_axis=seq_axis, batch_axis=batch_axis)
      outputs = tf.reverse(outputs, axis=[seq_axis])
  else:
    # # Fill the array with shape [batch] with value of max timesteps.
    # sequence_length = array_ops.fill([array_ops.shape(inputs)[1]],
    #                                  array_ops.shape(inputs)[0])
    if go_backwards:
      # Reverse axis 0 since the input is already convert to time major.
      inputs = tf.reverse(inputs, axis=[0])
    outputs, h, c, _ = tf.raw_ops.CudnnRNN(
        input=inputs, input_h=init_h, input_c=init_c, params=params,
        is_training=True, rnn_mode='lstm')

  last_output = outputs[-1]
  if not time_major and mask is None:
    outputs = tf.transpose(outputs, perm=[1, 0, 2])
  h = tf.squeeze(h, axis=seq_axis)
  c = tf.squeeze(c, axis=seq_axis)

  # In the case of variable length input, the cudnn kernel will fill zeros for
  # the output, whereas the default keras behavior is to bring over the previous
  # output for t-1, so that in the return_sequence=False case, user can quickly
  # get the final effect output instead just 0s at the last timestep.
  # In order to mimic the default keras behavior, we copy the final h state as
  # the last_output, since it is numerically same as the output.
  if mask is not None:
    last_output = h
  return last_output, outputs, h, c, _runtime(_RUNTIME_GPU)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 241:</b> &nbsp; 2 fragments, nominal size 80 lines, similarity 87%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4582')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent_v2.py: 697-825
</a>
<div class="mid" id="frag4582" style="display:none"><pre>
def gru_with_backend_selection(inputs, init_h, kernel, recurrent_kernel, bias,
                               mask, time_major, go_backwards, sequence_lengths,
                               zero_output_for_mask):
  """Call the GRU with optimized backend kernel selection.

  Under the hood, this function will create two TF function, one with the most
  generic kernel and can run on all device condition, and the second one with
  CuDNN specific kernel, which can only run on GPU.

  The first function will be called with normal_lstm_params, while the second
  function is not called, but only registered in the graph. The Grappler will
  do the proper graph rewrite and swap the optimized TF function based on the
  device placement.

  Args:
    inputs: Input tensor of GRU layer.
    init_h: Initial state tensor for the cell output.
    kernel: Weights for cell kernel.
    recurrent_kernel: Weights for cell recurrent kernel.
    bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias
      is used in this case.
    mask: Boolean tensor for mask out the steps within sequence.
      An individual `True` entry indicates that the corresponding timestep
      should be utilized, while a `False` entry indicates that the corresponding
      timestep should be ignored.
    time_major: Boolean, whether the inputs are in the format of
      [time, batch, feature] or [batch, time, feature].
    go_backwards: Boolean (default False). If True, process the input sequence
      backwards and return the reversed sequence.
    sequence_lengths: The lengths of all sequences coming from a variable length
      input, such as ragged tensors. If the input has a fixed timestep size,
      this should be None.
    zero_output_for_mask: Boolean, whether to output zero for masked timestep.

  Returns:
    List of output tensors, same as standard_gru.
  """
  params = {
      'inputs': inputs,
      'init_h': init_h,
      'kernel': kernel,
      'recurrent_kernel': recurrent_kernel,
      'bias': bias,
      'mask': mask,
      'time_major': time_major,
      'go_backwards': go_backwards,
      'sequence_lengths': sequence_lengths,
      'zero_output_for_mask': zero_output_for_mask,
  }

  def gpu_gru_with_fallback(inputs, init_h, kernel, recurrent_kernel, bias,
                            mask, time_major, go_backwards, sequence_lengths,
                            zero_output_for_mask):
    """Use CuDNN kernel when mask is none or strictly right padded."""
    if mask is None:
      return gpu_gru(
          inputs=inputs,
          init_h=init_h,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

    def cudnn_gru_fn():
      return gpu_gru(
          inputs=inputs,
          init_h=init_h,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

    def standard_gru_fn():
      return standard_gru(
          inputs=inputs,
          init_h=init_h,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths,
          zero_output_for_mask=zero_output_for_mask)

    return tf.cond(
        is_cudnn_supported_inputs(mask, time_major),
        true_fn=cudnn_gru_fn,
        false_fn=standard_gru_fn)

  if _use_new_code():
    # Chooses the implementation dynamically based on the running device.
    (last_output, outputs, new_h,
     runtime) = tf.__internal__.execute_fn_for_device(
         {
             _CPU_DEVICE_NAME: lambda: standard_gru(**params),
             _GPU_DEVICE_NAME: lambda: gpu_gru_with_fallback(**params)
         }, lambda: standard_gru(**params))
  else:
    # Each time a `tf.function` is called, we will give it a unique
    # identifiable API name, so that Grappler won't get confused when it
    # sees multiple GRU layers added into same graph, and it will be able
    # to pair up the different implementations across them.
    api_name = 'gru_' + str(uuid.uuid4())
    supportive_attribute = {
        'time_major': time_major,
        'go_backwards': go_backwards,
    }
    defun_standard_gru = _generate_defun_backend(api_name, _CPU_DEVICE_NAME,
                                                 standard_gru,
                                                 supportive_attribute)
    defun_gpu_gru = _generate_defun_backend(api_name, _GPU_DEVICE_NAME,
                                            gpu_gru_with_fallback,
                                            supportive_attribute)

    # Call the normal GRU impl and register the CuDNN impl function. The
    # grappler will kick in during session execution to optimize the graph.
    last_output, outputs, new_h, runtime = defun_standard_gru(**params)
    _function_register(defun_gpu_gru, **params)

  return last_output, outputs, new_h, runtime


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4595')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent_v2.py: 1519-1653
</a>
<div class="mid" id="frag4595" style="display:none"><pre>
def lstm_with_backend_selection(inputs, init_h, init_c, kernel,
                                recurrent_kernel, bias, mask, time_major,
                                go_backwards, sequence_lengths,
                                zero_output_for_mask):
  """Call the LSTM with optimized backend kernel selection.

  Under the hood, this function will create two TF function, one with the most
  generic kernel and can run on all device condition, and the second one with
  CuDNN specific kernel, which can only run on GPU.

  The first function will be called with normal_lstm_params, while the second
  function is not called, but only registered in the graph. The Grappler will
  do the proper graph rewrite and swap the optimized TF function based on the
  device placement.

  Args:
    inputs: Input tensor of LSTM layer.
    init_h: Initial state tensor for the cell output.
    init_c: Initial state tensor for the cell hidden state.
    kernel: Weights for cell kernel.
    recurrent_kernel: Weights for cell recurrent kernel.
    bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias
      is used in this case.
    mask: Boolean tensor for mask out the steps within sequence.
      An individual `True` entry indicates that the corresponding timestep
      should be utilized, while a `False` entry indicates that the corresponding
      timestep should be ignored.
    time_major: Boolean, whether the inputs are in the format of
      [time, batch, feature] or [batch, time, feature].
    go_backwards: Boolean (default False). If True, process the input sequence
      backwards and return the reversed sequence.
    sequence_lengths: The lengths of all sequences coming from a variable length
      input, such as ragged tensors. If the input has a fixed timestep size,
      this should be None.
    zero_output_for_mask: Boolean, whether to output zero for masked timestep.

  Returns:
    List of output tensors, same as standard_lstm.
  """
  params = {
      'inputs': inputs,
      'init_h': init_h,
      'init_c': init_c,
      'kernel': kernel,
      'recurrent_kernel': recurrent_kernel,
      'bias': bias,
      'mask': mask,
      'time_major': time_major,
      'go_backwards': go_backwards,
      'sequence_lengths': sequence_lengths,
      'zero_output_for_mask': zero_output_for_mask,
  }

  def gpu_lstm_with_fallback(inputs, init_h, init_c, kernel, recurrent_kernel,
                             bias, mask, time_major, go_backwards,
                             sequence_lengths, zero_output_for_mask):
    """Use CuDNN kernel when mask is none or strictly right padded."""
    if mask is None:
      return gpu_lstm(
          inputs=inputs,
          init_h=init_h,
          init_c=init_c,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

    def cudnn_lstm_fn():
      return gpu_lstm(
          inputs=inputs,
          init_h=init_h,
          init_c=init_c,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

    def stardard_lstm_fn():
      return standard_lstm(
          inputs=inputs,
          init_h=init_h,
          init_c=init_c,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths,
          zero_output_for_mask=zero_output_for_mask)

    return tf.cond(
        is_cudnn_supported_inputs(mask, time_major),
        true_fn=cudnn_lstm_fn,
        false_fn=stardard_lstm_fn)

  if _use_new_code():
    # Chooses the implementation dynamically based on the running device.
    (last_output, outputs, new_h, new_c,
     runtime) = tf.__internal__.execute_fn_for_device(
         {
             _CPU_DEVICE_NAME: lambda: standard_lstm(**params),
             _GPU_DEVICE_NAME: lambda: gpu_lstm_with_fallback(**params)
         }, lambda: standard_lstm(**params))
  else:
    # Each time a `tf.function` is called, we will give it a unique
    # identifiable API name, so that Grappler won't get confused when it
    # sees multiple LSTM layers added into same graph, and it will be able
    # to pair up the different implementations across them.
    api_name = 'lstm_' + str(uuid.uuid4())
    supportive_attribute = {
        'time_major': time_major,
        'go_backwards': go_backwards,
    }
    defun_standard_lstm = _generate_defun_backend(api_name, _CPU_DEVICE_NAME,
                                                  standard_lstm,
                                                  supportive_attribute)
    defun_gpu_lstm = _generate_defun_backend(api_name, _GPU_DEVICE_NAME,
                                             gpu_lstm_with_fallback,
                                             supportive_attribute)

    # Call the normal LSTM impl and register the CuDNN impl function. The
    # grappler will kick in during session execution to optimize the graph.
    last_output, outputs, new_h, new_c, runtime = defun_standard_lstm(**params)
    _function_register(defun_gpu_lstm, **params)

  return last_output, outputs, new_h, new_c, runtime


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 242:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4584')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent_v2.py: 763-774
</a>
<div class="mid" id="frag4584" style="display:none"><pre>
    def cudnn_gru_fn():
      return gpu_gru(
          inputs=inputs,
          init_h=init_h,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4585')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent_v2.py: 775-787
</a>
<div class="mid" id="frag4585" style="display:none"><pre>
    def standard_gru_fn():
      return standard_gru(
          inputs=inputs,
          init_h=init_h,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths,
          zero_output_for_mask=zero_output_for_mask)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 243:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4597')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent_v2.py: 1589-1601
</a>
<div class="mid" id="frag4597" style="display:none"><pre>
    def cudnn_lstm_fn():
      return gpu_lstm(
          inputs=inputs,
          init_h=init_h,
          init_c=init_c,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4598')" href="javascript:;">
keras-2.6.0/keras/layers/recurrent_v2.py: 1602-1615
</a>
<div class="mid" id="frag4598" style="display:none"><pre>
    def stardard_lstm_fn():
      return standard_lstm(
          inputs=inputs,
          init_h=init_h,
          init_c=init_c,
          kernel=kernel,
          recurrent_kernel=recurrent_kernel,
          bias=bias,
          mask=mask,
          time_major=time_major,
          go_backwards=go_backwards,
          sequence_lengths=sequence_lengths,
          zero_output_for_mask=zero_output_for_mask)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 244:</b> &nbsp; 3 fragments, nominal size 15 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4622')" href="javascript:;">
keras-2.6.0/keras/layers/advanced_activations_test.py: 91-109
</a>
<div class="mid" id="frag4622" style="display:none"><pre>
  def test_relu_with_invalid_negative_slope(self):
    with self.assertRaisesRegex(
        ValueError, 'negative_slope of a ReLU layer cannot be a negative '
        'value. Got: None'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'negative_slope': None},
          input_shape=(2, 3, 4),
          supports_masking=True)

    with self.assertRaisesRegex(
        ValueError, 'negative_slope of a ReLU layer cannot be a negative '
        'value. Got: -10'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'negative_slope': -10},
          input_shape=(2, 3, 4),
          supports_masking=True)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4623')" href="javascript:;">
keras-2.6.0/keras/layers/advanced_activations_test.py: 110-128
</a>
<div class="mid" id="frag4623" style="display:none"><pre>
  def test_relu_with_invalid_threshold(self):
    with self.assertRaisesRegex(
        ValueError, 'threshold of a ReLU layer cannot be a negative '
        'value. Got: None'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'threshold': None},
          input_shape=(2, 3, 4),
          supports_masking=True)

    with self.assertRaisesRegex(
        ValueError, 'threshold of a ReLU layer cannot be a negative '
        'value. Got: -10'):
      testing_utils.layer_test(
          keras.layers.ReLU,
          kwargs={'threshold': -10},
          input_shape=(2, 3, 4),
          supports_masking=True)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4627')" href="javascript:;">
keras-2.6.0/keras/layers/advanced_activations_test.py: 161-180
</a>
<div class="mid" id="frag4627" style="display:none"><pre>
  def test_threshold_relu_with_invalid_theta(self):
    with self.assertRaisesRegex(
        ValueError, 'Theta of a Thresholded ReLU layer cannot '
        'be None, requires a float. Got None'):
      testing_utils.layer_test(
          keras.layers.ThresholdedReLU,
          kwargs={'theta': None},
          input_shape=(2, 3, 4),
          supports_masking=True)

    with self.assertRaisesRegex(
        ValueError, 'The theta value of a Thresholded ReLU '
        'layer should be &gt;=0, got -10'):
      testing_utils.layer_test(
          keras.layers.ThresholdedReLU,
          kwargs={'theta': -10},
          input_shape=(2, 3, 4),
          supports_masking=True)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 245:</b> &nbsp; 2 fragments, nominal size 30 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4629')" href="javascript:;">
keras-2.6.0/keras/layers/merge_test.py: 65-99
</a>
<div class="mid" id="frag4629" style="display:none"><pre>
  def test_merge_subtract(self):
    i1 = keras.layers.Input(shape=(4, 5))
    i2 = keras.layers.Input(shape=(4, 5))
    i3 = keras.layers.Input(shape=(4, 5))

    subtract_layer = keras.layers.Subtract()
    o = subtract_layer([i1, i2])
    self.assertListEqual(o.shape.as_list(), [None, 4, 5])
    model = keras.models.Model([i1, i2], o)
    model.run_eagerly = testing_utils.should_run_eagerly()

    x1 = np.random.random((2, 4, 5))
    x2 = np.random.random((2, 4, 5))
    out = model.predict([x1, x2])
    self.assertEqual(out.shape, (2, 4, 5))
    self.assertAllClose(out, x1 - x2, atol=1e-4)

    self.assertEqual(subtract_layer.compute_mask([i1, i2], [None, None]), None)
    self.assertTrue(
        np.all(
            backend.eval(
                subtract_layer.compute_mask(
                    [i1, i2], [backend.variable(x1), backend.variable(x2)]))))

    with self.assertRaisesRegex(ValueError, '`mask` should be a list.'):
      subtract_layer.compute_mask([i1, i2], x1)
    with self.assertRaisesRegex(ValueError, '`inputs` should be a list.'):
      subtract_layer.compute_mask(i1, [None, None])
    with self.assertRaisesRegex(ValueError,
                                'layer should be called on exactly 2 inputs'):
      subtract_layer([i1, i2, i3])
    with self.assertRaisesRegex(ValueError,
                                'layer should be called on exactly 2 inputs'):
      subtract_layer([i1])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4634')" href="javascript:;">
keras-2.6.0/keras/layers/merge_test.py: 158-193
</a>
<div class="mid" id="frag4634" style="display:none"><pre>
  def test_merge_concatenate(self):
    i1 = keras.layers.Input(shape=(4, 5))
    i2 = keras.layers.Input(shape=(4, 5))
    concat_layer = keras.layers.Concatenate(axis=1)
    o = concat_layer([i1, i2])
    self.assertListEqual(o.shape.as_list(), [None, 8, 5])
    model = keras.models.Model([i1, i2], o)
    model.run_eagerly = testing_utils.should_run_eagerly()

    x1 = np.random.random((2, 4, 5))
    x2 = np.random.random((2, 4, 5))
    out = model.predict([x1, x2])
    self.assertEqual(out.shape, (2, 8, 5))
    self.assertAllClose(out, np.concatenate([x1, x2], axis=1), atol=1e-4)

    self.assertEqual(concat_layer.compute_mask([i1, i2], [None, None]), None)
    self.assertTrue(
        np.all(
            backend.eval(
                concat_layer.compute_mask(
                    [i1, i2], [backend.variable(x1), backend.variable(x2)]))))

    # Should work with unit-length input.
    unit_length_o = concat_layer([i1])
    self.assertListEqual(unit_length_o.shape.as_list(), i1.shape.as_list())

    with self.assertRaisesRegex(ValueError, '`mask` should be a list.'):
      concat_layer.compute_mask([i1, i2], x1)
    with self.assertRaisesRegex(ValueError, '`inputs` should be a list.'):
      concat_layer.compute_mask(i1, [None, None])
    with self.assertRaisesRegex(ValueError, 'should have the same length'):
      concat_layer.compute_mask([i1, i2], [None])
    with self.assertRaisesRegex(ValueError,
                                'layer should be called on a list of inputs'):
      concat_layer(i1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 246:</b> &nbsp; 3 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4631')" href="javascript:;">
keras-2.6.0/keras/layers/merge_test.py: 116-129
</a>
<div class="mid" id="frag4631" style="display:none"><pre>
  def test_merge_average(self):
    i1 = keras.layers.Input(shape=(4, 5))
    i2 = keras.layers.Input(shape=(4, 5))
    o = keras.layers.average([i1, i2])
    self.assertListEqual(o.shape.as_list(), [None, 4, 5])
    model = keras.models.Model([i1, i2], o)
    model.run_eagerly = testing_utils.should_run_eagerly()

    x1 = np.random.random((2, 4, 5))
    x2 = np.random.random((2, 4, 5))
    out = model.predict([x1, x2])
    self.assertEqual(out.shape, (2, 4, 5))
    self.assertAllClose(out, 0.5 * (x1 + x2), atol=1e-4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4632')" href="javascript:;">
keras-2.6.0/keras/layers/merge_test.py: 130-143
</a>
<div class="mid" id="frag4632" style="display:none"><pre>
  def test_merge_maximum(self):
    i1 = keras.layers.Input(shape=(4, 5))
    i2 = keras.layers.Input(shape=(4, 5))
    o = keras.layers.maximum([i1, i2])
    self.assertListEqual(o.shape.as_list(), [None, 4, 5])
    model = keras.models.Model([i1, i2], o)
    model.run_eagerly = testing_utils.should_run_eagerly()

    x1 = np.random.random((2, 4, 5))
    x2 = np.random.random((2, 4, 5))
    out = model.predict([x1, x2])
    self.assertEqual(out.shape, (2, 4, 5))
    self.assertAllClose(out, np.maximum(x1, x2), atol=1e-4)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4633')" href="javascript:;">
keras-2.6.0/keras/layers/merge_test.py: 144-157
</a>
<div class="mid" id="frag4633" style="display:none"><pre>
  def test_merge_minimum(self):
    i1 = keras.layers.Input(shape=(4, 5))
    i2 = keras.layers.Input(shape=(4, 5))
    o = keras.layers.minimum([i1, i2])
    self.assertListEqual(o.shape.as_list(), [None, 4, 5])
    model = keras.models.Model([i1, i2], o)
    model.run_eagerly = testing_utils.should_run_eagerly()

    x1 = np.random.random((2, 4, 5))
    x2 = np.random.random((2, 4, 5))
    out = model.predict([x1, x2])
    self.assertEqual(out.shape, (2, 4, 5))
    self.assertAllClose(out, np.minimum(x1, x2), atol=1e-4)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 247:</b> &nbsp; 3 fragments, nominal size 62 lines, similarity 79%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4670')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/normalization_test.py: 33-113
</a>
<div class="mid" id="frag4670" style="display:none"><pre>
def _get_layer_computation_test_cases():
  test_cases = ({
      "adapt_data": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),
      "axis": -1,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element"
  }, {
      "adapt_data": np.array([[1], [2], [3], [4], [5]], dtype=np.int32),
      "axis": -1,
      "test_data": np.array([[1], [2], [3]], np.int32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_int_data"
  }, {
      "adapt_data": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),
      "axis": None,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element_none_axis"
  }, {
      "adapt_data": np.array([[1., 2., 3., 4., 5.]], dtype=np.float32),
      "axis": None,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element_none_axis_flat_data"
  }, {
      "adapt_data":
          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],
                   np.float32),
      "axis":
          1,
      "test_data":
          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],
                   np.float32),
      "expected":
          np.array([[[-1.549193, -0.774597, 0.], [-1.549193, -0.774597, 0.]],
                    [[0., 0.774597, 1.549193], [0., 0.774597, 1.549193]]],
                   np.float32),
      "testcase_name":
          "3d_internal_axis"
  }, {
      "adapt_data":
          np.array(
              [[[1., 0., 3.], [2., 3., 4.]], [[3., -1., 5.], [4., 5., 8.]]],
              np.float32),
      "axis": (1, 2),
      "test_data":
          np.array(
              [[[3., 1., -1.], [2., 5., 4.]], [[3., 0., 5.], [2., 5., 8.]]],
              np.float32),
      "expected":
          np.array(
              [[[1., 3., -5.], [-1., 1., -1.]], [[1., 1., 1.], [-1., 1., 1.]]],
              np.float32),
      "testcase_name":
          "3d_multiple_axis"
  }, {
      "adapt_data":
          np.zeros((3, 4)),
      "axis": -1,
      "test_data":
          np.zeros((3, 4)),
      "expected":
          np.zeros((3, 4)),
      "testcase_name":
          "zero_variance"
  })

  crossed_test_cases = []
  # Cross above test cases with use_dataset in (True, False)
  for use_dataset in (True, False):
    for case in test_cases:
      case = case.copy()
      if use_dataset:
        case["testcase_name"] = case["testcase_name"] + "_with_dataset"
      case["use_dataset"] = use_dataset
      crossed_test_cases.append(case)

  return crossed_test_cases


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4911')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/normalization_tpu_test.py: 30-94
</a>
<div class="mid" id="frag4911" style="display:none"><pre>
def _get_layer_computation_test_cases():
  test_cases = ({
      "adapt_data": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),
      "axis": -1,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element"
  }, {
      "adapt_data": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),
      "axis": None,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element_none_axis"
  }, {
      "adapt_data": np.array([[1., 2., 3., 4., 5.]], dtype=np.float32),
      "axis": None,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element_none_axis_flat_data"
  }, {
      "adapt_data":
          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],
                   np.float32),
      "axis":
          1,
      "test_data":
          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],
                   np.float32),
      "expected":
          np.array([[[-1.549193, -0.774597, 0.], [-1.549193, -0.774597, 0.]],
                    [[0., 0.774597, 1.549193], [0., 0.774597, 1.549193]]],
                   np.float32),
      "testcase_name":
          "3d_internal_axis"
  }, {
      "adapt_data":
          np.array(
              [[[1., 0., 3.], [2., 3., 4.]], [[3., -1., 5.], [4., 5., 8.]]],
              np.float32),
      "axis": (1, 2),
      "test_data":
          np.array(
              [[[3., 1., -1.], [2., 5., 4.]], [[3., 0., 5.], [2., 5., 8.]]],
              np.float32),
      "expected":
          np.array(
              [[[1., 3., -5.], [-1., 1., -1.]], [[1., 1., 1.], [-1., 1., 1.]]],
              np.float32),
      "testcase_name":
          "3d_multiple_axis"
  })

  crossed_test_cases = []
  # Cross above test cases with use_dataset in (True, False)
  for use_dataset in (True, False):
    for case in test_cases:
      case = case.copy()
      if use_dataset:
        case["testcase_name"] = case["testcase_name"] + "_with_dataset"
      case["use_dataset"] = use_dataset
      crossed_test_cases.append(case)

  return crossed_test_cases


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5098')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/normalization_distribution_test.py: 28-92
</a>
<div class="mid" id="frag5098" style="display:none"><pre>
def _get_layer_computation_test_cases():
  test_cases = ({
      "adapt_data": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),
      "axis": -1,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element"
  }, {
      "adapt_data": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),
      "axis": None,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element_none_axis"
  }, {
      "adapt_data": np.array([[1., 2., 3., 4., 5.]], dtype=np.float32),
      "axis": None,
      "test_data": np.array([[1.], [2.], [3.]], np.float32),
      "expected": np.array([[-1.414214], [-.707107], [0]], np.float32),
      "testcase_name": "2d_single_element_none_axis_flat_data"
  }, {
      "adapt_data":
          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],
                   np.float32),
      "axis":
          1,
      "test_data":
          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],
                   np.float32),
      "expected":
          np.array([[[-1.549193, -0.774597, 0.], [-1.549193, -0.774597, 0.]],
                    [[0., 0.774597, 1.549193], [0., 0.774597, 1.549193]]],
                   np.float32),
      "testcase_name":
          "3d_internal_axis"
  }, {
      "adapt_data":
          np.array(
              [[[1., 0., 3.], [2., 3., 4.]], [[3., -1., 5.], [4., 5., 8.]]],
              np.float32),
      "axis": (1, 2),
      "test_data":
          np.array(
              [[[3., 1., -1.], [2., 5., 4.]], [[3., 0., 5.], [2., 5., 8.]]],
              np.float32),
      "expected":
          np.array(
              [[[1., 3., -5.], [-1., 1., -1.]], [[1., 1., 1.], [-1., 1., 1.]]],
              np.float32),
      "testcase_name":
          "3d_multiple_axis"
  })

  crossed_test_cases = []
  # Cross above test cases with use_dataset in (True, False)
  for use_dataset in (True, False):
    for case in test_cases:
      case = case.copy()
      if use_dataset:
        case["testcase_name"] = case["testcase_name"] + "_with_dataset"
      case["use_dataset"] = use_dataset
      crossed_test_cases.append(case)

  return crossed_test_cases


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 248:</b> &nbsp; 4 fragments, nominal size 16 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4681')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/normalization_test.py: 207-226
</a>
<div class="mid" id="frag4681" style="display:none"><pre>
  def test_layer_computation(self, adapt_data, axis, test_data, use_dataset,
                             expected):
    input_shape = tuple([test_data.shape[i] for i in range(1, test_data.ndim)])
    if use_dataset:
      # Keras APIs expect batched datasets
      adapt_data = tf.data.Dataset.from_tensor_slices(adapt_data).batch(
          test_data.shape[0] // 2)
      test_data = tf.data.Dataset.from_tensor_slices(test_data).batch(
          test_data.shape[0] // 2)

    layer = normalization.Normalization(axis=axis)
    layer.adapt(adapt_data)

    input_data = keras.Input(shape=input_shape)
    output = layer(input_data)
    model = keras.Model(input_data, output)
    model._run_eagerly = testing_utils.should_run_eagerly()
    output_data = model.predict(test_data)
    self.assertAllClose(expected, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4912')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/normalization_tpu_test.py: 101-122
</a>
<div class="mid" id="frag4912" style="display:none"><pre>
  def test_layer_computation(self, adapt_data, axis, test_data, use_dataset,
                             expected):
    input_shape = tuple([None for _ in range(test_data.ndim - 1)])
    if use_dataset:
      # Keras APIs expect batched datasets
      adapt_data = tf.data.Dataset.from_tensor_slices(adapt_data).batch(
          test_data.shape[0] // 2)
      test_data = tf.data.Dataset.from_tensor_slices(test_data).batch(
          test_data.shape[0] // 2)

    strategy = tpu_strategy_test_utils.get_tpu_strategy()

    with strategy.scope():
      input_data = keras.Input(shape=input_shape)
      layer = normalization.Normalization(axis=axis)
      layer.adapt(adapt_data)
      output = layer(input_data)
      model = keras.Model(input_data, output)
      output_data = model.predict(test_data)
    self.assertAllClose(expected, output_data)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5099')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/normalization_distribution_test.py: 102-121
</a>
<div class="mid" id="frag5099" style="display:none"><pre>
  def test_layer_computation(self, strategy, adapt_data, axis, test_data,
                             use_dataset, expected):
    input_shape = tuple([None for _ in range(test_data.ndim - 1)])
    if use_dataset:
      # Keras APIs expect batched datasets
      adapt_data = tf.data.Dataset.from_tensor_slices(adapt_data).batch(
          test_data.shape[0] // 2)
      test_data = tf.data.Dataset.from_tensor_slices(test_data).batch(
          test_data.shape[0] // 2)

    with strategy.scope():
      input_data = keras.Input(shape=input_shape)
      layer = normalization.Normalization(axis=axis)
      layer.adapt(adapt_data)
      output = layer(input_data)
      model = keras.Model(input_data, output)
      output_data = model.predict(test_data)
    self.assertAllClose(expected, output_data)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4848')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/discretization_test.py: 205-226
</a>
<div class="mid" id="frag4848" style="display:none"><pre>
  def test_layer_computation(self, adapt_data, test_data, use_dataset,
                             expected, num_bins=5, epsilon=0.01):

    input_shape = tuple(list(test_data.shape)[1:])
    np.random.shuffle(adapt_data)
    if use_dataset:
      # Keras APIs expect batched datasets
      adapt_data = tf.data.Dataset.from_tensor_slices(adapt_data).batch(
          test_data.shape[0] // 2)
      test_data = tf.data.Dataset.from_tensor_slices(test_data).batch(
          test_data.shape[0] // 2)

    layer = discretization.Discretization(epsilon=epsilon, num_bins=num_bins)
    layer.adapt(adapt_data)

    input_data = keras.Input(shape=input_shape)
    output = layer(input_data)
    model = keras.Model(input_data, output)
    model._run_eagerly = testing_utils.should_run_eagerly()
    output_data = model.predict(test_data)
    self.assertAllClose(expected, output_data)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 249:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4686')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/normalization_test.py: 276-299
</a>
<div class="mid" id="frag4686" style="display:none"><pre>
  def test_multiple_adapts(self):
    first_adapt = [[0], [2], [0], [2]]
    second_adapt = [[2], [4], [2], [4]]
    predict_input = [[2], [2]]
    expected_first_output = [[1], [1]]
    expected_second_output = [[-1], [-1]]

    inputs = keras.Input(shape=(1,), dtype=tf.int32)
    layer = normalization.Normalization(axis=-1)
    layer.adapt(first_adapt)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)

    actual_output = model.predict(predict_input)
    self.assertAllClose(actual_output, expected_first_output)

    # Re-adapt the layer on new inputs.
    layer.adapt(second_adapt)
    # Re-compile the model.
    model.compile()
    # `predict` should now use the new model state.
    actual_output = model.predict(predict_input)
    self.assertAllClose(actual_output, expected_second_output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4849')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/discretization_test.py: 227-250
</a>
<div class="mid" id="frag4849" style="display:none"><pre>
  def test_multiple_adapts(self):
    first_adapt = [[1], [2], [3]]
    second_adapt = [[4], [5], [6]]
    predict_input = [[2], [2]]
    expected_first_output = [[2], [2]]
    expected_second_output = [[0], [0]]

    inputs = keras.Input(shape=(1,), dtype=tf.int32)
    layer = discretization.Discretization(num_bins=3)
    layer.adapt(first_adapt)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)

    actual_output = model.predict(predict_input)
    self.assertAllClose(actual_output, expected_first_output)

    # Re-adapt the layer on new inputs.
    layer.adapt(second_adapt)
    # Re-compile the model.
    model.compile()
    # `predict` should now use the new model state.
    actual_output = model.predict(predict_input)
    self.assertAllClose(actual_output, expected_second_output)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 250:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4688')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/normalization_test.py: 337-366
</a>
<div class="mid" id="frag4688" style="display:none"><pre>
  def test_saved_model_keras(self, adapted):
    input_data = [[0.], [2.], [0.], [2.]]
    expected_output = [[-1.], [1.], [-1.], [1.]]

    cls = normalization.Normalization
    inputs = keras.Input(shape=(1,), dtype=tf.float32)
    if adapted:
      layer = cls(axis=-1)
      layer.adapt(input_data)
    else:
      layer = cls(mean=1., variance=1.)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)

    output_data = model.predict(input_data)
    self.assertAllClose(output_data, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")
    loaded_model = keras.models.load_model(
        output_path, custom_objects={"Normalization": cls})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_data = loaded_model.predict(input_data)
    self.assertAllClose(new_output_data, expected_output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4689')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/normalization_test.py: 371-399
</a>
<div class="mid" id="frag4689" style="display:none"><pre>
  def test_saved_weights_keras(self, adapted):
    input_data = [[0.], [2.], [0.], [2.]]
    expected_output = [[-1.], [1.], [-1.], [1.]]

    cls = normalization.Normalization
    inputs = keras.Input(shape=(1,), dtype=tf.float32)
    if adapted:
      layer = cls(axis=-1)
      layer.adapt(input_data)
    else:
      layer = cls(mean=1., variance=1.)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)

    output_data = model.predict(input_data)
    self.assertAllClose(output_data, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_weights")
    model.save_weights(output_path, save_format="tf")
    new_model = keras.Model.from_config(
        model.get_config(), custom_objects={"Normalization": cls})
    new_model.load_weights(output_path)

    # Validate correctness of the new model.
    new_output_data = new_model.predict(input_data)
    self.assertAllClose(new_output_data, expected_output)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 251:</b> &nbsp; 4 fragments, nominal size 23 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4704')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 77-113
</a>
<div class="mid" id="frag4704" style="display:none"><pre>
  def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,
                                       use_dataset, expected_output,
                                       input_dtype):
    cls = integer_lookup.IntegerLookup
    expected_output_dtype = tf.int64
    input_shape = input_data.shape

    if use_dataset:
      # Keras APIs expect batched datasets.
      # TODO(rachelim): `model.predict` predicts the result on each
      # dataset batch separately, then tries to concatenate the results
      # together. When the results have different shapes on the non-concat
      # axis (which can happen in the output_mode = INT case for
      # IntegerLookup), the concatenation fails. In real use cases, this may
      # not be an issue because users are likely to pipe the preprocessing layer
      # into other keras layers instead of predicting it directly. A workaround
      # for these unit tests is to have the dataset only contain one batch, so
      # no concatenation needs to happen with the result. For consistency with
      # numpy input, we should make `predict` join differently shaped results
      # together sensibly, with 0 padding.
      input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(
          input_shape[0])
      vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(
          input_shape[0])

    with CustomObjectScope({"IntegerLookup": cls}):
      output_data = testing_utils.layer_test(
          cls,
          kwargs=kwargs,
          input_shape=input_shape,
          input_data=input_data,
          input_dtype=input_dtype,
          expected_output_dtype=expected_output_dtype,
          validate_training=False,
          adapt_data=vocab_data)
    self.assertAllClose(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4759')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 275-312
</a>
<div class="mid" id="frag4759" style="display:none"><pre>
  def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,
                                       use_dataset, expected_output):
    cls = text_vectorization.TextVectorization
    if kwargs.get("output_mode") == text_vectorization.INT:
      expected_output_dtype = tf.int64
    else:
      expected_output_dtype = tf.float32
    input_shape = input_data.shape

    if use_dataset:
      # Keras APIs expect batched datasets.
      # TODO(rachelim): `model.predict` predicts the result on each
      # dataset batch separately, then tries to concatenate the results
      # together. When the results have different shapes on the non-concat
      # axis (which can happen in the output_mode = INT case for
      # TextVectorization), the concatenation fails. In real use cases, this may
      # not be an issue because users are likely to pipe the preprocessing layer
      # into other keras layers instead of predicting it directly. A workaround
      # for these unit tests is to have the dataset only contain one batch, so
      # no concatenation needs to happen with the result. For consistency with
      # numpy input, we should make `predict` join differently shaped results
      # together sensibly, with 0 padding.
      input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(
          input_shape[0])
      vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(
          input_shape[0])

    output_data = testing_utils.layer_test(
        cls,
        kwargs=kwargs,
        input_shape=input_shape,
        input_data=input_data,
        input_dtype=tf.string,
        expected_output_dtype=expected_output_dtype,
        validate_training=False,
        adapt_data=vocab_data)
    self.assertAllClose(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5230')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 302-348
</a>
<div class="mid" id="frag5230" style="display:none"><pre>
  def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,
                                       use_dataset, expected_output,
                                       input_dtype):
    cls = index_lookup.IndexLookup
    if "invert" in kwargs and kwargs["invert"]:
      expected_output_dtype = kwargs["dtype"]
    elif "output_mode" in kwargs and kwargs["output_mode"] != index_lookup.INT:
      expected_output_dtype = tf.float32
    else:
      expected_output_dtype = tf.int64

    input_shape = input_data.shape

    if use_dataset:
      # Keras APIs expect batched datasets.
      # TODO(rachelim): `model.predict` predicts the result on each
      # dataset batch separately, then tries to concatenate the results
      # together. When the results have different shapes on the non-concat
      # axis (which can happen in the output_mode = INT case for
      # IndexLookup), the concatenation fails. In real use cases, this may
      # not be an issue because users are likely to pipe the preprocessing layer
      # into other keras layers instead of predicting it directly. A workaround
      # for these unit tests is to have the dataset only contain one batch, so
      # no concatenation needs to happen with the result. For consistency with
      # numpy input, we should make `predict` join differently shaped results
      # together sensibly, with 0 padding.
      input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(
          input_shape[0])
      vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(
          input_shape[0])

    with CustomObjectScope({"IndexLookup": cls}):
      output_data = testing_utils.layer_test(
          cls,
          kwargs=kwargs,
          input_shape=input_shape,
          input_data=input_data,
          input_dtype=input_dtype,
          expected_output_dtype=expected_output_dtype,
          validate_training=False,
          adapt_data=vocab_data)
    if "invert" in kwargs and kwargs["invert"]:
      self.assertAllEqual(expected_output, output_data)
    else:
      self.assertAllClose(expected_output, output_data)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4914')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/string_lookup_test.py: 71-108
</a>
<div class="mid" id="frag4914" style="display:none"><pre>
  def test_layer_end_to_end_with_adapt(self, vocab_data, input_data, kwargs,
                                       use_dataset, expected_output,
                                       input_dtype):
    cls = string_lookup.StringLookup
    expected_output_dtype = tf.int64
    input_shape = input_data.shape

    if use_dataset:
      # Keras APIs expect batched datasets.
      # TODO(rachelim): `model.predict` predicts the result on each
      # dataset batch separately, then tries to concatenate the results
      # together. When the results have different shapes on the non-concat
      # axis (which can happen in the output_mode = INT case for
      # StringLookup), the concatenation fails. In real use cases, this may
      # not be an issue because users are likely to pipe the preprocessing layer
      # into other keras layers instead of predicting it directly. A workaround
      # for these unit tests is to have the dataset only contain one batch, so
      # no concatenation needs to happen with the result. For consistency with
      # numpy input, we should make `predict` join differently shaped results
      # together sensibly, with 0 padding.
      input_data = tf.data.Dataset.from_tensor_slices(input_data).batch(
          input_shape[0])
      vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(
          input_shape[0])

    with CustomObjectScope({"StringLookup": cls}):
      output_data = testing_utils.layer_test(
          cls,
          kwargs=kwargs,
          input_shape=input_shape,
          input_data=input_data,
          input_dtype=input_dtype,
          expected_output_dtype=expected_output_dtype,
          validate_training=False,
          adapt_data=vocab_data)
    self.assertAllClose(expected_output, output_data)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 252:</b> &nbsp; 7 fragments, nominal size 21 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4706')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 128-148
</a>
<div class="mid" id="frag4706" style="display:none"><pre>
  def test_sparse_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 32], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [4, 0]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = integer_lookup.IntegerLookup(max_tokens=None)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5242')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 596-621
</a>
<div class="mid" id="frag5242" style="display:none"><pre>
  def test_sparse_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 32], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4708')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 169-194
</a>
<div class="mid" id="frag4708" style="display:none"><pre>
  def test_sparse_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 133], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [6, 2]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = integer_lookup.IntegerLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=2,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5237')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 493-518
</a>
<div class="mid" id="frag5237" style="display:none"><pre>
  def test_sparse_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 133], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [6, 2]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=2,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5232')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 380-405
</a>
<div class="mid" id="frag5232" style="display:none"><pre>
  def test_sparse_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=np.array([13, 32], dtype=np.int64),
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5231')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 354-379
</a>
<div class="mid" id="frag5231" style="display:none"><pre>
  def test_sparse_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]],
        values=["fire", "michigan"],
        dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [5, 1]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.string, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5236')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 469-492
</a>
<div class="mid" id="frag5236" style="display:none"><pre>
  def test_sparse_string_input_multi_bucket(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.SparseTensor(
        indices=[[0, 0], [1, 2]], values=["fire", "ohio"], dense_shape=[3, 4])

    expected_indices = [[0, 0], [1, 2]]
    expected_values = [6, 2]
    expected_dense_shape = [3, 4]

    input_data = keras.Input(shape=(None,), dtype=tf.string, sparse=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_indices, output_data.indices)
    self.assertAllEqual(expected_values, output_data.values)
    self.assertAllEqual(expected_dense_shape, output_data.dense_shape)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 253:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4707')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 149-163
</a>
<div class="mid" id="frag4707" style="display:none"><pre>
  def test_ragged_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                              dtype=np.int64)
    expected_output = [[1, 2, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = integer_lookup.IntegerLookup(max_tokens=None)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4709')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 195-209
</a>
<div class="mid" id="frag4709" style="display:none"><pre>
  def test_ragged_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 133]],
                                              dtype=np.int64)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = integer_lookup.IntegerLookup(max_tokens=None, num_oov_indices=2)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 254:</b> &nbsp; 21 fragments, nominal size 11 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4714')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 256-268
</a>
<div class="mid" id="frag4714" style="display:none"><pre>
  def test_int_output(self):
    vocab_data = [42, 1138, 725, 1729]
    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup()
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4722')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 364-380
</a>
<div class="mid" id="frag4722" style="display:none"><pre>
  def test_forward_backward_adapted_vocab(self):
    adapt_data = [42, 1138, 725, 1729]
    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = np.array([[42, 1138, 725, 1729], [1729, 725, 42, -1]])

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup()
    layer.adapt(adapt_data)
    inverse_layer = integer_lookup.IntegerLookup(
        vocabulary=layer.get_vocabulary(), invert=True)
    int_data = layer(input_data)
    inverse_data = inverse_layer(int_data)
    model = keras.Model(inputs=input_data, outputs=inverse_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4728')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 445-457
</a>
<div class="mid" id="frag4728" style="display:none"><pre>
  def test_count_output(self):
    vocab_data = [2, 3, 4, 5]
    input_array = np.array([[2, 2, 3, 4], [0, 1, 5, 6]])
    expected_output = [[0, 2, 1, 1, 0], [3, 0, 0, 0, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(
        vocabulary=vocab_data, output_mode="count")
    res = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=res)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4727')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 432-444
</a>
<div class="mid" id="frag4727" style="display:none"><pre>
  def test_multi_hot_output(self):
    vocab_data = [2, 3, 4, 5]
    input_array = np.array([[2, 2, 3, 4], [0, 1, 5, 2]])
    expected_output = [[0, 1, 1, 1, 0], [1, 1, 0, 0, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(
        vocabulary=vocab_data, output_mode="multi_hot")
    res = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=res)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4724')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 395-406
</a>
<div class="mid" id="frag4724" style="display:none"><pre>
  def test_int_output_explicit_vocab(self):
    vocab_data = [42, 1138, 725, 1729]
    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(vocabulary=vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4732')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 489-502
</a>
<div class="mid" id="frag4732" style="display:none"><pre>
  def test_int_output_inverted_vocab_from_file(self):
    vocab_list = [42, 1138, 725, 1729]
    vocab_path = self._write_to_temp_file("vocab_file", vocab_list)

    input_array = np.array([[1, 2, 3, 4], [4, 3, 1, 0]])
    expected_output = [[42, 1138, 725, 1729], [1729, 725, 42, -1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(vocabulary=vocab_path, invert=True)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4731')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 475-488
</a>
<div class="mid" id="frag4731" style="display:none"><pre>
  def test_int_output_explicit_vocab_from_file(self):
    vocab_list = [42, 1138, 725, 1729]
    vocab_path = self._write_to_temp_file("vocab_file", vocab_list)

    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(vocabulary=vocab_path)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4720')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 336-348
</a>
<div class="mid" id="frag4720" style="display:none"><pre>
  def test_inverse_output(self):
    vocab_data = [-1, 42, 1138, 725, 1729]
    input_array = np.array([[1, 2, 3, 4], [4, 3, 1, 0]])
    expected_output = np.array([[42, 1138, 725, 1729], [1729, 725, 42, -1]])

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(invert=True)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4734')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 518-532
</a>
<div class="mid" id="frag4734" style="display:none"><pre>
  def test_int_output_explicit_vocab_from_file_via_setter(self):
    vocab_list = [42, 1138, 725, 1729]
    vocab_path = self._write_to_temp_file("vocab_file", vocab_list)

    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup()
    layer.set_vocabulary(vocab_path)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4716')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 275-287
</a>
<div class="mid" id="frag4716" style="display:none"><pre>
  def test_int_output_with_mask(self):
    vocab_data = [42, 1138, 725, 1729]
    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(max_tokens=None, mask_token=0)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4721')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 349-363
</a>
<div class="mid" id="frag4721" style="display:none"><pre>
  def test_forward_backward_explicit_vocab(self):
    vocab_data = [42, 1138, 725, 1729]
    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = np.array([[42, 1138, 725, 1729], [1729, 725, 42, -1]])

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(vocabulary=vocab_data)
    inverse_layer = integer_lookup.IntegerLookup(
        vocabulary=vocab_data, invert=True)
    int_data = layer(input_data)
    inverse_data = inverse_layer(int_data)
    model = keras.Model(inputs=input_data, outputs=inverse_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4733')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 503-517
</a>
<div class="mid" id="frag4733" style="display:none"><pre>
  def test_int_output_inverted_vocab_from_file_with_mask(self):
    vocab_list = [42, 1138, 725, 1729]
    vocab_path = self._write_to_temp_file("vocab_file", vocab_list)

    input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 0]])
    expected_output = [[42, 1138, 725, 1729], [1729, 725, 42, -10]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(
        vocabulary=vocab_path, invert=True, mask_value=-10)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4922')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/string_lookup_test.py: 207-220
</a>
<div class="mid" id="frag4922" style="display:none"><pre>
  def test_count_output(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "earth", "fire", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[0, 2, 0, 0, 2], [1, 1, 0, 1, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup(
        vocabulary=vocab_data, output_mode="count")
    res = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=res)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4921')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/string_lookup_test.py: 193-206
</a>
<div class="mid" id="frag4921" style="display:none"><pre>
  def test_multi_hot_output(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[0, 1, 1, 1, 1], [1, 1, 0, 1, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup(
        vocabulary=vocab_data, output_mode="multi_hot")
    res = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=res)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4916')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/string_lookup_test.py: 123-135
</a>
<div class="mid" id="frag4916" style="display:none"><pre>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup(vocabulary=vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4925')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/string_lookup_test.py: 244-258
</a>
<div class="mid" id="frag4925" style="display:none"><pre>
  def test_int_output_explicit_vocab_from_file(self):
    vocab_list = ["earth", "wind", "and", "fire"]
    vocab_path = self._write_to_temp_file("vocab_file", vocab_list)

    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup(vocabulary=vocab_path)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4926')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/string_lookup_test.py: 259-274
</a>
<div class="mid" id="frag4926" style="display:none"><pre>
  def test_int_output_explicit_vocab_from_file_via_setter(self):
    vocab_list = ["earth", "wind", "and", "fire"]
    vocab_path = self._write_to_temp_file("vocab_file", vocab_list)

    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup()
    layer.set_vocabulary(vocab_path)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4931')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/string_lookup_test.py: 316-330
</a>
<div class="mid" id="frag4931" style="display:none"><pre>
  def test_inverse_layer_from_file_with_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 0]])
    expected_output = np.array([["earth", "wind", "and", "fire"],
                                ["fire", "and", "earth", "[M]"]])
    vocab_path = self._write_to_temp_file("vocab_file", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = string_lookup.StringLookup(
        vocabulary=vocab_path, invert=True, mask_token="[M]")
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4930')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/string_lookup_test.py: 302-315
</a>
<div class="mid" id="frag4930" style="display:none"><pre>
  def test_inverse_layer_from_file(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([[1, 2, 3, 4], [4, 3, 1, 0]])
    expected_output = np.array([["earth", "wind", "and", "fire"],
                                ["fire", "and", "earth", "[UNK]"]])
    vocab_path = self._write_to_temp_file("vocab_file", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = string_lookup.StringLookup(vocabulary=vocab_path, invert=True)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4929')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/string_lookup_test.py: 288-301
</a>
<div class="mid" id="frag4929" style="display:none"><pre>
  def test_inverse_layer(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 0]])
    expected_output = np.array([["earth", "wind", "and", "fire"],
                                ["fire", "and", "earth", ""]])

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = string_lookup.StringLookup(
        vocabulary=vocab_data, invert=True, mask_token="")
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4917')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/string_lookup_test.py: 136-148
</a>
<div class="mid" id="frag4917" style="display:none"><pre>
  def test_int_output_explicit_vocab_with_special_tokens(self):
    vocab_data = ["", "[UNK]", "earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup(vocabulary=vocab_data, mask_token="")
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 255:</b> &nbsp; 62 fragments, nominal size 19 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4717')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 288-302
</a>
<div class="mid" id="frag4717" style="display:none"><pre>
  def test_int_output_explicit_vocab(self):
    vocab_data = [42, 1138, 725, 1729]
    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(
        vocabulary=vocab_data,
        max_tokens=None,
    )
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4718')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 303-318
</a>
<div class="mid" id="frag4718" style="display:none"><pre>
  def test_int_output_explicit_vocab_with_special_tokens(self):
    vocab_data = [0, -1, 42, 1138, 725, 1729]
    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        mask_token=0,
    )
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5281')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1432-1450
</a>
<div class="mid" id="frag5281" style="display:none"><pre>
  def test_int_output_int_file_vocab(self):
    vocab_data = ["10", "20", "30", "40"]
    input_array = np.array([[10, 20, 30, 40], [40, 0, 10, 42]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4788')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 784-804
</a>
<div class="mid" id="frag4788" style="display:none"><pre>
  def test_vocab_setting_with_oov_via_setter(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_path = self._write_to_temp_file("vocab_file", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT)
    layer.set_vocabulary(vocab_path)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5279')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1379-1399
</a>
<div class="mid" id="frag5279" style="display:none"><pre>
  def test_int_output_file_vocab_no_oov_or_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "wind", "earth", "and"]])
    expected_output = [[0, 1, 2, 3], [3, 1, 0, 2]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=0,
        oov_token=None,
        dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4790')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 837-854
</a>
<div class="mid" id="frag4790" style="display:none"><pre>
  def test_int_output(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5286')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1506-1524
</a>
<div class="mid" id="frag5286" style="display:none"><pre>
  def test_int_output_explicit_vocab_with_special_tokens(self):
    vocab_data = ["", "[OOV]", "earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5285')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1487-1505
</a>
<div class="mid" id="frag5285" style="display:none"><pre>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5272')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1219-1239
</a>
<div class="mid" id="frag5272" style="display:none"><pre>
  def test_int_output_file_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 0, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5252')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 759-777
</a>
<div class="mid" id="frag5252" style="display:none"><pre>
  def test_int_output_no_reserved_zero(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=None,
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5278')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1358-1378
</a>
<div class="mid" id="frag5278" style="display:none"><pre>
  def test_int_output_file_vocab_no_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 0, 1, 0]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        mask_token=None,
        num_oov_indices=1,
        oov_token="[OOV]",
        dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4786')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 744-763
</a>
<div class="mid" id="frag4786" style="display:none"><pre>
  def test_vocab_setting_via_init_file(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_path = self._write_to_temp_file("vocab_file", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT,
        vocabulary=vocab_path)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5256')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 855-873
</a>
<div class="mid" id="frag5256" style="display:none"><pre>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5248')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 693-711
</a>
<div class="mid" id="frag5248" style="display:none"><pre>
  def test_int_output(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4787')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 764-783
</a>
<div class="mid" id="frag4787" style="display:none"><pre>
  def test_vocab_setting_via_setter(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_path = self._write_to_temp_file("vocab_file", vocab_data)
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT)
    layer.set_vocabulary(vocab_path)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4785')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 725-743
</a>
<div class="mid" id="frag4785" style="display:none"><pre>
  def test_vocab_setting_via_init(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT,
        vocabulary=vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4789')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 810-831
</a>
<div class="mid" id="frag4789" style="display:none"><pre>
  def test_distribution_strategy_output(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    strategy = tf.distribute.OneDeviceStrategy("/cpu:0")
    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = text_vectorization.TextVectorization(
          max_tokens=None,
          standardize=None,
          split=None,
          output_mode=text_vectorization.INT)
      layer.set_vocabulary(vocab_data)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4772')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 490-507
</a>
<div class="mid" id="frag4772" style="display:none"><pre>
  def test_normalization(self):
    input_array = np.array([["Earth", "wInD", "aNd", "firE"],
                            ["fire|", "an&lt;&gt;d", "{earth}", "michigan@%$"]])
    expected_output = np.array([[b"earth", b"wind", b"and", b"fire"],
                                [b"fire", b"and", b"earth", b"michigan"]])

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=text_vectorization.LOWER_AND_STRIP_PUNCTUATION,
        split=None,
        ngrams=None,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4830')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1557-1587
</a>
<div class="mid" id="frag4830" style="display:none"><pre>
  def test_saving(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")

    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    # TODO(b/149526183): Can't clear session when TF2 is disabled.
    if tf.__internal__.tf2.enabled():
      keras.backend.clear_session()

    loaded_model = keras.models.load_model(output_path)
    self.assertAllEqual(loaded_model.predict(input_array), expected_output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4774')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 526-545
</a>
<div class="mid" id="frag4774" style="display:none"><pre>
  def test_custom_normalization(self):
    input_array = np.array([["Earth", "wInD", "aNd", "firE"],
                            ["fire|", "an&lt;&gt;d", "{earth}", "michigan@%$"]])
    expected_output = np.array(
        [[b"earth", b"wind", b"and", b"fire"],
         [b"fire|", b"an&lt;&gt;d", b"{earth}", b"michigan@%$"]])

    custom_standardization = tf.strings.lower
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=custom_standardization,
        split=None,
        ngrams=None,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5313')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1843-1862
</a>
<div class="mid" id="frag5313" style="display:none"><pre>
  def test_int_output_explicit_vocab(self):
    vocab_data = ["", "[OOV]", "earth", "wind", "and", "fire"]
    input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 1]])
    expected_output = np.array([["earth", "wind", "and", "fire"],
                                ["fire", "and", "earth", "[OOV]"]])

    input_data = keras.Input(shape=(None,), dtype=tf.int64)
    layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        invert=True)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5233')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 406-424
</a>
<div class="mid" id="frag5233" style="display:none"><pre>
  def test_ragged_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant(
        [["earth", "wind", "fire"], ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5243')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 622-640
</a>
<div class="mid" id="frag5243" style="display:none"><pre>
  def test_ragged_string_input(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant(
        [["earth", "wind", "fire"], ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5253')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 778-801
</a>
<div class="mid" id="frag5253" style="display:none"><pre>
  def test_int_output_no_oov(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5249')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 712-729
</a>
<div class="mid" id="frag5249" style="display:none"><pre>
  def test_int_output_rank_one(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_data = np.array(["earth", "wind", "and", "fire"])
    expected_output = [2, 3, 4, 5]

    inputs = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)
    output_dataset = model(input_data)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4797')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1027-1053
</a>
<div class="mid" id="frag4797" style="display:none"><pre>
  def test_bag_output_hard_maximum_set_vocabulary_after_build(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])

    # pyformat: disable
    expected_output = [[0, 1, 1, 1, 0],
                       [1, 1, 0, 1, 0]]
    # pyformat: enable
    max_tokens = 5
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=max_tokens,
        standardize=None,
        split=None,
        output_mode=text_vectorization.MULTI_HOT,
        pad_to_max_tokens=True)
    int_data = layer(input_data)
    layer.set_vocabulary(vocab_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4795')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 973-999
</a>
<div class="mid" id="frag4795" style="display:none"><pre>
  def test_binary_output_hard_maximum(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])

    # pyformat: disable
    expected_output = [[0, 1, 1, 1, 0, 0],
                       [1, 1, 0, 1, 0, 0]]
    # pyformat: enable
    max_tokens = 6
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=max_tokens,
        standardize=None,
        split=None,
        output_mode=text_vectorization.MULTI_HOT,
        pad_to_max_tokens=True)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5238')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 519-538
</a>
<div class="mid" id="frag5238" style="display:none"><pre>
  def test_ragged_string_input_multi_bucket(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = tf.ragged.constant([["earth", "wind", "fire"],
                                               ["fire", "and", "earth",
                                                "ohio"]])
    expected_output = [[3, 4, 6], [6, 5, 3, 2]]

    input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5254')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 802-827
</a>
<div class="mid" id="frag5254" style="display:none"><pre>
  def test_int_output_no_oov_ragged(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    valid_input = tf.RaggedTensor.from_tensor(valid_input)
    invalid_input = tf.RaggedTensor.from_tensor(invalid_input)
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5250')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 730-747
</a>
<div class="mid" id="frag5250" style="display:none"><pre>
  def test_int_output_rank_zero(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_data = tf.constant("earth")
    expected_output = 2

    inputs = keras.Input(shape=(), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)
    output_dataset = model(input_data)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4743')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_distribution_test.py: 38-66
</a>
<div class="mid" id="frag4743" style="display:none"><pre>
  def test_distribution_strategy_output(self, strategy):
    # TODO(b/180614455): remove this check when MLIR bridge is always enabled.
    if backend.is_tpu_strategy(strategy):
      self.skipTest("This test needs MLIR bridge on TPU.")

    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)

    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = text_vectorization.TextVectorization(
          max_tokens=None,
          standardize=None,
          split=None,
          output_mode=text_vectorization.INT)
      layer.set_vocabulary(vocab_data)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4831')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1588-1621
</a>
<div class="mid" id="frag4831" style="display:none"><pre>
  def test_saving_when_nested(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    outer_input = keras.Input(shape=(None,), dtype=tf.string)
    outer_output = model(outer_input)
    outer_model = keras.Model(inputs=outer_input, outputs=outer_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    outer_model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    # TODO(b/149526183): Can't clear session when TF2 is disabled.
    if tf.__internal__.tf2.enabled():
      keras.backend.clear_session()

    loaded_model = keras.models.load_model(output_path)
    self.assertAllEqual(loaded_model.predict(input_array), expected_output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5255')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 828-854
</a>
<div class="mid" id="frag5255" style="display:none"><pre>
  def test_int_output_no_oov_sparse(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    valid_input = tf.sparse.from_dense(valid_input)
    invalid_input = tf.sparse.from_dense(invalid_input)
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output,
                        tf.sparse.to_dense(output_data))
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5262')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 984-1013
</a>
<div class="mid" id="frag5262" style="display:none"><pre>
  def test_multi_hot_output_no_oov(self):
    """Check binary output when pad_to_max_tokens=True."""
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [
        [1, 1, 1, 1, 0],
        [1, 0, 1, 1, 0],
    ]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.MULTI_HOT,
        pad_to_max_tokens=True,
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    binary_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=binary_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4918')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/string_lookup_test.py: 149-167
</a>
<div class="mid" id="frag4918" style="display:none"><pre>
  def test_int_output_no_oov(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    valid_input = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", ""]])
    invalid_input = np.array([["earth", "wind", "and", "michigan"],
                              ["fire", "and", "earth", "michigan"]])
    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup(
        vocabulary=vocab_data, mask_token="", num_oov_indices=0)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_data = model.predict(valid_input)
    self.assertAllEqual(expected_output, output_data)
    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,
                                "found OOV values.*michigan"):
      _ = model.predict(invalid_input)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5325')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 2001-2038
</a>
<div class="mid" id="frag5325" style="display:none"><pre>
  def test_vocabulary_persistence_across_saving(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5259')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 929-945
</a>
<div class="mid" id="frag5259" style="display:none"><pre>
  def test_one_hot_output_rank_zero_no_oov(self):
    """Check binary output when pad_to_max_tokens=False."""
    vocab_data = ["earth", "wind", "and", "fire"]
    input_data = tf.constant("earth")
    expected_output = [1, 0, 0, 0]

    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.ONE_HOT,
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    output_data = layer(input_data)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4803')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1195-1221
</a>
<div class="mid" id="frag4803" style="display:none"><pre>
  def test_count_output_soft_maximum(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])

    # pyformat: disable
    expected_output = [[0, 2, 1, 1, 0],
                       [2, 1, 0, 1, 0]]
    # pyformat: enable
    max_tokens = 5
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=10,
        standardize=None,
        split=None,
        output_mode=text_vectorization.COUNT,
        pad_to_max_tokens=False)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4834')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1691-1720
</a>
<div class="mid" id="frag4834" style="display:none"><pre>
  def test_keras_vocab_trimming_example(self):
    vocab_data = np.array([
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ])
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])

    # pyformat: disable
    expected_output = [[1, 2, 1],
                       [3, 1, 0]]
    # pyformat: enable
    max_tokens = 3
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=max_tokens,
        standardize=None,
        split=None,
        output_mode=text_vectorization.COUNT,
        pad_to_max_tokens=True)
    int_data = layer(input_data)
    layer.adapt(vocab_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())
    model = keras.Model(input_data, int_data)
    output = model.predict(input_array)
    self.assertAllEqual(expected_output, output)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4805')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1253-1283
</a>
<div class="mid" id="frag4805" style="display:none"><pre>
  def test_tfidf_output_soft_maximum(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # OOV idf weight (bucket 0) should 0.5, the average of passed weights.
    idf_weights = [.4, .25, .75, .6]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "fire", "earth", "michigan"]])

    # pyformat: disable
    # pylint: disable=bad-whitespace
    expected_output = [[ 0, .8, .25, .75,  0],
                       [ 1, .4,   0,   0, .6]]
    # pylint: enable=bad-whitespace
    # pyformat: enable
    max_tokens = 5
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=10,
        standardize=None,
        split=None,
        output_mode=text_vectorization.TF_IDF,
        pad_to_max_tokens=False)
    layer.set_vocabulary(vocab_data, idf_weights=idf_weights)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllClose(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4802')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1168-1194
</a>
<div class="mid" id="frag4802" style="display:none"><pre>
  def test_count_output_hard_maximum(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])

    # pyformat: disable
    expected_output = [[0, 2, 1, 1, 0, 0],
                       [2, 1, 0, 1, 0, 0]]
    # pyformat: enable
    max_tokens = 6
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=6,
        standardize=None,
        split=None,
        output_mode=text_vectorization.COUNT,
        pad_to_max_tokens=True)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4796')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1000-1026
</a>
<div class="mid" id="frag4796" style="display:none"><pre>
  def test_binary_output_soft_maximum(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])

    # pyformat: disable
    expected_output = [[0, 1, 1, 1, 0],
                       [1, 1, 0, 1, 0]]
    # pyformat: enable
    max_tokens = 5
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=10,
        standardize=None,
        split=None,
        output_mode=text_vectorization.MULTI_HOT,
        pad_to_max_tokens=False)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4798')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1054-1083
</a>
<div class="mid" id="frag4798" style="display:none"><pre>
  def test_bag_output_hard_maximum_adapt_after_build(self):
    vocab_data = np.array([
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ])
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])

    # pyformat: disable
    expected_output = [[0, 1, 1, 1, 0],
                       [1, 1, 0, 1, 0]]
    # pyformat: enable
    max_tokens = 5
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=max_tokens,
        standardize=None,
        split=None,
        output_mode=text_vectorization.MULTI_HOT,
        pad_to_max_tokens=True)
    int_data = layer(input_data)
    layer.adapt(vocab_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4800')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1122-1149
</a>
<div class="mid" id="frag4800" style="display:none"><pre>
  def test_bag_output_soft_maximum_set_state_after_build(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])

    # pyformat: disable
    expected_output = [[0, 1, 1, 1, 0],
                       [1, 1, 0, 1, 0]]
    # pyformat: enable
    max_tokens = 5
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=10,
        standardize=None,
        split=None,
        output_mode=text_vectorization.MULTI_HOT,
        pad_to_max_tokens=False)
    layer.build(input_data.shape)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4806')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1284-1313
</a>
<div class="mid" id="frag4806" style="display:none"><pre>
  def test_tfidf_output_set_oov_weight(self):
    vocab_data = ["[UNK]", "earth", "wind", "and", "fire"]
    idf_weights = [.1, .4, .25, .75, .6]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "fire", "earth", "michigan"]])

    # pyformat: disable
    # pylint: disable=bad-whitespace
    expected_output = [[  0, .8, .25, .75,  0],
                       [ .2, .4,   0,   0, .6]]
    # pylint: enable=bad-whitespace
    # pyformat: enable
    max_tokens = 5
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=10,
        standardize=None,
        split=None,
        output_mode=text_vectorization.TF_IDF,
        pad_to_max_tokens=False)
    layer.set_vocabulary(vocab_data, idf_weights=idf_weights)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllClose(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4804')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1222-1252
</a>
<div class="mid" id="frag4804" style="display:none"><pre>
  def test_tfidf_output_hard_maximum(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # OOV idf weight (bucket 0) should 0.5, the average of passed weights.
    idf_weights = [.4, .25, .75, .6]
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "fire", "earth", "michigan"]])

    # pyformat: disable
    # pylint: disable=bad-whitespace
    expected_output = [[ 0, .8, .25, .75,  0, 0],
                       [ 1, .4,   0,   0, .6, 0]]
    # pylint: enable=bad-whitespace
    # pyformat: enable
    max_tokens = 6
    expected_output_shape = [None, max_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=6,
        standardize=None,
        split=None,
        output_mode=text_vectorization.TF_IDF,
        pad_to_max_tokens=True)
    layer.set_vocabulary(vocab_data, idf_weights=idf_weights)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllClose(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4792')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 883-910
</a>
<div class="mid" id="frag4792" style="display:none"><pre>
  def test_int_output_densifies_with_zeros_and_pads(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x6 tensor with a padding value in the
    # second example, since output_sequence_length is set to 6.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4, 1, 5, 0], [5, 4, 2, 1, 0, 0]]

    output_sequence_length = 6
    expected_output_shape = [None, output_sequence_length]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.SPLIT_ON_WHITESPACE,
        output_mode=text_vectorization.INT,
        output_sequence_length=output_sequence_length)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4793')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 911-937
</a>
<div class="mid" id="frag4793" style="display:none"><pre>
  def test_int_output_densifies_with_zeros_and_strips(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x3 tensor with a padding value in the
    # second example, since output_sequence_length is set to 3.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4], [5, 4, 2]]
    output_sequence_length = 3
    expected_output_shape = [None, output_sequence_length]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.SPLIT_ON_WHITESPACE,
        output_mode=text_vectorization.INT,
        output_sequence_length=output_sequence_length)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4756')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_distribution_test.py: 81-111
</a>
<div class="mid" id="frag4756" style="display:none"><pre>
  def test_strategy_with_file(self, strategy):
    # TODO(b/180614455): remove this check when MLIR bridge is always enabled.
    if backend.is_tpu_strategy(strategy):
      self.skipTest("This test needs MLIR bridge on TPU.")

    vocab_data = ["earth", "wind", "and", "fire"]
    vocab_file = self._write_to_temp_file("temp", vocab_data)

    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          dtype=tf.string,
          vocabulary=vocab_file)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)
    model.compile(loss="mse")
    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4757')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_distribution_test.py: 112-145
</a>
<div class="mid" id="frag4757" style="display:none"><pre>
  def test_tpu_with_multiple_oov(self, strategy):
    # TODO(b/180614455): remove this check when MLIR bridge is always enabled.
    if backend.is_tpu_strategy(strategy):
      self.skipTest("This test needs MLIR bridge on TPU.")

    vocab_data = [[
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ]]
    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)
    expected_output = [[3, 4, 5, 6], [6, 5, 3, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=2,
          mask_token="",
          oov_token="[OOV]",
          dtype=tf.string)
      layer.adapt(vocab_dataset)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)
    model.compile(loss="mse")
    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4744')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_distribution_test.py: 67-99
</a>
<div class="mid" id="frag4744" style="display:none"><pre>
  def test_distribution_strategy_output_with_adapt(self, strategy):
    # TODO(b/180614455): remove this check when MLIR bridge is always enabled.
    if backend.is_tpu_strategy(strategy):
      self.skipTest("This test needs MLIR bridge on TPU.")

    vocab_data = [[
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ]]
    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)

    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = text_vectorization.TextVectorization(
          max_tokens=None,
          standardize=None,
          split=None,
          output_mode=text_vectorization.INT)
      layer.adapt(vocab_dataset)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)

    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4755')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_distribution_test.py: 48-80
</a>
<div class="mid" id="frag4755" style="display:none"><pre>
  def test_strategy(self, strategy):
    # TODO(b/180614455): remove this check when MLIR bridge is always enabled.
    if backend.is_tpu_strategy(strategy):
      self.skipTest("This test needs MLIR bridge on TPU.")

    vocab_data = [[
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ]]
    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    input_dataset = tf.data.Dataset.from_tensor_slices(input_array).batch(
        2, drop_remainder=True)
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(None,), dtype=tf.string)
      layer = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          dtype=tf.string)
      layer.adapt(vocab_dataset)
      int_data = layer(input_data)
      model = keras.Model(inputs=input_data, outputs=int_data)
    model.compile(loss="mse")
    output_dataset = model.predict(input_dataset)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4933')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/string_lookup_test.py: 348-365
</a>
<div class="mid" id="frag4933" style="display:none"><pre>
  def test_forward_backward_adapted_vocab(self):
    adapt_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = np.array([["earth", "wind", "and", "fire"],
                                ["fire", "and", "earth", "[UNK]"]])

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup()
    layer.adapt(adapt_data)
    invert_layer = string_lookup.StringLookup(
        vocabulary=layer.get_vocabulary(), invert=True)
    int_data = layer(input_data)
    out_data = invert_layer(int_data)
    model = keras.Model(inputs=input_data, outputs=out_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4932')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/string_lookup_test.py: 331-347
</a>
<div class="mid" id="frag4932" style="display:none"><pre>
  def test_forward_backward_explicit_vocab(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = np.array([["earth", "wind", "and", "fire"],
                                ["fire", "and", "earth", "[UNK]"]])

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = string_lookup.StringLookup(vocabulary=vocab_data)
    invert_layer = string_lookup.StringLookup(
        vocabulary=vocab_data, invert=True)
    int_data = layer(input_data)
    out_data = invert_layer(int_data)
    model = keras.Model(inputs=input_data, outputs=out_data)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5328')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 2111-2150
</a>
<div class="mid" id="frag5328" style="display:none"><pre>
  def test_vocabulary_persistence_file_vocab_keras_save_tf_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = tf.saved_model.load(output_path)
    f = loaded_model.signatures["serving_default"]

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = f(tf.constant(input_array))["index_lookup"]
    self.assertAllEqual(new_output_dataset, expected_output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5326')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 2039-2070
</a>
<div class="mid" id="frag5326" style="display:none"><pre>
  def test_vocabulary_persistence_file_across_cloning(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]
    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Clone the model and set weights.
    new_model = keras.models.clone_model(model)
    new_model.set_weights(model.get_weights())

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, new_model)

    # Validate correctness of the new model.
    new_output_dataset = new_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5327')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 2071-2110
</a>
<div class="mid" id="frag5327" style="display:none"><pre>
  def test_persistence_file_vocabs_tf_save_tf_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    tf.saved_model.save(obj=model, export_dir=output_path)

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = tf.saved_model.load(output_path)
    f = loaded_model.signatures["serving_default"]

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = f(tf.constant(input_array))["index_lookup"]
    self.assertAllEqual(new_output_dataset, expected_output)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5304')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1737-1749
</a>
<div class="mid" id="frag5304" style="display:none"><pre>
  def test_vocab_without_idf_weights_tfidf_output_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.TF_IDF,
        dtype=tf.string)
    with self.assertRaisesRegex(
        ValueError, "`idf_weights` must be set if output_mode is TF_IDF"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4791')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 855-882
</a>
<div class="mid" id="frag4791" style="display:none"><pre>
  def test_int_output_densifies_with_zeros(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x5 tensor with a padding value in the
    # second example.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4, 1, 5], [5, 4, 2, 1, 0]]

    # This test doesn't explicitly set an output shape, so the 2nd dimension
    # should stay 'None'.
    expected_output_shape = [None, None]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.SPLIT_ON_WHITESPACE,
        output_mode=text_vectorization.INT)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4794')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 938-972
</a>
<div class="mid" id="frag4794" style="display:none"><pre>
  def test_int_output_dynamically_strips_and_pads(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    # Create an input array that has 5 elements in the first example and 4 in
    # the second. This should output a 2x3 tensor with a padding value in the
    # second example, since output_sequence_length is set to 3.
    input_array = np.array([["earth wind and also fire"],
                            ["fire and earth michigan"]])
    expected_output = [[2, 3, 4], [5, 4, 2]]
    output_sequence_length = 3
    expected_output_shape = [None, output_sequence_length]

    # The input shape here is explicitly 1 because we're tokenizing.
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.SPLIT_ON_WHITESPACE,
        output_mode=text_vectorization.INT,
        output_sequence_length=output_sequence_length)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

    # Create an input array that has 1 element in the first example and 2 in
    # the second. This should output a 2x3 tensor with a padding value in the
    # second example, since output_sequence_length is set to 3.
    input_array_2 = np.array([["wind"], ["fire and"]])
    expected_output_2 = [[3, 0, 0], [5, 4, 0]]
    output_dataset = model.predict(input_array_2)
    self.assertAllEqual(expected_output_2, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5303')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1723-1736
</a>
<div class="mid" id="frag5303" style="display:none"><pre>
  def test_vocab_with_idf_weights_length_mismatch_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    weight_data = [1, 1, 1, 1, 1]  # too long
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.TF_IDF,
        dtype=tf.string)
    with self.assertRaisesRegex(
        ValueError, "`idf_weights` must be the same length as vocab"):
      layer.set_vocabulary(vocab_data, idf_weights=weight_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5302')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1709-1722
</a>
<div class="mid" id="frag5302" style="display:none"><pre>
  def test_vocab_with_idf_weights_non_tfidf_output_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    weight_data = [1, 1, 1, 1, 1]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.MULTI_HOT,
        dtype=tf.string)
    with self.assertRaisesRegex(ValueError,
                                "`idf_weights` should only be set if"):
      layer.set_vocabulary(vocab_data, idf_weights=weight_data)

</pre></div>
</td>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 256:</b> &nbsp; 2 fragments, nominal size 17 lines, similarity 88%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4726')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 413-431
</a>
<div class="mid" id="frag4726" style="display:none"><pre>
  def test_one_hot_output(self):
    vocab_data = [2, 3, 4, 5]
    input_array = np.array([2, 3, 4, 5, 6])
    expected_output = [
        [0, 1, 0, 0, 0],
        [0, 0, 1, 0, 0],
        [0, 0, 0, 1, 0],
        [0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0],
    ]

    input_data = keras.Input(shape=(1,), dtype=tf.int64)
    layer = integer_lookup.IntegerLookup(
        vocabulary=vocab_data, output_mode="one_hot")
    res = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=res)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4920')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/string_lookup_test.py: 174-192
</a>
<div class="mid" id="frag4920" style="display:none"><pre>
  def test_one_hot_output(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array(["earth", "wind", "and", "fire", "michigan"])
    expected_output = [
        [0, 1, 0, 0, 0],
        [0, 0, 1, 0, 0],
        [0, 0, 0, 1, 0],
        [0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0],
    ]

    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = string_lookup.StringLookup(
        vocabulary=vocab_data, output_mode="one_hot")
    res = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=res)
    output_data = model.predict(input_array)
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 257:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4737')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/integer_lookup_test.py: 546-557
</a>
<div class="mid" id="frag4737" style="display:none"><pre>
  def test_tensor_vocab(self):
    vocab_data = [-1, 42, 1138, 725, 1729]
    vocab_tensor = tf.constant(vocab_data, tf.int64)
    layer = integer_lookup.IntegerLookup(vocabulary=vocab_tensor)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)
    self.assertAllEqual(layer.vocabulary_size(), 5)
    fn = tf.function(lambda: layer.set_vocabulary(vocab_tensor))
    with self.assertRaisesRegex(RuntimeError, "Cannot set a tensor vocabulary"):
      fn()


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4935')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/string_lookup_test.py: 381-391
</a>
<div class="mid" id="frag4935" style="display:none"><pre>
  def test_tensor_vocab(self):
    vocab_data = ["[UNK]", "wind", "and", "fire"]
    vocab_tensor = tf.constant(vocab_data)
    layer = string_lookup.StringLookup(vocabulary=vocab_tensor)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)
    self.assertAllEqual(layer.vocabulary_size(), 4)
    fn = tf.function(lambda: layer.set_vocabulary(vocab_tensor))
    with self.assertRaisesRegex(RuntimeError, "Cannot set a tensor vocabulary"):
      fn()

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 258:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4760')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 313-325
</a>
<div class="mid" id="frag4760" style="display:none"><pre>
  def test_scalar_input_int_mode_no_len_limit(self):
    vocab_data = [
        "fire earth earth", "earth earth", "wind wind", "and wind and"
    ]
    input_data = "earth wind and fire fire and earth michigan"
    layer = text_vectorization.TextVectorization()
    layer.adapt(vocab_data)
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [2, 3, 4, 5, 5, 4, 2, 1])
    layer.set_vocabulary(["earth", "wind", "and", "fire"])
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [2, 3, 4, 5, 5, 4, 2, 1])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4761')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 326-338
</a>
<div class="mid" id="frag4761" style="display:none"><pre>
  def test_scalar_input_int_mode_trim_to_len_limit(self):
    vocab_data = [
        "fire earth earth", "earth earth", "wind wind", "and wind and"
    ]
    input_data = "earth wind and fire fire and earth michigan"
    layer = text_vectorization.TextVectorization(output_sequence_length=3)
    layer.adapt(vocab_data)
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [2, 3, 4])
    layer.set_vocabulary(["earth", "wind", "and", "fire"])
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [2, 3, 4])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4762')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 339-351
</a>
<div class="mid" id="frag4762" style="display:none"><pre>
  def test_scalar_input_int_pad_to_len_limit(self):
    vocab_data = [
        "fire earth earth", "earth earth", "wind wind", "and wind and"
    ]
    input_data = "earth wind and fire fire and earth michigan"
    layer = text_vectorization.TextVectorization(output_sequence_length=10)
    layer.adapt(vocab_data)
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [2, 3, 4, 5, 5, 4, 2, 1, 0, 0])
    layer.set_vocabulary(["earth", "wind", "and", "fire"])
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [2, 3, 4, 5, 5, 4, 2, 1, 0, 0])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 259:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4763')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 352-362
</a>
<div class="mid" id="frag4763" style="display:none"><pre>
  def test_list_inputs_1d(self):
    vocab_data = ["two two two", "two three three", "three four four five"]
    input_data = ["two three", "four five"]
    layer = text_vectorization.TextVectorization()
    layer.adapt(vocab_data)
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])
    layer.set_vocabulary(["two", "three", "four", "five"])
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4765')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 375-386
</a>
<div class="mid" id="frag4765" style="display:none"><pre>
  def test_list_inputs_2d(self):
    vocab_data = [
        ["two two two"], ["two three three"], ["three four four five"]]
    input_data = [["two three"], ["four five"]]
    layer = text_vectorization.TextVectorization()
    layer.adapt(vocab_data)
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])
    layer.set_vocabulary(["two", "three", "four", "five"])
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4764')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 363-374
</a>
<div class="mid" id="frag4764" style="display:none"><pre>
  def test_tensor_inputs(self):
    vocab_data = tf.constant(
        ["two two two", "two three three", "three four four five"])
    input_data = tf.constant(["two three", "four five"])
    layer = text_vectorization.TextVectorization()
    layer.adapt(vocab_data)
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])
    layer.set_vocabulary(["two", "three", "four", "five"])
    out = layer(input_data)
    self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 260:</b> &nbsp; 3 fragments, nominal size 17 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4775')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 546-563
</a>
<div class="mid" id="frag4775" style="display:none"><pre>
  def test_string_splitting(self):
    input_array = np.array([["earth wind and fire"],
                            ["\tfire\tand\nearth    michigan  "]])
    expected_output = [[b"earth", b"wind", b"and", b"fire"],
                       [b"fire", b"and", b"earth", b"michigan"]]

    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.SPLIT_ON_WHITESPACE,
        ngrams=None,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4776')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 564-582
</a>
<div class="mid" id="frag4776" style="display:none"><pre>
  def test_custom_string_splitting(self):
    input_array = np.array([["earth&gt;wind&gt;and fire"],
                            ["\tfire&gt;and\nearth&gt;michigan"]])
    expected_output = [[b"earth", b"wind", b"and fire"],
                       [b"\tfire", b"and\nearth", b"michigan"]]

    custom_split = lambda x: tf.strings.split(x, sep="&gt;")
    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=custom_split,
        ngrams=None,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4833')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1663-1686
</a>
<div class="mid" id="frag4833" style="display:none"><pre>
  def test_serialization_with_custom_callables(self):
    input_array = np.array([["earth&gt;wind&gt;and Fire"],
                            ["\tfire&gt;And\nearth&gt;michigan"]])
    expected_output = [[b"earth", b"wind", b"and fire"],
                       [b"\tfire", b"and\nearth", b"michigan"]]

    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=custom_standardize_fn,
        split=custom_split_fn,
        ngrams=None,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

    serialized_model_data = model.get_config()
    new_model = keras.Model.from_config(serialized_model_data)
    new_output_dataset = new_model.predict(input_array)
    self.assertAllEqual(expected_output, new_output_dataset)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 261:</b> &nbsp; 3 fragments, nominal size 19 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4777')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 583-606
</a>
<div class="mid" id="frag4777" style="display:none"><pre>
  def test_single_ngram_value_ragged_inputs(self):
    input_array = tf.ragged.constant([["earth", "wind", "and", "fire"],
                                               ["fire", "and", "earth"]])
    # pyformat: disable
    expected_output = [[b"earth", b"wind", b"and", b"fire",
                        b"earth wind", b"wind and", b"and fire",
                        b"earth wind and", b"wind and fire"],
                       [b"fire", b"and", b"earth",
                        b"fire and", b"and earth",
                        b"fire and earth"]]
    # pyformat: enable

    input_data = keras.Input(shape=(None,), ragged=True, dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        ngrams=3,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4778')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 607-630
</a>
<div class="mid" id="frag4778" style="display:none"><pre>
  def test_single_ngram_value(self):
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    # pyformat: disable
    expected_output = [[b"earth", b"wind", b"and", b"fire",
                        b"earth wind", b"wind and", b"and fire",
                        b"earth wind and", b"wind and fire"],
                       [b"fire", b"and", b"earth", b"michigan",
                        b"fire and", b"and earth", b"earth michigan",
                        b"fire and earth", b"and earth michigan"]]
    # pyformat: enable

    input_data = keras.Input(shape=(4,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        ngrams=3,
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4779')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 631-652
</a>
<div class="mid" id="frag4779" style="display:none"><pre>
  def test_multiple_ngram_values(self):
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    # pyformat: disable
    expected_output = [[b"earth wind", b"wind and", b"and fire",
                        b"earth wind and", b"wind and fire"],
                       [b"fire and", b"and earth", b"earth michigan",
                        b"fire and earth", b"and earth michigan"]]
    # pyformat: enable

    input_data = keras.Input(shape=(4,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=None,
        ngrams=(2, 3),
        output_mode=None)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 262:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 81%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4781')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 687-697
</a>
<div class="mid" id="frag4781" style="display:none"><pre>
  def test_string_splitting_with_non_1d_array_fails(self):
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=None,
        standardize=None,
        split=text_vectorization.SPLIT_ON_WHITESPACE,
        output_mode=None)
    with self.assertRaisesRegex(RuntimeError,
                                ".*tokenize strings, the innermost dime.*"):
      _ = layer(input_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4782')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 698-709
</a>
<div class="mid" id="frag4782" style="display:none"><pre>
  def test_string_splitting_with_non_1d_raggedarray_fails(self):
    input_data = keras.Input(shape=(None,), ragged=True, dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        vocabulary=["a"],
        max_tokens=None,
        standardize=None,
        split=text_vectorization.SPLIT_ON_WHITESPACE,
        output_mode=None)
    with self.assertRaisesRegex(RuntimeError,
                                ".*tokenize strings, the innermost dime.*"):
      _ = layer(input_data)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 263:</b> &nbsp; 2 fragments, nominal size 32 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4799')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1084-1121
</a>
<div class="mid" id="frag4799" style="display:none"><pre>
  def test_bag_output_hard_maximum_multiple_adapts(self):
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])
    adapt_data = ["earth", "earth", "earth", "earth", "wind", "wind", "wind"]
    first_expected_output = [
        [1, 1, 1, 0, 0],
        [1, 1, 0, 0, 0],
    ]
    second_adapt_data = [
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ]
    second_expected_output = [
        [0, 1, 1, 1, 0],
        [1, 1, 0, 1, 0],
    ]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = text_vectorization.TextVectorization(
        max_tokens=5,
        standardize=None,
        split=None,
        output_mode=text_vectorization.MULTI_HOT,
        pad_to_max_tokens=True)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    # Test the first adapt
    layer.adapt(adapt_data)
    first_output = model.predict(input_array)
    # Test the second adapt
    layer.adapt(second_adapt_data)
    # We need to recompile the model to retrace our call graph.
    model.compile()
    second_output = model.predict(input_array)
    self.assertAllEqual(first_expected_output, first_output)
    self.assertAllEqual(second_expected_output, second_output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5263')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1014-1053
</a>
<div class="mid" id="frag5263" style="display:none"><pre>
  def test_multi_hot_output_hard_maximum_multiple_adapts(self):
    input_array = np.array([["earth", "wind", "and", "earth"],
                            ["ohio", "and", "earth", "michigan"]])
    adapt_data = ["earth", "earth", "earth", "earth", "wind", "wind", "wind"]
    first_expected_output = [
        [1, 1, 1, 0, 0],
        [1, 1, 0, 0, 0],
    ]
    second_adapt_data = [
        "earth", "earth", "earth", "earth", "wind", "wind", "wind", "and",
        "and", "fire"
    ]
    second_expected_output = [
        [0, 1, 1, 1, 0],
        [1, 1, 0, 1, 0],
    ]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.MULTI_HOT,
        pad_to_max_tokens=True,
        dtype=tf.string)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)

    # Test the first adapt
    layer.adapt(adapt_data)
    first_output = model.predict(input_array)
    # Test the second adapt
    layer.adapt(second_adapt_data)
    # We need to recompile the model to retrace our call graph.
    model.compile()
    second_output = model.predict(input_array)
    self.assertAllEqual(first_expected_output, first_output)
    self.assertAllEqual(second_expected_output, second_output)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 264:</b> &nbsp; 5 fragments, nominal size 10 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4813')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1441-1452
</a>
<div class="mid" id="frag4813" style="display:none"><pre>
  def test_too_long_vocab_fails_in_single_setting(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    layer = text_vectorization.TextVectorization(
        max_tokens=4,
        standardize=None,
        split=None,
        output_mode=text_vectorization.INT)
    with self.assertRaisesRegex(ValueError,
                                "vocabulary larger than the maximum vocab.*"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4816')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1477-1488
</a>
<div class="mid" id="frag4816" style="display:none"><pre>
  def test_set_tfidf_in_non_tfidf_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    idf_weights = [1, 2, 3, 4]
    layer = text_vectorization.TextVectorization(
        max_tokens=5,
        standardize=None,
        split=None,
        output_mode=text_vectorization.MULTI_HOT)
    with self.assertRaisesRegex(ValueError,
                                "`idf_weights` should only be set if"):
      layer.set_vocabulary(vocab_data, idf_weights)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5322')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1965-1977
</a>
<div class="mid" id="frag5322" style="display:none"><pre>
  def test_too_long_vocab_fails_in_single_setting(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    layer = index_lookup.IndexLookup(
        max_tokens=4,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    with self.assertRaisesRegex(ValueError,
                                "vocabulary larger than the maximum vocab.*"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4814')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1453-1464
</a>
<div class="mid" id="frag4814" style="display:none"><pre>
  def test_setting_vocab_without_idf_weights_fails_in_tfidf_mode(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    layer = text_vectorization.TextVectorization(
        max_tokens=5,
        standardize=None,
        split=None,
        output_mode=text_vectorization.TF_IDF)
    with self.assertRaisesRegex(
        ValueError, "`idf_weights` must be set if output_mode is TF_IDF"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4815')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/text_vectorization_test.py: 1465-1476
</a>
<div class="mid" id="frag4815" style="display:none"><pre>
  def test_idf_weights_length_mismatch_fails(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    idf_weights = [1, 2, 3]
    layer = text_vectorization.TextVectorization(
        max_tokens=5,
        standardize=None,
        split=None,
        output_mode=text_vectorization.TF_IDF)
    with self.assertRaisesRegex(
        ValueError, "`idf_weights` must be the same length as vocab"):
      layer.set_vocabulary(vocab_data, idf_weights)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 265:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 72%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4838')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/discretization_test.py: 36-50
</a>
<div class="mid" id="frag4838" style="display:none"><pre>
  def test_bucketize_with_explicit_buckets_integer(self):
    input_array = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])

    expected_output = [[0, 2, 3, 1], [1, 3, 2, 1]]
    expected_output_shape = [None, 4]

    input_data = keras.Input(shape=(4,))
    layer = discretization.Discretization(bin_boundaries=[0., 1., 2.])
    bucket_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, bucket_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=bucket_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4839')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/discretization_test.py: 51-65
</a>
<div class="mid" id="frag4839" style="display:none"><pre>
  def test_bucketize_with_explicit_buckets_int_input(self):
    input_array = np.array([[-1, 1, 3, 0], [0, 3, 1, 0]], dtype=np.int64)

    expected_output = [[0, 2, 3, 1], [1, 3, 2, 1]]
    expected_output_shape = [None, 4]

    input_data = keras.Input(shape=(4,), dtype=tf.int64)
    layer = discretization.Discretization(bin_boundaries=[-.5, 0.5, 1.5])
    bucket_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, bucket_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=bucket_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5100')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/discretization_distribution_test.py: 37-55
</a>
<div class="mid" id="frag5100" style="display:none"><pre>
  def test_distribution(self, strategy):
    input_array = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])

    expected_output = [[0, 2, 3, 1], [1, 3, 2, 1]]
    expected_output_shape = [None, 4]

    tf.config.set_soft_device_placement(True)

    with strategy.scope():
      input_data = keras.Input(shape=(4,))
      layer = discretization.Discretization(bin_boundaries=[0., 1., 2.])
      bucket_data = layer(input_data)
      self.assertAllEqual(expected_output_shape, bucket_data.shape.as_list())

      model = keras.Model(inputs=input_data, outputs=bucket_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 266:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 91%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4840')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/discretization_test.py: 66-79
</a>
<div class="mid" id="frag4840" style="display:none"><pre>
  def test_bucketize_with_explicit_buckets_sparse_float_input(self):
    indices = [[0, 1], [0, 2], [1, 1]]
    input_array = tf.SparseTensor(
        indices=indices, values=[-1.5, 1.0, 3.4], dense_shape=[2, 3])
    expected_output = [0, 2, 3]
    input_data = keras.Input(shape=(3,), dtype=tf.float32, sparse=True)
    layer = discretization.Discretization(bin_boundaries=[-.5, 0.5, 1.5])
    bucket_data = layer(input_data)

    model = keras.Model(inputs=input_data, outputs=bucket_data)
    output_dataset = model.predict(input_array, steps=1)
    self.assertAllEqual(indices, output_dataset.indices)
    self.assertAllEqual(expected_output, output_dataset.values)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4843')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/discretization_test.py: 111-124
</a>
<div class="mid" id="frag4843" style="display:none"><pre>
  def test_bucketize_with_explicit_buckets_sparse_int_input(self):
    indices = [[0, 1], [0, 2], [1, 1]]
    input_array = tf.SparseTensor(
        indices=indices, values=[-1, 1, 3], dense_shape=[2, 3])
    expected_output = [0, 2, 3]
    input_data = keras.Input(shape=(3,), dtype=tf.int32, sparse=True)
    layer = discretization.Discretization(bin_boundaries=[-.5, 0.5, 1.5])
    bucket_data = layer(input_data)

    model = keras.Model(inputs=input_data, outputs=bucket_data)
    output_dataset = model.predict(input_array, steps=1)
    self.assertAllEqual(indices, output_dataset.indices)
    self.assertAllEqual(expected_output, output_dataset.values)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 267:</b> &nbsp; 2 fragments, nominal size 19 lines, similarity 89%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4851')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/discretization_test.py: 278-305
</a>
<div class="mid" id="frag4851" style="display:none"><pre>
  def test_saved_model_keras(self):
    input_data = [[1], [2], [3]]
    predict_data = [[0.5], [1.5], [2.5]]
    expected_output = [[0], [1], [2]]

    cls = discretization.Discretization
    inputs = keras.Input(shape=(1,), dtype=tf.float32)
    layer = cls(num_bins=3)
    layer.adapt(input_data)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)

    output_data = model.predict(predict_data)
    self.assertAllClose(output_data, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")
    loaded_model = keras.models.load_model(
        output_path, custom_objects={"Discretization": cls})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_data = loaded_model.predict(predict_data)
    self.assertAllClose(new_output_data, expected_output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4852')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/discretization_test.py: 306-332
</a>
<div class="mid" id="frag4852" style="display:none"><pre>
  def test_saved_weights_keras(self):
    input_data = [[1], [2], [3]]
    predict_data = [[0.5], [1.5], [2.5]]
    expected_output = [[0], [1], [2]]

    cls = discretization.Discretization
    inputs = keras.Input(shape=(1,), dtype=tf.float32)
    layer = cls(num_bins=3)
    layer.adapt(input_data)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)

    output_data = model.predict(predict_data)
    self.assertAllClose(output_data, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_weights")
    model.save_weights(output_path, save_format="tf")
    new_model = keras.Model.from_config(
        model.get_config(), custom_objects={"Discretization": cls})
    new_model.load_weights(output_path)

    # Validate correctness of the new model.
    new_output_data = new_model.predict(predict_data)
    self.assertAllClose(new_output_data, expected_output)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 268:</b> &nbsp; 3 fragments, nominal size 24 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4874')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_encoding_test.py: 35-66
</a>
<div class="mid" id="frag4874" style="display:none"><pre>
  def test_dense_input_sparse_output(self):
    input_array = tf.constant([[1, 2, 3], [3, 3, 0]])

    # The expected output should be (X for missing value):
    # [[X, 1, 1, 1, X, X]
    #  [1, X, X, 2, X, X]]
    expected_indices = [[0, 1], [0, 2], [0, 3], [1, 0], [1, 3]]
    expected_values = [1, 1, 1, 1, 2]
    num_tokens = 6

    input_data = keras.Input(shape=(None,), dtype=tf.int32)
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.COUNT, sparse=True)
    int_data = layer(input_data)

    model = keras.Model(inputs=input_data, outputs=int_data)
    sp_output_dataset = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_values, sp_output_dataset.values)
    self.assertAllEqual(expected_indices, sp_output_dataset.indices)

    # Assert sparse output is same as dense output.
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens,
        output_mode=category_encoding.COUNT,
        sparse=False)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array, steps=1)
    self.assertAllEqual(
        tf.sparse.to_dense(sp_output_dataset, default_value=0),
        output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4877')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_encoding_test.py: 115-151
</a>
<div class="mid" id="frag4877" style="display:none"><pre>
  def test_sparse_input_sparse_output(self):
    sp_inp = tf.SparseTensor(
        indices=[[0, 0], [1, 1], [2, 0], [2, 1], [3, 1]],
        values=[0, 2, 1, 1, 0],
        dense_shape=[4, 2])
    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)

    # The expected output should be (X for missing value):
    # [[1, X, X, X]
    #  [X, X, 1, X]
    #  [X, 2, X, X]
    #  [1, X, X, X]]
    expected_indices = [[0, 0], [1, 2], [2, 1], [3, 0]]
    expected_values = [1, 1, 2, 1]
    num_tokens = 6

    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.COUNT, sparse=True)
    int_data = layer(input_data)

    model = keras.Model(inputs=input_data, outputs=int_data)
    sp_output_dataset = model.predict(sp_inp, steps=1)
    self.assertAllEqual(expected_values, sp_output_dataset.values)
    self.assertAllEqual(expected_indices, sp_output_dataset.indices)

    # Assert sparse output is same as dense output.
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens,
        output_mode=category_encoding.COUNT,
        sparse=False)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(sp_inp, steps=1)
    self.assertAllEqual(
        tf.sparse.to_dense(sp_output_dataset, default_value=0),
        output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4880')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_encoding_test.py: 201-232
</a>
<div class="mid" id="frag4880" style="display:none"><pre>
  def test_ragged_input_sparse_output(self):
    input_array = tf.ragged.constant([[1, 2, 3], [3, 3]])

    # The expected output should be (X for missing value):
    # [[X, 1, 1, 1]
    #  [X, X, X, 2]]
    expected_indices = [[0, 1], [0, 2], [0, 3], [1, 3]]
    expected_values = [1, 1, 1, 2]
    num_tokens = 6

    input_data = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.COUNT, sparse=True)
    int_data = layer(input_data)

    model = keras.Model(inputs=input_data, outputs=int_data)
    sp_output_dataset = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_values, sp_output_dataset.values)
    self.assertAllEqual(expected_indices, sp_output_dataset.indices)

    # Assert sparse output is same as dense output.
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens,
        output_mode=category_encoding.COUNT,
        sparse=False)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array, steps=1)
    self.assertAllEqual(
        tf.sparse.to_dense(sp_output_dataset, default_value=0),
        output_dataset)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 269:</b> &nbsp; 4 fragments, nominal size 14 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4875')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_encoding_test.py: 67-88
</a>
<div class="mid" id="frag4875" style="display:none"><pre>
  def test_sparse_input(self):
    input_array = np.array([[1, 2, 3, 0], [0, 3, 1, 0]], dtype=np.int64)
    sparse_tensor_data = tf.sparse.from_dense(input_array)

    # pyformat: disable
    expected_output = [[0, 1, 1, 1, 0, 0],
                       [0, 1, 0, 1, 0, 0]]
    # pyformat: enable
    num_tokens = 6
    expected_output_shape = [None, num_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)

    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.MULTI_HOT)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(sparse_tensor_data, steps=1)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4879')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_encoding_test.py: 179-200
</a>
<div class="mid" id="frag4879" style="display:none"><pre>
  def test_ragged_input(self):
    input_array = tf.ragged.constant([[1, 2, 3], [3, 1]])

    # pyformat: disable
    expected_output = [[0, 1, 1, 1, 0, 0],
                       [0, 1, 0, 1, 0, 0]]
    # pyformat: enable
    num_tokens = 6
    expected_output_shape = [None, num_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)

    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.MULTI_HOT)
    int_data = layer(input_data)

    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array, steps=1)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4893')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_encoding_test.py: 444-464
</a>
<div class="mid" id="frag4893" style="display:none"><pre>
  def test_count_output(self):
    input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])

    # pyformat: disable
    expected_output = [[0, 2, 1, 1, 0, 0],
                       [2, 1, 0, 1, 0, 0]]
    # pyformat: enable
    num_tokens = 6
    expected_output_shape = [None, num_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.int32)
    layer = category_encoding.CategoryEncoding(
        num_tokens=6, output_mode=category_encoding.COUNT)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4884')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_encoding_test.py: 282-298
</a>
<div class="mid" id="frag4884" style="display:none"><pre>
  def test_legacy_max_tokens_arg(self):
    input_array = np.array([[1, 2, 3, 1]])
    expected_output = [[0, 1, 1, 1, 0, 0]]
    num_tokens = 6
    expected_output_shape = [None, num_tokens]

    input_data = keras.Input(shape=(None,), dtype=tf.int32)
    layer = category_encoding.CategoryEncoding(
        max_tokens=num_tokens, output_mode=category_encoding.MULTI_HOT)
    int_data = layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())

    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 270:</b> &nbsp; 2 fragments, nominal size 15 lines, similarity 93%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4882')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_encoding_test.py: 248-264
</a>
<div class="mid" id="frag4882" style="display:none"><pre>
  def test_dense_oov_input(self):
    valid_array = tf.constant([[0, 1, 2], [0, 1, 2]])
    invalid_array = tf.constant([[0, 1, 2], [2, 3, 1]])
    num_tokens = 3
    expected_output_shape = [None, num_tokens]
    encoder_layer = category_encoding.CategoryEncoding(num_tokens)
    input_data = keras.Input(shape=(3,), dtype=tf.int32)
    int_data = encoder_layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())
    model = keras.Model(inputs=input_data, outputs=int_data)
    # Call predict once on valid input to compile a graph and test control flow.
    _ = model.predict(valid_array, steps=1)
    with self.assertRaisesRegex(
        tf.errors.InvalidArgumentError,
        ".*must be in the range 0 &lt;= values &lt; num_tokens.*"):
      _ = model.predict(invalid_array, steps=1)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4883')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_encoding_test.py: 265-281
</a>
<div class="mid" id="frag4883" style="display:none"><pre>
  def test_dense_negative(self):
    valid_array = tf.constant([[0, 1, 2], [0, 1, 2]])
    invalid_array = tf.constant([[1, 2, 0], [2, 2, -1]])
    num_tokens = 3
    expected_output_shape = [None, num_tokens]
    encoder_layer = category_encoding.CategoryEncoding(num_tokens)
    input_data = keras.Input(shape=(3,), dtype=tf.int32)
    int_data = encoder_layer(input_data)
    self.assertAllEqual(expected_output_shape, int_data.shape.as_list())
    model = keras.Model(inputs=input_data, outputs=int_data)
    # Call predict once on valid input to compile a graph and test control flow.
    _ = model.predict(valid_array, steps=1)
    with self.assertRaisesRegex(
        tf.errors.InvalidArgumentError,
        ".*must be in the range 0 &lt;= values &lt; num_tokens.*"):
      _ = model.predict(invalid_array, steps=1)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 271:</b> &nbsp; 5 fragments, nominal size 17 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4885')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_encoding_test.py: 304-323
</a>
<div class="mid" id="frag4885" style="display:none"><pre>
  def test_one_hot_output(self):
    input_data = np.array([[3], [2], [0], [1]])
    expected_output = [
        [0, 0, 0, 1],
        [0, 0, 1, 0],
        [1, 0, 0, 0],
        [0, 1, 0, 0],
    ]
    num_tokens = 4
    expected_output_shape = [None, num_tokens]

    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.ONE_HOT)
    inputs = keras.Input(shape=(1,), dtype=tf.int32)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)
    output_dataset = model(input_data)
    self.assertAllEqual(expected_output_shape, outputs.shape.as_list())
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4886')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_encoding_test.py: 324-348
</a>
<div class="mid" id="frag4886" style="display:none"><pre>
  def test_one_hot_output_rank_one_input(self):
    input_data = np.array([3, 2, 0, 1])
    expected_output = [
        [0, 0, 0, 1],
        [0, 0, 1, 0],
        [1, 0, 0, 0],
        [0, 1, 0, 0],
    ]
    num_tokens = 4
    expected_output_shape = [None, num_tokens]

    # Test call on layer directly.
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.ONE_HOT)
    output_data = layer(input_data)
    self.assertAllEqual(expected_output, output_data)

    # Test call on model.
    inputs = keras.Input(shape=(1,), dtype=tf.int32)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)
    output_data = model(input_data)
    self.assertAllEqual(expected_output_shape, outputs.shape.as_list())
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4890')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_encoding_test.py: 396-415
</a>
<div class="mid" id="frag4890" style="display:none"><pre>
  def test_multi_hot_output_rank_one_input(self):
    input_data = np.array([3, 2, 0, 1])
    expected_output = [1, 1, 1, 1, 0, 0]
    num_tokens = 6
    expected_output_shape = [None, num_tokens]

    # Test call on layer directly.
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.MULTI_HOT)
    output_data = layer(input_data)
    self.assertAllEqual(expected_output, output_data)

    # Test call on model.
    inputs = keras.Input(shape=(4,), dtype=tf.int32)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)
    output_data = model(input_data)
    self.assertAllEqual(expected_output_shape, outputs.shape.as_list())
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4891')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_encoding_test.py: 416-435
</a>
<div class="mid" id="frag4891" style="display:none"><pre>
  def test_multi_hot_output_rank_zero_input(self):
    input_data = np.array(3)
    expected_output = [0, 0, 0, 1, 0, 0]
    num_tokens = 6
    expected_output_shape = [None, num_tokens]

    # Test call on layer directly.
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.MULTI_HOT)
    output_data = layer(input_data)
    self.assertAllEqual(expected_output, output_data)

    # Test call on model.
    inputs = keras.Input(shape=(4,), dtype=tf.int32)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)
    output_data = model(input_data)
    self.assertAllEqual(expected_output_shape, outputs.shape.as_list())
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4887')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_encoding_test.py: 349-369
</a>
<div class="mid" id="frag4887" style="display:none"><pre>
  def test_one_hot_output_rank_zero_input(self):
    input_data = np.array(3)
    expected_output = [0, 0, 0, 1]
    num_tokens = 4
    expected_output_shape = [None, num_tokens]

    # Test call on layer directly.
    layer = category_encoding.CategoryEncoding(
        num_tokens=num_tokens, output_mode=category_encoding.ONE_HOT)
    output_data = layer(input_data)
    self.assertAllEqual(expected_output, output_data)

    # Test call on model.
    inputs = keras.Input(shape=(1,), dtype=tf.int32)
    outputs = layer(inputs)
    model = keras.Model(inputs=inputs, outputs=outputs)
    output_data = model(input_data)

    self.assertAllEqual(expected_output_shape, outputs.shape.as_list())
    self.assertAllEqual(expected_output, output_data)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 272:</b> &nbsp; 3 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4936')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_crossing_test.py: 30-41
</a>
<div class="mid" id="frag4936" style="display:none"><pre>
  def test_crossing_sparse_inputs(self):
    layer = category_crossing.CategoryCrossing()
    inputs_0 = tf.SparseTensor(
        indices=[[0, 0], [1, 0], [1, 1]],
        values=['a', 'b', 'c'],
        dense_shape=[2, 2])
    inputs_1 = tf.SparseTensor(
        indices=[[0, 1], [1, 2]], values=['d', 'e'], dense_shape=[2, 3])
    output = layer([inputs_0, inputs_1])
    self.assertAllClose(np.asarray([[0, 0], [1, 0], [1, 1]]), output.indices)
    self.assertAllEqual([b'a_X_d', b'b_X_e', b'c_X_e'], output.values)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4938')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_crossing_test.py: 54-65
</a>
<div class="mid" id="frag4938" style="display:none"><pre>
  def test_crossing_sparse_inputs_empty_sep(self):
    layer = category_crossing.CategoryCrossing(separator='')
    inputs_0 = tf.SparseTensor(
        indices=[[0, 0], [1, 0], [1, 1]],
        values=['a', 'b', 'c'],
        dense_shape=[2, 2])
    inputs_1 = tf.SparseTensor(
        indices=[[0, 1], [1, 2]], values=['d', 'e'], dense_shape=[2, 3])
    output = layer([inputs_0, inputs_1])
    self.assertAllClose(np.asarray([[0, 0], [1, 0], [1, 1]]), output.indices)
    self.assertAllEqual([b'ad', b'be', b'ce'], output.values)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag4937')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/category_crossing_test.py: 42-53
</a>
<div class="mid" id="frag4937" style="display:none"><pre>
  def test_crossing_sparse_inputs_custom_sep(self):
    layer = category_crossing.CategoryCrossing(separator='_Y_')
    inputs_0 = tf.SparseTensor(
        indices=[[0, 0], [1, 0], [1, 1]],
        values=['a', 'b', 'c'],
        dense_shape=[2, 2])
    inputs_1 = tf.SparseTensor(
        indices=[[0, 1], [1, 2]], values=['d', 'e'], dense_shape=[2, 3])
    output = layer([inputs_0, inputs_1])
    self.assertAllClose(np.asarray([[0, 0], [1, 0], [1, 1]]), output.indices)
    self.assertAllEqual([b'a_Y_d', b'b_Y_e', b'c_Y_e'], output.values)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 273:</b> &nbsp; 2 fragments, nominal size 21 lines, similarity 80%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag4991')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/embedding_varlen_benchmark.py: 31-65
</a>
<div class="mid" id="frag4991" style="display:none"><pre>
def embedding_varlen(batch_size, max_length):
  """Benchmark a variable-length embedding."""
  # Data and constants.
  embedding_size = 32768
  data = fc_bm.create_data(
      max_length, batch_size * NUM_REPEATS, embedding_size - 1, dtype=int)

  # Keras implementation
  model = keras.Sequential()
  model.add(
      keras.Input(shape=(None,), ragged=True, name="data", dtype=tf.int64))
  model.add(keras.layers.Embedding(embedding_size, 256))
  model.add(keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1)))

  # FC implementation
  fc = tf.feature_column.embedding_column(
      tf.feature_column.categorical_column_with_identity(
          "data", num_buckets=embedding_size - 1),
      dimension=256)

  # Wrap the FC implementation in a tf.function for a fair comparison
  @tf_function()
  def fc_fn(tensors):
    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

  # Benchmark runs
  keras_data = {"data": data}
  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

  fc_data = {"data": data.to_sparse()}
  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

  return k_avg_time, fc_avg_time


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5022')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/embedding_dense_benchmark.py: 31-64
</a>
<div class="mid" id="frag5022" style="display:none"><pre>
def embedding_varlen(batch_size, max_length):
  """Benchmark a variable-length embedding."""
  # Data and constants.
  embedding_size = 32768
  data = fc_bm.create_data(
      max_length, batch_size * NUM_REPEATS, embedding_size - 1, dtype=int)

  # Keras implementation
  model = keras.Sequential()
  model.add(keras.Input(shape=(None,), name="data", dtype=tf.int64))
  model.add(keras.layers.Embedding(embedding_size, 256))
  model.add(keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1)))

  # FC implementation
  fc = tf.feature_column.embedding_column(
      tf.feature_column.categorical_column_with_identity(
          "data", num_buckets=embedding_size - 1),
      dimension=256)

  # Wrap the FC implementation in a tf.function for a fair comparison
  @tf_function()
  def fc_fn(tensors):
    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

  # Benchmark runs
  keras_data = {"data": data.to_tensor(default_value=0)}
  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

  fc_data = {"data": data.to_tensor(default_value=0)}
  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

  return k_avg_time, fc_avg_time


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 274:</b> &nbsp; 8 fragments, nominal size 20 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5000')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/category_hash_dense_benchmark.py: 31-66
</a>
<div class="mid" id="frag5000" style="display:none"><pre>
def embedding_varlen(batch_size, max_length):
  """Benchmark a variable-length embedding."""
  # Data and constants.

  num_buckets = 10000
  vocab = fc_bm.create_vocabulary(32768)
  data = fc_bm.create_string_data(
      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.0)

  # Keras implementation
  model = keras.Sequential()
  model.add(keras.Input(shape=(max_length,), name="data", dtype=tf.string))
  model.add(hashing.Hashing(num_buckets))

  # FC implementation
  fc = tf.feature_column.sequence_categorical_column_with_hash_bucket("data", num_buckets)

  # Wrap the FC implementation in a tf.function for a fair comparison
  @tf_function()
  def fc_fn(tensors):
    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

  # Benchmark runs
  keras_data = {
      "data": data.to_tensor(default_value="", shape=(batch_size, max_length))
  }
  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

  fc_data = {
      "data": data.to_tensor(default_value="", shape=(batch_size, max_length))
  }
  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

  return k_avg_time, fc_avg_time


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5054')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/category_hash_varlen_benchmark.py: 31-64
</a>
<div class="mid" id="frag5054" style="display:none"><pre>
def embedding_varlen(batch_size, max_length):
  """Benchmark a variable-length embedding."""
  # Data and constants.

  num_buckets = 10000
  vocab = fc_bm.create_vocabulary(32768)
  data = fc_bm.create_string_data(
      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.0)

  # Keras implementation
  model = keras.Sequential()
  model.add(
      keras.Input(
          shape=(max_length,), name="data", ragged=True, dtype=tf.string))
  model.add(hashing.Hashing(num_buckets))

  # FC implementation
  fc = tf.feature_column.categorical_column_with_hash_bucket("data", num_buckets)

  # Wrap the FC implementation in a tf.function for a fair comparison
  @tf_function()
  def fc_fn(tensors):
    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

  # Benchmark runs
  keras_data = {"data": data}
  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

  fc_data = {"data": data.to_sparse()}
  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

  return k_avg_time, fc_avg_time


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5015')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/category_vocab_list_dense_benchmark.py: 31-65
</a>
<div class="mid" id="frag5015" style="display:none"><pre>
def embedding_varlen(batch_size, max_length):
  """Benchmark a variable-length embedding."""
  # Data and constants.
  vocab = fc_bm.create_vocabulary(32768)
  data = fc_bm.create_string_data(
      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.15)

  # Keras implementation
  model = keras.Sequential()
  model.add(keras.Input(shape=(max_length,), name="data", dtype=tf.string))
  model.add(string_lookup.StringLookup(vocabulary=vocab, mask_token=None))

  # FC implementation
  fc = tf.feature_column.categorical_column_with_vocabulary_list(
      key="data", vocabulary_list=vocab, num_oov_buckets=1)

  # Wrap the FC implementation in a tf.function for a fair comparison
  @tf_function()
  def fc_fn(tensors):
    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

  # Benchmark runs
  keras_data = {
      "data": data.to_tensor(default_value="", shape=(batch_size, max_length))
  }
  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

  fc_data = {
      "data": data.to_tensor(default_value="", shape=(batch_size, max_length))
  }
  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

  return k_avg_time, fc_avg_time


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5039')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/category_vocab_file_varlen_benchmark.py: 45-78
</a>
<div class="mid" id="frag5039" style="display:none"><pre>
  def embedding_varlen(self, batch_size, max_length):
    """Benchmark a variable-length embedding."""
    # Data and constants.
    vocab = fc_bm.create_vocabulary(32768)
    path = self._write_to_temp_file("tmp", vocab)

    data = fc_bm.create_string_data(
        max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.15)

    # Keras implementation
    model = keras.Sequential()
    model.add(
        keras.Input(
            shape=(max_length,), name="data", ragged=True, dtype=tf.string))
    model.add(string_lookup.StringLookup(vocabulary=path, mask_token=None))

    # FC implementation
    fc = tf.feature_column.sequence_categorical_column_with_vocabulary_list(
        key="data", vocabulary_list=vocab, num_oov_buckets=1)

    # Wrap the FC implementation in a tf.function for a fair comparison
    @tf_function()
    def fc_fn(tensors):
      fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

    # Benchmark runs
    keras_data = {"data": data}
    k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

    fc_data = {"data": data.to_sparse()}
    fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

    return k_avg_time, fc_avg_time

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5066')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/category_vocab_list_varlen_benchmark.py: 31-63
</a>
<div class="mid" id="frag5066" style="display:none"><pre>
def embedding_varlen(batch_size, max_length):
  """Benchmark a variable-length embedding."""
  # Data and constants.
  vocab = fc_bm.create_vocabulary(32768)
  data = fc_bm.create_string_data(
      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.15)

  # Keras implementation
  model = keras.Sequential()
  model.add(
      keras.Input(
          shape=(max_length,), name="data", ragged=True, dtype=tf.string))
  model.add(string_lookup.StringLookup(vocabulary=vocab, mask_token=None))

  # FC implementation
  fc = tf.feature_column.sequence_categorical_column_with_vocabulary_list(
      key="data", vocabulary_list=vocab, num_oov_buckets=1)

  # Wrap the FC implementation in a tf.function for a fair comparison
  @tf_function()
  def fc_fn(tensors):
    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

  # Benchmark runs
  keras_data = {"data": data}
  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

  fc_data = {"data": data.to_sparse()}
  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

  return k_avg_time, fc_avg_time


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5059')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/category_vocab_list_indicator_dense_benchmark.py: 32-71
</a>
<div class="mid" id="frag5059" style="display:none"><pre>
def embedding_varlen(batch_size, max_length):
  """Benchmark a variable-length embedding."""
  # Data and constants.
  vocab_size = 32768
  vocab = fc_bm.create_vocabulary(vocab_size)
  data = fc_bm.create_string_data(
      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.15)

  # Keras implementation
  model = keras.Sequential()
  model.add(keras.Input(shape=(max_length,), name="data", dtype=tf.string))
  model.add(string_lookup.StringLookup(vocabulary=vocab, mask_token=None))
  model.add(
      category_encoding.CategoryEncoding(
          num_tokens=vocab_size + 1, output_mode="count"))

  # FC implementation
  fc = tf.feature_column.indicator_column(
      tf.feature_column.categorical_column_with_vocabulary_list(
          key="data", vocabulary_list=vocab, num_oov_buckets=1))

  # Wrap the FC implementation in a tf.function for a fair comparison
  @tf_function()
  def fc_fn(tensors):
    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

  # Benchmark runs
  keras_data = {
      "data": data.to_tensor(default_value="", shape=(batch_size, max_length))
  }
  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

  fc_data = {
      "data": data.to_tensor(default_value="", shape=(batch_size, max_length))
  }
  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

  return k_avg_time, fc_avg_time


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5070')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/category_vocab_file_dense_benchmark.py: 45-83
</a>
<div class="mid" id="frag5070" style="display:none"><pre>
  def embedding_varlen(self, batch_size, max_length):
    """Benchmark a variable-length embedding."""
    # Data and constants.
    vocab = fc_bm.create_vocabulary(32768)

    path = self._write_to_temp_file("tmp", vocab)

    data = fc_bm.create_string_data(
        max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.15)

    # Keras implementation
    model = keras.Sequential()
    model.add(keras.Input(shape=(max_length,), name="data", dtype=tf.string))
    model.add(string_lookup.StringLookup(vocabulary=path, mask_token=None))

    # FC implementation
    fc = tf.feature_column.categorical_column_with_vocabulary_list(
        key="data", vocabulary_list=vocab, num_oov_buckets=1)

    # Wrap the FC implementation in a tf.function for a fair comparison
    @tf_function()
    def fc_fn(tensors):
      fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

    # Benchmark runs
    keras_data = {
        "data": data.to_tensor(
            default_value="", shape=(batch_size, max_length))
    }
    k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

    fc_data = {
        "data": data.to_tensor(
            default_value="", shape=(batch_size, max_length))
    }
    fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

    return k_avg_time, fc_avg_time

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5006')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/category_vocab_list_indicator_varlen_benchmark.py: 32-69
</a>
<div class="mid" id="frag5006" style="display:none"><pre>
def embedding_varlen(batch_size, max_length):
  """Benchmark a variable-length embedding."""
  # Data and constants.
  vocab_size = 32768
  vocab = fc_bm.create_vocabulary(vocab_size)
  data = fc_bm.create_string_data(
      max_length, batch_size * NUM_REPEATS, vocab, pct_oov=0.15)

  # Keras implementation
  model = keras.Sequential()
  model.add(
      keras.Input(
          shape=(max_length,), name="data", ragged=True, dtype=tf.string))
  model.add(string_lookup.StringLookup(vocabulary=vocab, mask_token=None))
  model.add(
      category_encoding.CategoryEncoding(
          num_tokens=vocab_size + 1, output_mode="count"))

  # FC implementation
  fc = tf.feature_column.indicator_column(
      tf.feature_column.sequence_categorical_column_with_vocabulary_list(
          key="data", vocabulary_list=vocab, num_oov_buckets=1))

  # Wrap the FC implementation in a tf.function for a fair comparison
  @tf_function()
  def fc_fn(tensors):
    fc.transform_feature(tf.__internal__.feature_column.FeatureTransformationCache(tensors), None)

  # Benchmark runs
  keras_data = {"data": data}
  k_avg_time = fc_bm.run_keras(keras_data, model, batch_size, NUM_REPEATS)

  fc_data = {"data": data.to_sparse()}
  fc_avg_time = fc_bm.run_fc(fc_data, fc_fn, batch_size, NUM_REPEATS)

  return k_avg_time, fc_avg_time


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 275:</b> &nbsp; 3 fragments, nominal size 19 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5012')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/image_preproc_benchmark.py: 77-101
</a>
<div class="mid" id="frag5012" style="display:none"><pre>
  def run_dataset_implementation(self, batch_size):
    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.from_tensor_slices(
          np.random.random((batch_size, 256, 256, 3)))
      ds = ds.shuffle(batch_size * 100)
      ds = ds.batch(batch_size)
      ds = ds.prefetch(batch_size)
      img_augmentation = functools.partial(
          image_augmentation, batch_size=batch_size)
      ds = ds.map(img_augmentation)
      starts.append(time.time())
      count = 0
      # Benchmarked code begins here.
      for i in ds:
        _ = i
        count += 1
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts)) / count
    return avg_time

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5035')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/category_crossing_benchmark.py: 40-62
</a>
<div class="mid" id="frag5035" style="display:none"><pre>
  def run_dataset_implementation(self, batch_size):
    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.from_generator(
          int_gen, (tf.int64, tf.int64),
          (tf.TensorShape([1]), tf.TensorShape([1])))
      ds = ds.shuffle(batch_size * 100)
      ds = ds.batch(batch_size)
      num_batches = 5
      ds = ds.take(num_batches)
      ds = ds.prefetch(num_batches)
      starts.append(time.time())
      # Benchmarked code begins here.
      for i in ds:
        _ = tf.sparse.cross([i[0], i[1]])
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts)) / num_batches
    return avg_time

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5043')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/hashing_benchmark.py: 42-63
</a>
<div class="mid" id="frag5043" style="display:none"><pre>
  def run_dataset_implementation(self, batch_size):
    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.from_generator(word_gen, tf.string,
                                              tf.TensorShape([]))
      ds = ds.shuffle(batch_size * 100)
      ds = ds.batch(batch_size)
      num_batches = 5
      ds = ds.take(num_batches)
      ds = ds.prefetch(num_batches)
      starts.append(time.time())
      # Benchmarked code begins here.
      for i in ds:
        _ = tf.strings.to_hash_bucket(i, num_buckets=2)
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts)) / num_batches
    return avg_time

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 276:</b> &nbsp; 2 fragments, nominal size 20 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5019')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/normalization_adapt_benchmark.py: 54-80
</a>
<div class="mid" id="frag5019" style="display:none"><pre>
  def run_dataset_implementation(self, num_elements, batch_size):
    input_t = keras.Input(shape=(1,))
    layer = normalization.Normalization()
    _ = layer(input_t)

    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.range(num_elements)
      ds = ds.map(
          lambda x: tf.expand_dims(tf.cast(x, tf.float32), -1))
      ds = ds.batch(batch_size)

      starts.append(time.time())
      # Benchmarked code begins here.
      k, n, ex, ex2 = ds.reduce((0.0, 0, 0.0, 0.0), reduce_fn)
      mean = k.numpy() + ex.numpy() / n.numpy()
      var = (ex2.numpy() - (ex.numpy() * ex.numpy()) / n.numpy()) / (
          n.numpy() - 1)
      layer.set_weights([mean, var])
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts))
    return avg_time

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5063')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/discretization_adapt_benchmark.py: 44-69
</a>
<div class="mid" id="frag5063" style="display:none"><pre>
  def run_dataset_implementation(self, num_elements, batch_size):
    input_t = keras.Input(shape=(1,))
    layer = discretization.Discretization()
    _ = layer(input_t)

    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.range(num_elements)
      ds = ds.map(
          lambda x: tf.expand_dims(tf.cast(x, tf.float32), -1))
      ds = ds.batch(batch_size)

      starts.append(time.time())
      # Benchmarked code begins here.
      state = ds.reduce((np.zeros((1, 2)),), reduce_fn)

      bins = discretization.get_bucket_boundaries(state, 100)
      layer.set_weights([bins])
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts))
    return avg_time

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 277:</b> &nbsp; 2 fragments, nominal size 25 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5020')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/normalization_adapt_benchmark.py: 81-113
</a>
<div class="mid" id="frag5020" style="display:none"><pre>
  def bm_adapt_implementation(self, num_elements, batch_size):
    """Test the KPL adapt implementation."""
    input_t = keras.Input(shape=(1,), dtype=tf.float32)
    layer = normalization.Normalization()
    _ = layer(input_t)

    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.range(num_elements)
      ds = ds.map(
          lambda x: tf.expand_dims(tf.cast(x, tf.float32), -1))
      ds = ds.batch(batch_size)

      starts.append(time.time())
      # Benchmarked code begins here.
      layer.adapt(ds)
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts))
    name = "normalization_adapt|%s_elements|batch_%s" % (num_elements,
                                                         batch_size)
    baseline = self.run_dataset_implementation(num_elements, batch_size)
    extras = {
        "tf.data implementation baseline": baseline,
        "delta seconds": (baseline - avg_time),
        "delta percent": ((baseline - avg_time) / baseline) * 100
    }
    self.report_benchmark(
        iters=num_repeats, wall_time=avg_time, extras=extras, name=name)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5064')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/discretization_adapt_benchmark.py: 70-102
</a>
<div class="mid" id="frag5064" style="display:none"><pre>
  def bm_adapt_implementation(self, num_elements, batch_size):
    """Test the KPL adapt implementation."""
    input_t = keras.Input(shape=(1,), dtype=tf.float32)
    layer = discretization.Discretization()
    _ = layer(input_t)

    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.range(num_elements)
      ds = ds.map(
          lambda x: tf.expand_dims(tf.cast(x, tf.float32), -1))
      ds = ds.batch(batch_size)

      starts.append(time.time())
      # Benchmarked code begins here.
      layer.adapt(ds)
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts))
    name = "discretization_adapt|%s_elements|batch_%s" % (num_elements,
                                                          batch_size)
    baseline = self.run_dataset_implementation(num_elements, batch_size)
    extras = {
        "tf.data implementation baseline": baseline,
        "delta seconds": (baseline - avg_time),
        "delta percent": ((baseline - avg_time) / baseline) * 100
    }
    self.report_benchmark(
        iters=num_repeats, wall_time=avg_time, extras=extras, name=name)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 278:</b> &nbsp; 2 fragments, nominal size 29 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5036')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/category_crossing_benchmark.py: 63-98
</a>
<div class="mid" id="frag5036" style="display:none"><pre>
  def bm_layer_implementation(self, batch_size):
    input_1 = keras.Input(shape=(1,), dtype=tf.int64, name="word")
    input_2 = keras.Input(shape=(1,), dtype=tf.int64, name="int")
    layer = category_crossing.CategoryCrossing()
    _ = layer([input_1, input_2])

    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.from_generator(
          int_gen, (tf.int64, tf.int64),
          (tf.TensorShape([1]), tf.TensorShape([1])))
      ds = ds.shuffle(batch_size * 100)
      ds = ds.batch(batch_size)
      num_batches = 5
      ds = ds.take(num_batches)
      ds = ds.prefetch(num_batches)
      starts.append(time.time())
      # Benchmarked code begins here.
      for i in ds:
        _ = layer([i[0], i[1]])
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts)) / num_batches
    name = "category_crossing|batch_%s" % batch_size
    baseline = self.run_dataset_implementation(batch_size)
    extras = {
        "dataset implementation baseline": baseline,
        "delta seconds": (baseline - avg_time),
        "delta percent": ((baseline - avg_time) / baseline) * 100
    }
    self.report_benchmark(
        iters=num_repeats, wall_time=avg_time, extras=extras, name=name)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5044')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/benchmarks/hashing_benchmark.py: 64-97
</a>
<div class="mid" id="frag5044" style="display:none"><pre>
  def bm_layer_implementation(self, batch_size):
    input_1 = keras.Input(shape=(None,), dtype=tf.string, name="word")
    layer = hashing.Hashing(num_bins=2)
    _ = layer(input_1)

    num_repeats = 5
    starts = []
    ends = []
    for _ in range(num_repeats):
      ds = tf.data.Dataset.from_generator(word_gen, tf.string,
                                              tf.TensorShape([]))
      ds = ds.shuffle(batch_size * 100)
      ds = ds.batch(batch_size)
      num_batches = 5
      ds = ds.take(num_batches)
      ds = ds.prefetch(num_batches)
      starts.append(time.time())
      # Benchmarked code begins here.
      for i in ds:
        _ = layer(i)
      # Benchmarked code ends here.
      ends.append(time.time())

    avg_time = np.mean(np.array(ends) - np.array(starts)) / num_batches
    name = "hashing|batch_%s" % batch_size
    baseline = self.run_dataset_implementation(batch_size)
    extras = {
        "dataset implementation baseline": baseline,
        "delta seconds": (baseline - avg_time),
        "delta percent": ((baseline - avg_time) / baseline) * 100
    }
    self.report_benchmark(
        iters=num_repeats, wall_time=avg_time, extras=extras, name=name)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 279:</b> &nbsp; 3 fragments, nominal size 10 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5088')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/hashing_test.py: 164-178
</a>
<div class="mid" id="frag5088" style="display:none"><pre>
  def test_hash_ragged_string_input_farmhash(self):
    layer = hashing.Hashing(num_bins=2)
    inp_data = tf.ragged.constant(
        [['omar', 'stringer', 'marlo', 'wire'], ['marlo', 'skywalker', 'wire']],
        dtype=tf.string)
    out_data = layer(inp_data)
    # Same hashed output as test_hash_sparse_input_farmhash
    expected_output = [[0, 0, 1, 0], [1, 0, 0]]
    self.assertAllEqual(expected_output, out_data)

    inp_t = input_layer.Input(shape=(None,), ragged=True, dtype=tf.string)
    out_t = layer(inp_t)
    model = training.Model(inputs=inp_t, outputs=out_t)
    self.assertAllClose(out_data, model.predict(inp_data))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5090')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/hashing_test.py: 195-207
</a>
<div class="mid" id="frag5090" style="display:none"><pre>
  def test_hash_ragged_int_input_farmhash(self):
    layer = hashing.Hashing(num_bins=3)
    inp_data = tf.ragged.constant([[0, 1, 3, 4], [2, 1, 0]], dtype=tf.int64)
    out_data = layer(inp_data)
    # Same hashed output as test_hash_sparse_input_farmhash
    expected_output = [[1, 0, 0, 2], [1, 0, 1]]
    self.assertAllEqual(expected_output, out_data)

    inp_t = input_layer.Input(shape=(None,), ragged=True, dtype=tf.int64)
    out_t = layer(inp_t)
    model = training.Model(inputs=inp_t, outputs=out_t)
    self.assertAllClose(out_data, model.predict(inp_data))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5092')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/hashing_test.py: 232-244
</a>
<div class="mid" id="frag5092" style="display:none"><pre>
  def test_hash_ragged_int_input_siphash(self):
    layer = hashing.Hashing(num_bins=3, salt=[133, 137])
    inp_data = tf.ragged.constant([[0, 1, 3, 4], [2, 1, 0]], dtype=tf.int64)
    out_data = layer(inp_data)
    # Same hashed output as test_hash_sparse_input_farmhash
    expected_output = [[1, 1, 0, 1], [2, 1, 1]]
    self.assertAllEqual(expected_output, out_data)

    inp_t = input_layer.Input(shape=(None,), ragged=True, dtype=tf.int64)
    out_t = layer(inp_t)
    model = training.Model(inputs=inp_t, outputs=out_t)
    self.assertAllClose(out_data, model.predict(inp_data))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 280:</b> &nbsp; 6 fragments, nominal size 14 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5121')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 36-50
</a>
<div class="mid" id="frag5121" style="display:none"><pre>
  def _run_test(self, kwargs, expected_height, expected_width):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    kwargs.update({'height': expected_height, 'width': expected_width})
    with testing_utils.use_gpu():
      testing_utils.layer_test(
          image_preprocessing.Resizing,
          kwargs=kwargs,
          input_shape=(num_samples, orig_height, orig_width, channels),
          expected_output_shape=(None, expected_height, expected_width,
                                 channels))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5192')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1195-1208
</a>
<div class="mid" id="frag5192" style="display:none"><pre>
  def _run_test(self, factor):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    kwargs = {'factor': factor}
    with testing_utils.use_gpu():
      testing_utils.layer_test(
          image_preprocessing.RandomRotation,
          kwargs=kwargs,
          input_shape=(num_samples, orig_height, orig_width, channels),
          expected_output_shape=(None, orig_height, orig_width, channels))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5173')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 642-655
</a>
<div class="mid" id="frag5173" style="display:none"><pre>
  def _run_test(self, height_factor, width_factor):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    kwargs = {'height_factor': height_factor, 'width_factor': width_factor}
    with testing_utils.use_gpu():
      testing_utils.layer_test(
          image_preprocessing.RandomTranslation,
          kwargs=kwargs,
          input_shape=(num_samples, orig_height, orig_width, channels),
          expected_output_shape=(None, orig_height, orig_width, channels))

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5198')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1263-1276
</a>
<div class="mid" id="frag5198" style="display:none"><pre>
  def _run_test(self, height_factor, width_factor):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    kwargs = {'height_factor': height_factor, 'width_factor': width_factor}
    with testing_utils.use_gpu():
      testing_utils.layer_test(
          image_preprocessing.RandomZoom,
          kwargs=kwargs,
          input_shape=(num_samples, orig_height, orig_width, channels),
          expected_output_shape=(None, orig_height, orig_width, channels))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5141')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 287-301
</a>
<div class="mid" id="frag5141" style="display:none"><pre>
  def _run_test(self, expected_height, expected_width):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    kwargs = {'height': expected_height, 'width': expected_width}
    with testing_utils.use_gpu():
      testing_utils.layer_test(
          image_preprocessing.RandomCrop,
          kwargs=kwargs,
          input_shape=(num_samples, orig_height, orig_width, channels),
          expected_output_shape=(None, expected_height, expected_width,
                                 channels))

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5134')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 215-235
</a>
<div class="mid" id="frag5134" style="display:none"><pre>
  def _run_test(self, expected_height, expected_width):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    kwargs = {'height': expected_height, 'width': expected_width}
    input_images = np.random.random(
        (num_samples, orig_height, orig_width, channels)).astype(np.float32)
    expected_output = get_numpy_center_crop(input_images, expected_height,
                                            expected_width)
    with testing_utils.use_gpu():
      testing_utils.layer_test(
          image_preprocessing.CenterCrop,
          kwargs=kwargs,
          input_shape=(num_samples, orig_height, orig_width, channels),
          input_data=input_images,
          expected_output=expected_output,
          expected_output_shape=(None, expected_height, expected_width,
                                 channels))

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 281:</b> &nbsp; 4 fragments, nominal size 12 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5124')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 91-106
</a>
<div class="mid" id="frag5124" style="display:none"><pre>
  def test_down_sampling_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 16), (1, 4, 4, 1)).astype(dtype)
        layer = image_preprocessing.Resizing(
            height=2, width=2, interpolation='nearest')
        output_image = layer(input_image)
        # pyformat: disable
        expected_output = np.asarray([
            [5, 7],
            [13, 15]
        ]).astype(dtype)
        # pyformat: enable
        expected_output = np.reshape(expected_output, (1, 2, 2, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5220')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1542-1557
</a>
<div class="mid" id="frag5220" style="display:none"><pre>
  def test_random_width_shorter_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 8), (2, 4, 1)).astype(dtype)
        layer = image_preprocessing.RandomWidth(
            factor=(-.5, -.5), interpolation='nearest')
        output_image = layer(np.expand_dims(input_image, axis=0))
        # pyformat: disable
        expected_output = np.asarray([
            [1, 3],
            [5, 7]
        ]).astype(dtype)
        # pyformat: enable
        expected_output = np.reshape(expected_output, (1, 2, 2, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5125')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 107-124
</a>
<div class="mid" id="frag5125" style="display:none"><pre>
  def test_up_sampling_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 4), (1, 2, 2, 1)).astype(dtype)
        layer = image_preprocessing.Resizing(
            height=4, width=4, interpolation='nearest')
        output_image = layer(input_image)
        # pyformat: disable
        expected_output = np.asarray([
            [0, 0, 1, 1],
            [0, 0, 1, 1],
            [2, 2, 3, 3],
            [2, 2, 3, 3]
        ]).astype(dtype)
        # pyformat: enable
        expected_output = np.reshape(expected_output, (1, 4, 4, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5211')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1433-1448
</a>
<div class="mid" id="frag5211" style="display:none"><pre>
  def test_random_height_shorter_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 8), (4, 2, 1)).astype(dtype)
        layer = image_preprocessing.RandomHeight(
            factor=(-.5, -.5), interpolation='nearest')
        output_image = layer(np.expand_dims(input_image, axis=0))
        # pyformat: disable
        expected_output = np.asarray([
            [2, 3],
            [6, 7]
        ]).astype(dtype)
        # pyformat: enable
        expected_output = np.reshape(expected_output, (1, 2, 2, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 282:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5130')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 156-167
</a>
<div class="mid" id="frag5130" style="display:none"><pre>
  def test_unbatched_image(self):
    with testing_utils.use_gpu():
      input_image = np.reshape(np.arange(0, 16), (4, 4, 1)).astype('float32')
      layer = image_preprocessing.Resizing(2, 2, interpolation='nearest')
      output_image = layer(input_image)
      expected_output = np.asarray([
          [5, 7],
          [13, 15],
      ]).astype('float32')
      expected_output = np.reshape(expected_output, (2, 2, 1))
      self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5140')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 271-283
</a>
<div class="mid" id="frag5140" style="display:none"><pre>
  def test_unbatched_image(self):
    with testing_utils.use_gpu():
      input_image = np.reshape(np.arange(0, 16), (4, 4, 1)).astype('float32')
      layer = image_preprocessing.CenterCrop(2, 2)
      output_image = layer(input_image)
      expected_output = np.asarray([
          [5, 6],
          [9, 10],
      ]).astype('float32')
      expected_output = np.reshape(expected_output, (2, 2, 1))
      self.assertAllEqual(expected_output, output_image)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 283:</b> &nbsp; 2 fragments, nominal size 10 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5148')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 352-362
</a>
<div class="mid" id="frag5148" style="display:none"><pre>
  def test_predicting_with_mock_longer_height(self):
    np.random.seed(1337)
    height, width = 3, 3
    inp = np.random.random((12, 10, 6, 3))
    with testing_utils.use_gpu():
      layer = image_preprocessing.RandomCrop(height, width)
      actual_output = layer(inp, training=0)
      resized_inp = tf.image.resize(inp, size=[5, 3])
      expected_output = resized_inp[:, 1:4, :, :]
      self.assertAllClose(expected_output, actual_output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5149')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 363-373
</a>
<div class="mid" id="frag5149" style="display:none"><pre>
  def test_predicting_with_mock_longer_width(self):
    np.random.seed(1337)
    height, width = 4, 6
    inp = np.random.random((12, 8, 16, 3))
    with testing_utils.use_gpu():
      layer = image_preprocessing.RandomCrop(height, width)
      actual_output = layer(inp, training=0)
      resized_inp = tf.image.resize(inp, size=[4, 8])
      expected_output = resized_inp[:, :, 1:7, :]
      self.assertAllClose(expected_output, actual_output)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 284:</b> &nbsp; 12 fragments, nominal size 16 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5175')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 662-679
</a>
<div class="mid" id="frag5175" style="display:none"><pre>
  def test_random_translation_up_numeric_reflect(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
        # Shifting by -.2 * 5 = 1 pixel.
        layer = image_preprocessing.RandomTranslation(
            height_factor=(-.2, -.2), width_factor=0.)
        output_image = layer(input_image)
        expected_output = np.asarray([
            [5, 6, 7, 8, 9],
            [10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19],
            [20, 21, 22, 23, 24],
            [20, 21, 22, 23, 24],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5184')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 810-827
</a>
<div class="mid" id="frag5184" style="display:none"><pre>
  def test_unbatched_image(self):
    with testing_utils.use_gpu():
      input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(np.int64)
      # Shifting by -.2 * 5 = 1 pixel.
      layer = image_preprocessing.RandomTranslation(
          height_factor=(-.2, -.2), width_factor=0.)
      output_image = layer(input_image)
      expected_output = np.asarray([
          [5, 6, 7, 8, 9],
          [10, 11, 12, 13, 14],
          [15, 16, 17, 18, 19],
          [20, 21, 22, 23, 24],
          [20, 21, 22, 23, 24],
      ]).astype(np.int64)
      expected_output = np.reshape(expected_output, (5, 5, 1))
      self.assertAllEqual(expected_output, output_image)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5202')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1306-1323
</a>
<div class="mid" id="frag5202" style="display:none"><pre>
  def test_random_zoom_out_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(dtype)
        layer = image_preprocessing.RandomZoom((.5, .5), (.8, .8),
                                               fill_mode='constant',
                                               interpolation='nearest')
        output_image = layer(np.expand_dims(input_image, axis=0))
        expected_output = np.asarray([
            [0, 0, 0, 0, 0],
            [0, 5, 7, 9, 0],
            [0, 10, 12, 14, 0],
            [0, 20, 22, 24, 0],
            [0, 0, 0, 0, 0],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5181')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 775-792
</a>
<div class="mid" id="frag5181" style="display:none"><pre>
  def test_random_translation_left_numeric_constant(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
        # Shifting by -.2 * 5 = 1 pixel.
        layer = image_preprocessing.RandomTranslation(
            height_factor=0., width_factor=(-.2, -.2), fill_mode='constant')
        output_image = layer(input_image)
        expected_output = np.asarray([
            [1, 2, 3, 4, 0],
            [6, 7, 8, 9, 0],
            [11, 12, 13, 14, 0],
            [16, 17, 18, 19, 0],
            [21, 22, 23, 24, 0],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5201')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1289-1305
</a>
<div class="mid" id="frag5201" style="display:none"><pre>
  def test_random_zoom_in_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(dtype)
        layer = image_preprocessing.RandomZoom((-.5, -.5), (-.5, -.5),
                                               interpolation='nearest')
        output_image = layer(np.expand_dims(input_image, axis=0))
        expected_output = np.asarray([
            [6, 7, 7, 8, 8],
            [11, 12, 12, 13, 13],
            [11, 12, 12, 13, 13],
            [16, 17, 17, 18, 18],
            [16, 17, 17, 18, 18],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5176')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 680-697
</a>
<div class="mid" id="frag5176" style="display:none"><pre>
  def test_random_translation_up_numeric_constant(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
        # Shifting by -.2 * 5 = 1 pixel.
        layer = image_preprocessing.RandomTranslation(
            height_factor=(-.2, -.2), width_factor=0., fill_mode='constant')
        output_image = layer(input_image)
        expected_output = np.asarray([
            [5, 6, 7, 8, 9],
            [10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19],
            [20, 21, 22, 23, 24],
            [0, 0, 0, 0, 0],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5177')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 698-715
</a>
<div class="mid" id="frag5177" style="display:none"><pre>
  def test_random_translation_down_numeric_reflect(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
        # Shifting by .2 * 5 = 1 pixel.
        layer = image_preprocessing.RandomTranslation(
            height_factor=(.2, .2), width_factor=0.)
        output_image = layer(input_image)
        expected_output = np.asarray([
            [0, 1, 2, 3, 4],
            [0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9],
            [10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5180')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 757-774
</a>
<div class="mid" id="frag5180" style="display:none"><pre>
  def test_random_translation_left_numeric_reflect(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
        # Shifting by .2 * 5 = 1 pixel.
        layer = image_preprocessing.RandomTranslation(
            height_factor=0., width_factor=(-.2, -.2))
        output_image = layer(input_image)
        expected_output = np.asarray([
            [1, 2, 3, 4, 4],
            [6, 7, 8, 9, 9],
            [11, 12, 13, 14, 14],
            [16, 17, 18, 19, 19],
            [21, 22, 23, 24, 24],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5179')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 739-756
</a>
<div class="mid" id="frag5179" style="display:none"><pre>
  def test_random_translation_down_numeric_constant(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)
        # Shifting by -.2 * 5 = 1 pixel.
        layer = image_preprocessing.RandomTranslation(
            height_factor=(.2, .2), width_factor=0., fill_mode='constant')
        output_image = layer(input_image)
        expected_output = np.asarray([
            [0, 0, 0, 0, 0],
            [0, 1, 2, 3, 4],
            [5, 6, 7, 8, 9],
            [10, 11, 12, 13, 14],
            [15, 16, 17, 18, 19],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5203')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1324-1341
</a>
<div class="mid" id="frag5203" style="display:none"><pre>
  def test_random_zoom_out_numeric_preserve_aspect_ratio(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(dtype)
        layer = image_preprocessing.RandomZoom((.5, .5),
                                               fill_mode='constant',
                                               interpolation='nearest')
        output_image = layer(np.expand_dims(input_image, axis=0))
        expected_output = np.asarray([
            [0, 0, 0, 0, 0],
            [0, 6, 7, 9, 0],
            [0, 11, 12, 14, 0],
            [0, 21, 22, 24, 0],
            [0, 0, 0, 0, 0],
        ]).astype(dtype)
        expected_output = np.reshape(expected_output, (1, 5, 5, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5206')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1358-1374
</a>
<div class="mid" id="frag5206" style="display:none"><pre>
  def test_unbatched_image(self):
    with testing_utils.use_gpu():
      input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(np.int64)
      layer = image_preprocessing.RandomZoom((-.5, -.5), (-.5, -.5),
                                             interpolation='nearest')
      output_image = layer(input_image)
      expected_output = np.asarray([
          [6, 7, 7, 8, 8],
          [11, 12, 12, 13, 13],
          [11, 12, 12, 13, 13],
          [16, 17, 17, 18, 18],
          [16, 17, 17, 18, 18],
      ]).astype(np.int64)
      expected_output = np.reshape(expected_output, (5, 5, 1))
      self.assertAllEqual(expected_output, output_image)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5197')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1243-1259
</a>
<div class="mid" id="frag5197" style="display:none"><pre>
  def test_unbatched_image(self):
    with testing_utils.use_gpu():
      input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(np.float32)
      # 180 rotation.
      layer = image_preprocessing.RandomRotation(factor=(0.5, 0.5))
      output_image = layer(input_image)
      expected_output = np.asarray([
          [24, 23, 22, 21, 20],
          [19, 18, 17, 16, 15],
          [14, 13, 12, 11, 10],
          [9, 8, 7, 6, 5],
          [4, 3, 2, 1, 0],
      ]).astype(np.float32)
      expected_output = np.reshape(expected_output, (5, 5, 1))
      self.assertAllClose(expected_output, output_image)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 285:</b> &nbsp; 5 fragments, nominal size 37 lines, similarity 76%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5186')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 847-902
</a>
<div class="mid" id="frag5186" style="display:none"><pre>
  def test_random_translation_reflect(self):
    # reflected output is (dcba|abcd|dcba)

    # Test down shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 1., 2.],
         [0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., -1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'reflect')

    # Test up shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11.],
         [12., 13., 14.],
         [12., 13., 14.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., 1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'reflect')

    # Test left shift by 1.
    # reflected output is (dcba|abcd|dcba)
    # pyformat: disable
    expected_output = np.asarray(
        [[1., 2., 2.],
         [4., 5., 5.],
         [7., 8., 8.],
         [10., 11., 11.],
         [13., 14., 14.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'reflect')

    # Test right shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 0., 1.],
         [3., 3., 4],
         [6., 6., 7.],
         [9., 9., 10.],
         [12., 12., 13.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., -1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'reflect')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5187')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 903-957
</a>
<div class="mid" id="frag5187" style="display:none"><pre>
  def test_random_translation_wrap(self):
    # warpped output is (abcd|abcd|abcd)

    # Test down shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[12., 13., 14.],
         [0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., -1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'wrap')

    # Test up shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11.],
         [12., 13., 14.],
         [0., 1., 2.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., 1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'wrap')

    # Test left shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[1., 2., 0.],
         [4., 5., 3.],
         [7., 8., 6.],
         [10., 11., 9.],
         [13., 14., 12.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'wrap')

    # Test right shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[2., 0., 1.],
         [5., 3., 4],
         [8., 6., 7.],
         [11., 9., 10.],
         [14., 12., 13.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., -1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'wrap')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5188')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 958-1012
</a>
<div class="mid" id="frag5188" style="display:none"><pre>
  def test_random_translation_nearest(self):
    # nearest output is (aaaa|abcd|dddd)

    # Test down shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 1., 2.],
         [0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., -1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'nearest')

    # Test up shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11.],
         [12., 13., 14.],
         [12., 13., 14.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., 1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'nearest')

    # Test left shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[1., 2., 2.],
         [4., 5., 5.],
         [7., 8., 8.],
         [10., 11., 11.],
         [13., 14., 14.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'nearest')

    # Test right shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 0., 1.],
         [3., 3., 4],
         [6., 6., 7.],
         [9., 9., 10.],
         [12., 12., 13.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., -1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'nearest')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5189')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1013-1067
</a>
<div class="mid" id="frag5189" style="display:none"><pre>
  def test_random_translation_constant_0(self):
    # constant output is (0000|abcd|0000)

    # Test down shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 0., 0.],
         [0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., -1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'constant')

    # Test up shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[3., 4., 5.],
         [6., 7., 8],
         [9., 10., 11.],
         [12., 13., 14.],
         [0., 0., 0.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 0., 0., 1., 1., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'constant')

    # Test left shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[1., 2., 0.],
         [4., 5., 0.],
         [7., 8., 0.],
         [10., 11., 0.],
         [13., 14., 0.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., 1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'constant')

    # Test right shift by 1.
    # pyformat: disable
    expected_output = np.asarray(
        [[0., 0., 1.],
         [0., 3., 4],
         [0., 6., 7.],
         [0., 9., 10.],
         [0., 12., 13.]]).reshape((1, 5, 3, 1)).astype(np.float32)
    # pyformat: enable
    transform_matrix = np.asarray([[1., 0., -1., 0., 1., 0., 0., 0.]])
    self._run_random_transform_with_mock(transform_matrix, expected_output,
                                         'constant')

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5190')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1068-1123
</a>
<div class="mid" id="frag5190" style="display:none"><pre>
  def test_random_translation_constant_1(self):
    with tf.compat.forward_compatibility_horizon(2020, 8, 6):
      # constant output is (1111|abcd|1111)

      # Test down shift by 1.
      # pyformat: disable
      expected_output = np.asarray(
          [[1., 1., 1.],
           [0., 1., 2.],
           [3., 4., 5.],
           [6., 7., 8],
           [9., 10., 11]]).reshape((1, 5, 3, 1)).astype(np.float32)
      # pyformat: enable
      transform_matrix = np.asarray([[1., 0., 0., 0., 1., -1., 0., 0.]])
      self._run_random_transform_with_mock(
          transform_matrix, expected_output, 'constant', fill_value=1.0)

      # Test up shift by 1.
      # pyformat: disable
      expected_output = np.asarray(
          [[3., 4., 5.],
           [6., 7., 8],
           [9., 10., 11.],
           [12., 13., 14.],
           [1., 1., 1.]]).reshape((1, 5, 3, 1)).astype(np.float32)
      # pyformat: enable
      transform_matrix = np.asarray([[1., 0., 0., 0., 1., 1., 0., 0.]])
      self._run_random_transform_with_mock(
          transform_matrix, expected_output, 'constant', fill_value=1.0)

      # Test left shift by 1.
      # pyformat: disable
      expected_output = np.asarray(
          [[1., 2., 1.],
           [4., 5., 1.],
           [7., 8., 1.],
           [10., 11., 1.],
           [13., 14., 1.]]).reshape((1, 5, 3, 1)).astype(np.float32)
      # pyformat: enable
      transform_matrix = np.asarray([[1., 0., 1., 0., 1., 0., 0., 0.]])
      self._run_random_transform_with_mock(
          transform_matrix, expected_output, 'constant', fill_value=1.0)

      # Test right shift by 1.
      # pyformat: disable
      expected_output = np.asarray(
          [[1., 0., 1.],
           [1., 3., 4],
           [1., 6., 7.],
           [1., 9., 10.],
           [1., 12., 13.]]).reshape((1, 5, 3, 1)).astype(np.float32)
      # pyformat: enable
      transform_matrix = np.asarray([[1., 0., -1., 0., 1., 0., 0., 0.]])
      self._run_random_transform_with_mock(
          transform_matrix, expected_output, 'constant', fill_value=1.0)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 286:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5207')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1378-1391
</a>
<div class="mid" id="frag5207" style="display:none"><pre>
  def _run_test(self, factor):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    with testing_utils.use_gpu():
      img = np.random.random((num_samples, orig_height, orig_width, channels))
      layer = image_preprocessing.RandomHeight(factor)
      img_out = layer(img, training=True)
      self.assertEqual(img_out.shape[0], 2)
      self.assertEqual(img_out.shape[2], 8)
      self.assertEqual(img_out.shape[3], 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5216')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1488-1501
</a>
<div class="mid" id="frag5216" style="display:none"><pre>
  def _run_test(self, factor):
    np.random.seed(1337)
    num_samples = 2
    orig_height = 5
    orig_width = 8
    channels = 3
    with testing_utils.use_gpu():
      img = np.random.random((num_samples, orig_height, orig_width, channels))
      layer = image_preprocessing.RandomWidth(factor)
      img_out = layer(img, training=True)
      self.assertEqual(img_out.shape[0], 2)
      self.assertEqual(img_out.shape[1], 5)
      self.assertEqual(img_out.shape[3], 3)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 287:</b> &nbsp; 4 fragments, nominal size 13 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5209')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1398-1412
</a>
<div class="mid" id="frag5209" style="display:none"><pre>
  def test_valid_random_height(self):
    # need (maxval - minval) * rnd + minval = 0.6
    mock_factor = 0
    with tf.compat.v1.test.mock.patch.object(
        gen_stateful_random_ops, 'stateful_uniform', return_value=mock_factor):
      with tf.compat.v1.test.mock.patch.object(
          gen_stateless_random_ops_v2,
          'stateless_random_uniform_v2',
          return_value=mock_factor):
        with testing_utils.use_gpu():
          img = np.random.random((12, 5, 8, 3))
          layer = image_preprocessing.RandomHeight(.4)
          img_out = layer(img, training=True)
          self.assertEqual(img_out.shape[1], 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5215')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1469-1484
</a>
<div class="mid" id="frag5215" style="display:none"><pre>
  def test_unbatched_image(self):
    # need (maxval - minval) * rnd + minval = 0.6
    mock_factor = 0
    with tf.compat.v1.test.mock.patch.object(
        gen_stateful_random_ops, 'stateful_uniform', return_value=mock_factor):
      with tf.compat.v1.test.mock.patch.object(
          gen_stateless_random_ops_v2,
          'stateless_random_uniform_v2',
          return_value=mock_factor):
        with testing_utils.use_gpu():
          img = np.random.random((5, 8, 3))
          layer = image_preprocessing.RandomHeight(.4)
          img_out = layer(img, training=True)
          self.assertEqual(img_out.shape[0], 3)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5218')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1508-1522
</a>
<div class="mid" id="frag5218" style="display:none"><pre>
  def test_valid_random_width(self):
    # need (maxval - minval) * rnd + minval = 0.6
    mock_factor = 0
    with tf.compat.v1.test.mock.patch.object(
        gen_stateful_random_ops, 'stateful_uniform', return_value=mock_factor):
      with tf.compat.v1.test.mock.patch.object(
          gen_stateless_random_ops_v2,
          'stateless_random_uniform_v2',
          return_value=mock_factor):
        with testing_utils.use_gpu():
          img = np.random.random((12, 8, 5, 3))
          layer = image_preprocessing.RandomWidth(.4)
          img_out = layer(img, training=True)
          self.assertEqual(img_out.shape[2], 3)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5224')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1578-1593
</a>
<div class="mid" id="frag5224" style="display:none"><pre>
  def test_unbatched_image(self):
    # need (maxval - minval) * rnd + minval = 0.6
    mock_factor = 0
    with tf.compat.v1.test.mock.patch.object(
        gen_stateful_random_ops, 'stateful_uniform', return_value=mock_factor):
      with tf.compat.v1.test.mock.patch.object(
          gen_stateless_random_ops_v2,
          'stateless_random_uniform_v2',
          return_value=mock_factor):
        with testing_utils.use_gpu():
          img = np.random.random((8, 5, 3))
          layer = image_preprocessing.RandomWidth(.4)
          img_out = layer(img, training=True)
          self.assertEqual(img_out.shape[1], 3)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 288:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 71%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5210')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1413-1432
</a>
<div class="mid" id="frag5210" style="display:none"><pre>
  def test_random_height_longer_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 6), (2, 3, 1)).astype(dtype)
        layer = image_preprocessing.RandomHeight(factor=(1., 1.))
        # Return type of RandomHeight() is float32 if `interpolation` is not
        # set to `ResizeMethod.NEAREST_NEIGHBOR`; cast `layer` to desired dtype.
        output_image = tf.cast(
            layer(np.expand_dims(input_image, axis=0)), dtype=dtype)
        # pyformat: disable
        expected_output = np.asarray([
            [0, 1, 2],
            [0.75, 1.75, 2.75],
            [2.25, 3.25, 4.25],
            [3, 4, 5]
        ]).astype(dtype)
        # pyformat: enable
        expected_output = np.reshape(expected_output, (1, 4, 3, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5219')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1523-1541
</a>
<div class="mid" id="frag5219" style="display:none"><pre>
  def test_random_width_longer_numeric(self):
    for dtype in (np.int64, np.float32):
      with testing_utils.use_gpu():
        input_image = np.reshape(np.arange(0, 6), (3, 2, 1)).astype(dtype)
        layer = image_preprocessing.RandomWidth(factor=(1., 1.))
        # Return type of RandomWidth() is float32 if `interpolation` is not
        # set to `ResizeMethod.NEAREST_NEIGHBOR`; cast `layer` to desired dtype.
        output_image = tf.cast(
            layer(np.expand_dims(input_image, axis=0)), dtype=dtype)
        # pyformat: disable
        expected_output = np.asarray([
            [0, 0.25, 0.75, 1],
            [2, 2.25, 2.75, 3],
            [4, 4.25, 4.75, 5]
        ]).astype(dtype)
        # pyformat: enable
        expected_output = np.reshape(expected_output, (1, 3, 4, 1))
        self.assertAllEqual(expected_output, output_image)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 289:</b> &nbsp; 2 fragments, nominal size 11 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5225')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1597-1609
</a>
<div class="mid" id="frag5225" style="display:none"><pre>
  def test_plain_call(self):
    layer = image_preprocessing.RandomWidth(.5, seed=123)
    shape = (12, 12, 3)
    img = np.random.random((12,) + shape)
    out = layer(img)  # Default to training=True
    self.assertNotEqual(tuple(int(i) for i in out.shape[1:]), shape)

    out = layer(img, training=True)
    self.assertNotEqual(tuple(int(i) for i in out.shape[1:]), shape)

    out = layer(img, training=False)
    self.assertEqual(tuple(int(i) for i in out.shape[1:]), shape)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5226')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/image_preprocessing_test.py: 1610-1626
</a>
<div class="mid" id="frag5226" style="display:none"><pre>
  def test_call_in_container(self):
    layer1 = image_preprocessing.RandomWidth(.5, seed=123)
    layer2 = image_preprocessing.RandomHeight(.5, seed=123)
    seq = sequential.Sequential([layer1, layer2])

    shape = (12, 12, 3)
    img = np.random.random((12,) + shape)
    out = seq(img)  # Default to training=True
    self.assertNotEqual(tuple(int(i) for i in out.shape[1:]), shape)

    out = seq(img, training=True)
    self.assertNotEqual(tuple(int(i) for i in out.shape[1:]), shape)

    out = seq(img, training=False)
    self.assertEqual(tuple(int(i) for i in out.shape[1:]), shape)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 290:</b> &nbsp; 4 fragments, nominal size 17 lines, similarity 100%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5234')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 425-443
</a>
<div class="mid" id="frag5234" style="display:none"><pre>
  def test_ragged_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                              dtype=np.int64)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5244')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 641-659
</a>
<div class="mid" id="frag5244" style="display:none"><pre>
  def test_ragged_int_input(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                              dtype=np.int64)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5235')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 444-463
</a>
<div class="mid" id="frag5235" style="display:none"><pre>
  def test_int32_input_with_int64_keys(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],
                                              dtype=np.int32)
    expected_output = [[2, 3, 5], [5, 4, 2, 1]]

    input_data = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5239')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 539-558
</a>
<div class="mid" id="frag5239" style="display:none"><pre>
  def test_ragged_int_input_multi_bucket(self):
    vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)
    input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 133]],
                                              dtype=np.int64)
    expected_output = [[3, 4, 6], [6, 5, 3, 2]]

    input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        dtype=tf.int64,
        num_oov_indices=2,
        mask_token=0,
        oov_token=-1)
    layer.set_vocabulary(vocab_data)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 291:</b> &nbsp; 2 fragments, nominal size 14 lines, similarity 73%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5240')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 564-580
</a>
<div class="mid" id="frag5240" style="display:none"><pre>
  def test_sparse_adapt(self):
    vocab_data = tf.SparseTensor(
        indices=[[0, 0], [0, 1], [1, 2]],
        values=["michigan", "fire", "michigan"],
        dense_shape=[3, 4])
    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)

    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.adapt(vocab_dataset)
    expected_vocabulary = ["", "[OOV]", "michigan", "fire"]
    self.assertAllEqual(expected_vocabulary, layer.get_vocabulary())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5241')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 581-595
</a>
<div class="mid" id="frag5241" style="display:none"><pre>
  def test_ragged_adapt(self):
    vocab_data = tf.ragged.constant([["michigan"],
                                              ["fire", "michigan"]])
    vocab_dataset = tf.data.Dataset.from_tensors(vocab_data)

    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.adapt(vocab_dataset)
    expected_vocabulary = ["", "[OOV]", "michigan", "fire"]
    self.assertAllEqual(expected_vocabulary, layer.get_vocabulary())

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 292:</b> &nbsp; 5 fragments, nominal size 11 lines, similarity 83%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5251')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 748-758
</a>
<div class="mid" id="frag5251" style="display:none"><pre>
  def test_int_output_shape(self):
    input_data = keras.Input(batch_size=16, shape=(4,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=2,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    int_data = layer(input_data)
    self.assertAllEqual(int_data.shape.as_list(), [16, 4])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5265')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1078-1090
</a>
<div class="mid" id="frag5265" style="display:none"><pre>
  def test_multi_hot_output_shape(self):
    input_data = keras.Input(batch_size=16, shape=(4,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=2,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.MULTI_HOT,
        vocabulary=["foo"],
        dtype=tf.string)
    binary_data = layer(input_data)
    self.assertAllEqual(binary_data.shape.as_list(), [16, 2])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5268')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1140-1152
</a>
<div class="mid" id="frag5268" style="display:none"><pre>
  def test_count_output_shape(self):
    input_data = keras.Input(batch_size=16, shape=(4,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=2,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.COUNT,
        vocabulary=["foo"],
        dtype=tf.string)
    count_data = layer(input_data)
    self.assertAllEqual(count_data.shape.as_list(), [16, 2])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5271')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1206-1218
</a>
<div class="mid" id="frag5271" style="display:none"><pre>
  def test_ifidf_output_shape(self):
    input_data = keras.Input(batch_size=16, shape=(4,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=2,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.TF_IDF,
        dtype=tf.string)
    layer.set_vocabulary(vocabulary=["foo"], idf_weights=[1.0])
    layer_output = layer(input_data)
    self.assertAllEqual(layer_output.shape.as_list(), [16, 2])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5260')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 946-958
</a>
<div class="mid" id="frag5260" style="display:none"><pre>
  def test_one_hot_output_shape(self):
    inputs = keras.Input(batch_size=16, shape=(1,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        vocabulary=["earth"],
        max_tokens=2,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.ONE_HOT,
        dtype=tf.string)
    outputs = layer(inputs)
    self.assertAllEqual(outputs.shape.as_list(), [16, 2])

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 293:</b> &nbsp; 8 fragments, nominal size 23 lines, similarity 75%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5257')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 874-901
</a>
<div class="mid" id="frag5257" style="display:none"><pre>
  def test_one_hot_output_hard_maximum(self):
    """Check binary output when pad_to_max_tokens=True."""
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array(["earth", "wind", "and", "fire", "michigan", ""])
    expected_output = [
        [0, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0],
        [0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0],
    ]

    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=6,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.ONE_HOT,
        pad_to_max_tokens=True,
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    binary_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=binary_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5266')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1091-1115
</a>
<div class="mid" id="frag5266" style="display:none"><pre>
  def test_count_output_hard_maxiumum(self):
    """Check count output when pad_to_max_tokens=True."""
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "wind", ""],
                            ["fire", "fire", "fire", "michigan", ""]])
    expected_output = [
        [0, 1, 2, 1, 0, 0],
        [1, 0, 0, 0, 3, 0],
    ]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=6,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.COUNT,
        pad_to_max_tokens=True,
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    count_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=count_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5261')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 959-983
</a>
<div class="mid" id="frag5261" style="display:none"><pre>
  def test_multi_hot_output_hard_maximum(self):
    """Check binary output when pad_to_max_tokens=True."""
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire", ""],
                            ["fire", "fire", "and", "earth", "michigan"]])
    expected_output = [
        [0, 1, 1, 1, 1, 0],
        [1, 1, 0, 1, 1, 0],
    ]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=6,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.MULTI_HOT,
        pad_to_max_tokens=True,
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    binary_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=binary_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5267')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1116-1139
</a>
<div class="mid" id="frag5267" style="display:none"><pre>
  def test_count_output_soft_maximum(self):
    """Check count output when pad_to_max_tokens=False."""
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "wind", ""],
                            ["fire", "fire", "fire", "michigan", ""]])
    expected_output = [
        [0, 1, 2, 1, 0],
        [1, 0, 0, 0, 3],
    ]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.COUNT,
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    count_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=count_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5270')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1180-1205
</a>
<div class="mid" id="frag5270" style="display:none"><pre>
  def test_ifidf_output_soft_maximum(self):
    """Check tf-idf output when pad_to_max_tokens=False."""
    vocab_data = ["earth", "wind", "and", "fire"]
    # OOV idf weight (bucket 0) should 0.5, the average of passed weights.
    idf_weights = [.4, .25, .75, .6]
    input_array = np.array([["earth", "wind", "and", "earth", ""],
                            ["ohio", "fire", "earth", "michigan", ""]])
    expected_output = [
        [0.00, 0.80, 0.25, 0.75, 0.00],
        [1.00, 0.40, 0.00, 0.00, 0.60],
    ]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.TF_IDF,
        dtype=tf.string)
    layer.set_vocabulary(vocab_data, idf_weights=idf_weights)
    layer_output = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=layer_output)
    output_dataset = model.predict(input_array)
    self.assertAllClose(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5269')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1153-1179
</a>
<div class="mid" id="frag5269" style="display:none"><pre>
  def test_ifidf_output_hard_maximum(self):
    """Check tf-idf output when pad_to_max_tokens=True."""
    vocab_data = ["earth", "wind", "and", "fire"]
    # OOV idf weight (bucket 0) should 0.5, the average of passed weights.
    idf_weights = [.4, .25, .75, .6]
    input_array = np.array([["earth", "wind", "and", "earth", ""],
                            ["ohio", "fire", "earth", "michigan", ""]])
    expected_output = [
        [0.00, 0.80, 0.25, 0.75, 0.00, 0.00],
        [1.00, 0.40, 0.00, 0.00, 0.60, 0.00],
    ]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=6,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.TF_IDF,
        pad_to_max_tokens=True,
        dtype=tf.string)
    layer.set_vocabulary(vocab_data, idf_weights=idf_weights)
    layer_output = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=layer_output)
    output_dataset = model.predict(input_array)
    self.assertAllClose(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5264')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1054-1077
</a>
<div class="mid" id="frag5264" style="display:none"><pre>
  def test_multi_hot_output_soft_maximum(self):
    """Check multi_hot output when pad_to_max_tokens=False."""
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire", ""],
                            ["fire", "and", "earth", "michigan", ""]])
    expected_output = [
        [0, 1, 1, 1, 1],
        [1, 1, 0, 1, 1],
    ]

    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.MULTI_HOT,
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    binary_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=binary_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5258')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 902-928
</a>
<div class="mid" id="frag5258" style="display:none"><pre>
  def test_one_hot_output_soft_maximum(self):
    """Check binary output when pad_to_max_tokens=False."""
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array(["earth", "wind", "and", "fire", "michigan", ""])
    expected_output = [
        [0, 1, 0, 0, 0],
        [0, 0, 1, 0, 0],
        [0, 0, 0, 1, 0],
        [0, 0, 0, 0, 1],
        [1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0],
    ]

    input_data = keras.Input(shape=(1,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        output_mode=index_lookup.ONE_HOT,
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    binary_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=binary_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(expected_output, output_dataset)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 294:</b> &nbsp; 30 fragments, nominal size 10 lines, similarity 70%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5274')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1254-1264
</a>
<div class="mid" id="frag5274" style="display:none"><pre>
    def compute(data):
      layer = index_lookup.IndexLookup(
          vocabulary=vocab_file,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          output_mode=index_lookup.MULTI_HOT,
          dtype=tf.string)
      return layer(data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5317')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1901-1911
</a>
<div class="mid" id="frag5317" style="display:none"><pre>
  def test_non_int_output_fails(self):
    with self.assertRaisesRegex(ValueError, "`output_mode` must be int"):
      _ = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          dtype=tf.string,
          output_mode=index_lookup.COUNT,
          invert=True)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5300')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1679-1689
</a>
<div class="mid" id="frag5300" style="display:none"><pre>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        dtype=tf.string)
    with self.assertRaisesRegex(ValueError, ".*Reserved mask.*"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5299')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1668-1678
</a>
<div class="mid" id="frag5299" style="display:none"><pre>
  def test_vocab_with_reserved_oov_element_fails(self):
    vocab_data = ["earth", "test", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    with self.assertRaisesRegex(ValueError, ".*Reserved OOV.*"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5298')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1657-1667
</a>
<div class="mid" id="frag5298" style="display:none"><pre>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    with self.assertRaisesRegex(ValueError, ".*repeated term.*earth.*"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5297')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1646-1656
</a>
<div class="mid" id="frag5297" style="display:none"><pre>
  def test_vocab_with_mask_but_no_oov_fails(self):
    vocab_data = ["", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    with self.assertRaisesRegex(ValueError, ".*does not have the OOV token.*"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5296')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1635-1645
</a>
<div class="mid" id="frag5296" style="display:none"><pre>
  def test_vocab_with_oov_and_no_mask_fails(self):
    vocab_data = ["[OOV]", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    with self.assertRaisesRegex(ValueError, ".*Reserved OOV.*"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5295')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1624-1634
</a>
<div class="mid" id="frag5295" style="display:none"><pre>
  def test_vocab_with_oov_and_wrong_mask_fails(self):
    vocab_data = ["custom_mask", "[OOV]", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    with self.assertRaisesRegex(ValueError, ".*does not have the mask token.*"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5294')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1613-1623
</a>
<div class="mid" id="frag5294" style="display:none"><pre>
  def test_non_unique_vocab_fails(self):
    vocab_data = ["earth", "wind", "and", "fire", "fire"]
    with self.assertRaisesRegex(ValueError, ".*repeated term.*fire.*"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          dtype=tf.string)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5316')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1889-1900
</a>
<div class="mid" id="frag5316" style="display:none"><pre>
  def test_non_unique_vocab_fails(self):
    vocab_data = ["earth", "wind", "and", "fire", "fire"]
    with self.assertRaisesRegex(ValueError, ".*repeated term.*fire.*"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token="",
          oov_token="[OOV]",
          dtype=tf.string,
          invert=True)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5319')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1924-1935
</a>
<div class="mid" id="frag5319" style="display:none"><pre>
  def test_vocab_with_reserved_mask_element_fails(self):
    vocab_data = ["earth", "mask_token", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="mask_token",
        oov_token="[OOV]",
        dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, ".*Reserved mask.*"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5318')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1912-1923
</a>
<div class="mid" id="frag5318" style="display:none"><pre>
  def test_vocab_with_repeated_element_fails(self):
    vocab_data = ["earth", "earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        invert=True)
    with self.assertRaisesRegex(ValueError, ".*repeated term.*earth.*"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5306')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1761-1771
</a>
<div class="mid" id="frag5306" style="display:none"><pre>
  def test_int_vocab_with_oov_and_wrong_mask_fails(self):
    vocab_data = [1234, -1, 11, 21, 13, 14]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "does not have the mask token `0`"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5310')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1805-1815
</a>
<div class="mid" id="frag5310" style="display:none"><pre>
  def test_int_vocab_with_reserved_oov_element_fails(self):
    vocab_data = [14, 38, -1, 34, 3, 84]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "Reserved OOV"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5292')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1589-1600
</a>
<div class="mid" id="frag5292" style="display:none"><pre>
  def test_vocab_with_multiple_oov_indices(self):
    vocab_data = ["", "[OOV]", "[OOV]", "[OOV]", "wind"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5288')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1538-1549
</a>
<div class="mid" id="frag5288" style="display:none"><pre>
  def test_vocab_multi_oov(self):
    vocab_data = ["", "[OOV]", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(returned_vocab, vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5308')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1783-1793
</a>
<div class="mid" id="frag5308" style="display:none"><pre>
  def test_int_vocab_with_mask_but_no_oov_fails(self):
    vocab_data = [0, 11, 12, 13, 14]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "does not have the OOV token `-1`"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5311')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1816-1826
</a>
<div class="mid" id="frag5311" style="display:none"><pre>
  def test_int_vocab_with_reserved_mask_element_fails(self):
    vocab_data = [125, 0, 3, 4, 94]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "Reserved mask"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5307')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1772-1782
</a>
<div class="mid" id="frag5307" style="display:none"><pre>
  def test_int_vocab_with_oov_and_no_mask_fails(self):
    vocab_data = [-1, 11, 12, 13, 14]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "Reserved OOV"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5309')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1794-1804
</a>
<div class="mid" id="frag5309" style="display:none"><pre>
  def test_int_vocab_with_repeated_element_fails(self):
    vocab_data = [11, 11, 34, 23, 124]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    with self.assertRaisesRegex(ValueError, "repeated term.*11"):
      layer.set_vocabulary(vocab_data)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5305')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1750-1760
</a>
<div class="mid" id="frag5305" style="display:none"><pre>
  def test_non_unique_int_vocab_fails(self):
    vocab_data = [12, 13, 14, 15, 15]
    with self.assertRaisesRegex(ValueError, "repeated term.*15"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token=0,
          oov_token=-1,
          dtype=tf.int64)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5320')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1936-1947
</a>
<div class="mid" id="frag5320" style="display:none"><pre>
  def test_non_unique_int_vocab_fails(self):
    vocab_data = [12, 13, 14, 15, 15]
    with self.assertRaisesRegex(ValueError, ".*repeated term.*15.*"):
      _ = index_lookup.IndexLookup(
          vocabulary=vocab_data,
          max_tokens=None,
          num_oov_indices=1,
          mask_token=0,
          oov_token=-1,
          dtype=tf.int64,
          invert=True)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5314')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1863-1875
</a>
<div class="mid" id="frag5314" style="display:none"><pre>
  def test_vocab_with_max_cap(self):
    vocab_data = ["", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        invert=True)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5321')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1948-1960
</a>
<div class="mid" id="frag5321" style="display:none"><pre>
  def test_int_vocab_with_repeated_element_fails(self):
    vocab_data = [11, 11, 34, 23, 124]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64,
        invert=True)
    with self.assertRaisesRegex(ValueError, ".*repeated term.*11.*"):
      layer.set_vocabulary(vocab_data)


</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5293')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1601-1612
</a>
<div class="mid" id="frag5293" style="display:none"><pre>
  def test_int_vocab_with_multiple_oov_indices(self):
    vocab_data = [0, -1, -1, -1, 42]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=3,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5289')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1550-1562
</a>
<div class="mid" id="frag5289" style="display:none"><pre>
  def test_vocab_multi_oov_not_present(self):
    vocab_data = ["wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=10,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(returned_vocab,
                        [""] + ["[OOV]"] * 10 + ["wind", "and", "fire"])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5290')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1563-1575
</a>
<div class="mid" id="frag5290" style="display:none"><pre>
  def test_vocab_with_max_cap(self):
    vocab_data = ["", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)
    self.assertAllEqual(layer.vocabulary_size(), 5)

</pre></div>
</td>
</tr><tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5315')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1876-1888
</a>
<div class="mid" id="frag5315" style="display:none"><pre>
  def test_int_vocab_with_max_cap(self):
    vocab_data = [0, -1, 42, 1276, 1138]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64,
        invert=True)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5291')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1576-1588
</a>
<div class="mid" id="frag5291" style="display:none"><pre>
  def test_int_vocab_with_max_cap(self):
    vocab_data = [0, -1, 42, 1276, 1138]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token=0,
        oov_token=-1,
        dtype=tf.int64)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary()
    self.assertAllEqual(vocab_data, returned_vocab)
    self.assertAllEqual(layer.vocabulary_size(), 5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5287')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1525-1537
</a>
<div class="mid" id="frag5287" style="display:none"><pre>
  def test_get_vocabulary_no_special_tokens(self):
    vocab_data = ["", "[OOV]", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=5,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)
    layer.set_vocabulary(vocab_data)
    returned_vocab = layer.get_vocabulary(include_special_tokens=False)
    self.assertAllEqual(returned_vocab, ["wind", "and", "fire"])
    self.assertAllEqual(layer.vocabulary_size(), 5)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 295:</b> &nbsp; 3 fragments, nominal size 25 lines, similarity 92%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5275')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1268-1297
</a>
<div class="mid" id="frag5275" style="display:none"><pre>
  def test_file_vocab_and_list_vocab_identical_attrs(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    file_layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)

    list_layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)

    expected_vocab = ["", "[OOV]", "earth", "wind", "and", "fire"]
    self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())
    expected_vocab_size = 6
    self.assertAllEqual(expected_vocab_size, list_layer.vocabulary_size())
    self.assertAllEqual(list_layer.get_vocabulary(),
                        file_layer.get_vocabulary())
    self.assertAllEqual(list_layer.vocabulary_size(),
                        file_layer.vocabulary_size())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5276')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1298-1327
</a>
<div class="mid" id="frag5276" style="display:none"><pre>
  def test_file_vocab_and_list_vocab_identical_attrs_multi_oov(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    file_layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)

    list_layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=2,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string)

    expected_vocab = ["", "[OOV]", "[OOV]", "earth", "wind", "and", "fire"]
    self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())
    expected_vocab_size = 7
    self.assertAllEqual(expected_vocab_size, list_layer.vocabulary_size())
    self.assertAllEqual(list_layer.get_vocabulary(),
                        file_layer.get_vocabulary())
    self.assertAllEqual(list_layer.vocabulary_size(),
                        file_layer.vocabulary_size())

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5277')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1328-1357
</a>
<div class="mid" id="frag5277" style="display:none"><pre>
  def test_file_vocab_and_list_vocab_identical_attrs_no_mask(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    file_layer = index_lookup.IndexLookup(
        vocabulary=vocab_file,
        max_tokens=None,
        num_oov_indices=2,
        mask_token=None,
        oov_token="[OOV]",
        dtype=tf.string)

    list_layer = index_lookup.IndexLookup(
        vocabulary=vocab_data,
        max_tokens=None,
        num_oov_indices=2,
        mask_token=None,
        oov_token="[OOV]",
        dtype=tf.string)

    expected_vocab = ["[OOV]", "[OOV]", "earth", "wind", "and", "fire"]
    self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())
    expected_vocab_size = 6
    self.assertAllEqual(expected_vocab_size, list_layer.vocabulary_size())
    self.assertAllEqual(list_layer.get_vocabulary(),
                        file_layer.get_vocabulary())
    self.assertAllEqual(list_layer.vocabulary_size(),
                        file_layer.vocabulary_size())

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 296:</b> &nbsp; 2 fragments, nominal size 13 lines, similarity 85%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5282')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1451-1463
</a>
<div class="mid" id="frag5282" style="display:none"><pre>
  def test_dataset_map_output(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=0,
        mask_token=None,
        oov_token="[OOV]",
        vocabulary=vocab_data,
        dtype=tf.string)
    ds = tf.data.Dataset.from_tensor_slices([["earth"], ["wind"], ["and"]])
    ds = ds.map(layer)
    self.assertAllEqual(list(ds.as_numpy_iterator()), [[0], [1], [2]])

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5283')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 1464-1481
</a>
<div class="mid" id="frag5283" style="display:none"><pre>
  def test_dataset_map_output_layer_created_in_function(self):
    vocab_data = ["earth", "wind", "and", "fire"]

    def apply_lookup(data):
      layer = index_lookup.IndexLookup(
          max_tokens=None,
          num_oov_indices=0,
          mask_token=None,
          oov_token="[OOV]",
          vocabulary=vocab_data,
          dtype=tf.string)
      return layer(data)

    ds = tf.data.Dataset.from_tensor_slices([["earth"], ["wind"], ["and"]])
    ds = ds.map(apply_lookup)
    self.assertAllEqual(list(ds.as_numpy_iterator()), [[0], [1], [2]])


</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 297:</b> &nbsp; 3 fragments, nominal size 40 lines, similarity 90%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5329')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 2151-2217
</a>
<div class="mid" id="frag5329" style="display:none"><pre>
  def test_persistence_file_vocab_keras_save_keras_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()
    tf.io.gfile.remove(vocab_file)

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Try re-saving the layer. This simulates saving a layer contained at
    # a hub Module.
    input_data_2 = keras.Input(shape=(None,), dtype=tf.string)
    output_2 = loaded_model(input_data_2)
    model_2 = keras.Model(inputs=input_data_2, outputs=output_2)
    new_output_dataset = model_2.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model_2")
    model_2.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5331')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 2285-2352
</a>
<div class="mid" id="frag5331" style="display:none"><pre>
  def test_persistence_file_vocab_keras_save_keras_load_keras_save_keras_load(
      self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()
    tf.io.gfile.remove(vocab_file)

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Try re-saving the layer. This simulates saving a layer contained at
    # a hub Module.
    input_data_2 = keras.Input(shape=(None,), dtype=tf.string)
    output_2 = loaded_model(input_data_2)
    model_2 = keras.Model(inputs=input_data_2, outputs=output_2)
    new_output_dataset = model_2.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model_2")
    model_2.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = model_2.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5330')" href="javascript:;">
keras-2.6.0/keras/layers/preprocessing/index_lookup_test.py: 2218-2284
</a>
<div class="mid" id="frag5330" style="display:none"><pre>
  def test_persistence_file_vocab_keras_save_keras_load_tf_save_tf_load(self):
    vocab_data = ["earth", "wind", "and", "fire"]
    input_array = np.array([["earth", "wind", "and", "fire"],
                            ["fire", "and", "earth", "michigan"]])
    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]

    vocab_file = self._write_to_temp_file("temp", vocab_data)

    # Build and validate a golden model.
    input_data = keras.Input(shape=(None,), dtype=tf.string)
    layer = index_lookup.IndexLookup(
        max_tokens=None,
        num_oov_indices=1,
        mask_token="",
        oov_token="[OOV]",
        dtype=tf.string,
        vocabulary=vocab_file)
    int_data = layer(input_data)
    model = keras.Model(inputs=input_data, outputs=int_data)
    output_dataset = model.predict(input_array)
    self.assertAllEqual(output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model")
    model.save(output_path, save_format="tf")

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()
    tf.io.gfile.remove(vocab_file)

    loaded_model = keras.models.load_model(
        output_path, custom_objects={"IndexLookup": index_lookup.IndexLookup})

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = loaded_model.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Try re-saving the layer. This simulates saving a layer contained at
    # a hub Module.
    input_data_2 = keras.Input(shape=(None,), dtype=tf.string)
    output_2 = loaded_model(input_data_2)
    model_2 = keras.Model(inputs=input_data_2, outputs=output_2)
    new_output_dataset = model_2.predict(input_array)
    self.assertAllEqual(new_output_dataset, expected_output)

    # Save the model to disk.
    output_path = os.path.join(self.get_temp_dir(), "tf_keras_saved_model_2")
    tf.saved_model.save(model_2, output_path)

    # Delete the session and graph to ensure that the loaded model is generated
    # from scratch.
    keras.backend.clear_session()

    loaded_model = tf.saved_model.load(output_path)
    f = loaded_model.signatures["serving_default"]

    # Ensure that the loaded model is unique (so that the save/load is real)
    self.assertIsNot(model, loaded_model)

    # Validate correctness of the new model.
    new_output_dataset = f(tf.constant(input_array))["model"]
    self.assertAllEqual(new_output_dataset, expected_output)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<br>
<table style="width:1000px; border:2px solid lightgrey; border-radius:8px;">
<tr><td style="background-color:white">
<p style="font-size:14pt"><b>Class 298:</b> &nbsp; 2 fragments, nominal size 12 lines, similarity 84%</p>
<table cellpadding=4 border=2>
<tr>
<td width="auto">
<a onclick="javascript:ShowHide('frag5350')" href="javascript:;">
keras-2.6.0/keras/regularizers_test.py: 130-141
</a>
<div class="mid" id="frag5350" style="display:none"><pre>
  def test_regularization_shared_layer(self, regularizer):
    dense_layer = keras.layers.Dense(
        NUM_CLASSES,
        kernel_regularizer=regularizer,
        activity_regularizer=regularizer)
    model = self.create_multi_input_model_from(dense_layer, dense_layer)
    model.compile(
        loss='categorical_crossentropy',
        optimizer='sgd',
        run_eagerly=testing_utils.should_run_eagerly())
    self.assertLen(model.losses, 5)

</pre></div>
</td>
<td width="auto">
<a onclick="javascript:ShowHide('frag5351')" href="javascript:;">
keras-2.6.0/keras/regularizers_test.py: 148-163
</a>
<div class="mid" id="frag5351" style="display:none"><pre>
  def test_regularization_shared_model(self, regularizer):
    dense_layer = keras.layers.Dense(
        NUM_CLASSES,
        kernel_regularizer=regularizer,
        activity_regularizer=regularizer)

    input_tensor = keras.layers.Input(shape=(DATA_DIM,))
    dummy_model = keras.models.Model(input_tensor, dense_layer(input_tensor))

    model = self.create_multi_input_model_from(dummy_model, dummy_model)
    model.compile(
        loss='categorical_crossentropy',
        optimizer='sgd',
        run_eagerly=testing_utils.should_run_eagerly())
    self.assertLen(model.losses, 6)

</pre></div>
</td>
</tr><tr>
</tr>
</table>
</td></tr>
</table>
<script language="JavaScript">
function ShowHide(divId) { 
    if(document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display='block';
    } else { 
        document.getElementById(divId).style.display = 'none';
    } 
}
</script>
</body>
</html>

<clones>
<systeminfo processor="nicad6" system="tfx-1.6.1" granularity="functions-blind" threshold="30%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="3207" npairs="568"/>
<runinfo ncompares="410142" cputime="325814"/>
<classinfo nclasses="146"/>

<class classid="1" nclones="4" nlines="15" similarity="75">
<source file="systems/tfx-1.6.1/tfx/components/infra_validator/executor_test.py" startline="214" endline="228" pcid="69">
  def testValidateOnce_LoadOnly_FailIfRunnerWaitRaises(self):
    infra_validator = executor.Executor(self._context)
    with mock.patch.object(self._serving_binary, 'MakeClient'):
      with mock.patch.object(
          executor, '_create_model_server_runner') as mock_runner_factory:
        mock_runner = mock_runner_factory.return_value
        mock_runner.WaitUntilRunning.side_effect = ValueError
        with self.assertRaises(ValueError):
          infra_validator._ValidateOnce(
              model_path=self._model_path,
              serving_binary=self._serving_binary,
              serving_spec=self._serving_spec,
              validation_spec=self._validation_spec,
              requests=[])

</source>
<source file="systems/tfx-1.6.1/tfx/components/infra_validator/executor_test.py" startline="229" endline="245" pcid="70">
  def testValidateOnce_LoadOnly_FailIfClientWaitRaises(self):
    infra_validator = executor.Executor(self._context)
    with mock.patch.object(self._serving_binary,
                           'MakeClient') as mock_client_factory:
      mock_client = mock_client_factory.return_value
      with mock.patch.object(
          executor, '_create_model_server_runner') as mock_runner_factory:
        mock_client.WaitUntilModelLoaded.side_effect = ValueError
        with self.assertRaises(ValueError):
          infra_validator._ValidateOnce(
              model_path=self._model_path,
              serving_binary=self._serving_binary,
              serving_spec=self._serving_spec,
              validation_spec=self._validation_spec,
              requests=[])
        mock_runner_factory.return_value.WaitUntilRunning.assert_called()

</source>
<source file="systems/tfx-1.6.1/tfx/components/infra_validator/executor_test.py" startline="263" endline="280" pcid="72">
  def testValidateOnce_LoadAndQuery_FailIfSendRequestsRaises(self):
    infra_validator = executor.Executor(self._context)
    with mock.patch.object(self._serving_binary,
                           'MakeClient') as mock_client_factory:
      mock_client = mock_client_factory.return_value
      with mock.patch.object(
          executor, '_create_model_server_runner') as mock_runner_factory:
        mock_client.SendRequests.side_effect = ValueError
        with self.assertRaises(ValueError):
          infra_validator._ValidateOnce(
              model_path=self._model_path,
              serving_binary=self._serving_binary,
              serving_spec=self._serving_spec,
              validation_spec=self._validation_spec,
              requests=['my_request'])
        mock_runner_factory.return_value.WaitUntilRunning.assert_called()
        mock_client.WaitUntilModelLoaded.assert_called()

</source>
<source file="systems/tfx-1.6.1/tfx/components/infra_validator/executor_test.py" startline="246" endline="262" pcid="71">
  def testValidateOnce_LoadAndQuery_Succeed(self):
    infra_validator = executor.Executor(self._context)
    with mock.patch.object(self._serving_binary,
                           'MakeClient') as mock_client_factory:
      mock_client = mock_client_factory.return_value
      with mock.patch.object(
          executor, '_create_model_server_runner') as mock_runner_factory:
        infra_validator._ValidateOnce(
            model_path=self._model_path,
            serving_binary=self._serving_binary,
            serving_spec=self._serving_spec,
            validation_spec=self._validation_spec,
            requests=['my_request'])
        mock_runner_factory.return_value.WaitUntilRunning.assert_called()
        mock_client.WaitUntilModelLoaded.assert_called()
        mock_client.SendRequests.assert_called()

</source>
</class>

<class classid="2" nclones="3" nlines="10" similarity="100">
<source file="systems/tfx-1.6.1/tfx/components/infra_validator/model_server_clients/tensorflow_serving_client_test.py" startline="54" endline="71" pcid="88">
  def testGetModelState_ReturnsReady_IfAllAvailable(self):
    # Prepare stub and client.
    self.model_stub.GetModelStatus.return_value = _make_response({
        'model_version_status': [
            {'state': 'AVAILABLE'},
            {'state': 'AVAILABLE'},
            {'state': 'AVAILABLE'}
        ]
    })
    client = tensorflow_serving_client.TensorFlowServingClient(
        'localhost:1234', 'a_model_name')

    # Call.
    result = client._GetServingStatus()

    # Check result.
    self.assertEqual(result, types.ModelServingStatus.READY)

</source>
<source file="systems/tfx-1.6.1/tfx/components/infra_validator/model_server_clients/tensorflow_serving_client_test.py" startline="90" endline="107" pcid="90">
  def testGetModelState_ReturnsUnavailable_IfAnyStateEnded(self):
    # Prepare stub and client.
    self.model_stub.GetModelStatus.return_value = _make_response({
        'model_version_status': [
            {'state': 'AVAILABLE'},
            {'state': 'AVAILABLE'},
            {'state': 'END'}
        ]
    })
    client = tensorflow_serving_client.TensorFlowServingClient(
        'localhost:1234', 'a_model_name')

    # Call.
    result = client._GetServingStatus()

    # Check result.
    self.assertEqual(result, types.ModelServingStatus.UNAVAILABLE)

</source>
<source file="systems/tfx-1.6.1/tfx/components/infra_validator/model_server_clients/tensorflow_serving_client_test.py" startline="72" endline="89" pcid="89">
  def testGetModelState_ReturnsNotReady_IfAnyStateNotAvailable(self):
    # Prepare stub and client.
    self.model_stub.GetModelStatus.return_value = _make_response({
        'model_version_status': [
            {'state': 'AVAILABLE'},
            {'state': 'AVAILABLE'},
            {'state': 'LOADING'}
        ]
    })
    client = tensorflow_serving_client.TensorFlowServingClient(
        'localhost:1234', 'a_model_name')

    # Call.
    result = client._GetServingStatus()

    # Check result.
    self.assertEqual(result, types.ModelServingStatus.NOT_READY)

</source>
</class>

<class classid="3" nclones="2" nlines="16" similarity="81">
<source file="systems/tfx-1.6.1/tfx/components/example_validator/component_test.py" startline="26" endline="43" pcid="98">
  def testConstruct(self):
    statistics_artifact = standard_artifacts.ExampleStatistics()
    statistics_artifact.split_names = artifact_utils.encode_split_names(
        ['train', 'eval'])
    exclude_splits = ['eval']
    example_validator = component.ExampleValidator(
        statistics=channel_utils.as_channel([statistics_artifact]),
        schema=channel_utils.as_channel([standard_artifacts.Schema()]),
        exclude_splits=exclude_splits)
    self.assertEqual(
        standard_artifacts.ExampleAnomalies.TYPE_NAME,
        example_validator.outputs[
            standard_component_specs.ANOMALIES_KEY].type_name)
    self.assertEqual(
        example_validator.spec.exec_properties[
            standard_component_specs.EXCLUDE_SPLITS_KEY], '["eval"]')


</source>
<source file="systems/tfx-1.6.1/tfx/components/schema_gen/component_test.py" startline="27" endline="43" pcid="440">
  def testConstruct(self):
    statistics_artifact = standard_artifacts.ExampleStatistics()
    statistics_artifact.split_names = artifact_utils.encode_split_names(
        ['train', 'eval'])
    exclude_splits = ['eval']
    schema_gen = component.SchemaGen(
        statistics=channel_utils.as_channel([statistics_artifact]),
        exclude_splits=exclude_splits)
    self.assertEqual(
        standard_artifacts.Schema.TYPE_NAME,
        schema_gen.outputs[standard_component_specs.SCHEMA_KEY].type_name)
    self.assertTrue(schema_gen.spec.exec_properties[
        standard_component_specs.INFER_FEATURE_SHAPE_KEY])
    self.assertEqual(
        schema_gen.spec.exec_properties[
            standard_component_specs.EXCLUDE_SPLITS_KEY], '["eval"]')

</source>
</class>

<class classid="4" nclones="2" nlines="16" similarity="76">
<source file="systems/tfx-1.6.1/tfx/components/example_validator/component.py" startline="65" endline="87" pcid="100">
  def __init__(self,
               statistics: types.BaseChannel,
               schema: types.BaseChannel,
               exclude_splits: Optional[List[str]] = None):
    """Construct an ExampleValidator component.

    Args:
      statistics: A BaseChannel of type `standard_artifacts.ExampleStatistics`.
      schema: A BaseChannel of type `standard_artifacts.Schema`. _required_
      exclude_splits: Names of splits that the example validator should not
        validate. Default behavior (when exclude_splits is set to None) is
        excluding no splits.
    """
    if exclude_splits is None:
      exclude_splits = []
      logging.info('Excluding no splits because exclude_splits is not set.')
    anomalies = types.Channel(type=standard_artifacts.ExampleAnomalies)
    spec = standard_component_specs.ExampleValidatorSpec(
        statistics=statistics,
        schema=schema,
        exclude_splits=json_utils.dumps(exclude_splits),
        anomalies=anomalies)
    super().__init__(spec=spec)
</source>
<source file="systems/tfx-1.6.1/tfx/components/statistics_gen/component.py" startline="53" endline="88" pcid="319">
  def __init__(self,
               examples: types.BaseChannel,
               schema: Optional[types.BaseChannel] = None,
               stats_options: Optional[tfdv.StatsOptions] = None,
               exclude_splits: Optional[List[str]] = None):
    """Construct a StatisticsGen component.

    Args:
      examples: A BaseChannel of `ExamplesPath` type, likely generated by the
        [ExampleGen component](https://www.tensorflow.org/tfx/guide/examplegen).
          This needs to contain two splits labeled `train` and `eval`.
          _required_
      schema: A `Schema` channel to use for automatically configuring the value
        of stats options passed to TFDV.
      stats_options: The StatsOptions instance to configure optional TFDV
        behavior. When stats_options.schema is set, it will be used instead of
        the `schema` channel input. Due to the requirement that stats_options be
        serialized, the slicer functions and custom stats generators are dropped
        and are therefore not usable.
      exclude_splits: Names of splits where statistics and sample should not be
        generated. Default behavior (when exclude_splits is set to None) is
        excluding no splits.
    """
    if exclude_splits is None:
      exclude_splits = []
      logging.info('Excluding no splits because exclude_splits is not set.')
    statistics = types.Channel(type=standard_artifacts.ExampleStatistics)
    # TODO(b/150802589): Move jsonable interface to tfx_bsl and use json_utils.
    stats_options_json = stats_options.to_json() if stats_options else None
    spec = standard_component_specs.StatisticsGenSpec(
        examples=examples,
        schema=schema,
        stats_options_json=stats_options_json,
        exclude_splits=json_utils.dumps(exclude_splits),
        statistics=statistics)
    super().__init__(spec=spec)
</source>
</class>

<class classid="5" nclones="2" nlines="27" similarity="85">
<source file="systems/tfx-1.6.1/tfx/components/bulk_inferrer/component.py" startline="58" endline="101" pcid="123">
  def __init__(
      self,
      examples: types.BaseChannel,
      model: Optional[types.BaseChannel] = None,
      model_blessing: Optional[types.BaseChannel] = None,
      data_spec: Optional[Union[bulk_inferrer_pb2.DataSpec,
                                data_types.RuntimeParameter]] = None,
      model_spec: Optional[Union[bulk_inferrer_pb2.ModelSpec,
                                 data_types.RuntimeParameter]] = None,
      output_example_spec: Optional[Union[bulk_inferrer_pb2.OutputExampleSpec,
                                          data_types.RuntimeParameter]] = None):
    """Construct an BulkInferrer component.

    Args:
      examples: A BaseChannel of type `standard_artifacts.Examples`, usually
        produced by an ExampleGen component. _required_
      model: A BaseChannel of type `standard_artifacts.Model`, usually produced
        by a Trainer component.
      model_blessing: A BaseChannel of type `standard_artifacts.ModelBlessing`,
        usually produced by a ModelValidator component.
      data_spec: bulk_inferrer_pb2.DataSpec instance that describes data
        selection.
      model_spec: bulk_inferrer_pb2.ModelSpec instance that describes model
        specification.
      output_example_spec: bulk_inferrer_pb2.OutputExampleSpec instance, specify
        if you want BulkInferrer to output examples instead of inference result.
    """
    if output_example_spec:
      output_examples = types.Channel(type=standard_artifacts.Examples)
      inference_result = None
    else:
      inference_result = types.Channel(type=standard_artifacts.InferenceResult)
      output_examples = None

    spec = standard_component_specs.BulkInferrerSpec(
        examples=examples,
        model=model,
        model_blessing=model_blessing,
        data_spec=data_spec or bulk_inferrer_pb2.DataSpec(),
        model_spec=model_spec or bulk_inferrer_pb2.ModelSpec(),
        output_example_spec=output_example_spec,
        inference_result=inference_result,
        output_examples=output_examples)
    super().__init__(spec=spec)
</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/bulk_inferrer/component.py" startline="81" endline="130" pcid="796">
  def __init__(
      self,
      examples: types.Channel,
      model: Optional[types.Channel] = None,
      model_blessing: Optional[types.Channel] = None,
      data_spec: Optional[Union[bulk_inferrer_pb2.DataSpec,
                                data_types.RuntimeParameter]] = None,
      output_example_spec: Optional[Union[bulk_inferrer_pb2.OutputExampleSpec,
                                          data_types.RuntimeParameter]] = None,
      custom_config: Optional[Dict[str, Any]] = None):
    """Construct an BulkInferrer component.

    Args:
      examples: A Channel of type `standard_artifacts.Examples`, usually
        produced by an ExampleGen component. _required_
      model: A Channel of type `standard_artifacts.Model`, usually produced by
        a Trainer component.
      model_blessing: A Channel of type `standard_artifacts.ModelBlessing`,
        usually produced by a ModelValidator component.
      data_spec: bulk_inferrer_pb2.DataSpec instance that describes data
        selection.
      output_example_spec: bulk_inferrer_pb2.OutputExampleSpec instance, specify
        if you want BulkInferrer to output examples instead of inference result.
      custom_config: A dict which contains the deployment job parameters to be
        passed to Google Cloud AI Platform.
        custom_config.ai_platform_serving_args need to contain the serving job
        parameters. For the full set of parameters, refer to
        https://cloud.google.com/ml-engine/reference/rest/v1/projects.models

    Raises:
      ValueError: Must not specify inference_result or output_examples depends
        on whether output_example_spec is set or not.
    """
    if output_example_spec:
      output_examples = types.Channel(type=standard_artifacts.Examples)
      inference_result = None
    else:
      inference_result = types.Channel(type=standard_artifacts.InferenceResult)
      output_examples = None

    spec = CloudAIBulkInferrerComponentSpec(
        examples=examples,
        model=model,
        model_blessing=model_blessing,
        data_spec=data_spec or bulk_inferrer_pb2.DataSpec(),
        output_example_spec=output_example_spec,
        custom_config=json_utils.dumps(custom_config),
        inference_result=inference_result,
        output_examples=output_examples)
    super().__init__(spec=spec)
</source>
</class>

<class classid="6" nclones="3" nlines="10" similarity="70">
<source file="systems/tfx-1.6.1/tfx/components/tuner/executor_test.py" startline="101" endline="113" pcid="137">
  def testDoWithModuleFile(self):
    self._exec_properties[
        standard_component_specs.MODULE_FILE_KEY] = os.path.join(
            self._testdata_dir, 'module_file', 'tuner_module.py')

    tuner = executor.Executor(self._context)
    tuner.Do(
        input_dict=self._input_dict,
        output_dict=self._output_dict,
        exec_properties=self._exec_properties)

    self._verify_output()

</source>
<source file="systems/tfx-1.6.1/tfx/components/tuner/executor_test.py" startline="126" endline="137" pcid="139">
  def testTuneArgs(self):
    with self.assertRaises(ValueError):
      self._exec_properties[
          standard_component_specs.TUNE_ARGS_KEY] = proto_utils.proto_to_json(
              tuner_pb2.TuneArgs(num_parallel_trials=3))

      tuner = executor.Executor(self._context)
      tuner.Do(
          input_dict=self._input_dict,
          output_dict=self._output_dict,
          exec_properties=self._exec_properties)

</source>
<source file="systems/tfx-1.6.1/tfx/components/tuner/executor_test.py" startline="171" endline="186" pcid="141">
  def testMultipleArtifacts(self):
    self._input_dict[
        standard_component_specs.EXAMPLES_KEY] = self._multiple_artifacts
    self._exec_properties[
        standard_component_specs.MODULE_FILE_KEY] = os.path.join(
            self._testdata_dir, 'module_file', 'tuner_module.py')

    tuner = executor.Executor(self._context)
    tuner.Do(
        input_dict=self._input_dict,
        output_dict=self._output_dict,
        exec_properties=self._exec_properties)

    self._verify_output()


</source>
</class>

<class classid="7" nclones="2" nlines="12" similarity="81">
<source file="systems/tfx-1.6.1/tfx/components/model_validator/executor_test.py" startline="61" endline="78" pcid="151">
  def testDoWithBlessedModel(self):
    # Create exe properties.
    exec_properties = {
        'blessed_model': os.path.join(self._source_data_dir, 'trainer/blessed'),
        'blessed_model_id': 123,
        'current_component_id': self.component_id,
    }

    # Run executor.
    model_validator = executor.Executor(self._context)
    model_validator.Do(self._input_dict, self._output_dict, exec_properties)

    # Check model validator outputs.
    self.assertTrue(fileio.exists(os.path.join(self._tmp_dir)))
    self.assertTrue(
        fileio.exists(
            os.path.join(self._blessing.uri, constants.BLESSED_FILE_NAME)))

</source>
<source file="systems/tfx-1.6.1/tfx/components/model_validator/executor_test.py" startline="79" endline="97" pcid="152">
  def testDoWithoutBlessedModel(self):
    # Create exe properties.
    exec_properties = {
        'blessed_model': None,
        'blessed_model_id': None,
        'current_component_id': self.component_id,
    }

    # Run executor.
    model_validator = executor.Executor(self._context)
    model_validator.Do(self._input_dict, self._output_dict, exec_properties)

    # Check model validator outputs.
    self.assertTrue(fileio.exists(os.path.join(self._tmp_dir)))
    self.assertTrue(
        fileio.exists(
            os.path.join(self._blessing.uri, constants.BLESSED_FILE_NAME)))


</source>
</class>

<class classid="8" nclones="7" nlines="11" similarity="71">
<source file="systems/tfx-1.6.1/tfx/components/evaluator/component_test.py" startline="43" endline="53" pcid="159">
  def testConstructWithBaselineModel(self):
    examples = standard_artifacts.Examples()
    model_exports = standard_artifacts.Model()
    baseline_model = standard_artifacts.Model()
    evaluator = component.Evaluator(
        examples=channel_utils.as_channel([examples]),
        model=channel_utils.as_channel([model_exports]),
        baseline_model=channel_utils.as_channel([baseline_model]))
    self.assertEqual(standard_artifacts.ModelEvaluation.TYPE_NAME,
                     evaluator.outputs['evaluation'].type_name)

</source>
<source file="systems/tfx-1.6.1/tfx/components/evaluator/component_test.py" startline="111" endline="122" pcid="164">
  def testConstructWithModuleFile(self):
    examples = standard_artifacts.Examples()
    model_exports = standard_artifacts.Model()
    evaluator = component.Evaluator(
        examples=channel_utils.as_channel([examples]),
        model=channel_utils.as_channel([model_exports]),
        example_splits=['eval'],
        module_file='path')
    self.assertEqual(standard_artifacts.ModelEvaluation.TYPE_NAME,
                     evaluator.outputs['evaluation'].type_name)
    self.assertEqual('path', evaluator.exec_properties['module_file'])

</source>
<source file="systems/tfx-1.6.1/tfx/components/evaluator/component_test.py" startline="123" endline="134" pcid="165">
  def testConstructWithModuleFn(self):
    examples = standard_artifacts.Examples()
    model_exports = standard_artifacts.Model()
    evaluator = component.Evaluator(
        examples=channel_utils.as_channel([examples]),
        model=channel_utils.as_channel([model_exports]),
        example_splits=['eval'],
        module_path='module')
    self.assertEqual(standard_artifacts.ModelEvaluation.TYPE_NAME,
                     evaluator.outputs['evaluation'].type_name)
    self.assertEqual('module', evaluator.exec_properties['module_path'])

</source>
<source file="systems/tfx-1.6.1/tfx/components/evaluator/component_test.py" startline="98" endline="110" pcid="163">
  def testConstructWithEvalConfig(self):
    examples = standard_artifacts.Examples()
    model_exports = standard_artifacts.Model()
    schema = standard_artifacts.Schema()
    evaluator = component.Evaluator(
        examples=channel_utils.as_channel([examples]),
        model=channel_utils.as_channel([model_exports]),
        eval_config=tfma.EvalConfig(
            slicing_specs=[tfma.SlicingSpec(feature_keys=['trip_start_hour'])]),
        schema=channel_utils.as_channel([schema]),)
    self.assertEqual(standard_artifacts.ModelEvaluation.TYPE_NAME,
                     evaluator.outputs['evaluation'].type_name)

</source>
<source file="systems/tfx-1.6.1/tfx/components/evaluator/component_test.py" startline="54" endline="66" pcid="160">
  def testConstructWithSliceSpec(self):
    examples = standard_artifacts.Examples()
    model_exports = standard_artifacts.Model()
    evaluator = component.Evaluator(
        examples=channel_utils.as_channel([examples]),
        model=channel_utils.as_channel([model_exports]),
        feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[
            evaluator_pb2.SingleSlicingSpec(
                column_for_slicing=['trip_start_hour'])
        ]))
    self.assertEqual(standard_artifacts.ModelEvaluation.TYPE_NAME,
                     evaluator.outputs['evaluation'].type_name)

</source>
<source file="systems/tfx-1.6.1/tfx/components/evaluator/component_test.py" startline="135" endline="147" pcid="166">
  def testConstructDuplicateUserModule(self):
    examples = standard_artifacts.Examples()
    model_exports = standard_artifacts.Model()

    with self.assertRaises(ValueError):
      _ = component.Evaluator(
          examples=channel_utils.as_channel([examples]),
          model=channel_utils.as_channel([model_exports]),
          example_splits=['eval'],
          module_file='module_file_path',
          module_path='python.path.module')


</source>
<source file="systems/tfx-1.6.1/tfx/components/evaluator/component_test.py" startline="67" endline="82" pcid="161">
  def testConstructWithFairnessThresholds(self):
    examples = standard_artifacts.Examples()
    model_exports = standard_artifacts.Model()
    evaluator = component.Evaluator(
        examples=channel_utils.as_channel([examples]),
        model=channel_utils.as_channel([model_exports]),
        feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[
            evaluator_pb2.SingleSlicingSpec(
                column_for_slicing=['trip_start_hour'])
        ]),
        fairness_indicator_thresholds=[0.1, 0.3, 0.5, 0.9])
    self.assertEqual(standard_artifacts.ModelEvaluation.TYPE_NAME,
                     evaluator.outputs['evaluation'].type_name)
    self.assertEqual('[0.1, 0.3, 0.5, 0.9]',
                     evaluator.exec_properties['fairness_indicator_thresholds'])

</source>
</class>

<class classid="9" nclones="2" nlines="11" similarity="72">
<source file="systems/tfx-1.6.1/tfx/components/experimental/data_view/binder_component.py" startline="70" endline="81" pcid="172">
  def __init__(self,
               input_examples: types.BaseChannel,
               data_view: types.BaseChannel,
               output_examples: Optional[types.Channel] = None):
    if not output_examples:
      output_examples = types.Channel(type=standard_artifacts.Examples)

    spec = _DataViewBinderComponentSpec(
        input_examples=input_examples,
        data_view=data_view,
        output_examples=output_examples)
    super().__init__(spec=spec)
</source>
<source file="systems/tfx-1.6.1/tfx/components/experimental/data_view/provider_component.py" startline="61" endline="88" pcid="174">
  def __init__(self,
               create_decoder_func: str,
               module_file: Optional[str] = None,
               data_view: Optional[types.Channel] = None):
    """Construct a StatisticsGen component.

    Args:
      create_decoder_func: If `module_file` is not None, this should be the name
        of the function in `module_file` that this component need to use to
        create the TfGraphRecordDecoder. Otherwise it should be the path
        (dot-delimited, e.g. "some_package.some_module.some_func") to such
        a function. The function must have the following signature:

        def create_decoder_func() -> tfx_bsl.coder.TfGraphRecordDecoder:
          ...
      module_file: The file path to a python module file, from which the
        function named after `create_decoder_func` will be loaded. If not
        provided, `create_decoder_func` is expected to be a path to a function.
      data_view: Output 'DataView' channel, in which a the decoder will be
        saved.
    """
    if data_view is None:
      data_view = types.Channel(type=standard_artifacts.DataView)
    spec = _TfGraphDataViewProviderSpec(
        module_file=module_file,
        create_decoder_func=create_decoder_func,
        data_view=data_view)
    super().__init__(spec=spec)
</source>
</class>

<class classid="10" nclones="2" nlines="17" similarity="75">
<source file="systems/tfx-1.6.1/tfx/components/experimental/data_view/provider_executor_test.py" startline="39" endline="56" pcid="178">
  def testExecutorModuleFileProvided(self):
    input_dict = {}
    output = standard_artifacts.DataView()
    output.uri = os.path.join(self._output_data_dir, 'output_data_view')
    output_dict = {'data_view': [output]}
    exec_properties = {
        'module_file':
            os.path.join(self._source_data_dir,
                         'module_file/data_view_module.py'),
        'create_decoder_func':
            'create_simple_decoder',
    }
    executor = provider_executor.TfGraphDataViewProviderExecutor()
    executor.Do(input_dict, output_dict, exec_properties)
    loaded_decoder = tf_graph_record_decoder.load_decoder(output.uri)
    self.assertIsInstance(
        loaded_decoder, tf_graph_record_decoder.LoadedDecoder)

</source>
<source file="systems/tfx-1.6.1/tfx/components/experimental/data_view/provider_executor_test.py" startline="57" endline="74" pcid="179">
  def testExecutorModuleFileNotProvided(self):
    input_dict = {}
    output = standard_artifacts.DataView()
    output.uri = os.path.join(self._output_data_dir, 'output_data_view')
    output_dict = {'data_view': [output]}
    exec_properties = {
        'module_file': None,
        'create_decoder_func':
            '%s.%s' % (data_view_module.create_simple_decoder.__module__,
                       data_view_module.create_simple_decoder.__name__),
    }
    executor = provider_executor.TfGraphDataViewProviderExecutor()
    executor.Do(input_dict, output_dict, exec_properties)
    loaded_decoder = tf_graph_record_decoder.load_decoder(output.uri)
    self.assertIsInstance(
        loaded_decoder, tf_graph_record_decoder.LoadedDecoder)


</source>
</class>

<class classid="11" nclones="2" nlines="10" similarity="70">
<source file="systems/tfx-1.6.1/tfx/components/pusher/component_test.py" startline="33" endline="43" pcid="186">
  def setUp(self):
    super().setUp()
    self._model = channel_utils.as_channel([standard_artifacts.Model()])
    self._model_blessing = channel_utils.as_channel(
        [standard_artifacts.ModelBlessing()])
    self._infra_blessing = channel_utils.as_channel(
        [standard_artifacts.InfraBlessing()])
    self._push_destination = pusher_pb2.PushDestination(
        filesystem=pusher_pb2.PushDestination.Filesystem(
            base_directory=self.get_temp_dir()))

</source>
<source file="systems/tfx-1.6.1/tfx/components/trainer/component_test.py" startline="29" endline="40" pcid="409">
  def setUp(self):
    super().setUp()

    self.examples = channel_utils.as_channel([standard_artifacts.Examples()])
    self.transform_graph = channel_utils.as_channel(
        [standard_artifacts.TransformGraph()])
    self.schema = channel_utils.as_channel([standard_artifacts.Schema()])
    self.hyperparameters = channel_utils.as_channel(
        [standard_artifacts.HyperParameters()])
    self.train_args = trainer_pb2.TrainArgs(splits=['train'], num_steps=100)
    self.eval_args = trainer_pb2.EvalArgs(splits=['eval'], num_steps=50)

</source>
</class>

<class classid="12" nclones="2" nlines="10" similarity="70">
<source file="systems/tfx-1.6.1/tfx/components/pusher/component_test.py" startline="53" endline="64" pcid="188">
  def testConstructWithParameter(self):
    push_dir = data_types.RuntimeParameter(name='push-dir', ptype=str)
    pusher = component.Pusher(
        model=self._model,
        model_blessing=self._model_blessing,
        push_destination={'filesystem': {
            'base_directory': push_dir
        }})
    self.assertEqual(
        standard_artifacts.PushedModel.TYPE_NAME,
        pusher.outputs[standard_component_specs.PUSHED_MODEL_KEY].type_name)

</source>
<source file="systems/tfx-1.6.1/tfx/components/pusher/component_test.py" startline="72" endline="82" pcid="190">
  def testConstructNoDestinationCustomExecutor(self):
    pusher = component.Pusher(
        model=self._model,
        model_blessing=self._model_blessing,
        custom_executor_spec=executor_spec.ExecutorClassSpec(
            self._MyCustomPusherExecutor),
    )
    self.assertEqual(
        standard_artifacts.PushedModel.TYPE_NAME,
        pusher.outputs[standard_component_specs.PUSHED_MODEL_KEY].type_name)

</source>
</class>

<class classid="13" nclones="2" nlines="10" similarity="100">
<source file="systems/tfx-1.6.1/tfx/components/pusher/executor_test.py" startline="156" endline="171" pcid="203">
  def testDo_NoModelBlessing_InfraBlessed_Pushed(self):
    # Prepare successful InfraBlessing only (without ModelBlessing).
    infra_blessing = standard_artifacts.InfraBlessing()
    infra_blessing.set_int_custom_property('blessed', 1)  # Blessed.
    input_dict = {
        standard_component_specs.MODEL_KEY:
            self._input_dict[standard_component_specs.MODEL_KEY],
        standard_component_specs.INFRA_BLESSING_KEY: [infra_blessing],
    }

    # Run executor
    self._executor.Do(input_dict, self._output_dict, self._exec_properties)

    # Check model is pushed.
    self.assertPushed()

</source>
<source file="systems/tfx-1.6.1/tfx/components/pusher/executor_test.py" startline="172" endline="187" pcid="204">
  def testDo_NoModelBlessing_InfraNotBlessed_NotPushed(self):
    # Prepare unsuccessful InfraBlessing only (without ModelBlessing).
    infra_blessing = standard_artifacts.InfraBlessing()
    infra_blessing.set_int_custom_property('blessed', 0)  # Not blessed.
    input_dict = {
        standard_component_specs.MODEL_KEY:
            self._input_dict[standard_component_specs.MODEL_KEY],
        standard_component_specs.INFRA_BLESSING_KEY: [infra_blessing],
    }

    # Run executor
    self._executor.Do(input_dict, self._output_dict, self._exec_properties)

    # Check model is not pushed.
    self.assertNotPushed()

</source>
</class>

<class classid="14" nclones="2" nlines="11" similarity="90">
<source file="systems/tfx-1.6.1/tfx/components/util/tfxio_utils_test.py" startline="148" endline="163" pcid="236">
  def decode_record(self, record):
    indices = tf.transpose(
        tf.stack([
            tf.range(tf.size(record), dtype=tf.int64),
            tf.zeros(tf.size(record), dtype=tf.int64)
        ]))

    return {
        'sparse_tensor':
            tf.SparseTensor(
                values=record,
                indices=indices,
                dense_shape=[tf.size(record), 1])
    }


</source>
<source file="systems/tfx-1.6.1/tfx/components/testdata/module_file/data_view_module.py" startline="24" endline="39" pcid="322">
  def decode_record(self, record: tf.Tensor) -> Dict[str, Any]:
    indices = tf.transpose(
        tf.stack([
            tf.range(tf.size(record), dtype=tf.int64),
            tf.zeros(tf.size(record), dtype=tf.int64)
        ]))

    return {
        "sparse_tensor":
            tf.SparseTensor(
                values=record,
                indices=indices,
                dense_shape=[tf.size(record), 1])
    }


</source>
</class>

<class classid="15" nclones="2" nlines="21" similarity="71">
<source file="systems/tfx-1.6.1/tfx/components/util/tfxio_utils.py" startline="140" endline="175" pcid="247">
def get_tf_dataset_factory_from_artifact(
    examples: List[artifact.Artifact],
    telemetry_descriptors: List[str],
) -> Callable[[
    List[str],
    dataset_options.TensorFlowDatasetOptions,
    Optional[schema_pb2.Schema],
], tf.data.Dataset]:
  """Returns a factory function that creates a tf.data.Dataset.

  Args:
    examples: The Examples artifacts that the TFXIO from which the Dataset is
      created from is intended to access.
    telemetry_descriptors: A set of descriptors that identify the component
      that is instantiating the TFXIO. These will be used to construct the
      namespace to contain metrics for profiling and are therefore expected to
      be identifiers of the component itself and not individual instances of
      source use.
  """
  payload_format, data_view_uri = resolve_payload_format_and_data_view_uri(
      examples)

  def dataset_factory(file_pattern: List[str],
                      options: dataset_options.TensorFlowDatasetOptions,
                      schema: Optional[schema_pb2.Schema]) -> tf.data.Dataset:
    return make_tfxio(
        file_pattern=file_pattern,
        telemetry_descriptors=telemetry_descriptors,
        payload_format=payload_format,
        data_view_uri=data_view_uri,
        schema=schema).TensorFlowDataset(
            options)

  return dataset_factory


</source>
<source file="systems/tfx-1.6.1/tfx/components/util/tfxio_utils.py" startline="176" endline="210" pcid="249">
def get_record_batch_factory_from_artifact(
    examples: List[artifact.Artifact],
    telemetry_descriptors: List[str],
) -> Callable[[
    List[str],
    dataset_options.RecordBatchesOptions,
    Optional[schema_pb2.Schema],
], Iterator[pa.RecordBatch]]:
  """Returns a factory function that creates Iterator[pa.RecordBatch].

  Args:
    examples: The Examples artifacts that the TFXIO from which the Dataset is
      created from is intended to access.
    telemetry_descriptors: A set of descriptors that identify the component that
      is instantiating the TFXIO. These will be used to construct the namespace
      to contain metrics for profiling and are therefore expected to be
      identifiers of the component itself and not individual instances of source
      use.
  """
  payload_format, data_view_uri = resolve_payload_format_and_data_view_uri(
      examples)

  def record_batch_factory(
      file_pattern: List[str], options: dataset_options.RecordBatchesOptions,
      schema: Optional[schema_pb2.Schema]) -> Iterator[pa.RecordBatch]:
    return make_tfxio(
        file_pattern=file_pattern,
        telemetry_descriptors=telemetry_descriptors,
        payload_format=payload_format,
        data_view_uri=data_view_uri,
        schema=schema).RecordBatches(options)

  return record_batch_factory


</source>
</class>

<class classid="16" nclones="9" nlines="12" similarity="71">
<source file="systems/tfx-1.6.1/tfx/components/transform/component_test.py" startline="85" endline="96" pcid="257">
  def test_construct_from_module_file(self):
    module_file = '/path/to/preprocessing.py'
    transform = component.Transform(
        examples=self.examples,
        schema=self.schema,
        module_file=module_file,
    )
    self._verify_outputs(transform)
    self.assertEqual(
        module_file,
        transform.exec_properties[standard_component_specs.MODULE_FILE_KEY])

</source>
<source file="systems/tfx-1.6.1/tfx/components/transform/component_test.py" startline="189" endline="203" pcid="266">
  def test_construct_with_splits_config(self):
    splits_config = transform_pb2.SplitsConfig(
        analyze=['train'], transform=['eval'])
    module_file = '/path/to/preprocessing.py'
    transform = component.Transform(
        examples=self.examples,
        schema=self.schema,
        module_file=module_file,
        splits_config=splits_config,
    )
    self._verify_outputs(transform)
    self.assertEqual(
        proto_utils.proto_to_json(splits_config),
        transform.exec_properties[standard_component_specs.SPLITS_CONFIG_KEY])

</source>
<source file="systems/tfx-1.6.1/tfx/components/trainer/component_test.py" startline="109" endline="120" pcid="415">
  def testConstructWithoutTransformOutput(self):
    module_file = '/path/to/module/file'
    trainer = component.Trainer(
        module_file=module_file,
        examples=self.examples,
        train_args=self.train_args,
        eval_args=self.eval_args)
    self._verify_outputs(trainer)
    self.assertEqual(
        module_file,
        trainer.spec.exec_properties[standard_component_specs.MODULE_FILE_KEY])

</source>
<source file="systems/tfx-1.6.1/tfx/components/transform/component_test.py" startline="214" endline="226" pcid="268">
  def test_construct_with_force_tf_compat_v1_override(self):
    transform = component.Transform(
        examples=self.examples,
        schema=self.schema,
        preprocessing_fn='my_preprocessing_fn',
        force_tf_compat_v1=True,
    )
    self._verify_outputs(transform)
    self.assertEqual(
        True,
        bool(transform.spec.exec_properties[
            standard_component_specs.FORCE_TF_COMPAT_V1_KEY]))

</source>
<source file="systems/tfx-1.6.1/tfx/components/trainer/component_test.py" startline="94" endline="108" pcid="414">
  def testConstructFromRunFn(self):
    run_fn = 'path.to.my_run_fn'
    trainer = component.Trainer(
        run_fn=run_fn,
        custom_executor_spec=executor_spec.ExecutorClassSpec(
            executor.GenericExecutor),
        examples=self.examples,
        transform_graph=self.transform_graph,
        train_args=self.train_args,
        eval_args=self.eval_args)
    self._verify_outputs(trainer)
    self.assertEqual(
        run_fn,
        trainer.spec.exec_properties[standard_component_specs.RUN_FN_KEY])

</source>
<source file="systems/tfx-1.6.1/tfx/components/trainer/component_test.py" startline="81" endline="93" pcid="413">
  def testConstructFromTrainerFn(self):
    trainer_fn = 'path.to.my_trainer_fn'
    trainer = component.Trainer(
        trainer_fn=trainer_fn,
        examples=self.examples,
        transform_graph=self.transform_graph,
        train_args=self.train_args,
        eval_args=self.eval_args)
    self._verify_outputs(trainer)
    self.assertEqual(
        trainer_fn,
        trainer.spec.exec_properties[standard_component_specs.TRAINER_FN_KEY])

</source>
<source file="systems/tfx-1.6.1/tfx/components/transform/component_test.py" startline="97" endline="109" pcid="258">
  def test_construct_with_parameter(self):
    module_file = data_types.RuntimeParameter(name='module-file', ptype=str)
    transform = component.Transform(
        examples=self.examples,
        schema=self.schema,
        module_file=module_file,
    )
    self._verify_outputs(transform)
    self.assertJsonEqual(
        str(module_file),
        str(transform.exec_properties[
            standard_component_specs.MODULE_FILE_KEY]))

</source>
<source file="systems/tfx-1.6.1/tfx/components/transform/component_test.py" startline="227" endline="240" pcid="269">
  def test_construct_with_stats_disabled(self):
    transform = component.Transform(
        examples=self.examples,
        schema=self.schema,
        preprocessing_fn='my_preprocessing_fn',
        disable_statistics=True,
    )
    self._verify_outputs(transform, disable_statistics=True)
    self.assertEqual(
        True,
        bool(transform.spec.exec_properties[
            standard_component_specs.DISABLE_STATISTICS_KEY]))


</source>
<source file="systems/tfx-1.6.1/tfx/components/trainer/component_test.py" startline="49" endline="64" pcid="411">
  def testConstructFromModuleFile(self):
    module_file = '/path/to/module/file'
    trainer = component.Trainer(
        module_file=module_file,
        examples=self.examples,
        transform_graph=self.transform_graph,
        schema=self.schema,
        custom_config={'test': 10})
    self._verify_outputs(trainer)
    self.assertEqual(
        module_file,
        trainer.spec.exec_properties[standard_component_specs.MODULE_FILE_KEY])
    self.assertEqual(
        '{"test": 10}', trainer.spec.exec_properties[
            standard_component_specs.CUSTOM_CONFIG_KEY])

</source>
</class>

<class classid="17" nclones="2" nlines="14" similarity="73">
<source file="systems/tfx-1.6.1/tfx/components/transform/component_test.py" startline="110" endline="123" pcid="259">
  def test_construct_from_preprocessing_fn(self):
    preprocessing_fn = 'path.to.my_preprocessing_fn'
    transform = component.Transform(
        examples=self.examples,
        schema=self.schema,
        preprocessing_fn=preprocessing_fn,
    )
    self._verify_outputs(transform)
    self.assertEqual(
        preprocessing_fn, transform.exec_properties[
            standard_component_specs.PREPROCESSING_FN_KEY])
    self.assertIsNone(transform.exec_properties[
        standard_component_specs.STATS_OPTIONS_UPDATER_FN_KEY])

</source>
<source file="systems/tfx-1.6.1/tfx/components/transform/component_test.py" startline="124" endline="139" pcid="260">
  def test_construct_from_preprocessing_fn_with_stats_options_updater_fn(self):
    preprocessing_fn = 'path.to.my_preprocessing_fn'
    stats_options_updater_fn = 'path.to.my.stats_options_updater_fn'
    transform = component.Transform(
        examples=self.examples,
        schema=self.schema,
        preprocessing_fn=preprocessing_fn,
        stats_options_updater_fn=stats_options_updater_fn)
    self._verify_outputs(transform)
    self.assertEqual(
        preprocessing_fn, transform.exec_properties[
            standard_component_specs.PREPROCESSING_FN_KEY])
    self.assertEqual(
        stats_options_updater_fn, transform.exec_properties[
            standard_component_specs.STATS_OPTIONS_UPDATER_FN_KEY])

</source>
</class>

<class classid="18" nclones="3" nlines="31" similarity="76">
<source file="systems/tfx-1.6.1/tfx/components/statistics_gen/executor_test.py" startline="47" endline="94" pcid="316">
  def testDo(self):
    source_data_dir = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), 'testdata')
    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    fileio.makedirs(output_data_dir)

    # Create input dict.
    examples = standard_artifacts.Examples()
    examples.uri = os.path.join(source_data_dir, 'csv_example_gen')
    examples.split_names = artifact_utils.encode_split_names(
        ['train', 'eval', 'test'])

    input_dict = {
        standard_component_specs.EXAMPLES_KEY: [examples],
    }

    exec_properties = {
        # List needs to be serialized before being passed into Do function.
        standard_component_specs.EXCLUDE_SPLITS_KEY:
            json_utils.dumps(['test']),
    }

    # Create output dict.
    stats = standard_artifacts.ExampleStatistics()
    stats.uri = output_data_dir
    output_dict = {
        standard_component_specs.STATISTICS_KEY: [stats],
    }

    # Run executor.
    stats_gen_executor = executor.Executor()
    stats_gen_executor.Do(input_dict, output_dict, exec_properties)

    self.assertEqual(
        artifact_utils.encode_split_names(['train', 'eval']), stats.split_names)

    # Check statistics_gen outputs.
    self._validate_stats_output(
        os.path.join(stats.uri, 'Split-train', 'FeatureStats.pb'))
    self._validate_stats_output(
        os.path.join(stats.uri, 'Split-eval', 'FeatureStats.pb'))

    # Assert 'test' split is excluded.
    self.assertFalse(
        fileio.exists(os.path.join(stats.uri, 'test', 'FeatureStats.pb')))

</source>
<source file="systems/tfx-1.6.1/tfx/components/statistics_gen/executor_test.py" startline="95" endline="139" pcid="317">
  def testDoWithSchemaAndStatsOptions(self):
    source_data_dir = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), 'testdata')
    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    fileio.makedirs(output_data_dir)

    # Create input dict.
    examples = standard_artifacts.Examples()
    examples.uri = os.path.join(source_data_dir, 'csv_example_gen')
    examples.split_names = artifact_utils.encode_split_names(['train', 'eval'])

    schema = standard_artifacts.Schema()
    schema.uri = os.path.join(source_data_dir, 'schema_gen')

    input_dict = {
        standard_component_specs.EXAMPLES_KEY: [examples],
        standard_component_specs.SCHEMA_KEY: [schema]
    }

    exec_properties = {
        standard_component_specs.STATS_OPTIONS_JSON_KEY:
            tfdv.StatsOptions(label_feature='company').to_json(),
        standard_component_specs.EXCLUDE_SPLITS_KEY:
            json_utils.dumps([])
    }

    # Create output dict.
    stats = standard_artifacts.ExampleStatistics()
    stats.uri = output_data_dir
    output_dict = {
        standard_component_specs.STATISTICS_KEY: [stats],
    }

    # Run executor.
    stats_gen_executor = executor.Executor()
    stats_gen_executor.Do(input_dict, output_dict, exec_properties)

    # Check statistics_gen outputs.
    self._validate_stats_output(
        os.path.join(stats.uri, 'Split-train', 'FeatureStats.pb'))
    self._validate_stats_output(
        os.path.join(stats.uri, 'Split-eval', 'FeatureStats.pb'))

</source>
<source file="systems/tfx-1.6.1/tfx/components/statistics_gen/executor_test.py" startline="140" endline="181" pcid="318">
  def testDoWithTwoSchemas(self):
    source_data_dir = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), 'testdata')
    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    fileio.makedirs(output_data_dir)

    # Create input dict.
    examples = standard_artifacts.Examples()
    examples.uri = os.path.join(source_data_dir, 'csv_example_gen')
    examples.split_names = artifact_utils.encode_split_names(['train', 'eval'])

    schema = standard_artifacts.Schema()
    schema.uri = os.path.join(source_data_dir, 'schema_gen')

    input_dict = {
        standard_component_specs.EXAMPLES_KEY: [examples],
        standard_component_specs.SCHEMA_KEY: [schema]
    }

    exec_properties = {
        standard_component_specs.STATS_OPTIONS_JSON_KEY:
            tfdv.StatsOptions(
                label_feature='company', schema=schema_pb2.Schema()).to_json(),
        standard_component_specs.EXCLUDE_SPLITS_KEY:
            json_utils.dumps([])
    }

    # Create output dict.
    stats = standard_artifacts.ExampleStatistics()
    stats.uri = output_data_dir
    output_dict = {
        standard_component_specs.STATISTICS_KEY: [stats],
    }

    # Run executor.
    stats_gen_executor = executor.Executor()
    with self.assertRaises(ValueError):
      stats_gen_executor.Do(input_dict, output_dict, exec_properties)


</source>
</class>

<class classid="19" nclones="3" nlines="14" similarity="73">
<source file="systems/tfx-1.6.1/tfx/components/testdata/module_file/tuner_module.py" startline="59" endline="86" pcid="325">
def _build_keras_model(hparams: keras_tuner.HyperParameters) -> tf.keras.Model:
  """Creates a DNN Keras model for classifying penguin data.

  Args:
    hparams: Holds HyperParameters for tuning.

  Returns:
    A Keras Model.
  """
  # The model below is built with Functional API, please refer to
  # https://www.tensorflow.org/guide/keras/overview for all API options.
  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]
  d = keras.layers.concatenate(inputs)
  for _ in range(int(hparams.get('num_layers'))):
    d = keras.layers.Dense(8, activation='relu')(d)
  outputs = keras.layers.Dense(3, activation='softmax')(d)

  model = keras.Model(inputs=inputs, outputs=outputs)
  model.compile(
      optimizer=keras.optimizers.Adam(hparams.get('learning_rate')),
      loss='sparse_categorical_crossentropy',
      metrics=[keras.metrics.SparseCategoricalAccuracy()])

  model.summary(print_fn=absl.logging.info)
  return model


# This will be called by TFX Tuner.
</source>
<source file="systems/tfx-1.6.1/tfx/examples/penguin/penguin_utils_cloud_tuner.py" startline="170" endline="200" pcid="3115">
def _build_keras_model(hparams: keras_tuner.HyperParameters) -> tf.keras.Model:
  """Creates a DNN Keras model for classifying penguin data.

  Args:
    hparams: Holds HyperParameters for tuning.

  Returns:
    A Keras Model.
  """
  # The model below is built with Functional API, please refer to
  # https://www.tensorflow.org/guide/keras/overview for all API options.
  inputs = [
      keras.layers.Input(shape=(1,), name=_transformed_name(f))
      for f in _FEATURE_KEYS
  ]
  d = keras.layers.concatenate(inputs)
  for _ in range(int(hparams.get('num_layers'))):
    d = keras.layers.Dense(8, activation='relu')(d)
  outputs = keras.layers.Dense(3, activation='softmax')(d)

  model = keras.Model(inputs=inputs, outputs=outputs)
  model.compile(
      optimizer=keras.optimizers.Adam(hparams.get('learning_rate')),
      loss='sparse_categorical_crossentropy',
      metrics=[keras.metrics.SparseCategoricalAccuracy()])

  model.summary(print_fn=logging.info)
  return model


# TFX Tuner will call this function.
</source>
<source file="systems/tfx-1.6.1/tfx/examples/penguin/penguin_utils_keras.py" startline="43" endline="73" pcid="3104">
def _make_keras_model(hparams: keras_tuner.HyperParameters) -> tf.keras.Model:
  """Creates a DNN Keras model for classifying penguin data.

  Args:
    hparams: Holds HyperParameters for tuning.

  Returns:
    A Keras Model.
  """
  # The model below is built with Functional API, please refer to
  # https://www.tensorflow.org/guide/keras/overview for all API options.
  inputs = [
      keras.layers.Input(shape=(1,), name=base.transformed_name(f))
      for f in base.FEATURE_KEYS
  ]
  d = keras.layers.concatenate(inputs)
  for _ in range(int(hparams.get('num_layers'))):
    d = keras.layers.Dense(8, activation='relu')(d)
  outputs = keras.layers.Dense(3, activation='softmax')(d)

  model = keras.Model(inputs=inputs, outputs=outputs)
  model.compile(
      optimizer=keras.optimizers.Adam(hparams.get('learning_rate')),
      loss='sparse_categorical_crossentropy',
      metrics=[keras.metrics.SparseCategoricalAccuracy()])

  model.summary(print_fn=absl.logging.info)
  return model


# TFX Tuner will call this function.
</source>
</class>

<class classid="20" nclones="4" nlines="26" similarity="100">
<source file="systems/tfx-1.6.1/tfx/components/testdata/module_file/trainer_module.py" startline="88" endline="133" pcid="331">
def _build_estimator(config, hidden_units=None, warm_start_from=None):
  """Build an estimator for predicting the tipping behavior of taxi riders.

  Args:
    config: tf.estimator.RunConfig defining the runtime environment for the
      estimator (including model_dir).
    hidden_units: [int], the layer sizes of the DNN (input layer first)
    warm_start_from: Optional directory to warm start from.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """
  real_valued_columns = [
      tf.feature_column.numeric_column(key, shape=())
      for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)
  ]
  categorical_columns = [
      tf.feature_column.categorical_column_with_identity(
          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)
      for key in _transformed_names(_VOCAB_FEATURE_KEYS)
  ]
  categorical_columns += [
      tf.feature_column.categorical_column_with_identity(
          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)
      for key in _transformed_names(_BUCKET_FEATURE_KEYS)
  ]
  categorical_columns += [
      tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension
          key,
          num_buckets=num_buckets,
          default_value=0) for key, num_buckets in zip(
              _transformed_names(_CATEGORICAL_FEATURE_KEYS),
              _MAX_CATEGORICAL_FEATURE_VALUES)
  ]
  return tf.estimator.DNNLinearCombinedClassifier(
      config=config,
      linear_feature_columns=categorical_columns,
      dnn_feature_columns=real_valued_columns,
      dnn_hidden_units=hidden_units or [100, 70, 50, 25],
      warm_start_from=warm_start_from)


</source>
<source file="systems/tfx-1.6.1/tfx/examples/chicago_taxi_pipeline/taxi_utils.py" startline="142" endline="187" pcid="3080">
def _build_estimator(config, hidden_units=None, warm_start_from=None):
  """Build an estimator for predicting the tipping behavior of taxi riders.

  Args:
    config: tf.estimator.RunConfig defining the runtime environment for the
      estimator (including model_dir).
    hidden_units: [int], the layer sizes of the DNN (input layer first)
    warm_start_from: Optional directory to warm start from.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """
  real_valued_columns = [
      tf.feature_column.numeric_column(key, shape=())
      for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)
  ]
  categorical_columns = [
      tf.feature_column.categorical_column_with_identity(
          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)
      for key in _transformed_names(_VOCAB_FEATURE_KEYS)
  ]
  categorical_columns += [
      tf.feature_column.categorical_column_with_identity(
          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)
      for key in _transformed_names(_BUCKET_FEATURE_KEYS)
  ]
  categorical_columns += [
      tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension
          key,
          num_buckets=num_buckets,
          default_value=0) for key, num_buckets in zip(
              _transformed_names(_CATEGORICAL_FEATURE_KEYS),
              _MAX_CATEGORICAL_FEATURE_VALUES)
  ]
  return tf.estimator.DNNLinearCombinedClassifier(
      config=config,
      linear_feature_columns=categorical_columns,
      dnn_feature_columns=real_valued_columns,
      dnn_hidden_units=hidden_units or [100, 70, 50, 25],
      warm_start_from=warm_start_from)


</source>
<source file="systems/tfx-1.6.1/tfx/examples/bigquery_ml/taxi_utils_bqml.py" startline="148" endline="193" pcid="2951">
def _build_estimator(config, hidden_units=None, warm_start_from=None):
  """Build an estimator for predicting the tipping behavior of taxi riders.

  Args:
    config: tf.estimator.RunConfig defining the runtime environment for the
      estimator (including model_dir).
    hidden_units: [int], the layer sizes of the DNN (input layer first)
    warm_start_from: Optional directory to warm start from.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """
  real_valued_columns = [
      tf.feature_column.numeric_column(key, shape=())
      for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)
  ]
  categorical_columns = [
      tf.feature_column.categorical_column_with_identity(
          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)
      for key in _transformed_names(_VOCAB_FEATURE_KEYS)
  ]
  categorical_columns += [
      tf.feature_column.categorical_column_with_identity(
          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)
      for key in _transformed_names(_BUCKET_FEATURE_KEYS)
  ]
  categorical_columns += [
      tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension
          key,
          num_buckets=num_buckets,
          default_value=0) for key, num_buckets in zip(
              _transformed_names(_CATEGORICAL_FEATURE_KEYS),
              _MAX_CATEGORICAL_FEATURE_VALUES)
  ]
  return tf.estimator.DNNLinearCombinedClassifier(
      config=config,
      linear_feature_columns=categorical_columns,
      dnn_feature_columns=real_valued_columns,
      dnn_hidden_units=hidden_units or [100, 70, 50, 25],
      warm_start_from=warm_start_from)


</source>
<source file="systems/tfx-1.6.1/tfx/examples/custom_components/slack/example/taxi_utils_slack.py" startline="143" endline="188" pcid="3053">
def _build_estimator(config, hidden_units=None, warm_start_from=None):
  """Build an estimator for predicting the tipping behavior of taxi riders.

  Args:
    config: tf.contrib.learn.RunConfig defining the runtime environment for the
      estimator (including model_dir).
    hidden_units: [int], the layer sizes of the DNN (input layer first)
    warm_start_from: Optional directory to warm start from.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """
  real_valued_columns = [
      tf.feature_column.numeric_column(key, shape=())
      for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)
  ]
  categorical_columns = [
      tf.feature_column.categorical_column_with_identity(
          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)
      for key in _transformed_names(_VOCAB_FEATURE_KEYS)
  ]
  categorical_columns += [
      tf.feature_column.categorical_column_with_identity(
          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)
      for key in _transformed_names(_BUCKET_FEATURE_KEYS)
  ]
  categorical_columns += [
      tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension
          key,
          num_buckets=num_buckets,
          default_value=0) for key, num_buckets in zip(
              _transformed_names(_CATEGORICAL_FEATURE_KEYS),
              _MAX_CATEGORICAL_FEATURE_VALUES)
  ]
  return tf.estimator.DNNLinearCombinedClassifier(
      config=config,
      linear_feature_columns=categorical_columns,
      dnn_feature_columns=real_valued_columns,
      dnn_hidden_units=hidden_units or [100, 70, 50, 25],
      warm_start_from=warm_start_from)


</source>
</class>

<class classid="21" nclones="5" nlines="10" similarity="80">
<source file="systems/tfx-1.6.1/tfx/components/testdata/module_file/trainer_module.py" startline="134" endline="157" pcid="332">
def _example_serving_receiver_fn(tf_transform_output, schema):
  """Build the serving in inputs.

  Args:
    tf_transform_output: A TFTransformOutput.
    schema: the schema of the input data.

  Returns:
    Tensorflow graph which parses examples, applying tf-transform to them.
  """
  raw_feature_spec = _get_raw_feature_spec(schema)
  raw_feature_spec.pop(_LABEL_KEY)

  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
      raw_feature_spec, default_batch_size=None)
  serving_input_receiver = raw_input_fn()

  transformed_features = tf_transform_output.transform_raw_features(
      serving_input_receiver.features)

  return tf.estimator.export.ServingInputReceiver(
      transformed_features, serving_input_receiver.receiver_tensors)


</source>
<source file="systems/tfx-1.6.1/tfx/examples/bigquery_ml/taxi_utils_bqml.py" startline="194" endline="219" pcid="2952">
def _flat_input_serving_receiver_fn(tf_transform_output, schema):
  """Build the serving function for flat list of Dense tensors as input.

  Args:
    tf_transform_output: A TFTransformOutput.
    schema: the schema of the input data.

  Returns:
    Tensorflow graph which parses examples, applying tf-transform to them.
  """
  raw_feature_spec = _get_raw_feature_spec(schema)
  raw_feature_spec.pop(_LABEL_KEY)

  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
      raw_feature_spec, default_batch_size=None)
  serving_input_receiver = raw_input_fn()

  transformed_features = tf_transform_output.transform_raw_features(
      serving_input_receiver.features)

  # We construct a receiver function that receives flat list of Dense tensors as
  # features. This is as per BigQuery ML serving requirements.
  return tf.estimator.export.ServingInputReceiver(
      transformed_features, serving_input_receiver.features)


</source>
<source file="systems/tfx-1.6.1/tfx/experimental/templates/taxi/models/estimator_model/model.py" startline="95" endline="118" pcid="2762">
def _example_serving_receiver_fn(tf_transform_output, schema):
  """Build the serving in inputs.

  Args:
    tf_transform_output: A TFTransformOutput.
    schema: the schema of the input data.

  Returns:
    Tensorflow graph which parses examples, applying tf-transform to them.
  """
  raw_feature_spec = _get_raw_feature_spec(schema)
  raw_feature_spec.pop(features.LABEL_KEY)

  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
      raw_feature_spec, default_batch_size=None)
  serving_input_receiver = raw_input_fn()

  transformed_features = tf_transform_output.transform_raw_features(
      serving_input_receiver.features)

  return tf.estimator.export.ServingInputReceiver(
      transformed_features, serving_input_receiver.receiver_tensors)


</source>
<source file="systems/tfx-1.6.1/tfx/examples/custom_components/slack/example/taxi_utils_slack.py" startline="189" endline="212" pcid="3054">
def _example_serving_receiver_fn(transform_output, schema):
  """Build the serving in inputs.

  Args:
    transform_output: a `tft.TFTransformOutput` object.
    schema: the schema of the input data.

  Returns:
    Tensorflow graph which parses examples, applying tf-transform to them.
  """
  raw_feature_spec = _get_raw_feature_spec(schema)
  raw_feature_spec.pop(_LABEL_KEY)

  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
      raw_feature_spec, default_batch_size=None)
  serving_input_receiver = raw_input_fn()

  _, transformed_features = transform_output.transform_raw_features(
      serving_input_receiver.features, drop_unused_features=True)

  return tf.estimator.export.ServingInputReceiver(
      transformed_features, serving_input_receiver.receiver_tensors)


</source>
<source file="systems/tfx-1.6.1/tfx/examples/chicago_taxi_pipeline/taxi_utils.py" startline="188" endline="211" pcid="3081">
def _example_serving_receiver_fn(tf_transform_output, schema):
  """Build the serving in inputs.

  Args:
    tf_transform_output: A TFTransformOutput.
    schema: the schema of the input data.

  Returns:
    Tensorflow graph which parses examples, applying tf-transform to them.
  """
  raw_feature_spec = _get_raw_feature_spec(schema)
  raw_feature_spec.pop(_LABEL_KEY)

  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
      raw_feature_spec, default_batch_size=None)
  serving_input_receiver = raw_input_fn()

  transformed_features = tf_transform_output.transform_raw_features(
      serving_input_receiver.features)

  return tf.estimator.export.ServingInputReceiver(
      transformed_features, serving_input_receiver.receiver_tensors)


</source>
</class>

<class classid="22" nclones="5" nlines="14" similarity="85">
<source file="systems/tfx-1.6.1/tfx/components/testdata/module_file/trainer_module.py" startline="158" endline="200" pcid="333">
def _eval_input_receiver_fn(tf_transform_output, schema):
  """Build everything needed for the tf-model-analysis to run the model.

  Args:
    tf_transform_output: A TFTransformOutput.
    schema: the schema of the input data.

  Returns:
    EvalInputReceiver function, which contains:
      - Tensorflow graph which parses raw untransformed features, applies the
        tf-transform preprocessing operators.
      - Set of raw, untransformed features.
      - Label against which predictions will be compared.
  """
  # Notice that the inputs are raw features, not transformed features here.
  raw_feature_spec = _get_raw_feature_spec(schema)

  serialized_tf_example = tf.compat.v1.placeholder(
      dtype=tf.string, shape=[None], name='input_example_tensor')

  # Add a parse_example operator to the tensorflow graph, which will parse
  # raw, untransformed, tf examples.
  features = tf.io.parse_example(
      serialized=serialized_tf_example, features=raw_feature_spec)

  # Now that we have our raw examples, process them through the tf-transform
  # function computed during the preprocessing step.
  transformed_features = tf_transform_output.transform_raw_features(
      features)

  # The key name MUST be 'examples'.
  receiver_tensors = {'examples': serialized_tf_example}

  # NOTE: Model is driven by transformed features (since training works on the
  # materialized output of TFT, but slicing will happen on raw features.
  features.update(transformed_features)

  return tfma.export.EvalInputReceiver(
      features=features,
      receiver_tensors=receiver_tensors,
      labels=transformed_features[_transformed_name(_LABEL_KEY)])


</source>
<source file="systems/tfx-1.6.1/tfx/examples/custom_components/slack/example/taxi_utils_slack.py" startline="213" endline="255" pcid="3055">
def _eval_input_receiver_fn(transform_output, schema):
  """Build everything needed for the tf-model-analysis to run the model.

  Args:
    transform_output: a `tft.TFTransformOutput` object.
    schema: the schema of the input data.

  Returns:
    EvalInputReceiver function, which contains:
      - Tensorflow graph which parses raw untransformed features, applies the
        tf-transform preprocessing operators.
      - Set of raw, untransformed features.
      - Label against which predictions will be compared.
  """
  # Notice that the inputs are raw features, not transformed features here.
  raw_feature_spec = _get_raw_feature_spec(schema)

  serialized_tf_example = tf.compat.v1.placeholder(
      dtype=tf.string, shape=[None], name='input_example_tensor')

  # Add a parse_example operator to the tensorflow graph, which will parse
  # raw, untransformed, tf examples.
  features = tf.io.parse_example(
      serialized=serialized_tf_example, features=raw_feature_spec)

  # Now that we have our raw examples, process them through the tf-transform
  # function computed during the preprocessing step.
  _, transformed_features = transform_output.transform_raw_features(
      features, drop_unused_features=True)

  # The key name MUST be 'examples'.
  receiver_tensors = {'examples': serialized_tf_example}

  # NOTE: Model is driven by transformed features (since training works on the
  # materialized output of TFT, but slicing will happen on raw features.
  features.update(transformed_features)

  return tfma.export.EvalInputReceiver(
      features=features,
      receiver_tensors=receiver_tensors,
      labels=transformed_features[_transformed_name(_LABEL_KEY)])


</source>
<source file="systems/tfx-1.6.1/tfx/experimental/templates/taxi/models/estimator_model/model.py" startline="119" endline="162" pcid="2763">
def _eval_input_receiver_fn(tf_transform_output, schema):
  """Build everything needed for the tf-model-analysis to run the model.

  Args:
    tf_transform_output: A TFTransformOutput.
    schema: the schema of the input data.

  Returns:
    EvalInputReceiver function, which contains:
      - Tensorflow graph which parses raw untransformed features, applies the
        tf-transform preprocessing operators.
      - Set of raw, untransformed features.
      - Label against which predictions will be compared.
  """
  # Notice that the inputs are raw features, not transformed features here.
  raw_feature_spec = _get_raw_feature_spec(schema)

  serialized_tf_example = tf.compat.v1.placeholder(
      dtype=tf.string, shape=[None], name='input_example_tensor')

  # Add a parse_example operator to the tensorflow graph, which will parse
  # raw, untransformed, tf examples.
  raw_features = tf.io.parse_example(
      serialized=serialized_tf_example, features=raw_feature_spec)

  # Now that we have our raw examples, process them through the tf-transform
  # function computed during the preprocessing step.
  transformed_features = tf_transform_output.transform_raw_features(
      raw_features)

  # The key name MUST be 'examples'.
  receiver_tensors = {'examples': serialized_tf_example}

  # NOTE: Model is driven by transformed features (since training works on the
  # materialized output of TFT, but slicing will happen on raw features.
  raw_features.update(transformed_features)

  return tfma.export.EvalInputReceiver(
      features=raw_features,
      receiver_tensors=receiver_tensors,
      labels=transformed_features[features.transformed_name(
          features.LABEL_KEY)])


</source>
<source file="systems/tfx-1.6.1/tfx/examples/bigquery_ml/taxi_utils_bqml.py" startline="220" endline="261" pcid="2953">
def _eval_input_receiver_fn(tf_transform_output, schema):
  """Build everything needed for the tf-model-analysis to run the model.

  Args:
    tf_transform_output: A TFTransformOutput.
    schema: the schema of the input data.

  Returns:
    EvalInputReceiver function, which contains:
      - Tensorflow graph which parses raw untransformed features, applies the
        tf-transform preprocessing operators.
      - Set of raw, untransformed features.
      - Label against which predictions will be compared.
  """
  # Notice that the inputs are raw features, not transformed features here.
  raw_feature_spec = _get_raw_feature_spec(schema)

  serialized_tf_example = tf.compat.v1.placeholder(
      dtype=tf.string, shape=[None], name='input_example_tensor')

  # Add a parse_example operator to the tensorflow graph, which will parse
  # raw, untransformed, tf examples.
  features = tf.io.parse_example(
      serialized=serialized_tf_example, features=raw_feature_spec)

  # Now that we have our raw examples, process them through the tf-transform
  # function computed during the preprocessing step.
  transformed_features = tf_transform_output.transform_raw_features(features)

  # The key name MUST be 'examples'.
  receiver_tensors = {'examples': serialized_tf_example}

  # NOTE: Model is driven by transformed features (since training works on the
  # materialized output of TFT, but slicing will happen on raw features.
  features.update(transformed_features)

  return tfma.export.EvalInputReceiver(
      features=features,
      receiver_tensors=receiver_tensors,
      labels=transformed_features[_transformed_name(_LABEL_KEY)])


</source>
<source file="systems/tfx-1.6.1/tfx/examples/chicago_taxi_pipeline/taxi_utils.py" startline="212" endline="254" pcid="3082">
def _eval_input_receiver_fn(tf_transform_output, schema):
  """Build everything needed for the tf-model-analysis to run the model.

  Args:
    tf_transform_output: A TFTransformOutput.
    schema: the schema of the input data.

  Returns:
    EvalInputReceiver function, which contains:
      - Tensorflow graph which parses raw untransformed features, applies the
        tf-transform preprocessing operators.
      - Set of raw, untransformed features.
      - Label against which predictions will be compared.
  """
  # Notice that the inputs are raw features, not transformed features here.
  raw_feature_spec = _get_raw_feature_spec(schema)

  serialized_tf_example = tf.compat.v1.placeholder(
      dtype=tf.string, shape=[None], name='input_example_tensor')

  # Add a parse_example operator to the tensorflow graph, which will parse
  # raw, untransformed, tf examples.
  features = tf.io.parse_example(
      serialized=serialized_tf_example, features=raw_feature_spec)

  # Now that we have our raw examples, process them through the tf-transform
  # function computed during the preprocessing step.
  transformed_features = tf_transform_output.transform_raw_features(
      features)

  # The key name MUST be 'examples'.
  receiver_tensors = {'examples': serialized_tf_example}

  # NOTE: Model is driven by transformed features (since training works on the
  # materialized output of TFT, but slicing will happen on raw features.
  features.update(transformed_features)

  return tfma.export.EvalInputReceiver(
      features=features,
      receiver_tensors=receiver_tensors,
      labels=transformed_features[_transformed_name(_LABEL_KEY)])


</source>
</class>

<class classid="23" nclones="5" nlines="46" similarity="72">
<source file="systems/tfx-1.6.1/tfx/components/testdata/module_file/trainer_module.py" startline="227" endline="314" pcid="335">
def trainer_fn(trainer_fn_args, schema):
  """Build the estimator using the high level API.

  Args:
    trainer_fn_args: Holds args used to train the model as name/value pairs.
    schema: Holds the schema of the training examples.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """
  if trainer_fn_args.hyperparameters:
    hp = trainer_fn_args.hyperparameters
    first_dnn_layer_size = hp.get('first_dnn_layer_size')
    num_dnn_layers = hp.get('num_dnn_layers')
    dnn_decay_factor = hp.get('dnn_decay_factor')
  else:
    # Number of nodes in the first layer of the DNN
    first_dnn_layer_size = 100
    num_dnn_layers = 4
    dnn_decay_factor = 0.7

  train_batch_size = 40
  eval_batch_size = 40

  tf_transform_output = tft.TFTransformOutput(trainer_fn_args.transform_output)

  train_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.train_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=train_batch_size)

  eval_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.eval_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=eval_batch_size)

  train_spec = tf.estimator.TrainSpec(  # pylint: disable=g-long-lambda
      train_input_fn,
      max_steps=trainer_fn_args.train_steps)

  serving_receiver_fn = lambda: _example_serving_receiver_fn(  # pylint: disable=g-long-lambda
      tf_transform_output, schema)

  exporter = tf.estimator.FinalExporter('chicago-taxi', serving_receiver_fn)
  eval_spec = tf.estimator.EvalSpec(
      eval_input_fn,
      steps=trainer_fn_args.eval_steps,
      exporters=[exporter],
      name='chicago-taxi-eval')

  run_config = tf.estimator.RunConfig(
      save_checkpoints_steps=999,
      # keep_checkpoint_max must be more than the number of worker replicas
      # nodes if training distributed, in order to avoid race condition.
      keep_checkpoint_max=5)

  export_dir = path_utils.serving_model_dir(trainer_fn_args.model_run_dir)
  run_config = run_config.replace(model_dir=export_dir)
  warm_start_from = trainer_fn_args.base_model

  estimator = _build_estimator(
      # Construct layers sizes with exponetial decay
      hidden_units=[
          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))
          for i in range(num_dnn_layers)
      ],
      config=run_config,
      warm_start_from=warm_start_from)

  # Create an input receiver for TFMA processing
  receiver_fn = lambda: _eval_input_receiver_fn(  # pylint: disable=g-long-lambda
      tf_transform_output, schema)

  return {
      'estimator': estimator,
      'train_spec': train_spec,
      'eval_spec': eval_spec,
      'eval_input_receiver_fn': receiver_fn
  }


# TFX generic trainer will call this function
</source>
<source file="systems/tfx-1.6.1/tfx/examples/bigquery_ml/taxi_utils_bqml.py" startline="287" endline="360" pcid="2955">
def trainer_fn(trainer_fn_args, schema):
  """Build the estimator using the high level API.

  Args:
    trainer_fn_args: Holds args used to train the model as name/value pairs.
    schema: Holds the schema of the training examples.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """
  # Number of nodes in the first layer of the DNN
  first_dnn_layer_size = 100
  num_dnn_layers = 4
  dnn_decay_factor = 0.7

  train_batch_size = 40
  eval_batch_size = 40

  tf_transform_output = tft.TFTransformOutput(trainer_fn_args.transform_output)

  train_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.train_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=train_batch_size)

  eval_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.eval_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=eval_batch_size)

  train_spec = tf.estimator.TrainSpec(  # pylint: disable=g-long-lambda
      train_input_fn,
      max_steps=trainer_fn_args.train_steps)

  serving_receiver_fn = lambda: _flat_input_serving_receiver_fn(  # pylint: disable=g-long-lambda
      tf_transform_output, schema)

  exporter = tf.estimator.FinalExporter('chicago-taxi', serving_receiver_fn)
  eval_spec = tf.estimator.EvalSpec(
      eval_input_fn,
      steps=trainer_fn_args.eval_steps,
      exporters=[exporter],
      name='chicago-taxi-eval')

  run_config = tf.estimator.RunConfig(
      save_checkpoints_steps=999, keep_checkpoint_max=1)

  run_config = run_config.replace(model_dir=trainer_fn_args.serving_model_dir)

  estimator = _build_estimator(
      # Construct layers sizes with exponential decay
      hidden_units=[
          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))
          for i in range(num_dnn_layers)
      ],
      config=run_config,
      warm_start_from=trainer_fn_args.base_model)

  # Create an input receiver for TFMA processing
  receiver_fn = lambda: _eval_input_receiver_fn(  # pylint: disable=g-long-lambda
      tf_transform_output, schema)

  return {
      'estimator': estimator,
      'train_spec': train_spec,
      'eval_spec': eval_spec,
      'eval_input_receiver_fn': receiver_fn
  }
</source>
<source file="systems/tfx-1.6.1/tfx/examples/chicago_taxi_pipeline/taxi_utils.py" startline="280" endline="357" pcid="3084">
def trainer_fn(trainer_fn_args, schema):
  """Build the estimator using the high level API.

  Args:
    trainer_fn_args: Holds args used to train the model as name/value pairs.
    schema: Holds the schema of the training examples.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """
  # Number of nodes in the first layer of the DNN
  first_dnn_layer_size = 100
  num_dnn_layers = 4
  dnn_decay_factor = 0.7

  train_batch_size = 40
  eval_batch_size = 40

  tf_transform_output = tft.TFTransformOutput(trainer_fn_args.transform_output)

  train_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.train_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=train_batch_size)

  eval_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.eval_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=eval_batch_size)

  train_spec = tf.estimator.TrainSpec(  # pylint: disable=g-long-lambda
      train_input_fn,
      max_steps=trainer_fn_args.train_steps)

  serving_receiver_fn = lambda: _example_serving_receiver_fn(  # pylint: disable=g-long-lambda
      tf_transform_output, schema)

  exporter = tf.estimator.FinalExporter('chicago-taxi', serving_receiver_fn)
  eval_spec = tf.estimator.EvalSpec(
      eval_input_fn,
      steps=trainer_fn_args.eval_steps,
      exporters=[exporter],
      name='chicago-taxi-eval')

  # Keep multiple checkpoint files for distributed training, note that
  # keep_max_checkpoint should be greater or equal to the number of replicas to
  # avoid race condition.
  run_config = tf.estimator.RunConfig(
      save_checkpoints_steps=999, keep_checkpoint_max=5)

  run_config = run_config.replace(model_dir=trainer_fn_args.serving_model_dir)
  warm_start_from = trainer_fn_args.base_model

  estimator = _build_estimator(
      # Construct layers sizes with exponetial decay
      hidden_units=[
          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))
          for i in range(num_dnn_layers)
      ],
      config=run_config,
      warm_start_from=warm_start_from)

  # Create an input receiver for TFMA processing
  receiver_fn = lambda: _eval_input_receiver_fn(  # pylint: disable=g-long-lambda
      tf_transform_output, schema)

  return {
      'estimator': estimator,
      'train_spec': train_spec,
      'eval_spec': eval_spec,
      'eval_input_receiver_fn': receiver_fn
  }
</source>
<source file="systems/tfx-1.6.1/tfx/examples/custom_components/slack/example/taxi_utils_slack.py" startline="281" endline="352" pcid="3057">
def trainer_fn(trainer_fn_args, schema):
  """Build the estimator using the high level API.

  Args:
    trainer_fn_args: Holds args used to train the model as name/value pairs.
    schema: Holds the schema of the training examples.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """
  # Number of nodes in the first layer of the DNN
  first_dnn_layer_size = 100
  num_dnn_layers = 4
  dnn_decay_factor = 0.7

  train_batch_size = 40
  eval_batch_size = 40

  tf_transform_output = tft.TFTransformOutput(trainer_fn_args.transform_output)

  train_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.train_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=train_batch_size)

  eval_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.eval_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=eval_batch_size)

  train_spec = tf.estimator.TrainSpec(
      train_input_fn, max_steps=trainer_fn_args.train_steps)

  serving_receiver_fn = (
      lambda: _example_serving_receiver_fn(tf_transform_output, schema))

  exporter = tf.estimator.FinalExporter('chicago-taxi', serving_receiver_fn)
  eval_spec = tf.estimator.EvalSpec(
      eval_input_fn,
      steps=trainer_fn_args.eval_steps,
      exporters=[exporter],
      name='chicago-taxi-eval')

  run_config = tf.estimator.RunConfig(
      save_checkpoints_steps=999, keep_checkpoint_max=1)

  run_config = run_config.replace(model_dir=trainer_fn_args.serving_model_dir)

  estimator = _build_estimator(
      # Construct layers sizes with exponetial decay
      hidden_units=[
          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))
          for i in range(num_dnn_layers)
      ],
      config=run_config,
      warm_start_from=trainer_fn_args.base_model)

  # Create an input receiver for TFMA processing
  receiver_fn = lambda: _eval_input_receiver_fn(tf_transform_output, schema)

  return {
      'estimator': estimator,
      'train_spec': train_spec,
      'eval_spec': eval_spec,
      'eval_input_receiver_fn': receiver_fn
  }
</source>
<source file="systems/tfx-1.6.1/tfx/experimental/templates/taxi/models/estimator_model/model.py" startline="185" endline="248" pcid="2765">
def _create_train_and_eval_spec(trainer_fn_args, schema):
  """Build the estimator using the high level API.

  Args:
    trainer_fn_args: Holds args used to train the model as name/value pairs.
    schema: Holds the schema of the training examples.

  Returns:
    A dict of the following:
      - estimator: The estimator that will be used for training and eval.
      - train_spec: Spec for training.
      - eval_spec: Spec for eval.
      - eval_input_receiver_fn: Input function for eval.
  """

  tf_transform_output = tft.TFTransformOutput(trainer_fn_args.transform_output)

  train_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.train_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=constants.TRAIN_BATCH_SIZE)

  eval_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda
      trainer_fn_args.eval_files,
      trainer_fn_args.data_accessor,
      tf_transform_output,
      batch_size=constants.EVAL_BATCH_SIZE)

  train_spec = tf.estimator.TrainSpec(  # pylint: disable=g-long-lambda
      train_input_fn,
      max_steps=trainer_fn_args.train_steps)

  serving_receiver_fn = lambda: _example_serving_receiver_fn(  # pylint: disable=g-long-lambda
      tf_transform_output, schema)

  exporter = tf.estimator.FinalExporter('chicago-taxi', serving_receiver_fn)
  eval_spec = tf.estimator.EvalSpec(
      eval_input_fn,
      steps=trainer_fn_args.eval_steps,
      exporters=[exporter],
      name='chicago-taxi-eval')

  run_config = tf.estimator.RunConfig(
      save_checkpoints_steps=999, keep_checkpoint_max=1)

  run_config = run_config.replace(model_dir=trainer_fn_args.serving_model_dir)

  estimator = _build_estimator(
      hidden_units=constants.HIDDEN_UNITS, config=run_config)

  # Create an input receiver for TFMA processing
  receiver_fn = lambda: _eval_input_receiver_fn(  # pylint: disable=g-long-lambda
      tf_transform_output, schema)

  return {
      'estimator': estimator,
      'train_spec': train_spec,
      'eval_spec': eval_spec,
      'eval_input_receiver_fn': receiver_fn
  }


# TFX will call this function
</source>
</class>

<class classid="24" nclones="4" nlines="23" similarity="82">
<source file="systems/tfx-1.6.1/tfx/components/testdata/module_file/transform_module.py" startline="92" endline="134" pcid="340">
def preprocessing_fn(inputs, custom_config):
  """tf.transform's callback function for preprocessing inputs.

  Args:
    inputs: map from feature keys to raw not-yet-transformed features.
    custom_config: additional properties for pre-processing.

  Returns:
    Map from string feature key to transformed features.
  """
  outputs = {}
  for key in _DENSE_FLOAT_FEATURE_KEYS:
    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.
    outputs[_transformed_name(key)] = tft.scale_to_z_score(
        _fill_in_missing(_identity(inputs[key])))

  for key in _VOCAB_FEATURE_KEYS:
    # Build a vocabulary for this feature.
    outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(
        _fill_in_missing(inputs[key]),
        top_k=custom_config.get('VOCAB_SIZE', _VOCAB_SIZE),
        num_oov_buckets=custom_config.get('OOV_SIZE', _OOV_SIZE))

  for key in _BUCKET_FEATURE_KEYS:
    outputs[_transformed_name(key)] = tft.bucketize(
        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)

  for key in _CATEGORICAL_FEATURE_KEYS:
    outputs[_transformed_name(key)] = _fill_in_missing(inputs[key])

  # Was this passenger a big tipper?
  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])
  tips = _fill_in_missing(inputs[_LABEL_KEY])
  outputs[_transformed_name(_LABEL_KEY)] = tf.compat.v1.where(
      tf.math.is_nan(taxi_fare),
      tf.cast(tf.zeros_like(taxi_fare), tf.int64),
      # Test if the tip was > 20% of the fare.
      tf.cast(
          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))

  return outputs


</source>
<source file="systems/tfx-1.6.1/tfx/examples/chicago_taxi_pipeline/taxi_utils.py" startline="100" endline="141" pcid="3079">
def preprocessing_fn(inputs):
  """tf.transform's callback function for preprocessing inputs.

  Args:
    inputs: map from feature keys to raw not-yet-transformed features.

  Returns:
    Map from string feature key to transformed feature operations.
  """
  outputs = {}
  for key in _DENSE_FLOAT_FEATURE_KEYS:
    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.
    outputs[_transformed_name(key)] = tft.scale_to_z_score(
        _fill_in_missing(inputs[key]))

  for key in _VOCAB_FEATURE_KEYS:
    # Build a vocabulary for this feature.
    outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(
        _fill_in_missing(inputs[key]),
        top_k=_VOCAB_SIZE,
        num_oov_buckets=_OOV_SIZE)

  for key in _BUCKET_FEATURE_KEYS:
    outputs[_transformed_name(key)] = tft.bucketize(
        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)

  for key in _CATEGORICAL_FEATURE_KEYS:
    outputs[_transformed_name(key)] = _fill_in_missing(inputs[key])

  # Was this passenger a big tipper?
  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])
  tips = _fill_in_missing(inputs[_LABEL_KEY])
  outputs[_transformed_name(_LABEL_KEY)] = tf.compat.v1.where(
      tf.math.is_nan(taxi_fare),
      tf.cast(tf.zeros_like(taxi_fare), tf.int64),
      # Test if the tip was > 20% of the fare.
      tf.cast(
          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))

  return outputs


</source>
<source file="systems/tfx-1.6.1/tfx/examples/custom_components/slack/example/taxi_utils_slack.py" startline="101" endline="142" pcid="3052">
def preprocessing_fn(inputs):
  """tf.transform's callback function for preprocessing inputs.

  Args:
    inputs: map from feature keys to raw not-yet-transformed features.

  Returns:
    Map from string feature key to transformed feature operations.
  """
  outputs = {}
  for key in _DENSE_FLOAT_FEATURE_KEYS:
    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.
    outputs[_transformed_name(key)] = tft.scale_to_z_score(
        _fill_in_missing(inputs[key]))

  for key in _VOCAB_FEATURE_KEYS:
    # Build a vocabulary for this feature.
    outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(
        _fill_in_missing(inputs[key]),
        top_k=_VOCAB_SIZE,
        num_oov_buckets=_OOV_SIZE)

  for key in _BUCKET_FEATURE_KEYS:
    outputs[_transformed_name(key)] = tft.bucketize(
        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)

  for key in _CATEGORICAL_FEATURE_KEYS:
    outputs[_transformed_name(key)] = _fill_in_missing(inputs[key])

  # Was this passenger a big tipper?
  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])
  tips = _fill_in_missing(inputs[_LABEL_KEY])
  outputs[_transformed_name(_LABEL_KEY)] = tf.compat.v1.where(
      tf.math.is_nan(taxi_fare),
      tf.cast(tf.zeros_like(taxi_fare), tf.int64),
      # Test if the tip was > 20% of the fare.
      tf.cast(
          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))

  return outputs


</source>
<source file="systems/tfx-1.6.1/tfx/examples/bigquery_ml/taxi_utils_bqml.py" startline="106" endline="147" pcid="2950">
def preprocessing_fn(inputs):
  """tf.transform's callback function for preprocessing inputs.

  Args:
    inputs: map from feature keys to raw not-yet-transformed features.

  Returns:
    Map from string feature key to transformed feature operations.
  """
  outputs = {}
  for key in _DENSE_FLOAT_FEATURE_KEYS:
    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.
    outputs[_transformed_name(key)] = tft.scale_to_z_score(
        _fill_in_missing(inputs[key]))

  for key in _VOCAB_FEATURE_KEYS:
    # Build a vocabulary for this feature.
    outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(
        _fill_in_missing(inputs[key]),
        top_k=_VOCAB_SIZE,
        num_oov_buckets=_OOV_SIZE)

  for key in _BUCKET_FEATURE_KEYS:
    outputs[_transformed_name(key)] = tft.bucketize(
        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)

  for key in _CATEGORICAL_FEATURE_KEYS:
    outputs[_transformed_name(key)] = _fill_in_missing(inputs[key])

  # Was this passenger a big tipper?
  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])
  tips = _fill_in_missing(inputs[_LABEL_KEY])
  outputs[_transformed_name(_LABEL_KEY)] = tf.compat.v1.where(
      tf.math.is_nan(taxi_fare),
      tf.cast(tf.zeros_like(taxi_fare), tf.int64),
      # Test if the tip was > 20% of the fare.
      tf.cast(
          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))

  return outputs


</source>
</class>

<class classid="25" nclones="4" nlines="19" similarity="73">
<source file="systems/tfx-1.6.1/tfx/components/trainer/rewriting/tflite_rewriter_test.py" startline="61" endline="80" pcid="344">
  def testInvokeTFLiteRewriterNoAssetsSucceeds(self, converter):
    m = self.ConverterMock()
    converter.return_value = m

    src_model, dst_model, _, dst_model_path = self.create_temp_model_template()

    tfrw = tflite_rewriter.TFLiteRewriter(name='myrw', filename='fname')
    tfrw.perform_rewrite(src_model, dst_model)

    converter.assert_called_once_with(
        saved_model_path=mock.ANY,
        quantization_optimizations=[],
        quantization_supported_types=[],
        representative_dataset=None,
        signature_key=None)
    expected_model = os.path.join(dst_model_path, 'fname')
    self.assertTrue(fileio.exists(expected_model))
    with fileio.open(expected_model, 'rb') as f:
      self.assertEqual(f.read(), b'model')

</source>
<source file="systems/tfx-1.6.1/tfx/components/trainer/rewriting/tflite_rewriter_test.py" startline="133" endline="155" pcid="346">
  def testInvokeTFLiteRewriterQuantizationHybridSucceeds(self, converter):
    m = self.ConverterMock()
    converter.return_value = m

    src_model, dst_model, _, dst_model_path = self.create_temp_model_template()

    tfrw = tflite_rewriter.TFLiteRewriter(
        name='myrw',
        filename='fname',
        quantization_optimizations=[tf.lite.Optimize.DEFAULT])
    tfrw.perform_rewrite(src_model, dst_model)

    converter.assert_called_once_with(
        saved_model_path=mock.ANY,
        quantization_optimizations=[tf.lite.Optimize.DEFAULT],
        quantization_supported_types=[],
        representative_dataset=None,
        signature_key=None)
    expected_model = os.path.join(dst_model_path, 'fname')
    self.assertTrue(fileio.exists(expected_model))
    with fileio.open(expected_model, 'rb') as f:
      self.assertEqual(f.read(), b'model')

</source>
<source file="systems/tfx-1.6.1/tfx/components/trainer/rewriting/tflite_rewriter_test.py" startline="158" endline="181" pcid="347">
  def testInvokeTFLiteRewriterQuantizationFloat16Succeeds(self, converter):
    m = self.ConverterMock()
    converter.return_value = m

    src_model, dst_model, _, dst_model_path = self.create_temp_model_template()

    tfrw = tflite_rewriter.TFLiteRewriter(
        name='myrw',
        filename='fname',
        quantization_optimizations=[tf.lite.Optimize.DEFAULT],
        quantization_supported_types=[tf.float16])
    tfrw.perform_rewrite(src_model, dst_model)

    converter.assert_called_once_with(
        saved_model_path=mock.ANY,
        quantization_optimizations=[tf.lite.Optimize.DEFAULT],
        quantization_supported_types=[tf.float16],
        representative_dataset=None,
        signature_key=None)
    expected_model = os.path.join(dst_model_path, 'fname')
    self.assertTrue(fileio.exists(expected_model))
    with fileio.open(expected_model, 'rb') as f:
      self.assertEqual(f.read(), b'model')

</source>
<source file="systems/tfx-1.6.1/tfx/components/trainer/rewriting/tflite_rewriter_test.py" startline="205" endline="233" pcid="349">
  def testInvokeTFLiteRewriterQuantizationFullIntegerSucceeds(self, converter):
    m = self.ConverterMock()
    converter.return_value = m

    src_model, dst_model, _, dst_model_path = self.create_temp_model_template()

    def representative_dataset():
      for i in range(2):
        yield [np.array(i)]

    tfrw = tflite_rewriter.TFLiteRewriter(
        name='myrw',
        filename='fname',
        quantization_optimizations=[tf.lite.Optimize.DEFAULT],
        quantization_enable_full_integer=True,
        representative_dataset=representative_dataset)
    tfrw.perform_rewrite(src_model, dst_model)

    converter.assert_called_once_with(
        saved_model_path=mock.ANY,
        quantization_optimizations=[tf.lite.Optimize.DEFAULT],
        quantization_supported_types=[],
        representative_dataset=representative_dataset,
        signature_key=None)
    expected_model = os.path.join(dst_model_path, 'fname')
    self.assertTrue(fileio.exists(expected_model))
    with fileio.open(expected_model, 'rb') as f:
      self.assertEqual(f.read(), b'model')

</source>
</class>

<class classid="26" nclones="2" nlines="13" similarity="84">
<source file="systems/tfx-1.6.1/tfx/components/example_gen/custom_executors/avro_component_test.py" startline="35" endline="53" pcid="506">
  def setUp(self):
    super().setUp()
    # Create input_base.
    input_data_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'testdata')
    self.avro_dir_path = os.path.join(input_data_dir, 'external')

    # Create input_config.
    self.input_config = example_gen_pb2.Input(splits=[
        example_gen_pb2.Input.Split(name='avro', pattern='avro/*.avro'),
    ])

    # Create output_config.
    self.output_config = example_gen_pb2.Output(
        split_config=example_gen_pb2.SplitConfig(splits=[
            example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=2),
            example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)
        ]))

</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/custom_executors/parquet_component_test.py" startline="35" endline="54" pcid="518">
  def setUp(self):
    super().setUp()
    # Create input_base.
    input_data_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'testdata')
    self.parquet_dir_path = os.path.join(input_data_dir, 'external')

    # Create input_config.
    self.input_config = example_gen_pb2.Input(splits=[
        example_gen_pb2.Input.Split(name='parquet',
                                    pattern='parquet/*.parquet'),
    ])

    # Create output_config.
    self.output_config = example_gen_pb2.Output(
        split_config=example_gen_pb2.SplitConfig(splits=[
            example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=2),
            example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)
        ]))

</source>
</class>

<class classid="27" nclones="2" nlines="33" similarity="100">
<source file="systems/tfx-1.6.1/tfx/components/example_gen/custom_executors/avro_component_test.py" startline="55" endline="97" pcid="507">
  def testRun(self, mock_publisher):
    mock_publisher.return_value.publish_execution.return_value = {}

    example_gen = FileBasedExampleGen(
        custom_executor_spec=executor_spec.ExecutorClassSpec(
            avro_executor.Executor),
        input_base=self.avro_dir_path,
        input_config=self.input_config,
        output_config=self.output_config).with_id('AvroExampleGen')

    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    pipeline_root = os.path.join(output_data_dir, 'Test')
    fileio.makedirs(pipeline_root)
    pipeline_info = data_types.PipelineInfo(
        pipeline_name='Test', pipeline_root=pipeline_root, run_id='123')

    driver_args = data_types.DriverArgs(enable_cache=True)

    connection_config = metadata_store_pb2.ConnectionConfig()
    connection_config.sqlite.SetInParent()
    metadata_connection = metadata.Metadata(connection_config)

    launcher = in_process_component_launcher.InProcessComponentLauncher.create(
        component=example_gen,
        pipeline_info=pipeline_info,
        driver_args=driver_args,
        metadata_connection=metadata_connection,
        beam_pipeline_args=[],
        additional_pipeline_args={})
    self.assertEqual(
        launcher._component_info.component_type,
        '.'.join([FileBasedExampleGen.__module__,
                  FileBasedExampleGen.__name__]))

    launcher.launch()
    mock_publisher.return_value.publish_execution.assert_called_once()

    # Check output paths.
    self.assertTrue(fileio.exists(os.path.join(pipeline_root, example_gen.id)))


</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/custom_executors/parquet_component_test.py" startline="56" endline="98" pcid="519">
  def testRun(self, mock_publisher):
    mock_publisher.return_value.publish_execution.return_value = {}

    example_gen = FileBasedExampleGen(
        custom_executor_spec=executor_spec.ExecutorClassSpec(
            parquet_executor.Executor),
        input_base=self.parquet_dir_path,
        input_config=self.input_config,
        output_config=self.output_config).with_id('ParquetExampleGen')

    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    pipeline_root = os.path.join(output_data_dir, 'Test')
    fileio.makedirs(pipeline_root)
    pipeline_info = data_types.PipelineInfo(
        pipeline_name='Test', pipeline_root=pipeline_root, run_id='123')

    driver_args = data_types.DriverArgs(enable_cache=True)

    connection_config = metadata_store_pb2.ConnectionConfig()
    connection_config.sqlite.SetInParent()
    metadata_connection = metadata.Metadata(connection_config)

    launcher = in_process_component_launcher.InProcessComponentLauncher.create(
        component=example_gen,
        pipeline_info=pipeline_info,
        driver_args=driver_args,
        metadata_connection=metadata_connection,
        beam_pipeline_args=[],
        additional_pipeline_args={})
    self.assertEqual(
        launcher._component_info.component_type,
        '.'.join([FileBasedExampleGen.__module__,
                  FileBasedExampleGen.__name__]))

    launcher.launch()
    mock_publisher.return_value.publish_execution.assert_called_once()

    # Check output paths.
    self.assertTrue(fileio.exists(os.path.join(pipeline_root, example_gen.id)))


</source>
</class>

<class classid="28" nclones="6" nlines="13" similarity="70">
<source file="systems/tfx-1.6.1/tfx/components/example_gen/custom_executors/parquet_executor_test.py" startline="38" endline="55" pcid="509">
  def testParquetToExample(self):
    with beam.Pipeline() as pipeline:
      examples = (
          pipeline
          | 'ToTFExample' >> parquet_executor._ParquetToExample(
              exec_properties={
                  standard_component_specs.INPUT_BASE_KEY: self._input_data_dir
              },
              split_pattern='parquet/*'))

      def check_result(got):
        # We use Python assertion here to avoid Beam serialization error in
        # pickling tf.test.TestCase.
        assert (10000 == len(got)), 'Unexpected example count'
        assert (18 == len(got[0].features.feature)), 'Example not match'

      util.assert_that(examples, check_result)

</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/csv_example_gen/executor_test.py" startline="38" endline="55" pcid="591">
  def testCsvToExample(self):
    with beam.Pipeline() as pipeline:
      examples = (
          pipeline
          | 'ToTFExample' >> executor._CsvToExample(
              exec_properties={
                  standard_component_specs.INPUT_BASE_KEY: self._input_data_dir
              },
              split_pattern='csv/*'))

      def check_results(results):
        # We use Python assertion here to avoid Beam serialization error.
        assert (15000 == len(results)), 'Unexpected example count {}.'.format(
            len(results))
        assert (18 == len(results[0].features.feature)), 'Example not match.'

      util.assert_that(examples, check_results)

</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/import_example_gen/executor_test.py" startline="51" endline="69" pcid="550">
  def testImportExample(self):
    with beam.Pipeline() as pipeline:
      examples = (
          pipeline
          | 'ToSerializedRecord' >> executor._ImportSerializedRecord(
              exec_properties={
                  standard_component_specs.INPUT_BASE_KEY: self._input_data_dir
              },
              split_pattern='tfrecord/*')
          | 'ToTFExample' >> beam.Map(tf.train.Example.FromString))

      def check_result(got):
        # We use Python assertion here to avoid Beam serialization error in
        # pickling tf.test.TestCase.
        assert (15000 == len(got)), 'Unexpected example count'
        assert (18 == len(got[0].features.feature)), 'Example not match'

      util.assert_that(examples, check_result)

</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/custom_executors/avro_executor_test.py" startline="38" endline="55" pcid="515">
  def testAvroToExample(self):
    with beam.Pipeline() as pipeline:
      examples = (
          pipeline
          | 'ToTFExample' >> avro_executor._AvroToExample(
              exec_properties={
                  standard_component_specs.INPUT_BASE_KEY: self._input_data_dir
              },
              split_pattern='avro/*.avro'))

      def check_result(got):
        # We use Python assertion here to avoid Beam serialization error in
        # pickling tf.test.TestCase.
        assert (10000 == len(got)), 'Unexpected example count'
        assert (18 == len(got[0].features.feature)), 'Example not match'

      util.assert_that(examples, check_result)

</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/csv_example_gen/executor_test.py" startline="56" endline="81" pcid="593">
  def testCsvToExampleWithEmptyColumn(self):
    with beam.Pipeline() as pipeline:
      examples = (
          pipeline
          | 'ToTFExample' >> executor._CsvToExample(
              exec_properties={
                  standard_component_specs.INPUT_BASE_KEY: self._input_data_dir
              },
              split_pattern='csv_empty/*'))

      def check_results(results):
        # We use Python assertion here to avoid Beam serialization error.
        assert (3 == len(results)), 'Unexpected example count {}.'.format(
            len(results))
        for example in results:
          assert (example.features.feature['A'].HasField('int64_list')
                 ), 'Column A should be int64 type.'
          assert (not example.features.feature['B'].WhichOneof('kind')
                 ), 'Column B should be empty.'
          assert (example.features.feature['C'].HasField('bytes_list')
                 ), 'Column C should be byte type.'
          assert (example.features.feature['D'].HasField('float_list')
                 ), 'Column D should be float type.'

      util.assert_that(examples, check_results)

</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/csv_example_gen/executor_test.py" startline="82" endline="104" pcid="595">
  def testCsvToExampleMultiLineString(self):
    with beam.Pipeline() as pipeline:
      examples = (
          pipeline
          | 'ToTFExample' >> executor._CsvToExample(
              exec_properties={
                  standard_component_specs.INPUT_BASE_KEY: self._input_data_dir
              },
              split_pattern='csv_multi_line_string/*'))

      def check_results(results):
        # We use Python assertion here to avoid Beam serialization error.
        assert (3 == len(results)), 'Unexpected example count: {}.'.format(
            len(results))
        instance = results[1]
        assert (instance.features.feature['B'].HasField('bytes_list')
               ), 'Column B should be bytes type. '
        value = instance.features.feature['B'].bytes_list.value
        assert (value ==
                [b'"2,\n"3",\n4\n5"']), 'Unexpected value: {}.'.format(value)

      util.assert_that(examples, check_results)

</source>
</class>

<class classid="29" nclones="4" nlines="38" similarity="77">
<source file="systems/tfx-1.6.1/tfx/components/example_gen/custom_executors/parquet_executor_test.py" startline="56" endline="106" pcid="511">
  def testDo(self):
    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    # Create output dict.
    examples = standard_artifacts.Examples()
    examples.uri = output_data_dir
    output_dict = {standard_component_specs.EXAMPLES_KEY: [examples]}

    # Create exec proterties.
    exec_properties = {
        standard_component_specs.INPUT_BASE_KEY:
            self._input_data_dir,
        standard_component_specs.INPUT_CONFIG_KEY:
            proto_utils.proto_to_json(
                example_gen_pb2.Input(splits=[
                    example_gen_pb2.Input.Split(
                        name='parquet', pattern='parquet/*'),
                ])),
        standard_component_specs.OUTPUT_CONFIG_KEY:
            proto_utils.proto_to_json(
                example_gen_pb2.Output(
                    split_config=example_gen_pb2.SplitConfig(splits=[
                        example_gen_pb2.SplitConfig.Split(
                            name='train', hash_buckets=2),
                        example_gen_pb2.SplitConfig.Split(
                            name='eval', hash_buckets=1)
                    ])))
    }

    # Run executor.
    parquet_example_gen = parquet_executor.Executor()
    parquet_example_gen.Do({}, output_dict, exec_properties)

    self.assertEqual(
        artifact_utils.encode_split_names(['train', 'eval']),
        examples.split_names)

    # Check Parquet example gen outputs.
    train_output_file = os.path.join(examples.uri, 'Split-train',
                                     'data_tfrecord-00000-of-00001.gz')
    eval_output_file = os.path.join(examples.uri, 'Split-eval',
                                    'data_tfrecord-00000-of-00001.gz')
    self.assertTrue(fileio.exists(train_output_file))
    self.assertTrue(fileio.exists(eval_output_file))
    self.assertGreater(
        fileio.open(train_output_file).size(),
        fileio.open(eval_output_file).size())


</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_big_query/example_gen/executor_test.py" startline="117" endline="172" pcid="780">
  def testDo(self, mock_client):
    # Mock query result schema for _BigQueryConverter.
    mock_client.return_value.query.return_value.result.return_value.schema = self._schema

    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    # Create output dict.
    examples = standard_artifacts.Examples()
    examples.uri = output_data_dir
    output_dict = {'examples': [examples]}

    # Create exe properties.
    exec_properties = {
        'input_config':
            proto_utils.proto_to_json(
                example_gen_pb2.Input(splits=[
                    example_gen_pb2.Input.Split(
                        name='bq', pattern='SELECT i, b, f, s FROM `fake`'),
                ])),
        'output_config':
            proto_utils.proto_to_json(
                example_gen_pb2.Output(
                    split_config=example_gen_pb2.SplitConfig(splits=[
                        example_gen_pb2.SplitConfig.Split(
                            name='train', hash_buckets=2),
                        example_gen_pb2.SplitConfig.Split(
                            name='eval', hash_buckets=1)
                    ])))
    }

    # Run executor.
    big_query_example_gen = executor.Executor(
        base_beam_executor.BaseBeamExecutor.Context(
            beam_pipeline_args=['--project=test-project']))
    big_query_example_gen.Do({}, output_dict, exec_properties)

    mock_client.assert_called_with(project='test-project')

    self.assertEqual(
        artifact_utils.encode_split_names(['train', 'eval']),
        examples.split_names)

    # Check BigQuery example gen outputs.
    train_output_file = os.path.join(examples.uri, 'Split-train',
                                     'data_tfrecord-00000-of-00001.gz')
    eval_output_file = os.path.join(examples.uri, 'Split-eval',
                                    'data_tfrecord-00000-of-00001.gz')
    self.assertTrue(fileio.exists(train_output_file))
    self.assertTrue(fileio.exists(eval_output_file))
    self.assertGreater(
        fileio.open(train_output_file).size(),
        fileio.open(eval_output_file).size())


</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/custom_executors/avro_executor_test.py" startline="56" endline="106" pcid="517">
  def testDo(self):
    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    # Create output dict.
    examples = standard_artifacts.Examples()
    examples.uri = output_data_dir
    output_dict = {standard_component_specs.EXAMPLES_KEY: [examples]}

    # Create exec proterties.
    exec_properties = {
        standard_component_specs.INPUT_BASE_KEY:
            self._input_data_dir,
        standard_component_specs.INPUT_CONFIG_KEY:
            proto_utils.proto_to_json(
                example_gen_pb2.Input(splits=[
                    example_gen_pb2.Input.Split(
                        name='avro', pattern='avro/*.avro'),
                ])),
        standard_component_specs.OUTPUT_CONFIG_KEY:
            proto_utils.proto_to_json(
                example_gen_pb2.Output(
                    split_config=example_gen_pb2.SplitConfig(splits=[
                        example_gen_pb2.SplitConfig.Split(
                            name='train', hash_buckets=2),
                        example_gen_pb2.SplitConfig.Split(
                            name='eval', hash_buckets=1)
                    ])))
    }

    # Run executor.
    avro_example_gen = avro_executor.Executor()
    avro_example_gen.Do({}, output_dict, exec_properties)

    self.assertEqual(
        artifact_utils.encode_split_names(['train', 'eval']),
        examples.split_names)

    # Check Avro example gen outputs.
    train_output_file = os.path.join(examples.uri, 'Split-train',
                                     'data_tfrecord-00000-of-00001.gz')
    eval_output_file = os.path.join(examples.uri, 'Split-eval',
                                    'data_tfrecord-00000-of-00001.gz')
    self.assertTrue(fileio.exists(train_output_file))
    self.assertTrue(fileio.exists(eval_output_file))
    self.assertGreater(
        fileio.open(train_output_file).size(),
        fileio.open(eval_output_file).size())


</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/csv_example_gen/executor_test.py" startline="105" endline="154" pcid="597">
  def testDo(self):
    output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.create_tempdir()),
        self._testMethodName)

    # Create output dict.
    examples = standard_artifacts.Examples()
    examples.uri = output_data_dir
    output_dict = {standard_component_specs.EXAMPLES_KEY: [examples]}

    # Create exec proterties.
    exec_properties = {
        standard_component_specs.INPUT_BASE_KEY:
            self._input_data_dir,
        standard_component_specs.INPUT_CONFIG_KEY:
            proto_utils.proto_to_json(
                example_gen_pb2.Input(splits=[
                    example_gen_pb2.Input.Split(name='csv', pattern='csv/*'),
                ])),
        standard_component_specs.OUTPUT_CONFIG_KEY:
            proto_utils.proto_to_json(
                example_gen_pb2.Output(
                    split_config=example_gen_pb2.SplitConfig(splits=[
                        example_gen_pb2.SplitConfig.Split(
                            name='train', hash_buckets=2),
                        example_gen_pb2.SplitConfig.Split(
                            name='eval', hash_buckets=1)
                    ])))
    }

    # Run executor.
    csv_example_gen = executor.Executor()
    csv_example_gen.Do({}, output_dict, exec_properties)

    self.assertEqual(
        artifact_utils.encode_split_names(['train', 'eval']),
        examples.split_names)

    # Check CSV example gen outputs.
    train_output_file = os.path.join(examples.uri, 'Split-train',
                                     'data_tfrecord-00000-of-00001.gz')
    eval_output_file = os.path.join(examples.uri, 'Split-eval',
                                    'data_tfrecord-00000-of-00001.gz')
    self.assertTrue(fileio.exists(train_output_file))
    self.assertTrue(fileio.exists(eval_output_file))
    self.assertGreater(
        fileio.open(train_output_file).size(),
        fileio.open(eval_output_file).size())


</source>
</class>

<class classid="30" nclones="2" nlines="16" similarity="75">
<source file="systems/tfx-1.6.1/tfx/components/example_gen/component_test.py" startline="158" endline="176" pcid="529">
  def testConstructWithOutputConfig(self):
    output_config = example_gen_pb2.Output(
        split_config=example_gen_pb2.SplitConfig(splits=[
            example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=2),
            example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1),
            example_gen_pb2.SplitConfig.Split(name='test', hash_buckets=1)
        ]))
    example_gen = TestFileBasedExampleGenComponent(
        input_base='path', output_config=output_config)
    self.assertEqual(
        standard_artifacts.Examples.TYPE_NAME,
        example_gen.outputs[standard_component_specs.EXAMPLES_KEY].type_name)

    stored_output_config = example_gen_pb2.Output()
    proto_utils.json_to_proto(
        example_gen.exec_properties[standard_component_specs.OUTPUT_CONFIG_KEY],
        stored_output_config)
    self.assertEqual(output_config, stored_output_config)

</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/component_test.py" startline="177" endline="194" pcid="530">
  def testConstructWithInputConfig(self):
    input_config = example_gen_pb2.Input(splits=[
        example_gen_pb2.Input.Split(name='train', pattern='train/*'),
        example_gen_pb2.Input.Split(name='eval', pattern='eval/*'),
        example_gen_pb2.Input.Split(name='test', pattern='test/*')
    ])
    example_gen = TestFileBasedExampleGenComponent(
        input_base='path', input_config=input_config)
    self.assertEqual(
        standard_artifacts.Examples.TYPE_NAME,
        example_gen.outputs[standard_component_specs.EXAMPLES_KEY].type_name)

    stored_input_config = example_gen_pb2.Input()
    proto_utils.json_to_proto(
        example_gen.exec_properties[standard_component_specs.INPUT_CONFIG_KEY],
        stored_input_config)
    self.assertEqual(input_config, stored_input_config)

</source>
</class>

<class classid="31" nclones="2" nlines="13" similarity="78">
<source file="systems/tfx-1.6.1/tfx/components/example_gen/component_test.py" startline="195" endline="208" pcid="531">
  def testConstructWithCustomConfig(self):
    custom_config = example_gen_pb2.CustomConfig(custom_config=any_pb2.Any())
    example_gen = component.FileBasedExampleGen(
        input_base='path',
        custom_config=custom_config,
        custom_executor_spec=executor_spec.BeamExecutorSpec(
            TestExampleGenExecutor))

    stored_custom_config = example_gen_pb2.CustomConfig()
    proto_utils.json_to_proto(
        example_gen.exec_properties[standard_component_specs.CUSTOM_CONFIG_KEY],
        stored_custom_config)
    self.assertEqual(custom_config, stored_custom_config)

</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/component_test.py" startline="209" endline="224" pcid="532">
  def testConstructWithStaticRangeConfig(self):
    range_config = range_config_pb2.RangeConfig(
        static_range=range_config_pb2.StaticRange(
            start_span_number=1, end_span_number=1))
    example_gen = component.FileBasedExampleGen(
        input_base='path',
        range_config=range_config,
        custom_executor_spec=executor_spec.BeamExecutorSpec(
            TestExampleGenExecutor))
    stored_range_config = range_config_pb2.RangeConfig()
    proto_utils.json_to_proto(
        example_gen.exec_properties[standard_component_specs.RANGE_CONFIG_KEY],
        stored_range_config)
    self.assertEqual(range_config, stored_range_config)


</source>
</class>

<class classid="32" nclones="2" nlines="20" similarity="100">
<source file="systems/tfx-1.6.1/tfx/components/example_gen/write_split_test.py" startline="34" endline="58" pcid="541">
  def testWriteSplitCounter_WithFormatUnspecified(self):
    count = 10

    def Pipeline(root):
      data = [tf.train.Example()] * count
      _ = (
          root
          | beam.Create(data)
          | write_split.WriteSplit(self._output_data_dir,
                                   example_gen_pb2.FILE_FORMAT_UNSPECIFIED))

    run_result = direct_runner.DirectRunner().run(Pipeline)
    run_result.wait_until_finish()

    num_instances = run_result.metrics().query(
        MetricsFilter().with_name('num_instances'))

    self.assertTrue(
        fileio.exists(
            os.path.join(self._output_data_dir,
                         'data_tfrecord-00000-of-00001.gz')))
    self.assertTrue(num_instances['counters'])
    self.assertEqual(len(num_instances['counters']), 1)
    self.assertEqual(num_instances['counters'][0].result, count)

</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/write_split_test.py" startline="59" endline="84" pcid="543">
  def testWriteSplitCounter_WithTFRECORDS_GZIP(self):
    count = 10

    def Pipeline(root):
      data = [tf.train.Example()] * count
      _ = (
          root
          | beam.Create(data)
          | write_split.WriteSplit(self._output_data_dir,
                                   example_gen_pb2.FORMAT_TFRECORDS_GZIP))

    run_result = direct_runner.DirectRunner().run(Pipeline)
    run_result.wait_until_finish()

    num_instances = run_result.metrics().query(
        MetricsFilter().with_name('num_instances'))

    self.assertTrue(
        fileio.exists(
            os.path.join(self._output_data_dir,
                         'data_tfrecord-00000-of-00001.gz')))
    self.assertTrue(num_instances['counters'])
    self.assertEqual(len(num_instances['counters']), 1)
    self.assertEqual(num_instances['counters'][0].result, count)


</source>
</class>

<class classid="33" nclones="3" nlines="15" similarity="75">
<source file="systems/tfx-1.6.1/tfx/components/example_gen/import_example_gen/component.py" startline="42" endline="74" pcid="556">
  def __init__(
      self,
      input_base: Optional[str] = None,
      input_config: Optional[Union[example_gen_pb2.Input,
                                   data_types.RuntimeParameter]] = None,
      output_config: Optional[Union[example_gen_pb2.Output,
                                    data_types.RuntimeParameter]] = None,
      range_config: Optional[Union[range_config_pb2.RangeConfig,
                                   data_types.RuntimeParameter]] = None,
      payload_format: Optional[int] = example_gen_pb2.FORMAT_TF_EXAMPLE):
    """Construct an ImportExampleGen component.

    Args:
      input_base: an external directory containing the TFRecord files.
      input_config: An example_gen_pb2.Input instance, providing input
        configuration. If unset, the files under input_base will be treated as a
        single split.
      output_config: An example_gen_pb2.Output instance, providing output
        configuration. If unset, default splits will be 'train' and 'eval' with
        size 2:1.
      range_config: An optional range_config_pb2.RangeConfig instance,
        specifying the range of span values to consider. If unset, driver will
        default to searching for latest span with no restrictions.
      payload_format: Payload format of input data. Should be one of
        example_gen_pb2.PayloadFormat enum. Note that payload format of output
        data is the same as input.
    """
    super().__init__(
        input_base=input_base,
        input_config=input_config,
        output_config=output_config,
        range_config=range_config,
        output_data_format=payload_format)
</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/csv_example_gen/component.py" startline="64" endline="91" pcid="598">
  def __init__(
      self,
      input_base: Optional[str] = None,
      input_config: Optional[Union[example_gen_pb2.Input,
                                   data_types.RuntimeParameter]] = None,
      output_config: Optional[Union[example_gen_pb2.Output,
                                    data_types.RuntimeParameter]] = None,
      range_config: Optional[Union[range_config_pb2.RangeConfig,
                                   data_types.RuntimeParameter]] = None):
    """Construct a CsvExampleGen component.

    Args:
      input_base: an external directory containing the CSV files.
      input_config: An example_gen_pb2.Input instance, providing input
        configuration. If unset, the files under input_base will be treated as a
        single split.
      output_config: An example_gen_pb2.Output instance, providing output
        configuration. If unset, default splits will be 'train' and 'eval' with
        size 2:1.
      range_config: An optional range_config_pb2.RangeConfig instance,
        specifying the range of span values to consider. If unset, driver will
        default to searching for latest span with no restrictions.
    """
    super().__init__(
        input_base=input_base,
        input_config=input_config,
        output_config=output_config,
        range_config=range_config)
</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_big_query/example_gen/component.py" startline="40" endline="76" pcid="781">
  def __init__(
      self,
      query: Optional[str] = None,
      input_config: Optional[Union[example_gen_pb2.Input,
                                   data_types.RuntimeParameter]] = None,
      output_config: Optional[Union[example_gen_pb2.Output,
                                    data_types.RuntimeParameter]] = None,
      range_config: Optional[Union[range_config_pb2.RangeConfig,
                                   data_types.RuntimeParameter]] = None):
    """Constructs a BigQueryExampleGen component.

    Args:
      query: BigQuery sql string, query result will be treated as a single
        split, can be overwritten by input_config.
      input_config: An example_gen_pb2.Input instance with Split.pattern as
        BigQuery sql string. If set, it overwrites the 'query' arg, and allows
        different queries per split. If any field is provided as a
        RuntimeParameter, input_config should be constructed as a dict with the
        same field names as Input proto message.
      output_config: An example_gen_pb2.Output instance, providing output
        configuration. If unset, default splits will be 'train' and 'eval' with
        size 2:1. If any field is provided as a RuntimeParameter,
        input_config should be constructed as a dict with the same field names
        as Output proto message.
      range_config: An optional range_config_pb2.RangeConfig instance,
        specifying the range of span values to consider.

    Raises:
      RuntimeError: Only one of query and input_config should be set.
    """
    if bool(query) == bool(input_config):
      raise RuntimeError('Exactly one of query and input_config should be set.')
    input_config = input_config or utils.make_default_input_config(query)
    super().__init__(
        input_config=input_config,
        output_config=output_config,
        range_config=range_config)
</source>
</class>

<class classid="34" nclones="2" nlines="28" similarity="75">
<source file="systems/tfx-1.6.1/tfx/components/example_gen/component.py" startline="58" endline="115" pcid="569">
  def __init__(
      self,
      input_config: Union[example_gen_pb2.Input, data_types.RuntimeParameter],
      output_config: Optional[Union[example_gen_pb2.Output,
                                    data_types.RuntimeParameter]] = None,
      custom_config: Optional[Union[example_gen_pb2.CustomConfig,
                                    data_types.RuntimeParameter]] = None,
      range_config: Optional[Union[range_config_pb2.RangeConfig,
                                   data_types.RuntimeParameter]] = None,
      output_data_format: Optional[int] = example_gen_pb2.FORMAT_TF_EXAMPLE,
      output_file_format: Optional[int] = example_gen_pb2.FORMAT_TFRECORDS_GZIP,
  ):
    """Construct a QueryBasedExampleGen component.

    Args:
      input_config: An
        [example_gen_pb2.Input](https://github.com/tensorflow/tfx/blob/master/tfx/proto/example_gen.proto)
        instance, providing input configuration. _required_
      output_config: An
        [example_gen_pb2.Output](https://github.com/tensorflow/tfx/blob/master/tfx/proto/example_gen.proto)
        instance, providing output configuration. If unset, the default splits
        will be labeled as 'train' and 'eval' with a distribution ratio of 2:1.
      custom_config: An
        [example_gen_pb2.CustomConfig](https://github.com/tensorflow/tfx/blob/master/tfx/proto/example_gen.proto)
        instance, providing custom configuration for ExampleGen.
      range_config: An optional range_config_pb2.RangeConfig instance,
        specifying the range of span values to consider.
      output_data_format: Payload format of generated data in output artifact,
        one of example_gen_pb2.PayloadFormat enum.
      output_file_format: File format of generated data in output artifact,
          one of example_gen_pb2.FileFormat enum.

    Raises:
      ValueError: The output_data_format, output_file_format value
        must be defined in the example_gen_pb2.PayloadFormat proto.
    """
    # Configure outputs.
    output_config = output_config or utils.make_default_output_config(
        input_config)
    example_artifacts = types.Channel(type=standard_artifacts.Examples)
    if output_data_format not in example_gen_pb2.PayloadFormat.values():
      raise ValueError('The value of output_data_format must be defined in'
                       'the example_gen_pb2.PayloadFormat proto.')
    if output_file_format not in example_gen_pb2.FileFormat.values():
      raise ValueError('The value of output_file_format must be defined in'
                       'the example_gen_pb2.FileFormat proto.')

    spec = standard_component_specs.QueryBasedExampleGenSpec(
        input_config=input_config,
        output_config=output_config,
        range_config=range_config,
        output_data_format=output_data_format,
        output_file_format=output_file_format,
        custom_config=custom_config,
        examples=example_artifacts)
    super().__init__(spec=spec)


</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/component.py" startline="145" endline="196" pcid="570">
  def __init__(
      self,
      input_base: Optional[str] = None,
      input_config: Optional[Union[example_gen_pb2.Input,
                                   data_types.RuntimeParameter]] = None,
      output_config: Optional[Union[example_gen_pb2.Output,
                                    data_types.RuntimeParameter]] = None,
      custom_config: Optional[Union[example_gen_pb2.CustomConfig,
                                    data_types.RuntimeParameter]] = None,
      range_config: Optional[Union[range_config_pb2.RangeConfig,
                                   data_types.RuntimeParameter]] = None,
      output_data_format: Optional[int] = example_gen_pb2.FORMAT_TF_EXAMPLE,
      output_file_format: Optional[int] = example_gen_pb2.FORMAT_TFRECORDS_GZIP,
      custom_executor_spec: Optional[executor_spec.ExecutorSpec] = None):
    """Construct a FileBasedExampleGen component.

    Args:
      input_base: an external directory containing the data files.
      input_config: An
        [`example_gen_pb2.Input`](https://github.com/tensorflow/tfx/blob/master/tfx/proto/example_gen.proto)
          instance, providing input configuration. If unset, input files will be
          treated as a single split.
      output_config: An example_gen_pb2.Output instance, providing the output
        configuration. If unset, default splits will be 'train' and
        'eval' with size 2:1.
      custom_config: An optional example_gen_pb2.CustomConfig instance,
        providing custom configuration for executor.
      range_config: An optional range_config_pb2.RangeConfig instance,
        specifying the range of span values to consider. If unset, driver will
        default to searching for latest span with no restrictions.
      output_data_format: Payload format of generated data in output artifact,
        one of example_gen_pb2.PayloadFormat enum.
      output_file_format: File format of generated data in output artifact,
        one of example_gen_pb2.FileFormat enum.
      custom_executor_spec: Optional custom executor spec overriding the default
        executor spec specified in the component attribute.
    """
    # Configure inputs and outputs.
    input_config = input_config or utils.make_default_input_config()
    output_config = output_config or utils.make_default_output_config(
        input_config)
    example_artifacts = types.Channel(type=standard_artifacts.Examples)
    spec = standard_component_specs.FileBasedExampleGenSpec(
        input_base=input_base,
        input_config=input_config,
        output_config=output_config,
        custom_config=custom_config,
        range_config=range_config,
        output_data_format=output_data_format,
        output_file_format=output_file_format,
        examples=example_artifacts)
    super().__init__(spec=spec, custom_executor_spec=custom_executor_spec)
</source>
</class>

<class classid="35" nclones="7" nlines="10" similarity="70">
<source file="systems/tfx-1.6.1/tfx/components/example_gen/utils_test.py" startline="267" endline="279" pcid="610">
  def testVersionWrongFormat(self):
    wrong_version = os.path.join(self._input_base_path, 'span01', 'versionx',
                                 'split1', 'data')
    io_utils.write_string_file(wrong_version, 'testing_wrong_version')

    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='span{SPAN}/version{VERSION}/split1/*')
    ]
    with self.assertRaisesRegex(ValueError, 'Cannot find version number'):
      utils.calculate_splits_fingerprint_span_and_version(
          self._input_base_path, splits)

</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/utils_test.py" startline="399" endline="413" pcid="619">
  def testDateBadFormat(self):
    # Test improperly formed date.
    split1 = os.path.join(self._input_base_path, 'yyyymmdd', 'split1', 'data')
    io_utils.write_string_file(split1, 'testing')

    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='{YYYY}{MM}{DD}/split1/*')
    ]

    with self.assertRaisesRegex(ValueError,
                                'Cannot find span number using date'):
      utils.calculate_splits_fingerprint_span_and_version(
          self._input_base_path, splits)

</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/utils_test.py" startline="528" endline="541" pcid="627">
  def testSpanVersionWidthNoSeperator(self):
    split1 = os.path.join(self._input_base_path, '1234', 'split1', 'data')
    io_utils.write_string_file(split1, 'testing')

    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='{SPAN:2}{VERSION:2}/split1/*')
    ]

    _, span, version = utils.calculate_splits_fingerprint_span_and_version(
        self._input_base_path, splits)
    self.assertEqual(span, 12)
    self.assertEqual(version, 34)

</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/utils_test.py" startline="427" endline="441" pcid="621">
  def testHaveDateNoVersion(self):
    # Test specific behavior when Date spec is present but Version is not.
    split1 = os.path.join(self._input_base_path, '19700102', 'split1', 'data')
    io_utils.write_string_file(split1, 'testing')

    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='{YYYY}{MM}{DD}/split1/*')
    ]

    _, span, version = utils.calculate_splits_fingerprint_span_and_version(
        self._input_base_path, splits)
    self.assertEqual(span, 1)
    self.assertIsNone(version)

</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/utils_test.py" startline="442" endline="457" pcid="622">
  def testHaveDateAndVersion(self):
    # Test specific behavior when both Date and Version are present.
    split1 = os.path.join(self._input_base_path, '19700102', 'ver1', 'split1',
                          'data')
    io_utils.write_string_file(split1, 'testing')

    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='{YYYY}{MM}{DD}/ver{VERSION}/split1/*')
    ]

    _, span, version = utils.calculate_splits_fingerprint_span_and_version(
        self._input_base_path, splits)
    self.assertEqual(span, 1)
    self.assertEqual(version, 1)

</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/utils_test.py" startline="320" endline="335" pcid="613">
  def testHaveSpanAndVersion(self):
    # Test specific behavior when both Span and Version are present.
    split1 = os.path.join(self._input_base_path, 'span1', 'version1', 'split1',
                          'data')
    io_utils.write_string_file(split1, 'testing')

    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='span{SPAN}/version{VERSION}/split1/*')
    ]

    _, span, version = utils.calculate_splits_fingerprint_span_and_version(
        self._input_base_path, splits)
    self.assertEqual(span, 1)
    self.assertEqual(version, 1)

</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/utils_test.py" startline="360" endline="378" pcid="616">
  def testNewSpanWithOlderVersionAlign(self):
    # Test specific behavior when a newer Span has older Version.
    span1_ver2 = os.path.join(self._input_base_path, 'span1', 'ver2', 'split1',
                              'data')
    io_utils.write_string_file(span1_ver2, 'testing')
    span2_ver1 = os.path.join(self._input_base_path, 'span2', 'ver1', 'split1',
                              'data')
    io_utils.write_string_file(span2_ver1, 'testing')

    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='span{SPAN}/ver{VERSION}/split1/*')
    ]

    _, span, version = utils.calculate_splits_fingerprint_span_and_version(
        self._input_base_path, splits)
    self.assertEqual(span, 2)
    self.assertEqual(version, 1)

</source>
</class>

<class classid="36" nclones="2" nlines="48" similarity="100">
<source file="systems/tfx-1.6.1/tfx/components/example_gen/utils_test.py" startline="542" endline="602" pcid="628">
  def testCalculateSplitsFingerprintSpanAndVersionWithSpan(self):
    # Test align of span and version numbers.
    span1_v1_split1 = os.path.join(self._input_base_path, 'span01', 'ver01',
                                   'split1', 'data')
    io_utils.write_string_file(span1_v1_split1, 'testing11')
    span1_v1_split2 = os.path.join(self._input_base_path, 'span01', 'ver01',
                                   'split2', 'data')
    io_utils.write_string_file(span1_v1_split2, 'testing12')
    span2_v1_split1 = os.path.join(self._input_base_path, 'span02', 'ver01',
                                   'split1', 'data')
    io_utils.write_string_file(span2_v1_split1, 'testing21')

    # Test if error raised when span does not align.
    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='span{SPAN}/ver{VERSION}/split1/*'),
        example_gen_pb2.Input.Split(
            name='s2', pattern='span{SPAN}/ver{VERSION}/split2/*')
    ]
    with self.assertRaisesRegex(
        ValueError, 'Latest span should be the same for each split'):
      utils.calculate_splits_fingerprint_span_and_version(
          self._input_base_path, splits)

    span2_v1_split2 = os.path.join(self._input_base_path, 'span02', 'ver01',
                                   'split2', 'data')
    io_utils.write_string_file(span2_v1_split2, 'testing22')
    span2_v2_split1 = os.path.join(self._input_base_path, 'span02', 'ver02',
                                   'split1', 'data')
    io_utils.write_string_file(span2_v2_split1, 'testing21')

    # Test if error raised when span aligns but version does not.
    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='span{SPAN}/ver{VERSION}/split1/*'),
        example_gen_pb2.Input.Split(
            name='s2', pattern='span{SPAN}/ver{VERSION}/split2/*')
    ]
    with self.assertRaisesRegex(
        ValueError, 'Latest version should be the same for each split'):
      utils.calculate_splits_fingerprint_span_and_version(
          self._input_base_path, splits)

    span2_v2_split2 = os.path.join(self._input_base_path, 'span02', 'ver02',
                                   'split2', 'data')
    io_utils.write_string_file(span2_v2_split2, 'testing22')

    # Test if latest span and version is selected when aligned for each split.
    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='span{SPAN}/ver{VERSION}/split1/*'),
        example_gen_pb2.Input.Split(
            name='s2', pattern='span{SPAN}/ver{VERSION}/split2/*')
    ]
    _, span, version = utils.calculate_splits_fingerprint_span_and_version(
        self._input_base_path, splits)
    self.assertEqual(span, 2)
    self.assertEqual(version, 2)
    self.assertEqual(splits[0].pattern, 'span02/ver02/split1/*')
    self.assertEqual(splits[1].pattern, 'span02/ver02/split2/*')

</source>
<source file="systems/tfx-1.6.1/tfx/components/example_gen/utils_test.py" startline="603" endline="662" pcid="629">
  def testCalculateSplitsFingerprintSpanAndVersionWithDate(self):
    # Test align of span and version numbers.
    span1_v1_split1 = os.path.join(self._input_base_path, '19700102', 'ver01',
                                   'split1', 'data')
    io_utils.write_string_file(span1_v1_split1, 'testing11')
    span1_v1_split2 = os.path.join(self._input_base_path, '19700102', 'ver01',
                                   'split2', 'data')
    io_utils.write_string_file(span1_v1_split2, 'testing12')
    span2_v1_split1 = os.path.join(self._input_base_path, '19700103', 'ver01',
                                   'split1', 'data')
    io_utils.write_string_file(span2_v1_split1, 'testing21')

    # Test if error raised when date does not align.
    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='{YYYY}{MM}{DD}/ver{VERSION}/split1/*'),
        example_gen_pb2.Input.Split(
            name='s2', pattern='{YYYY}{MM}{DD}/ver{VERSION}/split2/*')
    ]
    with self.assertRaisesRegex(
        ValueError, 'Latest span should be the same for each split'):
      utils.calculate_splits_fingerprint_span_and_version(
          self._input_base_path, splits)

    span2_v1_split2 = os.path.join(self._input_base_path, '19700103', 'ver01',
                                   'split2', 'data')
    io_utils.write_string_file(span2_v1_split2, 'testing22')
    span2_v2_split1 = os.path.join(self._input_base_path, '19700103', 'ver02',
                                   'split1', 'data')
    io_utils.write_string_file(span2_v2_split1, 'testing21')

    # Test if error raised when date aligns but version does not.
    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='{YYYY}{MM}{DD}/ver{VERSION}/split1/*'),
        example_gen_pb2.Input.Split(
            name='s2', pattern='{YYYY}{MM}{DD}/ver{VERSION}/split2/*')
    ]
    with self.assertRaisesRegex(
        ValueError, 'Latest version should be the same for each split'):
      utils.calculate_splits_fingerprint_span_and_version(
          self._input_base_path, splits)
    span2_v2_split2 = os.path.join(self._input_base_path, '19700103', 'ver02',
                                   'split2', 'data')
    io_utils.write_string_file(span2_v2_split2, 'testing22')

    # Test if latest span and version is selected when aligned for each split.
    splits = [
        example_gen_pb2.Input.Split(
            name='s1', pattern='{YYYY}{MM}{DD}/ver{VERSION}/split1/*'),
        example_gen_pb2.Input.Split(
            name='s2', pattern='{YYYY}{MM}{DD}/ver{VERSION}/split2/*')
    ]
    _, span, version = utils.calculate_splits_fingerprint_span_and_version(
        self._input_base_path, splits)
    self.assertEqual(span, 2)
    self.assertEqual(version, 2)
    self.assertEqual(splits[0].pattern, '19700103/ver02/split1/*')
    self.assertEqual(splits[1].pattern, '19700103/ver02/split2/*')

</source>
</class>

<class classid="37" nclones="2" nlines="12" similarity="76">
<source file="systems/tfx-1.6.1/tfx/types/artifact_test.py" startline="947" endline="963" pcid="678">
  def testTypeAnnotationIsNone(self):
    my_artifact_1 = _MyArtifact()
    self.assertIsNone(my_artifact_1.TYPE_ANNOTATION)
    self.assertEqual(my_artifact_1.artifact_type.base_type,
                     metadata_store_pb2.ArtifactType.UNSET)

    # _MyArtifact2/_MyArtifact3 classes are created from _ArtifactType method.
    my_artifact_2 = _MyArtifact2()
    self.assertIsNone(my_artifact_2.TYPE_ANNOTATION)
    self.assertEqual(my_artifact_2.artifact_type.base_type,
                     metadata_store_pb2.ArtifactType.UNSET)

    my_artifact_3 = _MyArtifact3()
    self.assertIsNone(my_artifact_3.TYPE_ANNOTATION)
    self.assertEqual(my_artifact_3.artifact_type.base_type,
                     metadata_store_pb2.ArtifactType.UNSET)

</source>
<source file="systems/tfx-1.6.1/tfx/types/artifact_test.py" startline="964" endline="977" pcid="679">
  def testArtifactTypeWithTypeAnnotation(self):
    my_artifact_with_annotation_1 = _MyArtifact4()
    self.assertEqual(my_artifact_with_annotation_1.artifact_type.base_type,
                     metadata_store_pb2.ArtifactType.DATASET)

    # _MyArtifact5/_MyArtifact6 classes are created from _ArtifactType method.
    my_artifact_with_annotation_2 = _MyArtifact5()
    self.assertEqual(my_artifact_with_annotation_2.artifact_type.base_type,
                     metadata_store_pb2.ArtifactType.DATASET)

    my_artifact_with_annotation_3 = _MyArtifact6()
    self.assertEqual(my_artifact_with_annotation_3.artifact_type.base_type,
                     metadata_store_pb2.ArtifactType.DATASET)

</source>
</class>

<class classid="38" nclones="2" nlines="19" similarity="78">
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_big_query/experimental/elwc_example_gen/component/component.py" startline="35" endline="78" pcid="755">
  def __init__(self,
               query: Optional[str] = None,
               elwc_config: Optional[elwc_config_pb2.ElwcConfig] = None,
               input_config: Optional[example_gen_pb2.Input] = None,
               output_config: Optional[example_gen_pb2.Output] = None):
    """Constructs a BigQueryElwcExampleGen component.

    Args:
      query: BigQuery sql string, query result will be treated as a single
        split, can be overwritten by input_config.
      elwc_config: The elwc config contains a list of context feature fields.
        The fields are used to build context feature. Examples with the same
        context feature will be converted to an ELWC(ExampleListWithContext)
        instance. For example, when there are two examples with the same context
        field, the two examples will be intergrated to a ELWC instance.
      input_config: An example_gen_pb2.Input instance with Split.pattern as
        BigQuery sql string. If set, it overwrites the 'query' arg, and allows
        different queries per split. If any field is provided as a
        RuntimeParameter, input_config should be constructed as a dict with the
        same field names as Input proto message.
      output_config: An example_gen_pb2.Output instance, providing output
        configuration. If unset, default splits will be 'train' and 'eval' with
        size 2:1. If any field is provided as a RuntimeParameter, input_config
          should be constructed as a dict with the same field names as Output
          proto message.

    Raises:
      RuntimeError: Only one of query and input_config should be set and
        elwc_config is required.
    """

    if bool(query) == bool(input_config):
      raise RuntimeError('Exactly one of query and input_config should be set.')
    if not elwc_config:
      raise RuntimeError(
          'elwc_config is required for BigQueryToElwcExampleGen.')
    input_config = input_config or utils.make_default_input_config(query)
    packed_custom_config = example_gen_pb2.CustomConfig()
    packed_custom_config.custom_config.Pack(elwc_config)
    super().__init__(
        input_config=input_config,
        output_config=output_config,
        output_data_format=example_gen_pb2.FORMAT_PROTO,
        custom_config=packed_custom_config)
</source>
<source file="systems/tfx-1.6.1/tfx/examples/custom_components/presto_example_gen/presto_component/component.py" startline="39" endline="78" pcid="3033">
  def __init__(self,
               conn_config: presto_config_pb2.PrestoConnConfig,
               query: Optional[str] = None,
               input_config: Optional[example_gen_pb2.Input] = None,
               output_config: Optional[example_gen_pb2.Output] = None):
    """Constructs a PrestoExampleGen component.

    Args:
      conn_config: Parameters for Presto connection client.
      query: Presto sql string, query result will be treated as a single split,
        can be overwritten by input_config.
      input_config: An example_gen_pb2.Input instance with Split.pattern as
        Presto sql string. If set, it overwrites the 'query' arg, and allows
        different queries per split.
      output_config: An example_gen_pb2.Output instance, providing output
        configuration. If unset, default splits will be 'train' and 'eval' with
        size 2:1.

    Raises:
      RuntimeError: Only one of query and input_config should be set. Or
      required host field in connection_config should be set.
    """
    if bool(query) == bool(input_config):
      raise RuntimeError('Exactly one of query and input_config should be set.')
    if not bool(conn_config.host):
      raise RuntimeError(
          'Required host field in connection config should be set.')

    input_config = input_config or utils.make_default_input_config(query)

    packed_custom_config = example_gen_pb2.CustomConfig()
    packed_custom_config.custom_config.Pack(conn_config)

    output_config = output_config or utils.make_default_output_config(
        input_config)

    super().__init__(
        input_config=input_config,
        output_config=output_config,
        custom_config=packed_custom_config)
</source>
</class>

<class classid="39" nclones="2" nlines="10" similarity="100">
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_big_query/pusher/component.py" startline="34" endline="57" pcid="765">
  def __init__(self,
               model: Optional[types.Channel] = None,
               model_blessing: Optional[types.Channel] = None,
               infra_blessing: Optional[types.Channel] = None,
               custom_config: Optional[Dict[str, Any]] = None):
    """Construct a Pusher component.

    Args:
      model: An optional Channel of type `standard_artifacts.Model`, usually
        produced by a Trainer component.
      model_blessing: An optional Channel of type
        `standard_artifacts.ModelBlessing`, usually produced from an Evaluator
        component.
      infra_blessing: An optional Channel of type
        `standard_artifacts.InfraBlessing`, usually produced from an
        InfraValidator component.
      custom_config: A dict which contains the deployment job parameters to be
        passed to Cloud platforms.
    """
    super().__init__(
        model=model,
        model_blessing=model_blessing,
        infra_blessing=infra_blessing,
        custom_config=custom_config)
</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/pusher/component.py" startline="29" endline="53" pcid="883">
  def __init__(self,
               model: Optional[types.Channel] = None,
               model_blessing: Optional[types.Channel] = None,
               infra_blessing: Optional[types.Channel] = None,
               custom_config: Optional[Dict[str, Any]] = None):
    """Construct a Pusher component.

    Args:
      model: An optional Channel of type `standard_artifacts.Model`, usually
        produced by a Trainer component, representing the model used for
        training.
      model_blessing: An optional Channel of type
        `standard_artifacts.ModelBlessing`, usually produced from an Evaluator
        component, containing the blessing model.
      infra_blessing: An optional Channel of type
        `standard_artifacts.InfraBlessing`, usually produced from an
        InfraValidator component, containing the validation result.
      custom_config: A dict which contains the deployment job parameters to be
        passed to Cloud platforms.
    """
    super().__init__(
        model=model,
        model_blessing=model_blessing,
        infra_blessing=infra_blessing,
        custom_config=custom_config)
</source>
</class>

<class classid="40" nclones="2" nlines="57" similarity="87">
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/bulk_inferrer/executor_test.py" startline="76" endline="139" pcid="793">
  def testDoWithBlessedModel(self, mock_runner, mock_run_model_inference, _):
    input_dict = {
        'examples': [self._examples],
        'model': [self._model],
        'model_blessing': [self._model_blessing],
    }
    output_dict = {
        'inference_result': [self._inference_result],
    }
    ai_platform_serving_args = {
        'model_name': 'model_name',
        'project_id': 'project_id'
    }
    # Create exe properties.
    exec_properties = {
        'data_spec':
            proto_utils.proto_to_json(bulk_inferrer_pb2.DataSpec()),
        'custom_config':
            json_utils.dumps({
                executor.SERVING_ARGS_KEY:
                    ai_platform_serving_args,
                constants.ENDPOINT_ARGS_KEY:
                    'https://us-central1-ml.googleapis.com',
            }),
    }
    mock_runner.get_service_name_and_api_version.return_value = ('ml', 'v1')
    mock_runner.create_model_for_aip_prediction_if_not_exist.return_value = True

    # Run executor.
    bulk_inferrer = executor.Executor(self._context)
    bulk_inferrer.Do(input_dict, output_dict, exec_properties)

    ai_platform_prediction_model_spec = (
        model_spec_pb2.AIPlatformPredictionModelSpec(
            project_id='project_id',
            model_name='model_name',
            version_name=self._model_version))
    ai_platform_prediction_model_spec.use_serialization_config = True
    inference_endpoint = model_spec_pb2.InferenceSpecType()
    inference_endpoint.ai_platform_prediction_model_spec.CopyFrom(
        ai_platform_prediction_model_spec)
    mock_run_model_inference.assert_called_once_with(mock.ANY, mock.ANY,
                                                     mock.ANY, mock.ANY,
                                                     mock.ANY,
                                                     inference_endpoint)
    executor_class_path = '%s.%s' % (bulk_inferrer.__class__.__module__,
                                     bulk_inferrer.__class__.__name__)
    with telemetry_utils.scoped_labels(
        {telemetry_utils.LABEL_TFX_EXECUTOR: executor_class_path}):
      job_labels = telemetry_utils.make_labels_dict()
    mock_runner.deploy_model_for_aip_prediction.assert_called_once_with(
        serving_path=path_utils.serving_model_path(self._model.uri),
        model_version_name=mock.ANY,
        ai_platform_serving_args=ai_platform_serving_args,
        labels=job_labels,
        api=mock.ANY,
        skip_model_endpoint_creation=True,
        set_default=False)
    mock_runner.delete_model_from_aip_if_exists.assert_called_once_with(
        model_version_name=mock.ANY,
        ai_platform_serving_args=ai_platform_serving_args,
        api=mock.ANY,
        delete_model_endpoint=True)

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/bulk_inferrer/executor_test.py" startline="145" endline="205" pcid="794">
  def testDoSkippedModelCreation(self, mock_runner, mock_run_model_inference,
                                 _):
    input_dict = {
        'examples': [self._examples],
        'model': [self._model],
        'model_blessing': [self._model_blessing],
    }
    output_dict = {
        'inference_result': [self._inference_result],
    }
    ai_platform_serving_args = {
        'model_name': 'model_name',
        'project_id': 'project_id'
    }
    # Create exe properties.
    exec_properties = {
        'data_spec':
            proto_utils.proto_to_json(bulk_inferrer_pb2.DataSpec()),
        'custom_config':
            json_utils.dumps(
                {executor.SERVING_ARGS_KEY: ai_platform_serving_args}),
    }
    mock_runner.get_service_name_and_api_version.return_value = ('ml', 'v1')
    mock_runner.create_model_for_aip_prediction_if_not_exist.return_value = False

    # Run executor.
    bulk_inferrer = executor.Executor(self._context)
    bulk_inferrer.Do(input_dict, output_dict, exec_properties)

    ai_platform_prediction_model_spec = (
        model_spec_pb2.AIPlatformPredictionModelSpec(
            project_id='project_id',
            model_name='model_name',
            version_name=self._model_version))
    ai_platform_prediction_model_spec.use_serialization_config = True
    inference_endpoint = model_spec_pb2.InferenceSpecType()
    inference_endpoint.ai_platform_prediction_model_spec.CopyFrom(
        ai_platform_prediction_model_spec)
    mock_run_model_inference.assert_called_once_with(mock.ANY, mock.ANY,
                                                     mock.ANY, mock.ANY,
                                                     mock.ANY,
                                                     inference_endpoint)
    executor_class_path = '%s.%s' % (bulk_inferrer.__class__.__module__,
                                     bulk_inferrer.__class__.__name__)
    with telemetry_utils.scoped_labels(
        {telemetry_utils.LABEL_TFX_EXECUTOR: executor_class_path}):
      job_labels = telemetry_utils.make_labels_dict()
    mock_runner.deploy_model_for_aip_prediction.assert_called_once_with(
        serving_path=path_utils.serving_model_path(self._model.uri),
        model_version_name=mock.ANY,
        ai_platform_serving_args=ai_platform_serving_args,
        labels=job_labels,
        api=mock.ANY,
        skip_model_endpoint_creation=True,
        set_default=False)
    mock_runner.delete_model_from_aip_if_exists.assert_called_once_with(
        model_version_name=mock.ANY,
        ai_platform_serving_args=ai_platform_serving_args,
        api=mock.ANY,
        delete_model_endpoint=False)

</source>
</class>

<class classid="41" nclones="2" nlines="11" similarity="90">
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/tuner/component_test.py" startline="41" endline="52" pcid="799">
  def testConstructWithCustomConfig(self):
    tuner = component.Tuner(
        examples=self.examples,
        schema=self.schema,
        train_args=self.train_args,
        eval_args=self.eval_args,
        tune_args=self.tune_args,
        module_file='/path/to/module/file',
        custom_config=self.custom_config,
    )
    self._verify_output(tuner)

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/tuner/component_test.py" startline="53" endline="64" pcid="800">
  def testConstructWithoutCustomConfig(self):
    tuner = component.Tuner(
        examples=self.examples,
        schema=self.schema,
        train_args=self.train_args,
        eval_args=self.eval_args,
        tune_args=self.tune_args,
        module_file='/path/to/module/file',
    )
    self._verify_output(tuner)


</source>
</class>

<class classid="42" nclones="2" nlines="26" similarity="80">
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/tuner/executor_test.py" startline="33" endline="63" pcid="801">
  def setUp(self):
    super().setUp()

    self._output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    self._job_dir = os.path.join(self._output_data_dir, 'jobDir')
    self._project_id = '12345'
    self._job_id = 'fake_job_id'
    self._inputs = {}
    self._outputs = {}
    # Dict format of exec_properties. custom_config needs to be serialized
    # before being passed into Do function.
    self._exec_properties = {
        'custom_config': {
            ai_platform_trainer_executor.JOB_ID_KEY: self._job_id,
            ai_platform_tuner_executor.TUNING_ARGS_KEY: {
                'project': self._project_id,
                'jobDir': self._job_dir,
            },
        },
    }
    self._executor_class_path = '%s.%s' % (
        ai_platform_tuner_executor._WorkerExecutor.__module__,
        ai_platform_tuner_executor._WorkerExecutor.__name__)

    self.addCleanup(mock.patch.stopall)
    self.mock_runner = mock.patch(
        'tfx.extensions.google_cloud_ai_platform.tuner.executor.runner').start(
        )

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/trainer/executor_test.py" startline="31" endline="62" pcid="895">
  def setUp(self):
    super().setUp()

    self._output_data_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    self._job_dir = os.path.join(self._output_data_dir, 'jobDir')
    self._project_id = '12345'
    self._inputs = {}
    self._outputs = {}
    # Dict format of exec_properties. custom_config needs to be serialized
    # before being passed into Do function.
    self._exec_properties = {
        standard_component_specs.CUSTOM_CONFIG_KEY: {
            ai_platform_trainer_executor.TRAINING_ARGS_KEY: {
                'project': self._project_id,
                'jobDir': self._job_dir,
            },
        },
    }
    self._executor_class_path = '%s.%s' % (
        tfx_trainer_executor.Executor.__module__,
        tfx_trainer_executor.Executor.__name__)
    self._generic_executor_class_path = '%s.%s' % (
        tfx_trainer_executor.GenericExecutor.__module__,
        tfx_trainer_executor.GenericExecutor.__name__)

    self.addCleanup(mock.patch.stopall)
    self.mock_runner = mock.patch(
        'tfx.extensions.google_cloud_ai_platform.trainer.executor.runner'
    ).start()

</source>
</class>

<class classid="43" nclones="5" nlines="14" similarity="75">
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/tuner/executor_test.py" startline="134" endline="150" pcid="807">
  def testDoWithEnableVertexOverride(self):
    executor = ai_platform_tuner_executor.Executor()
    enable_vertex = True
    vertex_region = 'us-central2'
    self._exec_properties[standard_component_specs.CUSTOM_CONFIG_KEY][
        constants.ENABLE_VERTEX_KEY] = enable_vertex
    self._exec_properties[standard_component_specs.CUSTOM_CONFIG_KEY][
        constants.VERTEX_REGION_KEY] = vertex_region
    executor.Do(self._inputs, self._outputs,
                self._serialize_custom_config_under_test())
    self.mock_runner.start_cloud_training.assert_called_with(
        self._inputs, self._outputs, self._serialize_custom_config_under_test(),
        self._executor_class_path, {
            'project': self._project_id,
            'jobDir': self._job_dir,
        }, self._job_id, enable_vertex, vertex_region)

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/trainer/executor_test.py" startline="106" endline="123" pcid="900">
  def testDoWithEnableVertexOverride(self):
    executor = ai_platform_trainer_executor.Executor()
    enable_vertex = True
    vertex_region = 'us-central2'
    self._exec_properties[standard_component_specs.CUSTOM_CONFIG_KEY][
        ai_platform_trainer_executor.ENABLE_VERTEX_KEY] = enable_vertex
    self._exec_properties[standard_component_specs.CUSTOM_CONFIG_KEY][
        ai_platform_trainer_executor.VERTEX_REGION_KEY] = vertex_region
    executor.Do(self._inputs, self._outputs,
                self._serialize_custom_config_under_test())
    self.mock_runner.start_cloud_training.assert_called_with(
        self._inputs, self._outputs, self._serialize_custom_config_under_test(),
        self._executor_class_path, {
            'project': self._project_id,
            'jobDir': self._job_dir,
        }, None, enable_vertex, vertex_region)


</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/trainer/executor_test.py" startline="81" endline="94" pcid="898">
  def testDoWithJobIdOverride(self):
    executor = ai_platform_trainer_executor.Executor()
    job_id = 'overridden_job_id'
    self._exec_properties[standard_component_specs.CUSTOM_CONFIG_KEY][
        ai_platform_trainer_executor.JOB_ID_KEY] = job_id
    executor.Do(self._inputs, self._outputs,
                self._serialize_custom_config_under_test())
    self.mock_runner.start_cloud_training.assert_called_with(
        self._inputs, self._outputs, self._serialize_custom_config_under_test(),
        self._executor_class_path, {
            'project': self._project_id,
            'jobDir': self._job_dir,
        }, job_id, False, None)

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/trainer/executor_test.py" startline="95" endline="105" pcid="899">
  def testDoWithGenericExecutorClass(self):
    executor = ai_platform_trainer_executor.GenericExecutor()
    executor.Do(self._inputs, self._outputs,
                self._serialize_custom_config_under_test())
    self.mock_runner.start_cloud_training.assert_called_with(
        self._inputs, self._outputs, self._serialize_custom_config_under_test(),
        self._generic_executor_class_path, {
            'project': self._project_id,
            'jobDir': self._job_dir,
        }, None, False, None)

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/trainer/executor_test.py" startline="70" endline="80" pcid="897">
  def testDo(self):
    executor = ai_platform_trainer_executor.Executor()
    executor.Do(self._inputs, self._outputs,
                self._serialize_custom_config_under_test())
    self.mock_runner.start_cloud_training.assert_called_with(
        self._inputs, self._outputs, self._serialize_custom_config_under_test(),
        self._executor_class_path, {
            'project': self._project_id,
            'jobDir': self._job_dir,
        }, None, False, None)

</source>
</class>

<class classid="44" nclones="2" nlines="28" similarity="75">
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="100" endline="136" pcid="833">
  def testStartCloudTraining(self, mock_discovery):
    mock_discovery.build.return_value = self._mock_api_client
    self._setUpTrainingMocks()

    class_path = 'foo.bar.class'

    runner.start_cloud_training(self._inputs, self._outputs,
                                self._serialize_custom_config_under_test(),
                                class_path, self._training_inputs, None)

    self._mock_create.assert_called_with(
        body=mock.ANY, parent='projects/{}'.format(self._project_id))
    kwargs = self._mock_create.call_args[1]
    body = kwargs['body']

    default_image = 'gcr.io/tfx-oss-public/tfx:{}'.format(
        version_utils.get_image_version())
    self.assertDictContainsSubset(
        {
            'masterConfig': {
                'imageUri':
                    default_image,
                'containerCommand':
                    runner._CONTAINER_COMMAND + [
                        '--executor_class_path', class_path, '--inputs', '{}',
                        '--outputs', '{}', '--exec-properties',
                        ('{"custom_config": '
                         '"{\\"ai_platform_training_args\\": {\\"project\\": \\"12345\\"'
                         '}}"}')
                    ],
            },
        }, body['training_input'])
    self.assertNotIn('project', body['training_input'])
    self.assertStartsWith(body['job_id'], 'tfx_')
    self._mock_get.execute.assert_called_with()
    self._mock_create_request.execute.assert_called_with()

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="139" endline="175" pcid="834">
  def testStartCloudTrainingWithUserContainer(self, mock_discovery):
    mock_discovery.build.return_value = self._mock_api_client
    self._setUpTrainingMocks()

    class_path = 'foo.bar.class'

    self._training_inputs['masterConfig'] = {'imageUri': 'my-custom-image'}
    self._exec_properties['custom_config'][executor.JOB_ID_KEY] = self._job_id
    runner.start_cloud_training(self._inputs, self._outputs,
                                self._serialize_custom_config_under_test(),
                                class_path, self._training_inputs, self._job_id)

    self._mock_create.assert_called_with(
        body=mock.ANY, parent='projects/{}'.format(self._project_id))
    kwargs = self._mock_create.call_args[1]
    body = kwargs['body']
    self.assertDictContainsSubset(
        {
            'masterConfig': {
                'imageUri':
                    'my-custom-image',
                'containerCommand':
                    runner._CONTAINER_COMMAND + [
                        '--executor_class_path', class_path, '--inputs', '{}',
                        '--outputs', '{}', '--exec-properties',
                        ('{"custom_config": '
                         '"{\\"ai_platform_training_args\\": '
                         '{\\"masterConfig\\": {\\"imageUri\\": \\"my-custom-image\\"}, '
                         '\\"project\\": \\"12345\\"}, '
                         '\\"ai_platform_training_job_id\\": \\"my_jobid\\"}"}')
                    ],
            }
        }, body['training_input'])
    self.assertEqual(body['job_id'], 'my_jobid')
    self._mock_get.execute.assert_called_with()
    self._mock_create_request.execute.assert_called_with()

</source>
</class>

<class classid="45" nclones="3" nlines="35" similarity="71">
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="177" endline="217" pcid="835">
  def testStartCloudTraining_Vertex(self, mock_gapic):
    mock_gapic.JobServiceClient.return_value = self._mock_api_client
    self._setUpVertexTrainingMocks()

    class_path = 'foo.bar.class'
    region = 'us-central1'

    runner.start_cloud_training(self._inputs, self._outputs,
                                self._serialize_custom_config_under_test(),
                                class_path, self._training_inputs, None, True,
                                region)

    self._mock_create.assert_called_with(
        parent='projects/{}/locations/{}'.format(self._project_id, region),
        custom_job=mock.ANY)
    kwargs = self._mock_create.call_args[1]
    body = kwargs['custom_job']

    default_image = 'gcr.io/tfx-oss-public/tfx:{}'.format(
        version_utils.get_image_version())
    self.assertDictContainsSubset(
        {
            'worker_pool_specs': [{
                'container_spec': {
                    'image_uri':
                        default_image,
                    'command':
                        runner._CONTAINER_COMMAND + [
                            '--executor_class_path', class_path, '--inputs',
                            '{}', '--outputs', '{}', '--exec-properties',
                            ('{"custom_config": '
                             '"{\\"ai_platform_training_args\\": '
                             '{\\"project\\": \\"12345\\"'
                             '}}"}')
                        ],
                },
            },],
        }, body['job_spec'])
    self.assertStartsWith(body['display_name'], 'tfx_')
    self._mock_get.assert_called_with(name='vertex_job_study_id')

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="219" endline="266" pcid="836">
  def testStartCloudTrainingWithUserContainer_Vertex(self, mock_gapic):
    mock_gapic.JobServiceClient.return_value = self._mock_api_client
    self._setUpVertexTrainingMocks()

    class_path = 'foo.bar.class'

    self._training_inputs['worker_pool_specs'] = [{
        'container_spec': {
            'image_uri': 'my-custom-image'
        }
    }]
    self._exec_properties['custom_config'][executor.JOB_ID_KEY] = self._job_id
    region = 'us-central2'
    runner.start_cloud_training(self._inputs, self._outputs,
                                self._serialize_custom_config_under_test(),
                                class_path, self._training_inputs, self._job_id,
                                True, region)

    self._mock_create.assert_called_with(
        parent='projects/{}/locations/{}'.format(self._project_id, region),
        custom_job=mock.ANY)
    kwargs = self._mock_create.call_args[1]
    body = kwargs['custom_job']
    self.assertDictContainsSubset(
        {
            'worker_pool_specs': [{
                'container_spec': {
                    'image_uri':
                        'my-custom-image',
                    'command':
                        runner._CONTAINER_COMMAND + [
                            '--executor_class_path', class_path, '--inputs',
                            '{}', '--outputs', '{}', '--exec-properties',
                            ('{"custom_config": '
                             '"{\\"ai_platform_training_args\\": '
                             '{\\"project\\": \\"12345\\", '
                             '\\"worker_pool_specs\\": '
                             '[{\\"container_spec\\": '
                             '{\\"image_uri\\": \\"my-custom-image\\"}}]}, '
                             '\\"ai_platform_training_job_id\\": '
                             '\\"my_jobid\\"}"}')
                        ],
                },
            },],
        }, body['job_spec'])
    self.assertEqual(body['display_name'], 'my_jobid')
    self._mock_get.assert_called_with(name='vertex_job_study_id')

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="268" endline="334" pcid="837">
  def testStartCloudTrainingWithVertexCustomJob(self, mock_gapic):
    mock_gapic.JobServiceClient.return_value = self._mock_api_client
    self._setUpVertexTrainingMocks()

    class_path = 'foo.bar.class'
    expected_encryption_spec = {
        'kms_key_name': 'my_kmskey',
    }
    user_provided_labels = {
        'l1': 'v1',
        'l2': 'v2',
    }

    self._training_inputs['display_name'] = 'valid_name'
    self._training_inputs['job_spec'] = {
        'worker_pool_specs': [{
            'container_spec': {
                'image_uri': 'my-custom-image'
            }
        }]
    }
    self._training_inputs['labels'] = user_provided_labels
    self._training_inputs['encryption_spec'] = expected_encryption_spec
    self._exec_properties['custom_config'][executor.JOB_ID_KEY] = self._job_id
    region = 'us-central2'
    runner.start_cloud_training(self._inputs, self._outputs,
                                self._serialize_custom_config_under_test(),
                                class_path, self._training_inputs, self._job_id,
                                True, region)

    self._mock_create.assert_called_with(
        parent='projects/{}/locations/{}'.format(self._project_id, region),
        custom_job=mock.ANY)
    kwargs = self._mock_create.call_args[1]
    body = kwargs['custom_job']
    self.assertDictContainsSubset(
        {
            'worker_pool_specs': [{
                'container_spec': {
                    'image_uri':
                        'my-custom-image',
                    'command':
                        runner._CONTAINER_COMMAND + [
                            '--executor_class_path', class_path, '--inputs',
                            '{}', '--outputs', '{}', '--exec-properties',
                            ('{"custom_config": '
                             '"{\\"ai_platform_training_args\\": '
                             '{\\"display_name\\": \\"valid_name\\", '
                             '\\"encryption_spec\\": {\\"kms_key_name\\": '
                             '\\"my_kmskey\\"}, \\"job_spec\\": '
                             '{\\"worker_pool_specs\\": '
                             '[{\\"container_spec\\": '
                             '{\\"image_uri\\": \\"my-custom-image\\"}}]}, '
                             '\\"labels\\": {\\"l1\\": \\"v1\\", '
                             '\\"l2\\": \\"v2\\"}, '
                             '\\"project\\": \\"12345\\"}, '
                             '\\"ai_platform_training_job_id\\": '
                             '\\"my_jobid\\"}"}')
                        ],
                },
            },],
        }, body['job_spec'])
    self.assertEqual(body['display_name'], 'valid_name')
    self.assertDictEqual(body['encryption_spec'], expected_encryption_spec)
    self.assertDictContainsSubset(user_provided_labels, body['labels'])
    self._mock_get.assert_called_with(name='vertex_job_study_id')

</source>
</class>

<class classid="46" nclones="3" nlines="34" similarity="75">
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="367" endline="414" pcid="839">
  def _setUpVertexPredictionMocks(self):
    importlib.reload(initializer)
    importlib.reload(aiplatform)

    self._serving_container_image_uri = 'gcr.io/path/to/container'
    self._serving_path = os.path.join(self._output_data_dir, 'serving_path')
    self._endpoint_name = 'endpoint-name'
    self._endpoint_region = 'us-central1'
    self._deployed_model_id = 'model_id'

    self._mock_create_client = mock.Mock()
    initializer.global_config.create_client = self._mock_create_client
    self._mock_create_client.return_value = mock.Mock(
        spec=endpoint_service_client.EndpointServiceClient)

    self._mock_get_endpoint = mock.Mock()
    endpoint_service_client.EndpointServiceClient.get_endpoint = self._mock_get_endpoint
    self._mock_get_endpoint.return_value = endpoint.Endpoint(
        display_name=self._endpoint_name,)

    aiplatform.init(
        project=self._project_id,
        location=None,
        credentials=mock.Mock(spec=auth_credentials.AnonymousCredentials()))

    self._mock_endpoint = aiplatform.Endpoint(
        endpoint_name='projects/{}/locations/us-central1/endpoints/1234'.format(
            self._project_id))

    self._mock_endpoint_create = mock.Mock()
    aiplatform.Endpoint.create = self._mock_endpoint_create
    self._mock_endpoint_create.return_value = self._mock_endpoint

    self._mock_endpoint_list = mock.Mock()
    aiplatform.Endpoint.list = self._mock_endpoint_list
    self._mock_endpoint_list.return_value = []

    self._mock_model_upload = mock.Mock()
    aiplatform.Model.upload = self._mock_model_upload

    self._mock_model_deploy = mock.Mock()
    self._mock_model_upload.return_value.deploy = self._mock_model_deploy

    self._ai_platform_serving_args_vertex = {
        'endpoint_name': self._endpoint_name,
        'project_id': self._project_id,
    }

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="832" endline="876" pcid="861">
  def _setUpDeleteVertexModelMocks(self):
    importlib.reload(initializer)
    importlib.reload(aiplatform)

    self._endpoint_name = 'endpoint_name'
    self._deployed_model_id = 'model_id'

    self._mock_create_client = mock.Mock()
    initializer.global_config.create_client = self._mock_create_client
    self._mock_create_client.return_value = mock.Mock(
        spec=endpoint_service_client.EndpointServiceClient)

    self._mock_get_endpoint = mock.Mock()
    endpoint_service_client.EndpointServiceClient.get_endpoint = self._mock_get_endpoint
    self._mock_get_endpoint.return_value = endpoint.Endpoint(
        display_name=self._endpoint_name)

    aiplatform.init(
        project=self._project_id,
        location=None,
        credentials=mock.Mock(spec=auth_credentials.AnonymousCredentials()))

    self._mock_endpoint = aiplatform.Endpoint(
        endpoint_name='projects/{}/locations/us-central1/endpoints/1234'.format(
            self._project_id))

    self._mock_endpoint_list = mock.Mock()
    aiplatform.Endpoint.list = self._mock_endpoint_list
    self._mock_endpoint_list.return_value = [self._mock_endpoint]

    self._mock_model_delete = mock.Mock()
    self._mock_endpoint.undeploy = self._mock_model_delete

    self._mock_list_models = mock.Mock()
    self._mock_list_models.return_value = [
        endpoint.DeployedModel(
            display_name=self._model_name, id=self._deployed_model_id)
    ]
    self._mock_endpoint.list_models = self._mock_list_models

    self._ai_platform_serving_args_vertex = {
        'endpoint_name': self._endpoint_name,
        'project_id': self._project_id,
    }

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="891" endline="928" pcid="864">
  def _setUpDeleteVertexEndpointMocks(self):
    importlib.reload(initializer)
    importlib.reload(aiplatform)

    self._endpoint_name = 'endpoint_name'

    self._mock_create_client = mock.Mock()
    initializer.global_config.create_client = self._mock_create_client
    self._mock_create_client.return_value = mock.Mock(
        spec=endpoint_service_client.EndpointServiceClient)

    self._mock_get_endpoint = mock.Mock()
    endpoint_service_client.EndpointServiceClient.get_endpoint = (
        self._mock_get_endpoint)
    self._mock_get_endpoint.return_value = endpoint.Endpoint(
        display_name=self._endpoint_name,)

    aiplatform.init(
        project=self._project_id,
        location=None,
        credentials=mock.Mock(spec=auth_credentials.AnonymousCredentials()))

    self._mock_endpoint = aiplatform.Endpoint(
        endpoint_name='projects/{}/locations/us-central1/endpoints/1234'.format(
            self._project_id))

    self._mock_endpoint_list = mock.Mock()
    aiplatform.Endpoint.list = self._mock_endpoint_list
    self._mock_endpoint_list.return_value = [self._mock_endpoint]

    self._mock_endpoint_delete = mock.Mock()
    self._mock_endpoint.delete = self._mock_endpoint_delete

    self._ai_platform_serving_args_vertex = {
        'endpoint_name': self._endpoint_name,
        'project_id': self._project_id,
    }

</source>
</class>

<class classid="47" nclones="3" nlines="15" similarity="70">
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="498" endline="515" pcid="842">
  def testDeployModelForAIPPrediction(self):
    self._setUpPredictionMocks()

    runner.deploy_model_for_aip_prediction(
        serving_path=self._serving_path,
        model_version_name=self._model_version,
        ai_platform_serving_args=self._ai_platform_serving_args,
        labels=self._job_labels,
        api=self._mock_api_client)

    expected_models_create_body = {
        'name': self._model_name,
        'regions': [],
        'labels': self._job_labels
    }
    self._assertDeployModelMockCalls(
        expected_models_create_body=expected_models_create_body)

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="584" endline="604" pcid="847">
  def testDeployModelForAIPPredictionWithCustomRuntime(self):
    self._setUpPredictionMocks()

    self._ai_platform_serving_args['runtime_version'] = '1.23.45'
    runner.deploy_model_for_aip_prediction(
        serving_path=self._serving_path,
        model_version_name=self._model_version,
        ai_platform_serving_args=self._ai_platform_serving_args,
        labels=self._job_labels,
        api=self._mock_api_client)

    expected_versions_create_body = {
        'name': self._model_version,
        'deployment_uri': self._serving_path,
        'runtime_version': '1.23.45',
        'python_version': '3.7',
        'labels': self._job_labels,
    }
    self._assertDeployModelMockCalls(
        expected_versions_create_body=expected_versions_create_body)

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="565" endline="583" pcid="846">
  def testDeployModelForAIPPredictionWithCustomRegion(self):
    self._setUpPredictionMocks()

    self._ai_platform_serving_args['regions'] = ['custom-region']
    runner.deploy_model_for_aip_prediction(
        serving_path=self._serving_path,
        model_version_name=self._model_version,
        ai_platform_serving_args=self._ai_platform_serving_args,
        labels=self._job_labels,
        api=self._mock_api_client)

    expected_models_create_body = {
        'name': self._model_name,
        'regions': ['custom-region'],
        'labels': self._job_labels
    }
    self._assertDeployModelMockCalls(
        expected_models_create_body=expected_models_create_body)

</source>
</class>

<class classid="48" nclones="2" nlines="12" similarity="81">
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="633" endline="648" pcid="849">
  def _setUpDeleteModelVersionMocks(self):
    self._model_version = 'model_version'

    self._mock_models_version_delete = mock.Mock()
    self._mock_api_client.projects().models().versions().delete = (
        self._mock_models_version_delete)
    self._mock_models_version_delete.return_value.execute.return_value = {
        'name': 'version_delete_op_name'
    }
    self._mock_get = mock.Mock()
    self._mock_api_client.projects().operations().get = self._mock_get

    self._mock_get.return_value.execute.return_value = {
        'done': True,
    }

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="668" endline="680" pcid="852">
  def _setUpDeleteModelMocks(self):
    self._mock_models_delete = mock.Mock()
    self._mock_api_client.projects().models().delete = (
        self._mock_models_delete)
    self._mock_models_delete.return_value.execute.return_value = {
        'name': 'model_delete_op_name'
    }
    self._mock_get = mock.Mock()
    self._mock_api_client.projects().operations().get = self._mock_get
    self._mock_get.return_value.execute.return_value = {
        'done': True,
    }

</source>
</class>

<class classid="49" nclones="2" nlines="30" similarity="89">
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="699" endline="730" pcid="855">
  def testDeployModelForVertexPrediction(self):
    self._setUpVertexPredictionMocks()
    self._mock_endpoint_list.side_effect = [[], [self._mock_endpoint]]

    runner.deploy_model_for_aip_prediction(
        serving_path=self._serving_path,
        model_version_name=self._model_name,
        ai_platform_serving_args=self._ai_platform_serving_args_vertex,
        labels=self._job_labels,
        serving_container_image_uri=self._serving_container_image_uri,
        endpoint_region=self._endpoint_region,
        enable_vertex=True)

    expected_endpoint_create_body = {
        'display_name': self._endpoint_name,
        'labels': self._job_labels,
    }
    expected_model_upload_body = {
        'display_name': self._model_name,
        'artifact_uri': self._serving_path,
        'serving_container_image_uri': self._serving_container_image_uri,
    }
    expected_model_deploy_body = {
        'endpoint': self._mock_endpoint,
        'traffic_percentage': 100,
    }

    self._assertDeployModelMockCallsVertex(
        expected_endpoint_create_body=expected_endpoint_create_body,
        expected_model_upload_body=expected_model_upload_body,
        expected_model_deploy_body=expected_model_deploy_body)

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="731" endline="766" pcid="856">
  def testDeployModelForVertexPredictionError(self):
    self._setUpVertexPredictionMocks()
    self._mock_endpoint_list.side_effect = [[], [self._mock_endpoint]]

    self._mock_model_deploy.side_effect = errors.HttpError(
        httplib2.Response(info={'status': 429}), b'')

    with self.assertRaises(RuntimeError):
      runner.deploy_model_for_aip_prediction(
          serving_path=self._serving_path,
          model_version_name=self._model_name,
          ai_platform_serving_args=self._ai_platform_serving_args_vertex,
          labels=self._job_labels,
          serving_container_image_uri=self._serving_container_image_uri,
          endpoint_region=self._endpoint_region,
          enable_vertex=True)

    expected_endpoint_create_body = {
        'display_name': self._endpoint_name,
        'labels': self._job_labels,
    }
    expected_model_upload_body = {
        'display_name': self._model_name,
        'artifact_uri': self._serving_path,
        'serving_container_image_uri': self._serving_container_image_uri,
    }
    expected_model_deploy_body = {
        'endpoint': self._mock_endpoint,
        'traffic_percentage': 100,
    }

    self._assertDeployModelMockCallsVertex(
        expected_endpoint_create_body=expected_endpoint_create_body,
        expected_model_upload_body=expected_model_upload_body,
        expected_model_deploy_body=expected_model_deploy_body)

</source>
</class>

<class classid="50" nclones="2" nlines="20" similarity="73">
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="786" endline="808" pcid="859">
  def testDeployModelForVertexPredictionWithCustomRegion(self):
    self._setUpVertexPredictionMocks()
    self._mock_endpoint_list.side_effect = [[], [self._mock_endpoint]]

    self._mock_init = mock.Mock()
    aiplatform.init = self._mock_init

    self._endpoint_region = 'custom-region'
    runner.deploy_model_for_aip_prediction(
        serving_path=self._serving_path,
        model_version_name=self._model_name,
        ai_platform_serving_args=self._ai_platform_serving_args_vertex,
        labels=self._job_labels,
        serving_container_image_uri=self._serving_container_image_uri,
        endpoint_region=self._endpoint_region,
        enable_vertex=True)

    expected_init_body = {
        'project': self._project_id,
        'location': 'custom-region',
    }
    self._mock_init.assert_called_with(**expected_init_body)

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/runner_test.py" startline="809" endline="831" pcid="860">
  def testDeployModelForVertexPredictionWithCustomMachineType(self):
    self._setUpVertexPredictionMocks()
    self._mock_endpoint_list.side_effect = [[], [self._mock_endpoint]]

    self._ai_platform_serving_args_vertex[
        'machine_type'] = 'custom_machine_type'
    runner.deploy_model_for_aip_prediction(
        serving_path=self._serving_path,
        model_version_name=self._model_name,
        ai_platform_serving_args=self._ai_platform_serving_args_vertex,
        labels=self._job_labels,
        serving_container_image_uri=self._serving_container_image_uri,
        endpoint_region=self._endpoint_region,
        enable_vertex=True)

    expected_model_deploy_body = {
        'endpoint': self._mock_endpoint,
        'traffic_percentage': 100,
        'machine_type': 'custom_machine_type',
    }
    self._assertDeployModelMockCallsVertex(
        expected_model_deploy_body=expected_model_deploy_body)

</source>
</class>

<class classid="51" nclones="3" nlines="28" similarity="73">
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/pusher/executor_test.py" startline="118" endline="145" pcid="876">
  def testDoBlessed(self, mock_runner, _):
    self._model_blessing.uri = os.path.join(self._source_data_dir,
                                            'model_validator/blessed')
    self._model_blessing.set_int_custom_property('blessed', 1)
    mock_runner.get_service_name_and_api_version.return_value = ('ml', 'v1')
    version = self._model_push.get_string_custom_property('pushed_version')
    mock_runner.deploy_model_for_aip_prediction.return_value = (
        'projects/project_id/models/model_name/versions/{}'.format(version))

    self._executor.Do(self._input_dict, self._output_dict,
                      self._serialize_custom_config_under_test())
    executor_class_path = '%s.%s' % (self._executor.__class__.__module__,
                                     self._executor.__class__.__name__)
    with telemetry_utils.scoped_labels(
        {telemetry_utils.LABEL_TFX_EXECUTOR: executor_class_path}):
      job_labels = telemetry_utils.make_labels_dict()
    mock_runner.deploy_model_for_aip_prediction.assert_called_once_with(
        serving_path=self._model_push.uri,
        model_version_name=mock.ANY,
        ai_platform_serving_args=mock.ANY,
        api=mock.ANY,
        labels=job_labels,
    )
    self.assertPushed()
    self.assertEqual(
        self._model_push.get_string_custom_property('pushed_destination'),
        'projects/project_id/models/model_name/versions/{}'.format(version))

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/pusher/executor_test.py" startline="186" endline="222" pcid="879">
  def testDoBlessedOnRegionalEndpoint(self, mock_runner, _):
    self._exec_properties = {
        'custom_config': {
            constants.SERVING_ARGS_KEY: {
                'model_name': 'model_name',
                'project_id': 'project_id'
            },
            constants.ENDPOINT_ARGS_KEY: 'https://ml-us-west1.googleapis.com',
        },
    }
    self._model_blessing.uri = os.path.join(self._source_data_dir,
                                            'model_validator/blessed')
    self._model_blessing.set_int_custom_property('blessed', 1)
    mock_runner.get_service_name_and_api_version.return_value = ('ml', 'v1')
    version = self._model_push.get_string_custom_property('pushed_version')
    mock_runner.deploy_model_for_aip_prediction.return_value = (
        'projects/project_id/models/model_name/versions/{}'.format(version))

    self._executor.Do(self._input_dict, self._output_dict,
                      self._serialize_custom_config_under_test())
    executor_class_path = '%s.%s' % (self._executor.__class__.__module__,
                                     self._executor.__class__.__name__)
    with telemetry_utils.scoped_labels(
        {telemetry_utils.LABEL_TFX_EXECUTOR: executor_class_path}):
      job_labels = telemetry_utils.make_labels_dict()
    mock_runner.deploy_model_for_aip_prediction.assert_called_once_with(
        serving_path=self._model_push.uri,
        model_version_name=mock.ANY,
        ai_platform_serving_args=mock.ANY,
        api=mock.ANY,
        labels=job_labels,
    )
    self.assertPushed()
    self.assertEqual(
        self._model_push.get_string_custom_property('pushed_destination'),
        'projects/project_id/models/model_name/versions/{}'.format(version))

</source>
<source file="systems/tfx-1.6.1/tfx/extensions/google_cloud_ai_platform/pusher/executor_test.py" startline="224" endline="250" pcid="880">
  def testDoBlessed_Vertex(self, mock_runner):
    endpoint_uri = 'projects/project_id/locations/us-central1/endpoints/12345'
    mock_runner.deploy_model_for_aip_prediction.return_value = endpoint_uri
    self._model_blessing.uri = os.path.join(self._source_data_dir,
                                            'model_validator/blessed')
    self._model_blessing.set_int_custom_property('blessed', 1)
    self._executor.Do(self._input_dict, self._output_dict,
                      self._serialize_custom_config_under_test_vertex())
    executor_class_path = '%s.%s' % (self._executor.__class__.__module__,
                                     self._executor.__class__.__name__)
    with telemetry_utils.scoped_labels(
        {telemetry_utils.LABEL_TFX_EXECUTOR: executor_class_path}):
      job_labels = telemetry_utils.make_labels_dict()
    mock_runner.deploy_model_for_aip_prediction.assert_called_once_with(
        serving_container_image_uri=self._container_image_uri_vertex,
        model_version_name=mock.ANY,
        ai_platform_serving_args=mock.ANY,
        labels=job_labels,
        serving_path=self._model_push.uri,
        endpoint_region='us-central1',
        enable_vertex=True,
    )
    self.assertPushed()
    self.assertEqual(
        self._model_push.get_string_custom_property('pushed_destination'),
        endpoint_uri)

</source>
</class>

<class classid="52" nclones="2" nlines="16" similarity="88">
<source file="systems/tfx-1.6.1/tfx/dsl/components/common/resolver_test.py" startline="31" endline="48" pcid="911">
  def testResolverDefinition(self):
    channel_to_resolve = types.Channel(type=standard_artifacts.Examples)
    rnode = resolver.Resolver(
        strategy_class=latest_artifact_strategy.LatestArtifactStrategy,
        config={'desired_num_of_artifacts': 5},
        channel_to_resolve=channel_to_resolve)
    self.assertDictEqual(
        rnode.exec_properties, {
            resolver.RESOLVER_STRATEGY_CLASS:
                latest_artifact_strategy.LatestArtifactStrategy,
            resolver.RESOLVER_CONFIG: {
                'desired_num_of_artifacts': 5
            }
        })
    self.assertEqual(rnode.inputs['channel_to_resolve'], channel_to_resolve)
    self.assertEqual(rnode.outputs['channel_to_resolve'].type_name,
                     channel_to_resolve.type_name)

</source>
<source file="systems/tfx-1.6.1/tfx/dsl/components/common/resolver_test.py" startline="49" endline="68" pcid="912">
  def testResolverUnionChannel(self):
    one_channel = types.Channel(type=standard_artifacts.Examples)
    another_channel = types.Channel(type=standard_artifacts.Examples)
    unioned_channel = channel.union([one_channel, another_channel])
    rnode = resolver.Resolver(
        strategy_class=latest_artifact_strategy.LatestArtifactStrategy,
        config={'desired_num_of_artifacts': 5},
        unioned_channel=unioned_channel)
    self.assertDictEqual(
        rnode.exec_properties, {
            resolver.RESOLVER_STRATEGY_CLASS:
                latest_artifact_strategy.LatestArtifactStrategy,
            resolver.RESOLVER_CONFIG: {
                'desired_num_of_artifacts': 5
            }
        })
    self.assertEqual(rnode.inputs['unioned_channel'], unioned_channel)
    self.assertEqual(rnode.outputs['unioned_channel'].type_name,
                     unioned_channel.type_name)

</source>
</class>

<class classid="53" nclones="2" nlines="14" similarity="71">
<source file="systems/tfx-1.6.1/tfx/dsl/components/common/resolver_test.py" startline="107" endline="122" pcid="917">
  def setUp(self):
    super().setUp()
    self.connection_config = metadata_store_pb2.ConnectionConfig()
    self.connection_config.sqlite.SetInParent()
    self.pipeline_info = data_types.PipelineInfo(
        pipeline_name='p_name', pipeline_root='p_root', run_id='run_id')
    self.component_info = data_types.ComponentInfo(
        component_type='c_type',
        component_id='c_id',
        pipeline_info=self.pipeline_info)
    self.driver_args = data_types.DriverArgs(enable_cache=True)
    self.source_channel_key = 'source_channel'
    self.source_channels = {
        self.source_channel_key: types.Channel(type=standard_artifacts.Examples)
    }

</source>
<source file="systems/tfx-1.6.1/tfx/dsl/input_resolution/strategies/conditional_strategy_test.py" startline="91" endline="104" pcid="1109">
  def setUp(self):
    super().setUp()
    self._connection_config = metadata_store_pb2.ConnectionConfig()
    self._connection_config.sqlite.SetInParent()
    self._metadata = self.enter_context(
        metadata.Metadata(connection_config=self._connection_config))
    self._store = self._metadata.store
    self._pipeline_info = data_types.PipelineInfo(
        pipeline_name='my_pipeline', pipeline_root='/tmp', run_id='my_run_id')
    self._component_info = data_types.ComponentInfo(
        component_type='a.b.c',
        component_id='my_component',
        pipeline_info=self._pipeline_info)

</source>
</class>

<class classid="54" nclones="2" nlines="10" similarity="80">
<source file="systems/tfx-1.6.1/tfx/dsl/components/base/executor_spec_test.py" startline="36" endline="49" pcid="941">
  def testExecutorClassSpecCopy(self):
    class _NestedExecutor(base_executor.BaseExecutor):
      pass
    spec = executor_spec.ExecutorClassSpec(_NestedExecutor)
    spec.add_extra_flags('a')
    spec_copy = spec.copy()
    del spec
    self.assertProtoEquals(
        """
        class_path: "__main__._NestedExecutor"
        extra_flags: "a"
        """,
        spec_copy.encode())

</source>
<source file="systems/tfx-1.6.1/tfx/dsl/components/base/executor_spec_test.py" startline="50" endline="68" pcid="942">
  def testBeamExecutorSpecCopy(self):

    class _NestedExecutor(base_executor.BaseExecutor):
      pass

    spec = executor_spec.BeamExecutorSpec(_NestedExecutor)
    spec.add_extra_flags('a')
    spec.add_beam_pipeline_args('b')
    spec_copy = spec.copy()
    del spec
    self.assertProtoEquals(
        """
        python_executor_spec: {
            class_path: "__main__._NestedExecutor"
            extra_flags: "a"
        }
        beam_pipeline_args: "b"
        """, spec_copy.encode())

</source>
</class>

<class classid="55" nclones="2" nlines="15" similarity="75">
<source file="systems/tfx-1.6.1/tfx/dsl/components/base/base_component_test.py" startline="90" endline="115" pcid="1004">
  def testComponentSpecClass(self):

    class MissingSpecComponent(base_component.BaseComponent):

      EXECUTOR_SPEC = executor_spec.ExecutorClassSpec(
          base_executor.BaseExecutor)

    with self.assertRaisesRegex(TypeError, "Can't instantiate abstract class"):
      MissingSpecComponent(spec=object())  # pytype: disable=wrong-arg-types

    with self.assertRaisesRegex(
        TypeError, "expects SPEC_CLASS property to be a subclass of "
        "types.ComponentSpec"):
      MissingSpecComponent._validate_component_class()

    class InvalidSpecComponent(base_component.BaseComponent):

      SPEC_CLASSES = object()
      EXECUTOR_SPEC = executor_spec.ExecutorClassSpec(
          base_executor.BaseExecutor)

    with self.assertRaisesRegex(
        TypeError, "expects SPEC_CLASS property to be a subclass of "
        "types.ComponentSpec"):
      InvalidSpecComponent._validate_component_class()

</source>
<source file="systems/tfx-1.6.1/tfx/dsl/components/base/base_component_test.py" startline="168" endline="191" pcid="1006">
  def testComponentExecutorClass(self):

    class MissingExecutorComponent(base_component.BaseComponent):

      SPEC_CLASS = _BasicComponentSpec

    with self.assertRaisesRegex(TypeError, "Can't instantiate abstract class"):
      MissingExecutorComponent(spec=object())  # pytype: disable=wrong-arg-types

    with self.assertRaisesRegex(
        TypeError, "expects EXECUTOR_SPEC property to be an instance of "
        "ExecutorSpec"):
      MissingExecutorComponent._validate_component_class()

    class InvalidExecutorComponent(base_component.BaseComponent):

      SPEC_CLASS = _BasicComponentSpec
      EXECUTOR_SPEC = object()

    with self.assertRaisesRegex(
        TypeError, "expects EXECUTOR_SPEC property to be an instance of "
        "ExecutorSpec"):
      InvalidExecutorComponent._validate_component_class()

</source>
</class>

<class classid="56" nclones="4" nlines="22" similarity="72">
<source file="systems/tfx-1.6.1/tfx/dsl/component/experimental/decorators_test.py" startline="179" endline="202" pcid="1046">
  def testBeamExecutionSuccess(self):
    """Test execution with return values; success case."""
    instance_1 = _injector_1(foo=9, bar='secret')
    instance_2 = _simple_component(
        a=instance_1.outputs['a'],
        b=instance_1.outputs['b'],
        c=instance_1.outputs['c'],
        d=instance_1.outputs['d'])
    instance_3 = _verify(
        e=instance_2.outputs['e'],
        f=instance_2.outputs['f'],
        g=instance_2.outputs['g'],
        h=instance_2.outputs['h'])  # pylint: disable=assignment-from-no-return

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    test_pipeline = pipeline.Pipeline(
        pipeline_name='test_pipeline_1',
        pipeline_root=self._test_dir,
        metadata_connection_config=metadata_config,
        components=[instance_1, instance_2, instance_3])

    beam_dag_runner.BeamDagRunner().run(test_pipeline)

</source>
<source file="systems/tfx-1.6.1/tfx/dsl/component/experimental/decorators_test.py" startline="203" endline="229" pcid="1047">
  def testBeamExecutionFailure(self):
    """Test execution with return values; failure case."""
    instance_1 = _injector_1(foo=9, bar='secret')
    instance_2 = _simple_component(
        a=instance_1.outputs['a'],
        b=instance_1.outputs['b'],
        c=instance_1.outputs['c'],
        d=instance_1.outputs['d'])
    # Swapped 'e' and 'f'.
    instance_3 = _verify(
        e=instance_2.outputs['f'],
        f=instance_2.outputs['e'],
        g=instance_2.outputs['g'],
        h=instance_2.outputs['h'])  # pylint: disable=assignment-from-no-return

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    test_pipeline = pipeline.Pipeline(
        pipeline_name='test_pipeline_1',
        pipeline_root=self._test_dir,
        metadata_connection_config=metadata_config,
        components=[instance_1, instance_2, instance_3])

    with self.assertRaisesRegex(
        RuntimeError, r'AssertionError: \(220.0, 32.0, \'OK\', None\)'):
      beam_dag_runner.BeamDagRunner().run(test_pipeline)

</source>
<source file="systems/tfx-1.6.1/tfx/dsl/component/experimental/decorators_test.py" startline="257" endline="285" pcid="1049">
  def testBeamExecutionNonNullableReturnError(self):
    """Test failure when None used for non-optional primitive return value."""
    instance_1 = _injector_3()  # pylint: disable=no-value-for-parameter
    self.assertEqual(1, len(instance_1.outputs['examples'].get()))
    instance_2 = _optionalarg_component(  # pylint: disable=assignment-from-no-return
        foo=9,
        bar='secret',
        examples=instance_1.outputs['examples'],
        a=instance_1.outputs['a'],
        b=instance_1.outputs['b'],
        c=instance_1.outputs['c'],
        d=instance_1.outputs['d'],
        e1=instance_1.outputs['e'],
        e2=instance_1.outputs['e'],
        g=999.0,
        optional_examples_1=instance_1.outputs['examples'])

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    test_pipeline = pipeline.Pipeline(
        pipeline_name='test_pipeline_1',
        pipeline_root=self._test_dir,
        metadata_connection_config=metadata_config,
        components=[instance_1, instance_2])
    with self.assertRaisesRegex(
        ValueError, 'Non-nullable output \'e\' received None return value'):
      beam_dag_runner.BeamDagRunner().run(test_pipeline)


</source>
<source file="systems/tfx-1.6.1/tfx/dsl/component/experimental/decorators_test.py" startline="230" endline="256" pcid="1048">
  def testBeamExecutionOptionalInputsAndParameters(self):
    """Test execution with optional inputs and parameters."""
    instance_1 = _injector_2()  # pylint: disable=no-value-for-parameter
    self.assertEqual(1, len(instance_1.outputs['examples'].get()))
    instance_2 = _optionalarg_component(  # pylint: disable=assignment-from-no-return
        foo=9,
        bar='secret',
        examples=instance_1.outputs['examples'],
        a=instance_1.outputs['a'],
        b=instance_1.outputs['b'],
        c=instance_1.outputs['c'],
        d=instance_1.outputs['d'],
        e1=instance_1.outputs['e'],
        e2=instance_1.outputs['e'],
        g=999.0,
        optional_examples_1=instance_1.outputs['examples'])

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    test_pipeline = pipeline.Pipeline(
        pipeline_name='test_pipeline_1',
        pipeline_root=self._test_dir,
        metadata_connection_config=metadata_config,
        components=[instance_1, instance_2])

    beam_dag_runner.BeamDagRunner().run(test_pipeline)

</source>
</class>

<class classid="57" nclones="2" nlines="16" similarity="81">
<source file="systems/tfx-1.6.1/tfx/dsl/input_resolution/strategies/conditional_strategy_test.py" startline="105" endline="123" pcid="1110">
  def testStrategy_IrMode_PredicateTrue(self):
    artifact_1 = standard_artifacts.Integer()
    artifact_1.uri = self.create_tempfile().full_path
    artifact_1.value = 0
    artifact_2 = standard_artifacts.Integer()
    artifact_2.uri = self.create_tempfile().full_path
    artifact_2.value = 1

    strategy = conditional_strategy.ConditionalStrategy([
        text_format.Parse(_TEST_PREDICATE_1,
                          placeholder_pb2.PlaceholderExpression()),
        text_format.Parse(_TEST_PREDICATE_2,
                          placeholder_pb2.PlaceholderExpression())
    ])
    input_dict = {'channel_1_key': [artifact_1], 'channel_2_key': [artifact_2]}
    result = strategy.resolve_artifacts(self._store, input_dict)
    self.assertIsNotNone(result)
    self.assertEqual(result, input_dict)

</source>
<source file="systems/tfx-1.6.1/tfx/dsl/input_resolution/strategies/conditional_strategy_test.py" startline="124" endline="141" pcid="1111">
  def testStrategy_IrMode_PredicateFalse(self):
    artifact_1 = standard_artifacts.Integer()
    artifact_1.uri = self.create_tempfile().full_path
    artifact_1.value = 0
    artifact_2 = standard_artifacts.Integer()
    artifact_2.uri = self.create_tempfile().full_path
    artifact_2.value = 42

    strategy = conditional_strategy.ConditionalStrategy([
        text_format.Parse(_TEST_PREDICATE_1,
                          placeholder_pb2.PlaceholderExpression()),
        text_format.Parse(_TEST_PREDICATE_2,
                          placeholder_pb2.PlaceholderExpression())
    ])
    input_dict = {'channel_1_key': [artifact_1], 'channel_2_key': [artifact_2]}
    with self.assertRaises(exceptions.SkipSignal):
      strategy.resolve_artifacts(self._store, input_dict)

</source>
</class>

<class classid="58" nclones="2" nlines="84" similarity="95">
<source file="systems/tfx-1.6.1/tfx/dsl/compiler/testdata/iris_pipeline_async.py" startline="38" endline="141" pcid="1138">
def create_test_pipeline():
  """Builds an Iris example pipeline with slight changes."""
  pipeline_name = "iris"
  iris_root = "iris_root"
  serving_model_dir = os.path.join(iris_root, "serving_model", pipeline_name)
  tfx_root = "tfx_root"
  data_path = os.path.join(tfx_root, "data_path")
  pipeline_root = os.path.join(tfx_root, "pipelines", pipeline_name)

  example_gen = CsvExampleGen(input_base=data_path)

  statistics_gen = StatisticsGen(examples=example_gen.outputs["examples"])

  importer = ImporterNode(
      source_uri="m/y/u/r/i",
      properties={
          "split_names": "['train', 'eval']",
      },
      custom_properties={
          "int_custom_property": 42,
          "str_custom_property": "42",
      },
      artifact_type=standard_artifacts.Examples).with_id("my_importer")

  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs["statistics"], infer_feature_shape=True)

  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs["statistics"],
      schema=schema_gen.outputs["schema"])

  trainer = Trainer(
      # Use RuntimeParameter as module_file to test out RuntimeParameter in
      # compiler.
      module_file=data_types.RuntimeParameter(
          name="module_file",
          default=os.path.join(iris_root, "iris_utils.py"),
          ptype=str),
      custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),
      examples=example_gen.outputs["examples"],
      schema=schema_gen.outputs["schema"],
      train_args=trainer_pb2.TrainArgs(num_steps=2000),
      # Attaching `TrainerArgs` as platform config is not sensible practice,
      # but is only for testing purpose.
      eval_args=trainer_pb2.EvalArgs(num_steps=5)).with_platform_config(
          config=trainer_pb2.TrainArgs(num_steps=2000))

  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_strategy.LatestBlessedModelStrategy,
      baseline_model=Channel(
          type=standard_artifacts.Model, producer_component_id="Trainer"),
      # Cannot add producer_component_id="Evaluator" for model_blessing as it
      # raises "producer component should have already been compiled" error.
      model_blessing=Channel(type=standard_artifacts.ModelBlessing)).with_id(
          "latest_blessed_model_resolver")

  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(signature_name="eval")],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(
              thresholds={
                  "sparse_categorical_accuracy":
                      tfma.MetricThreshold(
                          value_threshold=tfma.GenericValueThreshold(
                              lower_bound={"value": 0.6}),
                          change_threshold=tfma.GenericChangeThreshold(
                              direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                              absolute={"value": -1e-10}))
              })
      ])
  evaluator = Evaluator(
      examples=example_gen.outputs["examples"],
      model=trainer.outputs["model"],
      baseline_model=model_resolver.outputs["baseline_model"],
      eval_config=eval_config)

  pusher = Pusher(
      model=trainer.outputs["model"],
      model_blessing=evaluator.outputs["blessing"],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen,
          statistics_gen,
          importer,
          schema_gen,
          example_validator,
          trainer,
          model_resolver,
          evaluator,
          pusher,
      ],
      enable_cache=False,
      beam_pipeline_args=["--my_testing_beam_pipeline_args=bar"],
      # Attaching `TrainerArgs` as platform config is not sensible practice,
      # but is only for testing purpose.
      platform_config=trainer_pb2.TrainArgs(num_steps=2000),
      execution_mode=pipeline.ExecutionMode.ASYNC)
</source>
<source file="systems/tfx-1.6.1/tfx/dsl/compiler/testdata/iris_pipeline_sync.py" startline="38" endline="141" pcid="1139">
def create_test_pipeline():
  """Builds an Iris example pipeline with slight changes."""
  pipeline_name = "iris"
  iris_root = "iris_root"
  serving_model_dir = os.path.join(iris_root, "serving_model", pipeline_name)
  tfx_root = "tfx_root"
  data_path = os.path.join(tfx_root, "data_path")
  pipeline_root = os.path.join(tfx_root, "pipelines", pipeline_name)

  example_gen = CsvExampleGen(input_base=data_path)

  statistics_gen = StatisticsGen(examples=example_gen.outputs["examples"])

  importer = ImporterNode(
      source_uri="m/y/u/r/i",
      properties={
          "split_names": "['train', 'eval']",
      },
      custom_properties={
          "int_custom_property": 42,
          "str_custom_property": "42",
      },
      artifact_type=standard_artifacts.Examples).with_id("my_importer")
  another_statistics_gen = StatisticsGen(
      examples=importer.outputs["result"]).with_id("another_statistics_gen")

  schema_gen = SchemaGen(statistics=statistics_gen.outputs["statistics"])

  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs["statistics"],
      schema=schema_gen.outputs["schema"])

  trainer = Trainer(
      # Use RuntimeParameter as module_file to test out RuntimeParameter in
      # compiler.
      module_file=data_types.RuntimeParameter(
          name="module_file",
          default=os.path.join(iris_root, "iris_utils.py"),
          ptype=str),
      custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),
      examples=example_gen.outputs["examples"],
      schema=schema_gen.outputs["schema"],
      train_args=trainer_pb2.TrainArgs(num_steps=2000),
      # Attaching `TrainerArgs` as platform config is not sensible practice,
      # but is only for testing purpose.
      eval_args=trainer_pb2.EvalArgs(num_steps=5)).with_platform_config(
          config=trainer_pb2.TrainArgs(num_steps=2000))

  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_strategy.LatestBlessedModelStrategy,
      model=Channel(
          type=standard_artifacts.Model, producer_component_id=trainer.id),
      model_blessing=Channel(type=standard_artifacts.ModelBlessing)).with_id(
          "latest_blessed_model_resolver")

  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(signature_name="eval")],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(
              thresholds={
                  "sparse_categorical_accuracy":
                      tfma.MetricThreshold(
                          value_threshold=tfma.GenericValueThreshold(
                              lower_bound={"value": 0.6}),
                          change_threshold=tfma.GenericChangeThreshold(
                              direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                              absolute={"value": -1e-10}))
              })
      ])
  evaluator = Evaluator(
      examples=example_gen.outputs["examples"],
      model=trainer.outputs["model"],
      baseline_model=model_resolver.outputs["model"],
      eval_config=eval_config)

  pusher = Pusher(
      model=trainer.outputs["model"],
      model_blessing=evaluator.outputs["blessing"],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen,
          statistics_gen,
          another_statistics_gen,
          importer,
          schema_gen,
          example_validator,
          trainer,
          model_resolver,
          evaluator,
          pusher,
      ],
      enable_cache=True,
      beam_pipeline_args=["--my_testing_beam_pipeline_args=foo"],
      # Attaching `TrainerArgs` as platform config is not sensible practice,
      # but is only for testing purpose.
      platform_config=trainer_pb2.TrainArgs(num_steps=2000),
      execution_mode=pipeline.ExecutionMode.SYNC)
</source>
</class>

<class classid="59" nclones="2" nlines="10" similarity="90">
<source file="systems/tfx-1.6.1/tfx/dsl/compiler/placeholder_utils_test.py" startline="257" endline="287" pcid="1142">
  def testArtifactProperty(self):
    placeholder_expression = """
      operator {
        artifact_property_op {
          expression {
            operator {
              index_op{
                expression {
                  placeholder {
                    type: INPUT_ARTIFACT
                    key: "examples"
                  }
                }
                index: 0
              }
            }
          }
          key: "version"
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())
    self.assertEqual(
        placeholder_utils.resolve_placeholder_expression(
            pb, self._resolution_context), 42)

    self.assertEqual(
        placeholder_utils.debug_str(pb),
        "input(\"examples\")[0].property(\"version\")")

</source>
<source file="systems/tfx-1.6.1/tfx/dsl/compiler/placeholder_utils_test.py" startline="288" endline="319" pcid="1143">
  def testArtifactCustomProperty(self):
    placeholder_expression = """
      operator {
        artifact_property_op {
          expression {
            operator {
              index_op{
                expression {
                  placeholder {
                    type: INPUT_ARTIFACT
                    key: "examples"
                  }
                }
                index: 0
              }
            }
          }
          key: "custom_key"
          is_custom_property: True
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())
    self.assertEqual(
        placeholder_utils.resolve_placeholder_expression(
            pb, self._resolution_context), "custom_value")

    self.assertEqual(
        placeholder_utils.debug_str(pb),
        "input(\"examples\")[0].custom_property(\"custom_key\")")

</source>
</class>

<class classid="60" nclones="8" nlines="10" similarity="70">
<source file="systems/tfx-1.6.1/tfx/dsl/compiler/placeholder_utils_test.py" startline="385" endline="423" pcid="1146">
  def testProtoExecPropertyPrimitiveField(self):
    # Access a non-message type proto field
    placeholder_expression = """
      operator {
        index_op {
          expression {
            operator {
              proto_op {
                expression {
                  placeholder {
                    type: EXEC_PROPERTY
                    key: "proto_property"
                  }
                }
                proto_schema {
                  message_type: "tfx.components.infra_validator.ServingSpec"
                }
                proto_field_path: ".tensorflow_serving"
                proto_field_path: ".tags"
              }
            }
          }
          index: 1
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())

    # Prepare FileDescriptorSet
    fd = descriptor_pb2.FileDescriptorProto()
    infra_validator_pb2.ServingSpec().DESCRIPTOR.file.CopyToProto(fd)
    pb.operator.index_op.expression.operator.proto_op.proto_schema.file_descriptors.file.append(
        fd)

    self.assertEqual(
        placeholder_utils.resolve_placeholder_expression(
            pb, self._resolution_context), "1.15.0-gpu")

</source>
<source file="systems/tfx-1.6.1/tfx/dsl/compiler/placeholder_utils_test.py" startline="525" endline="557" pcid="1150">
  def testProtoExecPropertyMessageFieldTextFormat(self):
    # Access a message type proto field
    placeholder_expression = """
      operator {
        proto_op {
          expression {
            placeholder {
              type: EXEC_PROPERTY
              key: "proto_property"
            }
          }
          proto_schema {
            message_type: "tfx.components.infra_validator.ServingSpec"
          }
          proto_field_path: ".tensorflow_serving"
          serialization_format: TEXT_FORMAT
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())

    fd = descriptor_pb2.FileDescriptorProto()
    infra_validator_pb2.ServingSpec().DESCRIPTOR.file.CopyToProto(fd)
    pb.operator.proto_op.proto_schema.file_descriptors.file.append(fd)

    # If proto_field_path points to a message type field, the message will
    # be rendered using text_format.
    self.assertEqual(
        placeholder_utils.resolve_placeholder_expression(
            pb, self._resolution_context),
        "tags: \"latest\"\ntags: \"1.15.0-gpu\"\n")

</source>
<source file="systems/tfx-1.6.1/tfx/dsl/compiler/placeholder_utils_test.py" startline="683" endline="713" pcid="1156">
  def testProtoRuntimeInfoNoneAccess(self):
    # Access a missing platform config.
    placeholder_expression = """
      operator {
        proto_op {
          expression {
            placeholder {
              type: RUNTIME_INFO
              key: "platform_config"
            }
          }
          proto_schema {
            message_type: "tfx.components.infra_validator.ServingSpec"
          }
          proto_field_path: ".tensorflow_serving"
          proto_field_path: ".tags"
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())

    # Prepare FileDescriptorSet
    fd = descriptor_pb2.FileDescriptorProto()
    infra_validator_pb2.ServingSpec().DESCRIPTOR.file.CopyToProto(fd)
    pb.operator.proto_op.proto_schema.file_descriptors.file.append(fd)

    self.assertIsNone(
        placeholder_utils.resolve_placeholder_expression(
            pb, self._none_resolution_context))

</source>
<source file="systems/tfx-1.6.1/tfx/dsl/compiler/placeholder_utils_test.py" startline="714" endline="752" pcid="1157">
  def testProtoSerializationJSON(self):
    placeholder_expression = """
      operator {
        proto_op {
          expression {
            placeholder {
              type: EXEC_PROPERTY
              key: "proto_property"
            }
          }
          proto_schema {
            message_type: "tfx.components.infra_validator.ServingSpec"
          }
          serialization_format: JSON
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())

    # Prepare FileDescriptorSet
    fd = descriptor_pb2.FileDescriptorProto()
    infra_validator_pb2.ServingSpec().DESCRIPTOR.file.CopyToProto(fd)
    pb.operator.proto_op.proto_schema.file_descriptors.file.append(fd)

    expected_json_serialization = """\
{
  "tensorflow_serving": {
    "tags": [
      "latest",
      "1.15.0-gpu"
    ]
  }
}"""

    self.assertEqual(
        placeholder_utils.resolve_placeholder_expression(
            pb, self._resolution_context), expected_json_serialization)

</source>
<source file="systems/tfx-1.6.1/tfx/dsl/compiler/placeholder_utils_test.py" startline="558" endline="588" pcid="1151">
  def testProtoExecPropertyRepeatedField(self):
    # Access a repeated field.
    placeholder_expression = """
      operator {
        proto_op {
          expression {
            placeholder {
              type: EXEC_PROPERTY
              key: "proto_property"
            }
          }
          proto_schema {
            message_type: "tfx.components.infra_validator.ServingSpec"
          }
          proto_field_path: ".tensorflow_serving"
          proto_field_path: ".tags"
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())

    # Prepare FileDescriptorSet
    fd = descriptor_pb2.FileDescriptorProto()
    infra_validator_pb2.ServingSpec().DESCRIPTOR.file.CopyToProto(fd)
    pb.operator.proto_op.proto_schema.file_descriptors.file.append(fd)

    self.assertEqual(
        placeholder_utils.resolve_placeholder_expression(
            pb, self._resolution_context), ["latest", "1.15.0-gpu"])

</source>
<source file="systems/tfx-1.6.1/tfx/dsl/compiler/placeholder_utils_test.py" startline="619" endline="649" pcid="1153">
  def testProtoExecPropertyNoneAccess(self):
    # Access a missing optional exec property.
    placeholder_expression = """
      operator {
        proto_op {
          expression {
            placeholder {
              type: EXEC_PROPERTY
              key: "proto_property"
            }
          }
          proto_schema {
            message_type: "tfx.components.infra_validator.ServingSpec"
          }
          proto_field_path: ".tensorflow_serving"
          proto_field_path: ".tags"
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())

    # Prepare FileDescriptorSet
    fd = descriptor_pb2.FileDescriptorProto()
    infra_validator_pb2.ServingSpec().DESCRIPTOR.file.CopyToProto(fd)
    pb.operator.proto_op.proto_schema.file_descriptors.file.append(fd)

    self.assertIsNone(
        placeholder_utils.resolve_placeholder_expression(
            pb, self._none_resolution_context))

</source>
<source file="systems/tfx-1.6.1/tfx/dsl/compiler/placeholder_utils_test.py" startline="589" endline="618" pcid="1152">
  def testProtoExecPropertyInvalidField(self):
    # Access a repeated field.
    placeholder_expression = """
      operator {
        proto_op {
          expression {
            placeholder {
              type: EXEC_PROPERTY
              key: "proto_property"
            }
          }
          proto_schema {
            message_type: "tfx.components.infra_validator.ServingSpec"
          }
          proto_field_path: ".some_invalid_field"
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())

    # Prepare FileDescriptorSet
    fd = descriptor_pb2.FileDescriptorProto()
    infra_validator_pb2.ServingSpec().DESCRIPTOR.file.CopyToProto(fd)
    pb.operator.proto_op.proto_schema.file_descriptors.file.append(fd)

    with self.assertRaises(ValueError):
      placeholder_utils.resolve_placeholder_expression(pb,
                                                       self._resolution_context)

</source>
<source file="systems/tfx-1.6.1/tfx/dsl/compiler/placeholder_utils_test.py" startline="753" endline="780" pcid="1158">
  def testProtoWithoutSerializationFormat(self):
    placeholder_expression = """
      operator {
        proto_op {
          expression {
            placeholder {
              type: EXEC_PROPERTY
              key: "proto_property"
            }
          }
          proto_schema {
            message_type: "tfx.components.infra_validator.ServingSpec"
          }
        }
      }
    """
    pb = text_format.Parse(placeholder_expression,
                           placeholder_pb2.PlaceholderExpression())

    # Prepare FileDescriptorSet
    fd = descriptor_pb2.FileDescriptorProto()
    infra_validator_pb2.ServingSpec().DESCRIPTOR.file.CopyToProto(fd)
    pb.operator.proto_op.proto_schema.file_descriptors.file.append(fd)

    with self.assertRaises(ValueError):
      placeholder_utils.resolve_placeholder_expression(pb,
                                                       self._resolution_context)

</source>
</class>

<class classid="61" nclones="2" nlines="12" similarity="91">
<source file="systems/tfx-1.6.1/tfx/dsl/context_managers/context_manager_test.py" startline="102" endline="115" pcid="1184">
  def testRegistry_AllContexts(self):
    registry = self.reset_registry()
    bg = registry.background_context

    self.assertEqual(registry.all_contexts, [bg])
    with _FakeContextManager() as c1:
      self.assertEqual(registry.all_contexts, [bg, c1])
      with _FakeContextManager() as c2:
        self.assertEqual(registry.all_contexts, [bg, c1, c2])
        with _FakeContextManager() as c3:
          self.assertEqual(registry.all_contexts, [bg, c1, c2, c3])
      with _FakeContextManager() as c4:
        self.assertEqual(registry.all_contexts, [bg, c1, c2, c3, c4])

</source>
<source file="systems/tfx-1.6.1/tfx/dsl/context_managers/context_manager_test.py" startline="116" endline="129" pcid="1185">
  def testRegistry_ActiveContexts(self):
    registry = self.reset_registry()
    bg = registry.background_context

    self.assertEqual(registry.active_contexts, [bg])
    with _FakeContextManager() as c1:
      self.assertEqual(registry.active_contexts, [bg, c1])
      with _FakeContextManager() as c2:
        self.assertEqual(registry.active_contexts, [bg, c1, c2])
        with _FakeContextManager() as c3:
          self.assertEqual(registry.active_contexts, [bg, c1, c2, c3])
      with _FakeContextManager() as c4:
        self.assertEqual(registry.active_contexts, [bg, c1, c4])

</source>
</class>

<class classid="62" nclones="2" nlines="22" similarity="86">
<source file="systems/tfx-1.6.1/tfx/dsl/io/plugins/local_test.py" startline="26" endline="62" pcid="1232">
  def testNotFound(self):
    temp_dir = tempfile.mkdtemp()

    with self.assertRaises(NotFoundError):
      LocalFilesystem.open(os.path.join(temp_dir, 'foo')).read()

    with self.assertRaises(NotFoundError):
      LocalFilesystem.copy(
          os.path.join(temp_dir, 'foo'), os.path.join(temp_dir, 'bar'))

    # No exception raised.
    self.assertEqual(LocalFilesystem.glob(os.path.join(temp_dir, 'foo/*')), [])

    # No exception raised.
    self.assertEqual(
        LocalFilesystem.isdir(os.path.join(temp_dir, 'foo/bar')), False)

    with self.assertRaises(NotFoundError):
      LocalFilesystem.listdir(os.path.join(temp_dir, 'foo'))

    with self.assertRaises(NotFoundError):
      LocalFilesystem.mkdir(os.path.join(temp_dir, 'foo/bar'))

    with self.assertRaises(NotFoundError):
      LocalFilesystem.remove(os.path.join(temp_dir, 'foo'))

    with self.assertRaises(NotFoundError):
      LocalFilesystem.rmtree(os.path.join(temp_dir, 'foo'))

    with self.assertRaises(NotFoundError):
      LocalFilesystem.stat(os.path.join(temp_dir, 'foo'))

    # No exception raised.
    self.assertEqual(
        list(LocalFilesystem.walk(os.path.join(temp_dir, 'foo'))), [])


</source>
<source file="systems/tfx-1.6.1/tfx/dsl/io/plugins/tensorflow_gfile_test.py" startline="27" endline="65" pcid="1233">
  def testNotFound(self):
    temp_dir = tempfile.mkdtemp()

    # Because the GFile implementation delays I/O until necessary, we cannot
    # catch `NotFoundError` here, so this does not raise an error.
    TensorflowFilesystem.open(os.path.join(temp_dir, 'foo'))

    with self.assertRaises(NotFoundError):
      TensorflowFilesystem.copy(
          os.path.join(temp_dir, 'foo'), os.path.join(temp_dir, 'bar'))

    # No exception raised.
    self.assertEqual(
        TensorflowFilesystem.glob(os.path.join(temp_dir, 'foo/*')), [])

    # No exception raised.
    self.assertEqual(
        TensorflowFilesystem.isdir(os.path.join(temp_dir, 'foo/bar')), False)

    with self.assertRaises(NotFoundError):
      TensorflowFilesystem.listdir(os.path.join(temp_dir, 'foo'))

    with self.assertRaises(NotFoundError):
      TensorflowFilesystem.mkdir(os.path.join(temp_dir, 'foo/bar'))

    with self.assertRaises(NotFoundError):
      TensorflowFilesystem.remove(os.path.join(temp_dir, 'foo'))

    with self.assertRaises(NotFoundError):
      TensorflowFilesystem.rmtree(os.path.join(temp_dir, 'foo'))

    with self.assertRaises(NotFoundError):
      TensorflowFilesystem.stat(os.path.join(temp_dir, 'foo'))

    # No exception raised.
    self.assertEqual(
        list(TensorflowFilesystem.walk(os.path.join(temp_dir, 'foo'))), [])


</source>
</class>

<class classid="63" nclones="2" nlines="14" similarity="100">
<source file="systems/tfx-1.6.1/tfx/orchestration/kubeflow/v2/kubeflow_v2_dag_runner_test.py" startline="67" endline="82" pcid="1301">
  def testCompileTwoStepPipeline(self, fake_now, fake_sys_version):
    fake_now.return_value = datetime.date(2020, 1, 1)
    fake_sys_version.major = 3
    fake_sys_version.minor = 7
    runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(
        output_dir=_TEST_DIR,
        output_filename=_TEST_FILE_NAME,
        config=kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(
            display_name='my-pipeline',
            default_image='gcr.io/my-tfx:latest'))

    self._compare_against_testdata(
        runner=runner,
        pipeline=test_utils.two_step_pipeline(),
        golden_file='expected_two_step_pipeline_job.json')

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/kubeflow/v2/kubeflow_v2_dag_runner_test.py" startline="86" endline="102" pcid="1302">
  def testCompileFullTaxiPipeline(self, fake_now, fake_sys_version):
    fake_now.return_value = datetime.date(2020, 1, 1)
    fake_sys_version.major = 3
    fake_sys_version.minor = 7
    runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(
        output_dir=_TEST_DIR,
        output_filename=_TEST_FILE_NAME,
        config=kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(
            display_name='my-pipeline',
            default_image='tensorflow/tfx:latest'))

    self._compare_against_testdata(
        runner=runner,
        pipeline=test_utils.full_taxi_pipeline(),
        golden_file='expected_full_taxi_pipeline_job.json')


</source>
</class>

<class classid="64" nclones="2" nlines="16" similarity="76">
<source file="systems/tfx-1.6.1/tfx/orchestration/kubeflow/v2/e2e_tests/bigquery_integration_test.py" startline="51" endline="76" pcid="1315">
  def testSimpleEnd2EndPipeline(self):
    """End-to-End test for a simple pipeline."""
    pipeline_name = 'kubeflow-v2-bqeg-test-{}'.format(test_utils.random_id())

    components = kubeflow_v2_test_utils.create_pipeline_components(
        pipeline_root=self._pipeline_root(pipeline_name),
        transform_module=self._MODULE_FILE,
        trainer_module=self._MODULE_FILE,
        bigquery_query=_BIGQUERY_QUERY)

    beam_pipeline_args = [
        '--temp_location=' +
        os.path.join(self._pipeline_root(pipeline_name), 'dataflow', 'temp'),
        '--project={}'.format(self._GCP_PROJECT_ID),
        # TODO(b/171733562): Remove `use_runner_v2` once it is the default for
        # Dataflow.
        '--experiments=use_runner_v2',
        '--worker_harness_container_image=%s' % self.container_image,
    ]

    pipeline = self._create_pipeline(pipeline_name, components,
                                     beam_pipeline_args)

    self._run_pipeline(pipeline)


</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/kubeflow/v2/e2e_tests/csv_example_gen_integration_test.py" startline="31" endline="52" pcid="1317">
  def testSimpleEnd2EndPipeline(self):
    """End-to-End test for a simple pipeline."""
    pipeline_name = 'kubeflow-v2-fbeg-test-{}'.format(test_utils.random_id())

    components = kubeflow_v2_test_utils.create_pipeline_components(
        pipeline_root=self._pipeline_root(pipeline_name),
        transform_module=self._MODULE_FILE,
        trainer_module=self._MODULE_FILE,
        csv_input_location=_TEST_DATA_ROOT)

    beam_pipeline_args = [
        '--temp_location=' +
        os.path.join(self._pipeline_root(pipeline_name), 'dataflow', 'temp'),
        '--project={}'.format(self._GCP_PROJECT_ID)
    ]

    pipeline = self._create_pipeline(pipeline_name, components,
                                     beam_pipeline_args)

    self._run_pipeline(pipeline)


</source>
</class>

<class classid="65" nclones="2" nlines="15" similarity="93">
<source file="systems/tfx-1.6.1/tfx/orchestration/kubeflow/container_entrypoint.py" startline="260" endline="289" pcid="1391">
  def _dump_input_populated_artifacts(
      node_inputs: MutableMapping[str, pipeline_pb2.InputSpec],
      name_to_artifacts: Dict[str, List[artifact.Artifact]]) -> List[str]:
    """Dump artifacts markdown string for inputs.

    Args:
      node_inputs: maps from input name to input sepc proto.
      name_to_artifacts: maps from input key to list of populated artifacts.

    Returns:
      A list of dumped markdown string, each of which represents a channel.
    """
    rendered_list = []
    for name, spec in node_inputs.items():
      # Need to look for materialized artifacts in the execution decision.
      rendered_artifacts = ''.join([
          _render_artifact_as_mdstr(single_artifact)
          for single_artifact in name_to_artifacts.get(name, [])
      ])
      # There must be at least a channel in a input, and all channels in a input
      # share the same artifact type.
      artifact_type = spec.channels[0].artifact_query.type.name
      rendered_list.append(
          '## {name}\n\n**Type**: {channel_type}\n\n{artifacts}'.format(
              name=_sanitize_underscore(name),
              channel_type=_sanitize_underscore(artifact_type),
              artifacts=rendered_artifacts))

    return rendered_list

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/kubeflow/container_entrypoint.py" startline="290" endline="319" pcid="1392">
  def _dump_output_populated_artifacts(
      node_outputs: MutableMapping[str, pipeline_pb2.OutputSpec],
      name_to_artifacts: Dict[str, List[artifact.Artifact]]) -> List[str]:
    """Dump artifacts markdown string for outputs.

    Args:
      node_outputs: maps from output name to output sepc proto.
      name_to_artifacts: maps from output key to list of populated artifacts.

    Returns:
      A list of dumped markdown string, each of which represents a channel.
    """
    rendered_list = []
    for name, spec in node_outputs.items():
      # Need to look for materialized artifacts in the execution decision.
      rendered_artifacts = ''.join([
          _render_artifact_as_mdstr(single_artifact)
          for single_artifact in name_to_artifacts.get(name, [])
      ])
      # There must be at least a channel in a input, and all channels in a input
      # share the same artifact type.
      artifact_type = spec.artifact_spec.type.name
      rendered_list.append(
          '## {name}\n\n**Type**: {channel_type}\n\n{artifacts}'.format(
              name=_sanitize_underscore(name),
              channel_type=_sanitize_underscore(artifact_type),
              artifacts=rendered_artifacts))

    return rendered_list

</source>
</class>

<class classid="66" nclones="2" nlines="11" similarity="75">
<source file="systems/tfx-1.6.1/tfx/orchestration/kubeflow/e2e_tests/kubeflow_dataflow_integration_test.py" startline="77" endline="89" pcid="1400">
  def testTransformOnDataflowRunner(self):
    """Transform-only test pipeline on DataflowRunner."""
    pipeline_name = 'kubeflow-transform-dataflow-test-{}'.format(
        test_utils.random_id())
    pipeline = self._create_dataflow_pipeline(pipeline_name, [
        self.raw_examples_importer, self.schema_importer,
        Transform(
            examples=self.raw_examples_importer.outputs['result'],
            schema=self.schema_importer.outputs['result'],
            module_file=self._transform_module)
    ])
    self._compile_and_run_pipeline(pipeline)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/kubeflow/e2e_tests/kubeflow_dataflow_integration_test.py" startline="90" endline="106" pcid="1401">
  def testEvaluatorOnDataflowRunner(self):
    """Evaluator-only test pipeline on DataflowRunner."""
    pipeline_name = 'kubeflow-evaluator-dataflow-test-{}'.format(
        test_utils.random_id())
    pipeline = self._create_dataflow_pipeline(pipeline_name, [
        self.raw_examples_importer, self.model_1_importer,
        Evaluator(
            examples=self.raw_examples_importer.outputs['result'],
            model=self.model_1_importer.outputs['result'],
            feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[
                evaluator_pb2.SingleSlicingSpec(
                    column_for_slicing=['trip_start_hour'])
            ]))
    ])
    self._compile_and_run_pipeline(pipeline)


</source>
</class>

<class classid="67" nclones="2" nlines="21" similarity="90">
<source file="systems/tfx-1.6.1/tfx/orchestration/kubeflow/e2e_tests/kubeflow_gcp_integration_test.py" startline="177" endline="200" pcid="1420">
  def testAIPlatformTrainerPipeline(self):
    """Trainer-only test pipeline on AI Platform Training."""
    pipeline_name = self._make_unique_pipeline_name('kubeflow-aip-trainer')
    pipeline = self._create_pipeline(pipeline_name, [
        self.schema_importer, self.transformed_examples_importer,
        self.transform_graph_importer,
        Trainer(
            custom_executor_spec=executor_spec.ExecutorClassSpec(
                ai_platform_trainer_executor.Executor),
            module_file=self._trainer_module,
            transformed_examples=self.transformed_examples_importer
            .outputs['result'],
            schema=self.schema_importer.outputs['result'],
            transform_graph=self.transform_graph_importer.outputs['result'],
            train_args=trainer_pb2.TrainArgs(num_steps=10),
            eval_args=trainer_pb2.EvalArgs(num_steps=5),
            custom_config={
                ai_platform_trainer_executor.TRAINING_ARGS_KEY:
                    self._getCaipTrainingArgsForDistributed(pipeline_name)
            })
    ])
    self._compile_and_run_pipeline(pipeline)
    self._assertNumberOfTrainerOutputIsOne(pipeline_name)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/kubeflow/e2e_tests/kubeflow_gcp_integration_test.py" startline="201" endline="229" pcid="1421">
  def testAIPlatformGenericTrainerPipeline(self):
    """Trainer-only pipeline on AI Platform Training with GenericTrainer."""
    pipeline_name = self._make_unique_pipeline_name(
        'kubeflow-aip-generic-trainer')
    pipeline = self._create_pipeline(pipeline_name, [
        self.schema_importer, self.transformed_examples_importer,
        self.transform_graph_importer,
        Trainer(
            custom_executor_spec=executor_spec.ExecutorClassSpec(
                ai_platform_trainer_executor.GenericExecutor),
            module_file=self._trainer_module,
            transformed_examples=self.transformed_examples_importer
            .outputs['result'],
            schema=self.schema_importer.outputs['result'],
            transform_graph=self.transform_graph_importer.outputs['result'],
            train_args=trainer_pb2.TrainArgs(num_steps=10),
            eval_args=trainer_pb2.EvalArgs(num_steps=5),
            custom_config={
                ai_platform_trainer_executor.TRAINING_ARGS_KEY:
                    self._getCaipTrainingArgs(pipeline_name)
            })
    ])
    self._compile_and_run_pipeline(pipeline)
    self._assertNumberOfTrainerOutputIsOne(pipeline_name)

  # TODO(b/150661783): Add tests using distributed training with a generic
  #  trainer.
  # TODO(b/150576271): Add Trainer tests using Keras models.

</source>
</class>

<class classid="68" nclones="2" nlines="22" similarity="70">
<source file="systems/tfx-1.6.1/tfx/orchestration/kubeflow/e2e_tests/kubeflow_gcp_integration_test.py" startline="243" endline="271" pcid="1423">
  def testVertexDistributedTunerPipeline(self):
    """Tuner-only pipeline for distributed Tuner flock on Vertex AI Training."""
    pipeline_name = self._make_unique_pipeline_name(
        'kubeflow-vertex-dist-tuner')
    pipeline = self._create_pipeline(
        pipeline_name,
        [
            self.penguin_examples_importer,
            self.penguin_schema_importer,
            ai_platform_tuner_component.Tuner(
                examples=self.penguin_examples_importer.outputs['result'],
                module_file=self._penguin_tuner_module,
                schema=self.penguin_schema_importer.outputs['result'],
                train_args=trainer_pb2.TrainArgs(num_steps=10),
                eval_args=trainer_pb2.EvalArgs(num_steps=5),
                # 3 worker parallel tuning.
                tune_args=tuner_pb2.TuneArgs(num_parallel_trials=3),
                custom_config={
                    ai_platform_tuner_executor.TUNING_ARGS_KEY:
                        self._getVertexTrainingArgs(pipeline_name),
                    constants.ENABLE_VERTEX_KEY:
                        True,
                    constants.VERTEX_REGION_KEY:
                        self._GCP_REGION
                })
        ])
    self._compile_and_run_pipeline(pipeline)
    self._assertHyperparametersAreWritten(pipeline_name)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/kubeflow/e2e_tests/kubeflow_gcp_integration_test.py" startline="272" endline="295" pcid="1424">
  def testAIPlatformDistributedTunerPipeline(self):
    """Tuner-only pipeline for distributed Tuner flock on AIP Training."""
    pipeline_name = self._make_unique_pipeline_name('kubeflow-aip-dist-tuner')
    pipeline = self._create_pipeline(
        pipeline_name,
        [
            self.penguin_examples_importer,
            self.penguin_schema_importer,
            ai_platform_tuner_component.Tuner(
                examples=self.penguin_examples_importer.outputs['result'],
                module_file=self._penguin_tuner_module,
                schema=self.penguin_schema_importer.outputs['result'],
                train_args=trainer_pb2.TrainArgs(num_steps=10),
                eval_args=trainer_pb2.EvalArgs(num_steps=5),
                # 3 worker parallel tuning.
                tune_args=tuner_pb2.TuneArgs(num_parallel_trials=3),
                custom_config={
                    ai_platform_tuner_executor.TUNING_ARGS_KEY:
                        self._getCaipTrainingArgs(pipeline_name)
                })
        ])
    self._compile_and_run_pipeline(pipeline)
    self._assertHyperparametersAreWritten(pipeline_name)

</source>
</class>

<class classid="69" nclones="2" nlines="13" similarity="76">
<source file="systems/tfx-1.6.1/tfx/orchestration/kubeflow/e2e_tests/kubeflow_gcp_integration_test.py" startline="323" endline="338" pcid="1427">
    def _pusher(model_importer, model_blessing_importer, bigquery_dataset_id):
      return Pusher(
          custom_executor_spec=executor_spec.ExecutorClassSpec(
              bigquery_pusher_executor.Executor),
          model=model_importer.outputs['result'],
          model_blessing=model_blessing_importer.outputs['result'],
          custom_config={
              bigquery_pusher_executor.SERVING_ARGS_KEY: {
                  'bq_dataset_id': bigquery_dataset_id,
                  'model_name': pipeline_name,
                  'project_id': self._GCP_PROJECT_ID,
              }
          },
      )

    # The model list should be empty
</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/kubeflow/e2e_tests/kubeflow_gcp_integration_test.py" startline="378" endline="393" pcid="1431">
    def _pusher(model_importer, model_blessing_importer):
      return Pusher(
          custom_executor_spec=executor_spec.ExecutorClassSpec(
              ai_platform_pusher_executor.Executor),
          model=model_importer.outputs['result'],
          model_blessing=model_blessing_importer.outputs['result'],
          custom_config={
              tfx.extensions.google_cloud_ai_platform.experimental
              .PUSHER_SERVING_ARGS_KEY: {
                  'model_name': model_name,
                  'project_id': self._GCP_PROJECT_ID,
              }
          },
      )

    # Use default service_name / api_version.
</source>
</class>

<class classid="70" nclones="2" nlines="77" similarity="80">
<source file="systems/tfx-1.6.1/tfx/orchestration/launcher/kubernetes_component_launcher.py" startline="52" endline="162" pcid="1458">
  def _run_executor(self, execution_id: int,
                    input_dict: Dict[str, List[types.Artifact]],
                    output_dict: Dict[str, List[types.Artifact]],
                    exec_properties: Dict[str, Any]) -> None:
    """Execute underlying component implementation.

    Runs executor container in a Kubernetes Pod and wait until it goes into
    `Succeeded` or `Failed` state.

    Args:
      execution_id: The ID of the execution.
      input_dict: Input dict from input key to a list of Artifacts. These are
        often outputs of another component in the pipeline and passed to the
        component by the orchestration system.
      output_dict: Output dict from output key to a list of Artifacts. These are
        often consumed by a dependent component.
      exec_properties: A dict of execution properties. These are inputs to
        pipeline with primitive types (int, string, float) and fully
        materialized when a pipeline is constructed. No dependency to other
        component or later injection from orchestration systems is necessary or
        possible on these values.

    Raises:
      RuntimeError: when the pod is in `Failed` state or unexpected failure from
      Kubernetes API.

    """

    container_spec = cast(executor_spec.ExecutorContainerSpec,
                          self._component_executor_spec)

    # Replace container spec with jinja2 template.
    container_spec = container_common.resolve_container_template(
        container_spec, input_dict, output_dict, exec_properties)
    pod_name = self._build_pod_name(execution_id)
    # TODO(hongyes): replace the default value from component config.
    try:
      namespace = kube_utils.get_kfp_namespace()
    except RuntimeError:
      namespace = 'kubeflow'

    pod_manifest = self._build_pod_manifest(pod_name, container_spec)
    core_api = kube_utils.make_core_v1_api()

    if kube_utils.is_inside_kfp():
      launcher_pod = kube_utils.get_current_kfp_pod(core_api)
      pod_manifest['spec']['serviceAccount'] = launcher_pod.spec.service_account
      pod_manifest['spec'][
          'serviceAccountName'] = launcher_pod.spec.service_account_name
      pod_manifest['metadata'][
          'ownerReferences'] = container_common.to_swagger_dict(
              launcher_pod.metadata.owner_references)
    else:
      pod_manifest['spec']['serviceAccount'] = kube_utils.TFX_SERVICE_ACCOUNT
      pod_manifest['spec'][
          'serviceAccountName'] = kube_utils.TFX_SERVICE_ACCOUNT

    logging.info('Looking for pod "%s:%s".', namespace, pod_name)
    resp = kube_utils.get_pod(core_api, pod_name, namespace)
    if not resp:
      logging.info('Pod "%s:%s" does not exist. Creating it...',
                   namespace, pod_name)
      logging.info('Pod manifest: %s', pod_manifest)
      try:
        resp = core_api.create_namespaced_pod(
            namespace=namespace, body=pod_manifest)
      except client.rest.ApiException as e:
        raise RuntimeError(
            'Failed to created container executor pod!\nReason: %s\nBody: %s' %
            (e.reason, e.body))

    # Wait up to 300 seconds for the pod to move from pending to another status.
    logging.info('Waiting for pod "%s:%s" to start.', namespace, pod_name)
    kube_utils.wait_pod(
        core_api,
        pod_name,
        namespace,
        exit_condition_lambda=kube_utils.pod_is_not_pending,
        condition_description='non-pending status',
        timeout_sec=300)

    logging.info('Start log streaming for pod "%s:%s".', namespace, pod_name)
    try:
      logs = core_api.read_namespaced_pod_log(
          name=pod_name,
          namespace=namespace,
          container=kube_utils.ARGO_MAIN_CONTAINER_NAME,
          follow=True,
          _preload_content=False).stream()
    except client.rest.ApiException as e:
      raise RuntimeError(
          'Failed to stream the logs from the pod!\nReason: %s\nBody: %s' %
          (e.reason, e.body))

    for log in logs:
      logging.info(log.decode().rstrip('\n'))

    # Wait indefinitely for the pod to complete.
    resp = kube_utils.wait_pod(
        core_api,
        pod_name,
        namespace,
        exit_condition_lambda=kube_utils.pod_is_done,
        condition_description='done state')

    if resp.status.phase == kube_utils.PodPhase.FAILED.value:
      raise RuntimeError('Pod "%s:%s" failed with status "%s".' %
                         (namespace, pod_name, resp.status))

    logging.info('Pod "%s:%s" is done.', namespace, pod_name)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/kubernetes_executor_operator.py" startline="46" endline="161" pcid="1882">
  def run_executor(
      self, execution_info: data_types.ExecutionInfo
  ) -> execution_result_pb2.ExecutorOutput:
    """Execute underlying component implementation.

    Runs executor container in a Kubernetes Pod and wait until it goes into
    `Succeeded` or `Failed` state.

    Args:
      execution_info: All the information that the launcher provides.

    Raises:
      RuntimeError: when the pod is in `Failed` state or unexpected failure from
      Kubernetes API.

    Returns:
      An ExecutorOutput instance

    """

    context = placeholder_utils.ResolutionContext(
        exec_info=execution_info,
        executor_spec=self._executor_spec,
        platform_config=self._platform_config)

    container_spec = executor_specs.TemplatedExecutorContainerSpec(
        image=self._container_executor_spec.image,
        command=[
            placeholder_utils.resolve_placeholder_expression(cmd, context)
            for cmd in self._container_executor_spec.commands
        ] or None,
        args=[
            placeholder_utils.resolve_placeholder_expression(arg, context)
            for arg in self._container_executor_spec.args
        ] or None,
    )

    pod_name = self._build_pod_name(execution_info)
    # TODO(hongyes): replace the default value from component config.
    try:
      namespace = kube_utils.get_kfp_namespace()
    except RuntimeError:
      namespace = 'kubeflow'

    pod_manifest = self._build_pod_manifest(pod_name, container_spec)
    core_api = kube_utils.make_core_v1_api()

    if kube_utils.is_inside_kfp():
      launcher_pod = kube_utils.get_current_kfp_pod(core_api)
      pod_manifest['spec']['serviceAccount'] = launcher_pod.spec.service_account
      pod_manifest['spec'][
          'serviceAccountName'] = launcher_pod.spec.service_account_name
      pod_manifest['metadata'][
          'ownerReferences'] = container_common.to_swagger_dict(
              launcher_pod.metadata.owner_references)
    else:
      pod_manifest['spec']['serviceAccount'] = kube_utils.TFX_SERVICE_ACCOUNT
      pod_manifest['spec'][
          'serviceAccountName'] = kube_utils.TFX_SERVICE_ACCOUNT

    logging.info('Looking for pod "%s:%s".', namespace, pod_name)
    resp = kube_utils.get_pod(core_api, pod_name, namespace)
    if not resp:
      logging.info('Pod "%s:%s" does not exist. Creating it...',
                   namespace, pod_name)
      logging.info('Pod manifest: %s', pod_manifest)
      try:
        resp = core_api.create_namespaced_pod(
            namespace=namespace, body=pod_manifest)
      except client.rest.ApiException as e:
        raise RuntimeError(
            'Failed to created container executor pod!\nReason: %s\nBody: %s' %
            (e.reason, e.body))

    # Wait up to 300 seconds for the pod to move from pending to another status.
    logging.info('Waiting for pod "%s:%s" to start.', namespace, pod_name)
    kube_utils.wait_pod(
        core_api,
        pod_name,
        namespace,
        exit_condition_lambda=kube_utils.pod_is_not_pending,
        condition_description='non-pending status',
        timeout_sec=300)

    logging.info('Start log streaming for pod "%s:%s".', namespace, pod_name)
    try:
      logs = core_api.read_namespaced_pod_log(
          name=pod_name,
          namespace=namespace,
          container=kube_utils.ARGO_MAIN_CONTAINER_NAME,
          follow=True,
          _preload_content=False).stream()
    except client.rest.ApiException as e:
      raise RuntimeError(
          'Failed to stream the logs from the pod!\nReason: %s\nBody: %s' %
          (e.reason, e.body))

    for log in logs:
      logging.info(log.decode().rstrip('\n'))

    # Wait indefinitely for the pod to complete.
    resp = kube_utils.wait_pod(
        core_api,
        pod_name,
        namespace,
        exit_condition_lambda=kube_utils.pod_is_done,
        condition_description='done state')

    if resp.status.phase == kube_utils.PodPhase.FAILED.value:
      raise RuntimeError('Pod "%s:%s" failed with status "%s".' %
                         (namespace, pod_name, resp.status))

    logging.info('Pod "%s:%s" is done.', namespace, pod_name)

    return execution_result_pb2.ExecutorOutput()

</source>
</class>

<class classid="71" nclones="2" nlines="25" similarity="100">
<source file="systems/tfx-1.6.1/tfx/orchestration/launcher/docker_component_launcher_test.py" startline="104" endline="138" pcid="1472">
  def _create_launcher_context(self, component_config=None):
    test_dir = self.get_temp_dir()

    connection_config = metadata_store_pb2.ConnectionConfig()
    connection_config.sqlite.SetInParent()
    metadata_connection = metadata.Metadata(connection_config)

    pipeline_root = os.path.join(test_dir, 'Test')

    input_artifact = test_utils._InputArtifact()
    input_artifact.uri = os.path.join(test_dir, 'input')

    component = test_utils._FakeComponent(
        name='FakeComponent',
        input_channel=channel_utils.as_channel([input_artifact]),
        custom_executor_spec=executor_spec.ExecutorContainerSpec(
            image='gcr://test', args=['{{input_dict["input"][0].uri}}']))

    pipeline_info = data_types.PipelineInfo(
        pipeline_name='Test', pipeline_root=pipeline_root, run_id='123')

    driver_args = data_types.DriverArgs(enable_cache=True)

    launcher = docker_component_launcher.DockerComponentLauncher.create(
        component=component,
        pipeline_info=pipeline_info,
        driver_args=driver_args,
        metadata_connection=metadata_connection,
        beam_pipeline_args=[],
        additional_pipeline_args={},
        component_config=component_config)

    return {'launcher': launcher, 'input_artifact': input_artifact}


</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/launcher/kubernetes_component_launcher_test.py" startline="256" endline="289" pcid="1488">
  def _create_launcher_context(self, component_config=None):
    test_dir = self.get_temp_dir()

    connection_config = metadata_store_pb2.ConnectionConfig()
    connection_config.sqlite.SetInParent()
    metadata_connection = metadata.Metadata(connection_config)

    pipeline_root = os.path.join(test_dir, 'Test')

    input_artifact = test_utils._InputArtifact()
    input_artifact.uri = os.path.join(test_dir, 'input')

    component = test_utils._FakeComponent(
        name='FakeComponent',
        input_channel=channel_utils.as_channel([input_artifact]),
        custom_executor_spec=executor_spec.ExecutorContainerSpec(
            image='gcr://test', args=['{{input_dict["input"][0].uri}}']))

    pipeline_info = data_types.PipelineInfo(
        pipeline_name='Test', pipeline_root=pipeline_root, run_id='123')

    driver_args = data_types.DriverArgs(enable_cache=True)

    launcher = kubernetes_component_launcher.KubernetesComponentLauncher.create(
        component=component,
        pipeline_info=pipeline_info,
        driver_args=driver_args,
        metadata_connection=metadata_connection,
        beam_pipeline_args=[],
        additional_pipeline_args={},
        component_config=component_config)

    return {'launcher': launcher, 'input_artifact': input_artifact}

</source>
</class>

<class classid="72" nclones="5" nlines="47" similarity="73">
<source file="systems/tfx-1.6.1/tfx/orchestration/launcher/kubernetes_component_launcher_test.py" startline="65" endline="119" pcid="1485">
  def testLaunch_loadInClusterSucceed(self, mock_core_api_cls,
                                      mock_incluster_config, mock_publisher,
                                      mock_is_inside_kfp, mock_kfp_namespace):
    mock_publisher.return_value.publish_execution.return_value = {}
    core_api = mock_core_api_cls.return_value
    core_api.read_namespaced_pod.side_effect = [
        self._mock_launcher_pod(),
        client.rest.ApiException(status=404),  # Mock no existing pod state.
        self._mock_executor_pod(
            'Pending'),  # Mock pending state after creation.
        self._mock_executor_pod('Running'),  # Mock running state after pending.
        self._mock_executor_pod('Succeeded'),  # Mock Succeeded state.
    ]
    # Mock successful pod creation.
    core_api.create_namespaced_pod.return_value = client.V1Pod()
    core_api.read_namespaced_pod_log.return_value.stream.return_value = [
        b'log-1'
    ]
    context = self._create_launcher_context()

    context['launcher'].launch()

    self.assertEqual(5, core_api.read_namespaced_pod.call_count)
    core_api.create_namespaced_pod.assert_called_once()
    core_api.read_namespaced_pod_log.assert_called_once()
    _, mock_kwargs = core_api.create_namespaced_pod.call_args
    self.assertEqual(_KFP_NAMESPACE, mock_kwargs['namespace'])
    pod_manifest = mock_kwargs['body']
    self.assertDictEqual(
        {
            'apiVersion': 'v1',
            'kind': 'Pod',
            'metadata': {
                'name':
                    'test-123-fakecomponent-fakecomponent-123',
                'ownerReferences': [{
                    'apiVersion': 'argoproj.io/v1alpha1',
                    'kind': 'Workflow',
                    'name': 'wf-1',
                    'uid': 'wf-uid-1'
                }]
            },
            'spec': {
                'restartPolicy': 'Never',
                'containers': [{
                    'name': 'main',
                    'image': 'gcr://test',
                    'command': None,
                    'args': [context['input_artifact'].uri],
                }],
                'serviceAccount': 'sa-1',
                'serviceAccountName': None
            }
        }, pod_manifest)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/launcher/kubernetes_component_launcher_test.py" startline="124" endline="175" pcid="1486">
  def testLaunch_loadKubeConfigSucceed(self, mock_core_api_cls,
                                       mock_kube_config, mock_incluster_config,
                                       mock_publisher):
    mock_publisher.return_value.publish_execution.return_value = {}
    mock_incluster_config.side_effect = config.config_exception.ConfigException(
    )
    core_api = mock_core_api_cls.return_value
    core_api.read_namespaced_pod.side_effect = [
        client.rest.ApiException(status=404),  # Mock no existing pod state.
        self._mock_executor_pod(
            'Pending'),  # Mock pending state after creation.
        self._mock_executor_pod('Running'),  # Mock running state after pending.
        self._mock_executor_pod('Succeeded'),  # Mock Succeeded state.
    ]
    # Mock successful pod creation.
    core_api.create_namespaced_pod.return_value = client.V1Pod()
    core_api.read_namespaced_pod_log.return_value.stream.return_value = [
        b'log-1'
    ]
    context = self._create_launcher_context()

    context['launcher'].launch()

    self.assertEqual(4, core_api.read_namespaced_pod.call_count)
    core_api.create_namespaced_pod.assert_called_once()
    core_api.read_namespaced_pod_log.assert_called_once()
    _, mock_kwargs = core_api.create_namespaced_pod.call_args
    self.assertEqual('kubeflow', mock_kwargs['namespace'])
    pod_manifest = mock_kwargs['body']
    self.assertDictEqual(
        {
            'apiVersion': 'v1',
            'kind': 'Pod',
            'metadata': {
                'name': 'test-123-fakecomponent-fakecomponent-123',
            },
            'spec': {
                'restartPolicy':
                    'Never',
                'serviceAccount':
                    'tfx-service-account',
                'serviceAccountName':
                    'tfx-service-account',
                'containers': [{
                    'name': 'main',
                    'image': 'gcr://test',
                    'command': None,
                    'args': [context['input_artifact'].uri],
                }],
            }
        }, pod_manifest)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/kubernetes_executor_operator_test.py" startline="85" endline="141" pcid="1886">
  def testLaunch_loadInClusterSucceed(self, mock_core_api_cls,
                                      mock_incluster_config, mock_publisher,
                                      mock_is_inside_kfp, mock_kfp_namespace):
    mock_publisher.return_value.publish_execution.return_value = {}
    core_api = mock_core_api_cls.return_value
    core_api.read_namespaced_pod.side_effect = [
        self._mock_launcher_pod(),
        client.rest.ApiException(status=404),  # Mock no existing pod state.
        self._mock_executor_pod(
            'Pending'),  # Mock pending state after creation.
        self._mock_executor_pod('Running'),  # Mock running state after pending.
        self._mock_executor_pod('Succeeded'),  # Mock Succeeded state.
    ]
    # Mock successful pod creation.
    core_api.create_namespaced_pod.return_value = client.V1Pod()
    core_api.read_namespaced_pod_log.return_value.stream.return_value = [
        b'log-1'
    ]
    context = self._create_launcher_context()

    execution_info = self._set_up_test_execution_info(
        input_dict={'input': [context['input_artifact']]})
    context['operator'].run_executor(execution_info)

    self.assertEqual(5, core_api.read_namespaced_pod.call_count)
    core_api.create_namespaced_pod.assert_called_once()
    core_api.read_namespaced_pod_log.assert_called_once()
    _, mock_kwargs = core_api.create_namespaced_pod.call_args
    self.assertEqual(_KFP_NAMESPACE, mock_kwargs['namespace'])
    pod_manifest = mock_kwargs['body']
    self.assertDictEqual(
        {
            'apiVersion': 'v1',
            'kind': 'Pod',
            'metadata': {
                'name':
                    'test-123-fakecomponent-fakecomponent-123',
                'ownerReferences': [{
                    'apiVersion': 'argoproj.io/v1alpha1',
                    'kind': 'Workflow',
                    'name': 'wf-1',
                    'uid': 'wf-uid-1'
                }]
            },
            'spec': {
                'restartPolicy': 'Never',
                'containers': [{
                    'name': 'main',
                    'image': 'gcr://test',
                    'command': [],
                    'args': [context['input_artifact'].uri],
                }],
                'serviceAccount': 'sa-1',
                'serviceAccountName': None
            }
        }, pod_manifest)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/launcher/kubernetes_component_launcher_test.py" startline="187" endline="255" pcid="1487">
  def testLaunch_withComponentConfig(self, mock_core_api_cls,
                                     mock_incluster_config, mock_publisher,
                                     mock_is_inside_kfp, mock_kfp_namespace):
    mock_publisher.return_value.publish_execution.return_value = {}
    core_api = mock_core_api_cls.return_value
    core_api.read_namespaced_pod.side_effect = [
        self._mock_launcher_pod(),
        client.rest.ApiException(status=404),  # Mock no existing pod state.
        self._mock_executor_pod(
            'Pending'),  # Mock pending state after creation.
        self._mock_executor_pod('Running'),  # Mock running state after pending.
        self._mock_executor_pod('Succeeded'),  # Mock Succeeded state.
    ]
    # Mock successful pod creation.
    core_api.create_namespaced_pod.return_value = client.V1Pod()
    core_api.read_namespaced_pod_log.return_value.stream.return_value = [
        b'log-1'
    ]
    component_config = kubernetes_component_config.KubernetesComponentConfig(
        client.V1Pod(
            spec=client.V1PodSpec(containers=[
                client.V1Container(
                    name='main', resources={'limits': {
                        'memory': '200mi'
                    }})
            ])))
    context = self._create_launcher_context(component_config)

    context['launcher'].launch()

    self.assertEqual(5, core_api.read_namespaced_pod.call_count)
    core_api.create_namespaced_pod.assert_called_once()
    core_api.read_namespaced_pod_log.assert_called_once()
    _, mock_kwargs = core_api.create_namespaced_pod.call_args
    self.assertEqual(_KFP_NAMESPACE, mock_kwargs['namespace'])
    pod_manifest = mock_kwargs['body']
    print(pod_manifest)
    self.assertDictEqual(
        {
            'apiVersion': 'v1',
            'kind': 'Pod',
            'metadata': {
                'name':
                    'test-123-fakecomponent-fakecomponent-123',
                'ownerReferences': [{
                    'apiVersion': 'argoproj.io/v1alpha1',
                    'kind': 'Workflow',
                    'name': 'wf-1',
                    'uid': 'wf-uid-1'
                }]
            },
            'spec': {
                'restartPolicy': 'Never',
                'containers': [{
                    'name': 'main',
                    'image': 'gcr://test',
                    'command': None,
                    'args': [context['input_artifact'].uri],
                    'resources': {
                        'limits': {
                            'memory': '200mi'
                        }
                    }
                }],
                'serviceAccount': 'sa-1',
                'serviceAccountName': None
            }
        }, pod_manifest)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/kubernetes_executor_operator_test.py" startline="146" endline="199" pcid="1887">
  def testLaunch_loadKubeConfigSucceed(self, mock_core_api_cls,
                                       mock_kube_config, mock_incluster_config,
                                       mock_publisher):
    mock_publisher.return_value.publish_execution.return_value = {}
    mock_incluster_config.side_effect = config.config_exception.ConfigException(
    )
    core_api = mock_core_api_cls.return_value
    core_api.read_namespaced_pod.side_effect = [
        client.rest.ApiException(status=404),  # Mock no existing pod state.
        self._mock_executor_pod(
            'Pending'),  # Mock pending state after creation.
        self._mock_executor_pod('Running'),  # Mock running state after pending.
        self._mock_executor_pod('Succeeded'),  # Mock Succeeded state.
    ]
    # Mock successful pod creation.
    core_api.create_namespaced_pod.return_value = client.V1Pod()
    core_api.read_namespaced_pod_log.return_value.stream.return_value = [
        b'log-1'
    ]
    context = self._create_launcher_context()

    execution_info = self._set_up_test_execution_info(
        input_dict={'input': [context['input_artifact']]})
    context['operator'].run_executor(execution_info)

    self.assertEqual(4, core_api.read_namespaced_pod.call_count)
    core_api.create_namespaced_pod.assert_called_once()
    core_api.read_namespaced_pod_log.assert_called_once()
    _, mock_kwargs = core_api.create_namespaced_pod.call_args
    self.assertEqual('kubeflow', mock_kwargs['namespace'])
    pod_manifest = mock_kwargs['body']
    self.assertDictEqual(
        {
            'apiVersion': 'v1',
            'kind': 'Pod',
            'metadata': {
                'name': 'test-123-fakecomponent-fakecomponent-123',
            },
            'spec': {
                'restartPolicy':
                    'Never',
                'serviceAccount':
                    'tfx-service-account',
                'serviceAccountName':
                    'tfx-service-account',
                'containers': [{
                    'name': 'main',
                    'image': 'gcr://test',
                    'command': [],
                    'args': [context['input_artifact'].uri],
                }],
            }
        }, pod_manifest)

</source>
</class>

<class classid="73" nclones="2" nlines="16" similarity="100">
<source file="systems/tfx-1.6.1/tfx/orchestration/launcher/docker_component_launcher_e2e_test.py" startline="51" endline="69" pcid="1492">
def _create_pipeline(
    pipeline_name,
    pipeline_root,
    metadata_path,
    name,
):
  hello_world = _HelloWorldComponent(name=name)

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[hello_world],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      additional_pipeline_args={},
  )


</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/docker_executor_operator_e2e_test.py" startline="51" endline="69" pcid="1958">
def _create_pipeline(
    pipeline_name,
    pipeline_root,
    metadata_path,
    name,
):
  hello_world = _HelloWorldComponent(name=name)

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[hello_world],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      additional_pipeline_args={},
  )


</source>
</class>

<class classid="74" nclones="13" nlines="12" similarity="71">
<source file="systems/tfx-1.6.1/tfx/orchestration/launcher/docker_component_launcher_e2e_test.py" startline="72" endline="83" pcid="1493">
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'docker_e2e_test'
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/docker_executor_operator_e2e_test.py" startline="72" endline="83" pcid="1959">
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'docker_e2e_test'
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</source>
<source file="systems/tfx-1.6.1/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_local_e2e_test.py" startline="28" endline="42" pcid="3085">
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'beam_test'
    self._data_root = os.path.join(os.path.dirname(__file__), 'data', 'simple')
    self._module_file = os.path.join(os.path.dirname(__file__), 'taxi_utils.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</source>
<source file="systems/tfx-1.6.1/tfx/examples/tfjs_next_page_prediction/tfjs_next_page_prediction_e2e_test.py" startline="28" endline="43" pcid="3014">
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'page_prediction_test'
    self._data_root = os.path.join(os.path.dirname(__file__), 'data')
    self._module_file = os.path.join(
        os.path.dirname(__file__), 'tfjs_next_page_prediction_util.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</source>
<source file="systems/tfx-1.6.1/tfx/examples/bert/mrpc/bert_mrpc_pipeline_e2e_test.py" startline="31" endline="46" pcid="2902">
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'keras_test'
    self._data_root = os.path.join(os.path.dirname(__file__), 'data')
    self._module_file = os.path.join(
        os.path.dirname(__file__), 'bert_mrpc_utils.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</source>
<source file="systems/tfx-1.6.1/tfx/examples/imdb/imdb_pipeline_native_keras_e2e_test.py" startline="31" endline="46" pcid="2995">
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'keras_test'
    self._data_root = os.path.join(os.path.dirname(__file__), 'data')
    self._module_file = os.path.join(
        os.path.dirname(__file__), 'imdb_utils_native_keras.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</source>
<source file="systems/tfx-1.6.1/tfx/examples/custom_components/hello_world/example/taxi_pipeline_hello_e2e_test.py" startline="27" endline="39" pcid="3036">
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'hello_test'
    self._data_root = os.path.join(os.path.dirname(__file__), '..', 'data')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</source>
<source file="systems/tfx-1.6.1/tfx/examples/bert/cola/bert_cola_pipeline_e2e_test.py" startline="31" endline="46" pcid="2882">
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'keras_test'
    self._data_root = os.path.join(os.path.dirname(__file__), 'data')
    self._module_file = os.path.join(
        os.path.dirname(__file__), 'bert_cola_utils.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</source>
<source file="systems/tfx-1.6.1/tfx/examples/penguin/penguin_pipeline_local_infraval_e2e_test.py" startline="41" endline="58" pcid="3118">
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'penguin_test'
    self._data_root = os.path.join(os.path.dirname(__file__), 'data')
    self._schema_path = os.path.join(
        os.path.dirname(__file__), 'schema', 'user_provided', 'schema.pbtxt')
    self._module_file = os.path.join(
        os.path.dirname(__file__), 'penguin_utils_keras.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</source>
<source file="systems/tfx-1.6.1/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_native_keras_e2e_test.py" startline="32" endline="48" pcid="3069">
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'native_keras_test'
    self._data_root = os.path.join(
        os.path.dirname(__file__), 'data', 'simple')
    self._module_file = os.path.join(
        os.path.dirname(__file__), 'taxi_utils_native_keras.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</source>
<source file="systems/tfx-1.6.1/tfx/examples/cifar10/cifar10_pipeline_native_keras_e2e_test.py" startline="28" endline="45" pcid="2922">
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'keras_test'
    self._data_root = os.path.join(os.path.dirname(__file__), 'data')
    self._module_file = os.path.join(
        os.path.dirname(__file__), 'cifar10_utils_native_keras.py')
    self._serving_model_dir_lite = os.path.join(self._test_dir,
                                                'serving_model_lite')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')
    self._labels_path = os.path.join(self._data_root, 'labels.txt')

</source>
<source file="systems/tfx-1.6.1/tfx/examples/mnist/mnist_pipeline_native_keras_e2e_test.py" startline="31" endline="50" pcid="2937">
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    self._pipeline_name = 'keras_test'
    self._data_root = os.path.join(os.path.dirname(__file__), 'data')
    self._module_file = os.path.join(
        os.path.dirname(__file__), 'mnist_utils_native_keras.py')
    self._module_file_lite = os.path.join(
        os.path.dirname(__file__), 'mnist_utils_native_keras_lite.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._serving_model_dir_lite = os.path.join(
        self._test_dir, 'serving_model_lite')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</source>
<source file="systems/tfx-1.6.1/tfx/examples/penguin/experimental/penguin_pipeline_sklearn_local_e2e_test.py" startline="29" endline="48" pcid="3136">
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    self._experimental_root = os.path.dirname(__file__)
    self._penguin_root = os.path.dirname(self._experimental_root)

    self._pipeline_name = 'sklearn_test'
    self._data_root = os.path.join(self._penguin_root, 'data')
    self._trainer_module_file = os.path.join(
        self._experimental_root, 'penguin_utils_sklearn.py')
    self._evaluator_module_file = os.path.join(
        self._experimental_root, 'sklearn_predict_extractor.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')

</source>
</class>

<class classid="75" nclones="2" nlines="11" similarity="100">
<source file="systems/tfx-1.6.1/tfx/orchestration/launcher/docker_component_launcher_e2e_test.py" startline="84" endline="98" pcid="1494">
  def testDockerComponentLauncherInBeam(self):

    beam_dag_runner.BeamDagRunner().run(
        _create_pipeline(
            pipeline_name=self._pipeline_name,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            name='docker_e2e_test_in_beam'))

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      self.assertEqual(1, len(m.store.get_executions()))


</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/docker_executor_operator_e2e_test.py" startline="84" endline="98" pcid="1960">
  def testDockerComponentLauncherInBeam(self):

    beam_dag_runner.BeamDagRunner().run(
                _create_pipeline(
                    pipeline_name=self._pipeline_name,
                    pipeline_root=self._pipeline_root,
                    metadata_path=self._metadata_path,
                    name='docker_e2e_test_in_beam'))

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      self.assertEqual(1, len(m.store.get_executions()))


</source>
</class>

<class classid="76" nclones="2" nlines="34" similarity="79">
<source file="systems/tfx-1.6.1/tfx/orchestration/beam/legacy/beam_dag_runner_test.py" startline="125" endline="163" pcid="1497">
  def testRun(self):
    component_a = _FakeComponent(
        _FakeComponentSpecA(output=types.Channel(type=_ArtifactTypeA)))
    component_b = _FakeComponent(
        _FakeComponentSpecB(
            a=component_a.outputs['output'],
            output=types.Channel(type=_ArtifactTypeB)))
    component_c = _FakeComponent(
        _FakeComponentSpecC(
            a=component_a.outputs['output'],
            output=types.Channel(type=_ArtifactTypeC)))
    component_c.add_upstream_node(component_b)
    component_d = _FakeComponent(
        _FakeComponentSpecD(
            b=component_b.outputs['output'],
            c=component_c.outputs['output'],
            output=types.Channel(type=_ArtifactTypeD)))
    component_e = _FakeComponent(
        _FakeComponentSpecE(
            a=component_a.outputs['output'],
            b=component_b.outputs['output'],
            d=component_d.outputs['output'],
            output=types.Channel(type=_ArtifactTypeE)))

    test_pipeline = pipeline.Pipeline(
        pipeline_name='x',
        pipeline_root='y',
        metadata_connection_config=metadata_store_pb2.ConnectionConfig(),
        components=[
            component_d, component_c, component_a, component_b, component_e
        ])

    beam_dag_runner.BeamDagRunner().run(test_pipeline)
    self.assertEqual(_executed_components, [
        '_FakeComponent.A', '_FakeComponent.B', '_FakeComponent.C',
        '_FakeComponent.D', '_FakeComponent.E'
    ])


</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/local/legacy/local_dag_runner_test.py" startline="124" endline="160" pcid="2026">
  def _getTestPipeline(self):  # pylint: disable=invalid-name
    component_a = _get_fake_component(
        _FakeComponentSpecA(output=types.Channel(type=_ArtifactTypeA)))
    component_b = _get_fake_component(
        _FakeComponentSpecB(
            a=component_a.outputs['output'],
            output=types.Channel(type=_ArtifactTypeB)))
    component_c = _get_fake_component(
        _FakeComponentSpecC(
            a=component_a.outputs['output'],
            output=types.Channel(type=_ArtifactTypeC)))
    component_c.add_upstream_node(component_b)
    component_d = _get_fake_component(
        _FakeComponentSpecD(
            b=component_b.outputs['output'],
            c=component_c.outputs['output'],
            output=types.Channel(type=_ArtifactTypeD)))
    component_e = _get_fake_component(
        _FakeComponentSpecE(
            a=component_a.outputs['output'],
            b=component_b.outputs['output'],
            d=component_d.outputs['output'],
            output=types.Channel(type=_ArtifactTypeE)))

    temp_path = tempfile.mkdtemp()
    pipeline_root_path = os.path.join(temp_path, 'pipeline_root')
    metadata_path = os.path.join(temp_path, 'metadata.db')
    test_pipeline = pipeline.Pipeline(
        pipeline_name='test_pipeline',
        pipeline_root=pipeline_root_path,
        metadata_connection_config=sqlite_metadata_connection_config(
            metadata_path),
        components=[
            component_d, component_c, component_a, component_b, component_e
        ])
    return test_pipeline

</source>
</class>

<class classid="77" nclones="2" nlines="49" similarity="100">
<source file="systems/tfx-1.6.1/tfx/orchestration/beam/beam_dag_runner_test.py" startline="193" endline="244" pcid="1501">
  def testRunWithLocalDeploymentConfig(self):
    self._pipeline.deployment_config.Pack(_LOCAL_DEPLOYMENT_CONFIG)
    beam_dag_runner.BeamDagRunner().run_with_ir(self._pipeline)
    self.assertEqual(
        _component_executors, {
            'my_example_gen':
                text_format.Parse(
                    'class_path: "tfx.components.example_gen_executor"',
                    _PythonClassExecutableSpec()),
            'my_transform':
                text_format.Parse(
                    'class_path: "tfx.components.transform_executor"',
                    _PythonClassExecutableSpec()),
            'my_trainer':
                text_format.Parse('image: "path/to/docker/image"',
                                  _ContainerExecutableSpec()),
            'my_importer':
                None,
        })
    self.assertEqual(
        _component_drivers, {
            'my_example_gen':
                text_format.Parse(
                    'class_path: "tfx.components.example_gen_driver"',
                    _PythonClassExecutableSpec()),
            'my_transform':
                None,
            'my_trainer':
                None,
            'my_importer':
                None,
        })
    self.assertEqual(
        _component_platform_configs, {
            'my_example_gen':
                None,
            'my_transform':
                None,
            'my_trainer':
                text_format.Parse('docker_server_url: "docker/server/url"',
                                  _DockerPlatformConfig()),
            'my_importer':
                None,
        })
    # 'my_importer' has no upstream and can be executed in any order.
    self.assertIn('my_importer', _executed_components)
    _executed_components.remove('my_importer')
    self.assertEqual(_executed_components,
                     ['my_example_gen', 'my_transform', 'my_trainer'])
    # Verifies that every component gets a not-None pipeline_run.
    self.assertTrue(all(_component_to_pipeline_run.values()))

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/beam/beam_dag_runner_test.py" startline="249" endline="300" pcid="1502">
  def testRunWithIntermediateDeploymentConfig(self):
    self._pipeline.deployment_config.Pack(_INTERMEDIATE_DEPLOYMENT_CONFIG)
    beam_dag_runner.BeamDagRunner().run_with_ir(self._pipeline)
    self.assertEqual(
        _component_executors, {
            'my_example_gen':
                text_format.Parse(
                    'class_path: "tfx.components.example_gen_executor"',
                    _PythonClassExecutableSpec()),
            'my_transform':
                text_format.Parse(
                    'class_path: "tfx.components.transform_executor"',
                    _PythonClassExecutableSpec()),
            'my_trainer':
                text_format.Parse('image: "path/to/docker/image"',
                                  _ContainerExecutableSpec()),
            'my_importer':
                None,
        })
    self.assertEqual(
        _component_drivers, {
            'my_example_gen':
                text_format.Parse(
                    'class_path: "tfx.components.example_gen_driver"',
                    _PythonClassExecutableSpec()),
            'my_transform':
                None,
            'my_trainer':
                None,
            'my_importer':
                None,
        })
    self.assertEqual(
        _component_platform_configs, {
            'my_example_gen':
                None,
            'my_transform':
                None,
            'my_trainer':
                text_format.Parse('docker_server_url: "docker/server/url"',
                                  _DockerPlatformConfig()),
            'my_importer':
                None,
        })
    # 'my_importer' has no upstream and can be executed in any order.
    self.assertIn('my_importer', _executed_components)
    _executed_components.remove('my_importer')
    self.assertEqual(_executed_components,
                     ['my_example_gen', 'my_transform', 'my_trainer'])
    # Verifies that every component gets a not-None pipeline_run.
    self.assertTrue(all(_component_to_pipeline_run.values()))

</source>
</class>

<class classid="78" nclones="2" nlines="10" similarity="100">
<source file="systems/tfx-1.6.1/tfx/orchestration/beam/beam_dag_runner_test.py" startline="305" endline="315" pcid="1503">
  def testPartialRunWithLocalDeploymentConfig(self):
    self._pipeline.deployment_config.Pack(_LOCAL_DEPLOYMENT_CONFIG)
    pr_opts = pipeline_pb2.PartialRun()
    pr_opts.from_nodes.append('my_trainer')
    pr_opts.to_nodes.append('my_trainer')
    pr_opts.snapshot_settings.latest_pipeline_run_strategy.SetInParent()
    beam_dag_runner.BeamDagRunner().run_with_ir(
        self._pipeline,
        run_options=pipeline_pb2.RunOptions(partial_run=pr_opts))
    self.assertEqual(_executed_components, ['my_trainer'])

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/beam/beam_dag_runner_test.py" startline="320" endline="330" pcid="1504">
  def testPartialRunWithIntermediateDeploymentConfig(self):
    self._pipeline.deployment_config.Pack(_INTERMEDIATE_DEPLOYMENT_CONFIG)
    pr_opts = pipeline_pb2.PartialRun()
    pr_opts.from_nodes.append('my_trainer')
    pr_opts.to_nodes.append('my_trainer')
    pr_opts.snapshot_settings.latest_pipeline_run_strategy.SetInParent()
    beam_dag_runner.BeamDagRunner().run_with_ir(
        self._pipeline,
        run_options=pipeline_pb2.RunOptions(partial_run=pr_opts))
    self.assertEqual(_executed_components, ['my_trainer'])

</source>
</class>

<class classid="79" nclones="2" nlines="16" similarity="75">
<source file="systems/tfx-1.6.1/tfx/orchestration/pipeline_test.py" startline="304" endline="322" pcid="1556">
  def testPipelineWithBeamPipelineArgs(self):
    expected_args = [
        '--my_first_beam_pipeline_args=foo',
        '--my_second_beam_pipeline_args=bar'
    ]
    p = pipeline.Pipeline(
        pipeline_name='a',
        pipeline_root='b',
        log_root='c',
        components=[
            _make_fake_component_instance(
                'component_a', _OutputTypeA, {}, {},
                with_beam=True).with_beam_pipeline_args([expected_args[1]])
        ],
        beam_pipeline_args=[expected_args[0]],
        metadata_connection_config=self._metadata_connection_config)
    self.assertEqual(expected_args,
                     p.components[0].executor_spec.beam_pipeline_args)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/pipeline_test.py" startline="323" endline="342" pcid="1557">
  def testComponentsSetAfterCreationWithBeamPipelineArgs(self):
    expected_args = [
        '--my_first_beam_pipeline_args=foo',
        '--my_second_beam_pipeline_args=bar'
    ]
    p = pipeline.Pipeline(
        pipeline_name='a',
        pipeline_root='b',
        log_root='c',
        beam_pipeline_args=[expected_args[0]],
        metadata_connection_config=self._metadata_connection_config)
    p.components = [
        _make_fake_component_instance(
            'component_a', _OutputTypeA, {}, {},
            with_beam=True).with_beam_pipeline_args([expected_args[1]])
    ]
    self.assertEqual(expected_args,
                     p.components[0].executor_spec.beam_pipeline_args)


</source>
</class>

<class classid="80" nclones="2" nlines="12" similarity="100">
<source file="systems/tfx-1.6.1/tfx/orchestration/data_types_utils.py" startline="73" endline="87" pcid="1563">
def build_metadata_value_dict(
    value_dict: Mapping[str, types.ExecPropertyTypes]
) -> Dict[str, metadata_store_pb2.Value]:
  """Converts plain value dict into MLMD value dict."""
  result = {}
  if not value_dict:
    return result
  for k, v in value_dict.items():
    if v is None:
      continue
    value = metadata_store_pb2.Value()
    result[k] = set_metadata_value(value, v)
  return result


</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/data_types_utils.py" startline="88" endline="102" pcid="1564">
def build_pipeline_value_dict(
    value_dict: Dict[str, types.ExecPropertyTypes]
) -> Dict[str, pipeline_pb2.Value]:
  """Converts plain value dict into pipeline_pb2.Value dict."""
  result = {}
  if not value_dict:
    return result
  for k, v in value_dict.items():
    if v is None:
      continue
    value = pipeline_pb2.Value()
    result[k] = set_parameter_value(value, v)
  return result


</source>
</class>

<class classid="81" nclones="10" nlines="72" similarity="74">
<source file="systems/tfx-1.6.1/tfx/orchestration/experimental/kubernetes/examples/taxi_pipeline_kubernetes.py" startline="69" endline="168" pcid="1588">
def create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                    module_file: str, serving_model_dir: str,
                    beam_pipeline_args: List[str]) -> pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'],
      infer_feature_shape=False)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that implements a model.
  trainer = Trainer(
      module_file=module_file,
      transformed_examples=transform.outputs['transformed_examples'],
      schema=schema_gen.outputs['schema'],
      transform_graph=transform.outputs['transform_graph'],
      train_args=trainer_pb2.TrainArgs(num_steps=10000),
      eval_args=trainer_pb2.EvalArgs(num_steps=5000))

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute a evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(signature_name='eval')],
      slicing_specs=[
          tfma.SlicingSpec(),
          tfma.SlicingSpec(feature_keys=['trip_start_hour'])
      ],
      metrics_specs=[
          tfma.MetricsSpec(
              thresholds={
                  'accuracy':
                      tfma.MetricThreshold(
                          value_threshold=tfma.GenericValueThreshold(
                              lower_bound={'value': 0.6}),
                          # Change threshold will be ignored if there is no
                          # baseline model resolved from MLMD (first run).
                          change_threshold=tfma.GenericChangeThreshold(
                              direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                              absolute={'value': -1e-10}))
              })
      ])
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  config = kubernetes_dag_runner.get_default_kubernetes_metadata_config()
  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen,
          statistics_gen,
          schema_gen,
          example_validator,
          transform,
          trainer,
          model_resolver,
          evaluator,
          pusher,
      ],
      enable_cache=False,
      metadata_connection_config=config,
      beam_pipeline_args=beam_pipeline_args)


</source>
<source file="systems/tfx-1.6.1/tfx/examples/airflow_workshop/setup/dags/taxi_pipeline_solution.py" startline="81" endline="179" pcid="2960">
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir: str,
                     metadata_path: str,
                     beam_pipeline_args: List[str]) -> pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""
  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  infer_schema = SchemaGen(
      statistics=statistics_gen.outputs['statistics'],
      infer_feature_shape=False)

  # Performs anomaly detection based on statistics and data schema.
  validate_stats = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=infer_schema.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=infer_schema.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that implements a model.
  trainer = Trainer(
      module_file=module_file,
      custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),
      examples=transform.outputs['transformed_examples'],
      transform_graph=transform.outputs['transform_graph'],
      schema=infer_schema.outputs['schema'],
      train_args=trainer_pb2.TrainArgs(num_steps=10000),
      eval_args=trainer_pb2.EvalArgs(num_steps=5000))

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute a evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(label_key='tips')],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='BinaryAccuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          lower_bound={'value': 0.6}),
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-10})))
          ])
      ])

  model_analyzer = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      # Change threshold will be ignored if there is no baseline (first run).
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=model_analyzer.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen,
          statistics_gen,
          infer_schema,
          validate_stats,
          transform,
          trainer,
          model_resolver,
          model_analyzer,
          pusher,
      ],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      beam_pipeline_args=beam_pipeline_args)


# 'DAG' below need to be kept for Airflow to detect dag.
</source>
<source file="systems/tfx-1.6.1/tfx/examples/bert/mrpc/bert_mrpc_pipeline.py" startline="70" endline="176" pcid="2894">
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir: str,
                     metadata_path: str,
                     beam_pipeline_args: List[str]) -> pipeline.Pipeline:
  """Implements the Bert classication on mrpc dataset pipline with TFX."""
  input_config = example_gen_pb2.Input(splits=[
      example_gen_pb2.Input.Split(name='train', pattern='train/*'),
      example_gen_pb2.Input.Split(name='eval', pattern='validation/*')
  ])

  # Brings data into the pipline
  example_gen = CsvExampleGen(input_base=data_root, input_config=input_config)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that trains a model.
  trainer = Trainer(
      module_file=module_file,
      examples=transform.outputs['transformed_examples'],
      transform_graph=transform.outputs['transform_graph'],
      schema=schema_gen.outputs['schema'],
      # Adjust these steps when training on the full dataset.
      train_args=trainer_pb2.TrainArgs(num_steps=1),
      eval_args=trainer_pb2.EvalArgs(num_steps=1))

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(label_key='label')],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='SparseCategoricalAccuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          # Adjust the threshold when training on the
                          # full dataset.
                          lower_bound={'value': 0.5}),
                      # Change threshold will be ignored if there is no
                      # baseline model resolved from MLMD (first run).
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-2})))
          ])
      ])
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  components = [
      example_gen,
      statistics_gen,
      schema_gen,
      example_validator,
      transform,
      trainer,
      model_resolver,
      evaluator,
      pusher,
  ]

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=components,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      enable_cache=True,
      beam_pipeline_args=beam_pipeline_args,
  )


</source>
<source file="systems/tfx-1.6.1/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_simple.py" startline="82" endline="183" pcid="3073">
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir: str,
                     metadata_path: str,
                     beam_pipeline_args: List[str]) -> pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""
  # Parametrize data root so it can be replaced on runtime. See the
  # "Passing Parameters when triggering dags" section of
  # https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html
  # for more details.
  data_root_runtime = data_types.RuntimeParameter(
      'data_root', ptype=str, default=data_root)

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root_runtime)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'],
      infer_feature_shape=False)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that implements a model.
  trainer = Trainer(
      module_file=module_file,
      custom_executor_spec=executor_spec.ExecutorClassSpec(Executor),
      transformed_examples=transform.outputs['transformed_examples'],
      schema=schema_gen.outputs['schema'],
      transform_graph=transform.outputs['transform_graph'],
      train_args=trainer_pb2.TrainArgs(num_steps=10000),
      eval_args=trainer_pb2.EvalArgs(num_steps=5000))

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute a evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(signature_name='eval')],
      slicing_specs=[
          tfma.SlicingSpec(),
          tfma.SlicingSpec(feature_keys=['trip_start_hour'])
      ],
      metrics_specs=[
          tfma.MetricsSpec(
              thresholds={
                  'accuracy':
                      tfma.MetricThreshold(
                          value_threshold=tfma.GenericValueThreshold(
                              lower_bound={'value': 0.6}),
                          # Change threshold will be ignored if there is no
                          # baseline model resolved from MLMD (first run).
                          change_threshold=tfma.GenericChangeThreshold(
                              direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                              absolute={'value': -1e-10}))
              })
      ])
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen, statistics_gen, schema_gen, example_validator, transform,
          trainer, model_resolver, evaluator, pusher
      ],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      beam_pipeline_args=beam_pipeline_args)


# 'DAG' below need to be kept for Airflow to detect dag.
</source>
<source file="systems/tfx-1.6.1/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_native_keras.py" startline="72" endline="174" pcid="3093">
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir: str,
                     metadata_path: str,
                     beam_pipeline_args: List[str]) -> pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""
  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'],
      infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that implements a model.
  trainer = Trainer(
      module_file=module_file,
      examples=transform.outputs['transformed_examples'],
      transform_graph=transform.outputs['transform_graph'],
      schema=schema_gen.outputs['schema'],
      train_args=trainer_pb2.TrainArgs(num_steps=1000),
      eval_args=trainer_pb2.EvalArgs(num_steps=150))

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute a evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[
          tfma.ModelSpec(
              signature_name='serving_default', label_key='tips_xf',
              preprocessing_function_names=['transform_features'])
      ],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='BinaryAccuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          lower_bound={'value': 0.6}),
                      # Change threshold will be ignored if there is no
                      # baseline model resolved from MLMD (first run).
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-10})))
          ])
      ])
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen,
          statistics_gen,
          schema_gen,
          example_validator,
          transform,
          trainer,
          model_resolver,
          evaluator,
          pusher,
      ],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      beam_pipeline_args=beam_pipeline_args)


# To run this pipeline from the python CLI:
#   $python taxi_pipeline_native_keras.py
</source>
<source file="systems/tfx-1.6.1/tfx/examples/bert/cola/bert_cola_pipeline.py" startline="70" endline="176" pcid="2886">
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir: str,
                     metadata_path: str,
                     beam_pipeline_args: List[str]) -> pipeline.Pipeline:
  """Implements the Bert classication on Cola dataset pipline with TFX."""
  input_config = example_gen_pb2.Input(splits=[
      example_gen_pb2.Input.Split(name='train', pattern='train/*'),
      example_gen_pb2.Input.Split(name='eval', pattern='validation/*')
  ])

  # Brings data into the pipline
  example_gen = CsvExampleGen(input_base=data_root, input_config=input_config)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that trains a model.
  trainer = Trainer(
      module_file=module_file,
      examples=transform.outputs['transformed_examples'],
      transform_graph=transform.outputs['transform_graph'],
      schema=schema_gen.outputs['schema'],
      # Adjust these steps when training on the full dataset.
      train_args=trainer_pb2.TrainArgs(num_steps=2),
      eval_args=trainer_pb2.EvalArgs(num_steps=1))

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(label_key='label')],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='SparseCategoricalAccuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          # Adjust the threshold when training on the
                          # full dataset.
                          lower_bound={'value': 0.5}),
                      # Change threshold will be ignored if there is no
                      # baseline model resolved from MLMD (first run).
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-2})))
          ])
      ])
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  components = [
      example_gen,
      statistics_gen,
      schema_gen,
      example_validator,
      transform,
      trainer,
      model_resolver,
      evaluator,
      pusher,
  ]

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=components,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      enable_cache=True,
      beam_pipeline_args=beam_pipeline_args,
  )


</source>
<source file="systems/tfx-1.6.1/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_local.py" startline="75" endline="184" pcid="3074">
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir: str,
                     metadata_path: str,
                     beam_pipeline_args: List[str]) -> pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'],
      infer_feature_shape=False)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Get the latest model so that we can warm start from the model.
  latest_model_resolver = resolver.Resolver(
      strategy_class=latest_artifacts_resolver.LatestArtifactsResolver,
      latest_model=Channel(type=Model)).with_id('latest_model_resolver')

  # Uses user-provided Python function that implements a model.
  trainer = Trainer(
      module_file=module_file,
      custom_executor_spec=executor_spec.ExecutorClassSpec(Executor),
      transformed_examples=transform.outputs['transformed_examples'],
      schema=schema_gen.outputs['schema'],
      base_model=latest_model_resolver.outputs['latest_model'],
      transform_graph=transform.outputs['transform_graph'],
      train_args=trainer_pb2.TrainArgs(num_steps=10000),
      eval_args=trainer_pb2.EvalArgs(num_steps=5000))

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute a evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(signature_name='eval')],
      slicing_specs=[
          tfma.SlicingSpec(),
          tfma.SlicingSpec(feature_keys=['trip_start_hour'])
      ],
      metrics_specs=[
          tfma.MetricsSpec(
              thresholds={
                  'accuracy':
                      tfma.MetricThreshold(
                          value_threshold=tfma.GenericValueThreshold(
                              lower_bound={'value': 0.6}),
                          change_threshold=tfma.GenericChangeThreshold(
                              direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                              absolute={'value': -1e-10}))
              })
      ])
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      # Change threshold will be ignored if there is no baseline (first run).
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen,
          statistics_gen,
          schema_gen,
          example_validator,
          transform,
          latest_model_resolver,
          trainer,
          model_resolver,
          evaluator,
          pusher,
      ],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      beam_pipeline_args=beam_pipeline_args)


# To run this pipeline from the python CLI:
#   $python taxi_pipeline_beam.py
</source>
<source file="systems/tfx-1.6.1/tfx/examples/imdb/imdb_pipeline_native_keras.py" startline="72" endline="179" pcid="2994">
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir: str,
                     metadata_path: str,
                     beam_pipeline_args: List[str]) -> pipeline.Pipeline:
  """Implements the imdb sentiment analysis pipline with TFX."""
  output = example_gen_pb2.Output(
      split_config=example_gen_pb2.SplitConfig(splits=[
          example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=9),
          example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=1)
      ]))

  # Brings data in to the pipline
  example_gen = CsvExampleGen(input_base=data_root, output_config=output)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that trains a model.
  trainer = Trainer(
      module_file=module_file,
      examples=transform.outputs['transformed_examples'],
      transform_graph=transform.outputs['transform_graph'],
      schema=schema_gen.outputs['schema'],
      train_args=trainer_pb2.TrainArgs(num_steps=500),
      eval_args=trainer_pb2.EvalArgs(num_steps=200))

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(label_key='label')],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='BinaryAccuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          # Increase this threshold when training on complete
                          # dataset.
                          lower_bound={'value': 0.01}),
                      # Change threshold will be ignored if there is no
                      # baseline model resolved from MLMD (first run).
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-2})))
          ])
      ])

  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  components = [
      example_gen,
      statistics_gen,
      schema_gen,
      example_validator,
      transform,
      trainer,
      model_resolver,
      evaluator,
      pusher,
  ]
  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=components,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      enable_cache=True,
      beam_pipeline_args=beam_pipeline_args)


# To run this pipeline from the python CLI:
# $python imdb_pipeline_native_keras.py
</source>
<source file="systems/tfx-1.6.1/tfx/examples/cifar10/cifar10_pipeline_native_keras.py" startline="82" endline="200" pcid="2926">
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir_lite: str,
                     metadata_path: str, labels_path: str,
                     beam_pipeline_args: List[str]) -> pipeline.Pipeline:
  """Implements the CIFAR10 image classification pipeline using TFX."""
  # This is needed for datasets with pre-defined splits
  # Change the pattern argument to train_whole/* and test_whole/* to train
  # on the whole CIFAR-10 dataset
  input_config = example_gen_pb2.Input(splits=[
      example_gen_pb2.Input.Split(name='train', pattern='train/*'),
      example_gen_pb2.Input.Split(name='eval', pattern='test/*')
  ])

  # Brings data into the pipeline.
  example_gen = ImportExampleGen(
      input_base=data_root, input_config=input_config)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that trains a model.
  # When traning on the whole dataset, use 18744 for train steps, 156 for eval
  # steps. 18744 train steps correspond to 24 epochs on the whole train set, and
  # 156 eval steps correspond to 1 epoch on the whole test set. The
  # configuration below is for training on the dataset we provided in the data
  # folder, which has 128 train and 128 test samples. The 160 train steps
  # correspond to 40 epochs on this tiny train set, and 4 eval steps correspond
  # to 1 epoch on this tiny test set.
  trainer = Trainer(
      module_file=module_file,
      examples=transform.outputs['transformed_examples'],
      transform_graph=transform.outputs['transform_graph'],
      schema=schema_gen.outputs['schema'],
      train_args=trainer_pb2.TrainArgs(num_steps=160),
      eval_args=trainer_pb2.EvalArgs(num_steps=4),
      custom_config={'labels_path': labels_path})

  # Get the latest blessed model for model validation.
  model_resolver = resolver.Resolver(
      strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(
          type=ModelBlessing)).with_id('latest_blessed_model_resolver')

  # Uses TFMA to compute evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compare to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(label_key='label_xf', model_type='tf_lite')],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='SparseCategoricalAccuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          lower_bound={'value': 0.55}),
                      # Change threshold will be ignored if there is no
                      # baseline model resolved from MLMD (first run).
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-3})))
          ])
      ])

  # Uses TFMA to compute the evaluation statistics over features of a model.
  # We evaluate using the materialized examples that are output by Transform
  # because
  # 1. the decoding_png function currently performed within Transform are not
  # compatible with TFLite.
  # 2. MLKit requires deserialized (float32) tensor image inputs
  # Note that for deployment, the same logic that is performed within Transform
  # must be reproduced client-side.
  evaluator = Evaluator(
      examples=transform.outputs['transformed_examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir_lite)))

  components = [
      example_gen, statistics_gen, schema_gen, example_validator, transform,
      trainer, model_resolver, evaluator, pusher
  ]

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=components,
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      beam_pipeline_args=beam_pipeline_args)


# To run this pipeline from the python CLI:
#   $python cifar_pipeline_native_keras.py
</source>
<source file="systems/tfx-1.6.1/tfx/examples/tfjs_next_page_prediction/tfjs_next_page_prediction_pipeline.py" startline="71" endline="186" pcid="3018">
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     module_file: str, serving_model_dir: str,
                     metadata_path: str,
                     beam_pipeline_args: List[str]) -> dsl.Pipeline:
  """Implements the page prediction pipline with TFX."""
  input_config = proto.Input(
      splits=[proto.Input.Split(name='input', pattern='*.tfrecord.gz')])
  output_config = proto.Output(
      split_config=proto.SplitConfig(splits=[
          proto.SplitConfig.Split(name='train', hash_buckets=9),
          proto.SplitConfig.Split(name='eval', hash_buckets=1)
      ]))

  # Brings data in to the pipline
  example_gen = ImportExampleGen(
      input_base=data_root,
      input_config=input_config,
      output_config=output_config)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(
      examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that trains a model.
  trainer = Trainer(
      module_file=module_file,
      examples=transform.outputs['transformed_examples'],
      transform_graph=transform.outputs['transform_graph'],
      schema=schema_gen.outputs['schema'],
      train_args=proto.TrainArgs(num_steps=100000),
      eval_args=proto.EvalArgs(num_steps=200))

  # Get the latest blessed model for model validation.
  model_resolver = dsl.Resolver(
      strategy_class=dsl.experimental.LatestBlessedModelStrategy,
      model=dsl.Channel(type=types.standard_artifacts.Model),
      model_blessing=dsl.Channel(
          type=types.standard_artifacts.ModelBlessing)).with_id(
              'latest_blessed_model_resolver')

  # Uses TFMA to compute evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      # Directly evaluates the tfjs model.
      model_specs=[tfma.ModelSpec(label_key='label', model_type='tf_js')],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='SparseCategoricalAccuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          # Increase this threshold when training on complete
                          # dataset.
                          lower_bound={'value': 0.01}),
                      # Change threshold will be ignored if there is no
                      # baseline model resolved from MLMD (first run).
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-2})))
          ])
      ])

  evaluator = Evaluator(
      examples=transform.outputs['transformed_examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=proto.PushDestination(
          filesystem=proto.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  components = [
      example_gen,
      statistics_gen,
      schema_gen,
      example_validator,
      transform,
      trainer,
      model_resolver,
      evaluator,
      pusher,
  ]
  return dsl.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=components,
      metadata_connection_config=orchestration.metadata
      .sqlite_metadata_connection_config(metadata_path),
      enable_cache=True,
      beam_pipeline_args=beam_pipeline_args)


# To run this pipeline from the python CLI:
# $python imdb_pipeline_native_keras.py
</source>
</class>

<class classid="82" nclones="2" nlines="14" similarity="86">
<source file="systems/tfx-1.6.1/tfx/orchestration/experimental/interactive/standard_visualizations.py" startline="35" endline="51" pcid="1602">
  def display(self, artifact: types.Artifact):
    from IPython.core.display import display  # pylint: disable=g-import-not-at-top
    from IPython.core.display import HTML  # pylint: disable=g-import-not-at-top
    for split in artifact_utils.decode_split_names(artifact.split_names):
      display(HTML('<div><b>%r split:</b></div><br/>' % split))
      anomalies_path = io_utils.get_only_uri_in_dir(
          artifact_utils.get_split_uri([artifact], split))
      if artifact_utils.is_artifact_version_older_than(
          artifact, artifact_utils._ARTIFACT_VERSION_FOR_ANOMALIES_UPDATE):  # pylint: disable=protected-access
        anomalies = tfdv.load_anomalies_text(anomalies_path)
      else:
        anomalies = anomalies_pb2.Anomalies()
        anomalies_bytes = io_utils.read_bytes_file(anomalies_path)
        anomalies.ParseFromString(anomalies_bytes)
      tfdv.display_anomalies(anomalies)


</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/experimental/interactive/standard_visualizations.py" startline="57" endline="71" pcid="1603">
  def display(self, artifact: types.Artifact):
    from IPython.core.display import display  # pylint: disable=g-import-not-at-top
    from IPython.core.display import HTML  # pylint: disable=g-import-not-at-top
    for split in artifact_utils.decode_split_names(artifact.split_names):
      display(HTML('<div><b>%r split:</b></div><br/>' % split))
      stats_path = io_utils.get_only_uri_in_dir(
          artifact_utils.get_split_uri([artifact], split))
      if artifact_utils.is_artifact_version_older_than(
          artifact, artifact_utils._ARTIFACT_VERSION_FOR_STATS_UPDATE):  # pylint: disable=protected-access
        stats = tfdv.load_statistics(stats_path)
      else:
        stats = tfdv.load_stats_binary(stats_path)
      tfdv.visualize_statistics(stats)


</source>
</class>

<class classid="83" nclones="2" nlines="22" similarity="70">
<source file="systems/tfx-1.6.1/tfx/orchestration/experimental/interactive/interactive_context_test.py" startline="100" endline="126" pcid="1631">
  def testBasicRun(self):

    class _FakeComponentSpec(types.ComponentSpec):
      PARAMETERS = {}
      INPUTS = {}
      OUTPUTS = {}

    class _FakeExecutor(base_executor.BaseExecutor):
      CALLED = False

      def Do(self, input_dict: Dict[str, List[types.Artifact]],
             output_dict: Dict[str, List[types.Artifact]],
             exec_properties: Dict[str, Any]) -> None:
        _FakeExecutor.CALLED = True

    class _FakeComponent(base_component.BaseComponent):
      SPEC_CLASS = _FakeComponentSpec
      EXECUTOR_SPEC = executor_spec.ExecutorClassSpec(_FakeExecutor)

      def __init__(self, spec: types.ComponentSpec):
        super().__init__(spec=spec)

    c = interactive_context.InteractiveContext()
    component = _FakeComponent(_FakeComponentSpec())
    c.run(component)
    self.assertTrue(_FakeExecutor.CALLED)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/experimental/interactive/interactive_context_test.py" startline="133" endline="164" pcid="1635">
  def testUnresolvedChannel(self):

    class _FakeComponentSpec(types.ComponentSpec):
      PARAMETERS = {}
      INPUTS = {
          'input':
              component_spec.ChannelParameter(type=standard_artifacts.Examples)
      }
      OUTPUTS = {}

    class _FakeExecutor(base_executor.BaseExecutor):
      CALLED = False

      def Do(self, input_dict: Dict[str, List[types.Artifact]],
             output_dict: Dict[str, List[types.Artifact]],
             exec_properties: Dict[str, Any]) -> None:
        _FakeExecutor.CALLED = True

    class _FakeComponent(base_component.BaseComponent):
      SPEC_CLASS = _FakeComponentSpec
      EXECUTOR_SPEC = executor_spec.ExecutorClassSpec(_FakeExecutor)

      def __init__(self, spec: types.ComponentSpec):
        super().__init__(spec=spec)

    c = interactive_context.InteractiveContext()
    foo = types.Channel(type=standard_artifacts.Examples).set_artifacts(
        [standard_artifacts.Examples()])
    component = _FakeComponent(_FakeComponentSpec(input=foo))
    with self.assertRaisesRegex(ValueError, 'Unresolved input channel'):
      c.run(component)

</source>
</class>

<class classid="84" nclones="3" nlines="13" similarity="75">
<source file="systems/tfx-1.6.1/tfx/orchestration/experimental/core/mlmd_state_test.py" startline="80" endline="91" pcid="1660">
  def setUp(self):
    super().setUp()
    pipeline_root = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self.id())
    metadata_path = os.path.join(pipeline_root, 'metadata', 'metadata.db')
    connection_config = metadata.sqlite_metadata_connection_config(
        metadata_path)
    connection_config.sqlite.SetInParent()
    self._mlmd_connection = metadata.Metadata(
        connection_config=connection_config)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/experimental/core/task_schedulers/manual_task_scheduler_test.py" startline="39" endline="55" pcid="1757">
  def setUp(self):
    super().setUp()

    pipeline_root = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self.id())

    metadata_path = os.path.join(pipeline_root, 'metadata', 'metadata.db')
    connection_config = metadata.sqlite_metadata_connection_config(
        metadata_path)
    connection_config.sqlite.SetInParent()
    self._mlmd_connection = metadata.Metadata(
        connection_config=connection_config)

    self._pipeline = self._make_pipeline(pipeline_root, str(uuid.uuid4()))
    self._manual_node = self._pipeline.nodes[0].pipeline_node

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/experimental/core/task_schedulers/resolver_task_scheduler_test.py" startline="38" endline="57" pcid="1762">
  def setUp(self):
    super().setUp()

    pipeline_root = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self.id())

    metadata_path = os.path.join(pipeline_root, 'metadata', 'metadata.db')
    connection_config = metadata.sqlite_metadata_connection_config(
        metadata_path)
    connection_config.sqlite.SetInParent()
    self._mlmd_connection = metadata.Metadata(
        connection_config=connection_config)

    pipeline = self._make_pipeline(pipeline_root, str(uuid.uuid4()))
    self._pipeline = pipeline
    self._trainer = self._pipeline.nodes[0].pipeline_node
    self._resolver_node = self._pipeline.nodes[1].pipeline_node
    self._consumer_node = self._pipeline.nodes[2].pipeline_node

</source>
</class>

<class classid="85" nclones="2" nlines="37" similarity="76">
<source file="systems/tfx-1.6.1/tfx/orchestration/experimental/core/async_pipeline_task_gen_test.py" startline="382" endline="426" pcid="1687">
  def test_triggering_upon_exec_properties_change(self):
    test_utils.fake_example_gen_run(self._mlmd_connection, self._example_gen, 1,
                                    1)

    [exec_transform_task] = self._generate_and_test(
        False,
        num_initial_executions=1,
        num_tasks_generated=1,
        num_new_executions=1,
        num_active_executions=1,
        expected_exec_nodes=[self._transform],
        ignore_update_node_state_tasks=True)

    # Fail the registered execution.
    with self._mlmd_connection as m:
      with mlmd_state.mlmd_execution_atomic_op(
          m, exec_transform_task.execution_id) as execution:
        execution.last_known_state = metadata_store_pb2.Execution.FAILED

    # Try to generate with same execution properties. This should not trigger
    # as there are no changes since last run.
    self._generate_and_test(
        False,
        num_initial_executions=2,
        num_tasks_generated=0,
        num_new_executions=0,
        num_active_executions=0,
        ignore_update_node_state_tasks=True)

    # Change execution properties of last run.
    with self._mlmd_connection as m:
      with mlmd_state.mlmd_execution_atomic_op(
          m, exec_transform_task.execution_id) as execution:
        execution.custom_properties['a_param'].int_value = 20

    # Generating with different execution properties should trigger.
    self._generate_and_test(
        False,
        num_initial_executions=2,
        num_tasks_generated=1,
        num_new_executions=1,
        num_active_executions=1,
        expected_exec_nodes=[self._transform],
        ignore_update_node_state_tasks=True)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/experimental/core/async_pipeline_task_gen_test.py" startline="427" endline="475" pcid="1688">
  def test_triggering_upon_executor_spec_change(self):
    test_utils.fake_example_gen_run(self._mlmd_connection, self._example_gen, 1,
                                    1)

    with mock.patch.object(task_gen_utils,
                           'get_executor_spec') as mock_get_executor_spec:
      mock_get_executor_spec.side_effect = _fake_executor_spec(1)
      [exec_transform_task] = self._generate_and_test(
          False,
          num_initial_executions=1,
          num_tasks_generated=1,
          num_new_executions=1,
          num_active_executions=1,
          expected_exec_nodes=[self._transform],
          ignore_update_node_state_tasks=True)

    # Fail the registered execution.
    with self._mlmd_connection as m:
      with mlmd_state.mlmd_execution_atomic_op(
          m, exec_transform_task.execution_id) as execution:
        execution.last_known_state = metadata_store_pb2.Execution.FAILED

    # Try to generate with same executor spec. This should not trigger as
    # there are no changes since last run.
    with mock.patch.object(task_gen_utils,
                           'get_executor_spec') as mock_get_executor_spec:
      mock_get_executor_spec.side_effect = _fake_executor_spec(1)
      self._generate_and_test(
          False,
          num_initial_executions=2,
          num_tasks_generated=0,
          num_new_executions=0,
          num_active_executions=0,
          ignore_update_node_state_tasks=True)

    # Generating with a different executor spec should trigger.
    with mock.patch.object(task_gen_utils,
                           'get_executor_spec') as mock_get_executor_spec:
      mock_get_executor_spec.side_effect = _fake_executor_spec(2)
      self._generate_and_test(
          False,
          num_initial_executions=2,
          num_tasks_generated=1,
          num_new_executions=1,
          num_active_executions=1,
          expected_exec_nodes=[self._transform],
          ignore_update_node_state_tasks=True)


</source>
</class>

<class classid="86" nclones="2" nlines="39" similarity="74">
<source file="systems/tfx-1.6.1/tfx/orchestration/experimental/core/pipeline_ops_test.py" startline="178" endline="225" pcid="1713">
  def test_initiate_pipeline_start_with_partial_run(self, mock_snapshot):
    with self._mlmd_connection as m:
      pipeline = _test_pipeline('test_pipeline', pipeline_pb2.Pipeline.SYNC)
      node_example_gen = pipeline.nodes.add().pipeline_node
      node_example_gen.node_info.id = 'ExampleGen'
      node_example_gen.downstream_nodes.extend(['Transform'])
      node_transform = pipeline.nodes.add().pipeline_node
      node_transform.node_info.id = 'Transform'
      node_transform.upstream_nodes.extend(['ExampleGen'])
      node_transform.downstream_nodes.extend(['Trainer'])
      node_trainer = pipeline.nodes.add().pipeline_node
      node_trainer.node_info.id = 'Trainer'
      node_trainer.upstream_nodes.extend(['Transform'])

      latest_pipeline_snapshot_settings = pipeline_pb2.SnapshotSettings()
      latest_pipeline_snapshot_settings.latest_pipeline_run_strategy.SetInParent(
      )

      incorrect_partial_run_option = pipeline_pb2.PartialRun(
          from_nodes=['InvalidaNode'],
          to_nodes=['Trainer'],
          snapshot_settings=latest_pipeline_snapshot_settings)
      with self.assertRaisesRegex(
          status_lib.StatusNotOkError,
          'specified in from_nodes/to_nodes are not present in the pipeline.'):
        pipeline_ops.initiate_pipeline_start(
            m, pipeline, partial_run_option=incorrect_partial_run_option)

      expected_pipeline = copy.deepcopy(pipeline)
      expected_pipeline.runtime_spec.snapshot_settings.latest_pipeline_run_strategy.SetInParent(
      )
      expected_pipeline.nodes[
          0].pipeline_node.execution_options.skip.reuse_artifacts = True
      expected_pipeline.nodes[
          1].pipeline_node.execution_options.run.perform_snapshot = True
      expected_pipeline.nodes[
          1].pipeline_node.execution_options.run.depends_on_snapshot = True
      expected_pipeline.nodes[
          2].pipeline_node.execution_options.run.SetInParent()

      partial_run_option = pipeline_pb2.PartialRun(
          from_nodes=['Transform'],
          to_nodes=['Trainer'],
          snapshot_settings=latest_pipeline_snapshot_settings)
      with pipeline_ops.initiate_pipeline_start(
          m, pipeline, partial_run_option=partial_run_option) as pipeline_state:
        self.assertEqual(expected_pipeline, pipeline_state.pipeline)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/experimental/core/pipeline_ops_test.py" startline="227" endline="264" pcid="1714">
  def test_initiate_pipeline_start_with_partial_run_default_to_nodes(
      self, mock_snapshot):
    with self._mlmd_connection as m:
      pipeline = _test_pipeline('test_pipeline', pipeline_pb2.Pipeline.SYNC)
      node_example_gen = pipeline.nodes.add().pipeline_node
      node_example_gen.node_info.id = 'ExampleGen'
      node_example_gen.downstream_nodes.extend(['Transform'])
      node_transform = pipeline.nodes.add().pipeline_node
      node_transform.node_info.id = 'Transform'
      node_transform.upstream_nodes.extend(['ExampleGen'])
      node_transform.downstream_nodes.extend(['Trainer'])
      node_trainer = pipeline.nodes.add().pipeline_node
      node_trainer.node_info.id = 'Trainer'
      node_trainer.upstream_nodes.extend(['Transform'])

      latest_pipeline_snapshot_settings = pipeline_pb2.SnapshotSettings()
      latest_pipeline_snapshot_settings.latest_pipeline_run_strategy.SetInParent(
      )

      expected_pipeline = copy.deepcopy(pipeline)
      expected_pipeline.runtime_spec.snapshot_settings.latest_pipeline_run_strategy.SetInParent(
      )
      expected_pipeline.nodes[
          0].pipeline_node.execution_options.skip.reuse_artifacts = True
      expected_pipeline.nodes[
          1].pipeline_node.execution_options.run.perform_snapshot = True
      expected_pipeline.nodes[
          1].pipeline_node.execution_options.run.depends_on_snapshot = True
      expected_pipeline.nodes[
          2].pipeline_node.execution_options.run.SetInParent()

      partial_run_option = pipeline_pb2.PartialRun(
          from_nodes=['Transform'],
          snapshot_settings=latest_pipeline_snapshot_settings)
      with pipeline_ops.initiate_pipeline_start(
          m, pipeline, partial_run_option=partial_run_option) as pipeline_state:
        self.assertEqual(expected_pipeline, pipeline_state.pipeline)

</source>
</class>

<class classid="87" nclones="2" nlines="43" similarity="77">
<source file="systems/tfx-1.6.1/tfx/orchestration/experimental/core/pipeline_ops_test.py" startline="863" endline="920" pcid="1734">
  def test_executor_node_stop_then_start_flow(self, pipeline,
                                              mock_async_task_gen,
                                              mock_sync_task_gen):
    service_job_manager = service_jobs.DummyServiceJobManager()
    with self._mlmd_connection as m:
      pipeline_uid = task_lib.PipelineUid.from_pipeline(pipeline)
      pipeline.nodes.add().pipeline_node.node_info.id = 'Trainer'
      trainer_node_uid = task_lib.NodeUid.from_pipeline_node(
          pipeline, pipeline.nodes[0].pipeline_node)

      # Start pipeline and stop trainer.
      pipeline_ops.initiate_pipeline_start(m, pipeline)
      with pstate.PipelineState.load(m, pipeline_uid) as pipeline_state:
        with pipeline_state.node_state_update_context(
            trainer_node_uid) as node_state:
          node_state.update(pstate.NodeState.STOPPING,
                            status_lib.Status(code=status_lib.Code.CANCELLED))

      task_queue = tq.TaskQueue()

      # Simulate ExecNodeTask for trainer already present in the task queue.
      trainer_task = test_utils.create_exec_node_task(node_uid=trainer_node_uid)
      task_queue.enqueue(trainer_task)

      pipeline_ops.orchestrate(m, task_queue, service_job_manager)

      # Dequeue pre-existing trainer task.
      task = task_queue.dequeue()
      task_queue.task_done(task)
      self.assertEqual(trainer_task, task)

      # Dequeue CancelNodeTask for trainer.
      task = task_queue.dequeue()
      task_queue.task_done(task)
      self.assertTrue(task_lib.is_cancel_node_task(task))
      self.assertEqual(trainer_node_uid, task.node_uid)

      self.assertTrue(task_queue.is_empty())

      with pstate.PipelineState.load(m, pipeline_uid) as pipeline_state:
        node_state = pipeline_state.get_node_state(trainer_node_uid)
        self.assertEqual(pstate.NodeState.STOPPING, node_state.state)
        self.assertEqual(status_lib.Code.CANCELLED, node_state.status.code)

      pipeline_ops.orchestrate(m, task_queue, service_job_manager)

      with pstate.PipelineState.load(m, pipeline_uid) as pipeline_state:
        node_state = pipeline_state.get_node_state(trainer_node_uid)
        self.assertEqual(pstate.NodeState.STOPPED, node_state.state)
        self.assertEqual(status_lib.Code.CANCELLED, node_state.status.code)

      pipeline_ops.initiate_node_start(m, trainer_node_uid)
      pipeline_ops.orchestrate(m, task_queue, service_job_manager)

      with pstate.PipelineState.load(m, pipeline_uid) as pipeline_state:
        node_state = pipeline_state.get_node_state(trainer_node_uid)
        self.assertEqual(pstate.NodeState.STARTED, node_state.state)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/experimental/core/pipeline_ops_test.py" startline="970" endline="1036" pcid="1736">
  def test_mixed_service_node_stop_then_start_flow(self, pipeline,
                                                   mock_async_task_gen,
                                                   mock_sync_task_gen):
    with self._mlmd_connection as m:
      pipeline_uid = task_lib.PipelineUid.from_pipeline(pipeline)
      pipeline.nodes.add().pipeline_node.node_info.id = 'Transform'

      transform_node_uid = task_lib.NodeUid.from_pipeline_node(
          pipeline, pipeline.nodes[0].pipeline_node)

      pipeline_ops.initiate_pipeline_start(m, pipeline)
      with pstate.PipelineState.load(
          m, task_lib.PipelineUid.from_pipeline(pipeline)) as pipeline_state:
        # Stop Transform.
        with pipeline_state.node_state_update_context(
            transform_node_uid) as node_state:
          node_state.update(pstate.NodeState.STOPPING,
                            status_lib.Status(code=status_lib.Code.CANCELLED))

      task_queue = tq.TaskQueue()

      # Simulate ExecNodeTask for Transform already present in the task queue.
      transform_task = test_utils.create_exec_node_task(
          node_uid=transform_node_uid)
      task_queue.enqueue(transform_task)

      pipeline_ops.orchestrate(m, task_queue, self._mock_service_job_manager)

      # stop_node_services should not be called as there was an active
      # ExecNodeTask for Transform which is a mixed service node.
      self._mock_service_job_manager.stop_node_services.assert_not_called()

      # Dequeue pre-existing transform task.
      task = task_queue.dequeue()
      task_queue.task_done(task)
      self.assertEqual(transform_task, task)

      # Dequeue CancelNodeTask for transform.
      task = task_queue.dequeue()
      task_queue.task_done(task)
      self.assertTrue(task_lib.is_cancel_node_task(task))
      self.assertEqual(transform_node_uid, task.node_uid)

      with pstate.PipelineState.load(m, pipeline_uid) as pipeline_state:
        node_state = pipeline_state.get_node_state(transform_node_uid)
        self.assertEqual(pstate.NodeState.STOPPING, node_state.state)
        self.assertEqual(status_lib.Code.CANCELLED, node_state.status.code)

      pipeline_ops.orchestrate(m, task_queue, self._mock_service_job_manager)

      # stop_node_services should be called for Transform which is a mixed
      # service node and corresponding ExecNodeTask has been dequeued.
      self._mock_service_job_manager.stop_node_services.assert_called_once_with(
          mock.ANY, 'Transform')

      with pstate.PipelineState.load(m, pipeline_uid) as pipeline_state:
        node_state = pipeline_state.get_node_state(transform_node_uid)
        self.assertEqual(pstate.NodeState.STOPPED, node_state.state)
        self.assertEqual(status_lib.Code.CANCELLED, node_state.status.code)

      pipeline_ops.initiate_node_start(m, transform_node_uid)
      pipeline_ops.orchestrate(m, task_queue, self._mock_service_job_manager)

      with pstate.PipelineState.load(m, pipeline_uid) as pipeline_state:
        node_state = pipeline_state.get_node_state(transform_node_uid)
        self.assertEqual(pstate.NodeState.STARTED, node_state.state)

</source>
</class>

<class classid="88" nclones="2" nlines="11" similarity="81">
<source file="systems/tfx-1.6.1/tfx/orchestration/experimental/core/task_scheduler_test.py" startline="58" endline="71" pcid="1781">
  def test_register_using_executor_spec_type_url(self):
    # Register a fake task scheduler.
    ts.TaskSchedulerRegistry.register(self._spec_type_url, _FakeTaskScheduler)

    # Create a task and verify that the correct scheduler is instantiated.
    task = test_utils.create_exec_node_task(
        node_uid=task_lib.NodeUid(
            pipeline_uid=task_lib.PipelineUid(pipeline_id='pipeline'),
            node_id='Trainer'),
        pipeline=self._pipeline)
    task_scheduler = ts.TaskSchedulerRegistry.create_task_scheduler(
        mock.Mock(), self._pipeline, task)
    self.assertIsInstance(task_scheduler, _FakeTaskScheduler)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/experimental/core/task_scheduler_test.py" startline="72" endline="86" pcid="1782">
  def test_register_using_node_type_name(self):
    # Register a fake task scheduler.
    ts.TaskSchedulerRegistry.register(constants.IMPORTER_NODE_TYPE,
                                      _FakeTaskScheduler)

    # Create a task and verify that the correct scheduler is instantiated.
    task = test_utils.create_exec_node_task(
        node_uid=task_lib.NodeUid(
            pipeline_uid=task_lib.PipelineUid(pipeline_id='pipeline'),
            node_id='Importer'),
        pipeline=self._pipeline)
    task_scheduler = ts.TaskSchedulerRegistry.create_task_scheduler(
        mock.Mock(), self._pipeline, task)
    self.assertIsInstance(task_scheduler, _FakeTaskScheduler)

</source>
</class>

<class classid="89" nclones="3" nlines="30" similarity="83">
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/execution_publish_utils_test.py" startline="55" endline="106" pcid="1861">
  def testRegisterExecution(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      contexts = self._generate_contexts(m)
      input_example = standard_artifacts.Examples()
      execution_publish_utils.register_execution(
          m,
          self._execution_type,
          contexts,
          input_artifacts={'examples': [input_example]},
          exec_properties={
              'p1': 1,
          })
      [execution] = m.store.get_executions()
      self.assertProtoPartiallyEquals(
          """
          id: 1
          custom_properties {
            key: 'p1'
            value {int_value: 1}
          }
          last_known_state: RUNNING
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])
      [event] = m.store.get_events_by_execution_ids([execution.id])
      self.assertProtoPartiallyEquals(
          """
          artifact_id: 1
          execution_id: 1
          path {
            steps {
              key: 'examples'
            }
            steps {
              index: 0
            }
          }
          type: INPUT
          """,
          event,
          ignored_fields=['milliseconds_since_epoch'])
      # Verifies the context-execution edges are set up.
      self.assertCountEqual(
          [c.id for c in contexts],
          [c.id for c in m.store.get_contexts_by_execution(execution.id)])
      self.assertCountEqual(
          [c.id for c in contexts],
          [c.id for c in m.store.get_contexts_by_artifact(input_example.id)])

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/execution_publish_utils_test.py" startline="107" endline="153" pcid="1862">
  def testPublishCachedExecution(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      contexts = self._generate_contexts(m)
      execution_id = execution_publish_utils.register_execution(
          m, self._execution_type, contexts).id
      output_example = standard_artifacts.Examples()
      execution_publish_utils.publish_cached_execution(
          m,
          contexts,
          execution_id,
          output_artifacts={'examples': [output_example]})
      [execution] = m.store.get_executions()
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: CACHED
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])
      [event] = m.store.get_events_by_execution_ids([execution.id])
      self.assertProtoPartiallyEquals(
          """
          artifact_id: 1
          execution_id: 1
          path {
            steps {
              key: 'examples'
            }
            steps {
              index: 0
            }
          }
          type: OUTPUT
          """,
          event,
          ignored_fields=['milliseconds_since_epoch'])
      # Verifies the context-execution edges are set up.
      self.assertCountEqual(
          [c.id for c in contexts],
          [c.id for c in m.store.get_contexts_by_execution(execution.id)])
      self.assertCountEqual(
          [c.id for c in contexts],
          [c.id for c in m.store.get_contexts_by_artifact(output_example.id)])

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/execution_publish_utils_test.py" startline="559" endline="606" pcid="1872">
  def testPublishInternalExecution(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      contexts = self._generate_contexts(m)
      execution_id = execution_publish_utils.register_execution(
          m, self._execution_type, contexts).id
      output_example = standard_artifacts.Examples()
      execution_publish_utils.publish_internal_execution(
          m,
          contexts,
          execution_id,
          output_artifacts={'examples': [output_example]})
      [execution] = m.store.get_executions()
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: COMPLETE
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])
      [event] = m.store.get_events_by_execution_ids([execution.id])
      self.assertProtoPartiallyEquals(
          """
          artifact_id: 1
          execution_id: 1
          path {
            steps {
              key: 'examples'
            }
            steps {
              index: 0
            }
          }
          type: INTERNAL_OUTPUT
          """,
          event,
          ignored_fields=['milliseconds_since_epoch'])
      # Verifies the context-execution edges are set up.
      self.assertCountEqual(
          [c.id for c in contexts],
          [c.id for c in m.store.get_contexts_by_execution(execution.id)])
      self.assertCountEqual(
          [c.id for c in contexts],
          [c.id for c in m.store.get_contexts_by_artifact(output_example.id)])


</source>
</class>

<class classid="90" nclones="2" nlines="11" similarity="81">
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/execution_publish_utils_test.py" startline="227" endline="239" pcid="1864">
  def testPublishSuccessExecutionFailNewKey(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      contexts = self._generate_contexts(m)
      execution_id = execution_publish_utils.register_execution(
          m, self._execution_type, contexts).id
      executor_output = execution_result_pb2.ExecutorOutput()
      executor_output.output_artifacts['new_key'].artifacts.add()

      with self.assertRaisesRegex(RuntimeError, 'contains more keys'):
        execution_publish_utils.publish_succeeded_execution(
            m, execution_id, contexts,
            {'examples': [standard_artifacts.Examples()]}, executor_output)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/execution_publish_utils_test.py" startline="359" endline="371" pcid="1866">
  def testPublishSuccessExecutionFailChangedType(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      contexts = self._generate_contexts(m)
      execution_id = execution_publish_utils.register_execution(
          m, self._execution_type, contexts).id
      executor_output = execution_result_pb2.ExecutorOutput()
      executor_output.output_artifacts['examples'].artifacts.add().type_id = 10

      with self.assertRaisesRegex(RuntimeError, 'change artifact type'):
        execution_publish_utils.publish_succeeded_execution(
            m, execution_id, contexts,
            {'examples': [standard_artifacts.Examples(),]}, executor_output)

</source>
</class>

<class classid="91" nclones="4" nlines="18" similarity="77">
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/execution_publish_utils_test.py" startline="407" endline="452" pcid="1868">
  def testPublishSuccessExecutionUpdatesCustomProperties(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      executor_output = text_format.Parse(
          """
          execution_properties {
          key: "int"
          value {
            int_value: 1
          }
          }
          execution_properties {
            key: "string"
            value {
              string_value: "string_value"
            }
          }
           """, execution_result_pb2.ExecutorOutput())
      contexts = self._generate_contexts(m)
      execution_id = execution_publish_utils.register_execution(
          m, self._execution_type, contexts).id
      execution_publish_utils.publish_succeeded_execution(
          m, execution_id, contexts, {}, executor_output)
      [execution] = m.store.get_executions_by_id([execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: COMPLETE
          custom_properties {
            key: "int"
            value {
              int_value: 1
            }
          }
          custom_properties {
            key: "string"
            value {
              string_value: "string_value"
            }
          }
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/execution_publish_utils_test.py" startline="493" endline="518" pcid="1870">
  def testPublishSuccessExecutionDropsEmptyResult(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      executor_output = text_format.Parse(
          """
        execution_result {
          code: 0
         }
      """, execution_result_pb2.ExecutorOutput())
      contexts = self._generate_contexts(m)
      execution_id = execution_publish_utils.register_execution(
          m, self._execution_type, contexts).id
      execution_publish_utils.publish_failed_execution(m, contexts,
                                                       execution_id,
                                                       executor_output)
      [execution] = m.store.get_executions_by_id([execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: FAILED
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/execution_publish_utils_test.py" startline="519" endline="558" pcid="1871">
  def testPublishFailedExecution(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      executor_output = text_format.Parse(
          """
        execution_result {
          code: 1
          result_message: 'error message.'
         }
      """, execution_result_pb2.ExecutorOutput())
      contexts = self._generate_contexts(m)
      execution_id = execution_publish_utils.register_execution(
          m, self._execution_type, contexts).id
      execution_publish_utils.publish_failed_execution(m, contexts,
                                                       execution_id,
                                                       executor_output)
      [execution] = m.store.get_executions_by_id([execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: FAILED
          custom_properties {
            key: '__execution_result__'
            value {
              string_value: '{\\n  "resultMessage": "error message.",\\n  "code": 1\\n}'
            }
          }
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])
      # No events because there is no artifact published.
      events = m.store.get_events_by_execution_ids([execution.id])
      self.assertEmpty(events)
      # Verifies the context-execution edges are set up.
      self.assertCountEqual(
          [c.id for c in contexts],
          [c.id for c in m.store.get_contexts_by_execution(execution.id)])

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/execution_publish_utils_test.py" startline="453" endline="492" pcid="1869">
  def testPublishSuccessExecutionRecordExecutionResult(self):
    with metadata.Metadata(connection_config=self._connection_config) as m:
      executor_output = text_format.Parse(
          """
        execution_result {
          code: 0
          result_message: 'info message.'
         }
      """, execution_result_pb2.ExecutorOutput())
      contexts = self._generate_contexts(m)
      execution_id = execution_publish_utils.register_execution(
          m, self._execution_type, contexts).id
      execution_publish_utils.publish_failed_execution(m, contexts,
                                                       execution_id,
                                                       executor_output)
      [execution] = m.store.get_executions_by_id([execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: FAILED
          custom_properties {
            key: '__execution_result__'
            value {
              string_value: '{\\n  "resultMessage": "info message."\\n}'
            }
          }
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])
      # No events because there is no artifact published.
      events = m.store.get_events_by_execution_ids([execution.id])
      self.assertEmpty(events)
      # Verifies the context-execution edges are set up.
      self.assertCountEqual(
          [c.id for c in contexts],
          [c.id for c in m.store.get_contexts_by_execution(execution.id)])

</source>
</class>

<class classid="92" nclones="2" nlines="15" similarity="73">
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/docker_executor_operator_test.py" startline="103" endline="117" pcid="1873">
  def _set_up_test_execution_info(self,
                                  input_dict=None,
                                  output_dict=None,
                                  exec_properties=None):
    return data_types.ExecutionInfo(
        input_dict=input_dict or {},
        output_dict=output_dict or {},
        exec_properties=exec_properties or {},
        execution_output_uri='/testing/executor/output/',
        stateful_working_dir='/testing/stateful/dir',
        pipeline_node=pipeline_pb2.PipelineNode(
            node_info=pipeline_pb2.NodeInfo(
                type=metadata_store_pb2.ExecutionType(name='Docker_executor'))),
        pipeline_info=pipeline_pb2.PipelineInfo(id='test_pipeline_id'))

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/kubernetes_executor_operator_test.py" startline="227" endline="242" pcid="1891">
  def _set_up_test_execution_info(self,
                                  input_dict=None,
                                  output_dict=None,
                                  exec_properties=None):
    return data_types.ExecutionInfo(
        execution_id=123,
        input_dict=input_dict or {},
        output_dict=output_dict or {},
        exec_properties=exec_properties or {},
        execution_output_uri='/testing/executor/output/',
        stateful_working_dir='/testing/stateful/dir',
        pipeline_node=pipeline_pb2.PipelineNode(
            node_info=pipeline_pb2.NodeInfo(id='fakecomponent-fakecomponent')),
        pipeline_info=pipeline_pb2.PipelineInfo(id='Test'),
        pipeline_run_id='123')

</source>
</class>

<class classid="93" nclones="2" nlines="11" similarity="100">
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/python_executor_operator_test.py" startline="91" endline="117" pcid="1904">
  def testRunExecutor_with_InprocessExecutor(self):
    executor_sepc = text_format.Parse(
        """
      class_path: "tfx.orchestration.portable.python_executor_operator_test.InprocessExecutor"
    """, executable_spec_pb2.PythonClassExecutableSpec())
    operator = python_executor_operator.PythonExecutorOperator(executor_sepc)
    input_dict = {'input_key': [standard_artifacts.Examples()]}
    output_dict = {'output_key': [standard_artifacts.Model()]}
    exec_properties = {'key': 'value'}
    executor_output = operator.run_executor(
        self._get_execution_info(input_dict, output_dict, exec_properties))
    self.assertProtoPartiallyEquals(
        """
          execution_properties {
            key: "key"
            value {
              string_value: "value"
            }
          }
          output_artifacts {
            key: "output_key"
            value {
              artifacts {
              }
            }
          }""", executor_output)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/python_executor_operator_test.py" startline="118" endline="144" pcid="1905">
  def testRunExecutor_with_NotInprocessExecutor(self):
    executor_sepc = text_format.Parse(
        """
      class_path: "tfx.orchestration.portable.python_executor_operator_test.NotInprocessExecutor"
    """, executable_spec_pb2.PythonClassExecutableSpec())
    operator = python_executor_operator.PythonExecutorOperator(executor_sepc)
    input_dict = {'input_key': [standard_artifacts.Examples()]}
    output_dict = {'output_key': [standard_artifacts.Model()]}
    exec_properties = {'key': 'value'}
    executor_output = operator.run_executor(
        self._get_execution_info(input_dict, output_dict, exec_properties))
    self.assertProtoPartiallyEquals(
        """
          execution_properties {
            key: "key"
            value {
              string_value: "value"
            }
          }
          output_artifacts {
            key: "output_key"
            value {
              artifacts {
              }
            }
          }""", executor_output)

</source>
</class>

<class classid="94" nclones="2" nlines="26" similarity="88">
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/resolver_node_handler_test.py" startline="37" endline="71" pcid="1911">
  def setUp(self):
    super().setUp()
    pipeline_root = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self.id())

    # Makes sure multiple connections within a test always connect to the same
    # MLMD instance.
    metadata_path = os.path.join(pipeline_root, 'metadata', 'metadata.db')
    connection_config = metadata.sqlite_metadata_connection_config(
        metadata_path)
    connection_config.sqlite.SetInParent()
    self._mlmd_connection = metadata.Metadata(
        connection_config=connection_config)
    self._testdata_dir = os.path.join(os.path.dirname(__file__), 'testdata')

    # Sets up pipelines
    pipeline = pipeline_pb2.Pipeline()
    self.load_proto_from_text(
        os.path.join(
            os.path.dirname(__file__), 'testdata',
            'pipeline_for_resolver_test.pbtxt'), pipeline)
    self._pipeline_info = pipeline.pipeline_info
    self._pipeline_runtime_spec = pipeline.runtime_spec
    runtime_parameter_utils.substitute_runtime_parameter(
        pipeline, {
            constants.PIPELINE_RUN_ID_PARAMETER_NAME: 'my_pipeline_run',
        })

    # Extracts components
    self._my_trainer = pipeline.nodes[0].pipeline_node
    self._my_resolver = pipeline.nodes[1].pipeline_node
    self._model_type = (
        self._my_trainer.outputs.outputs['model'].artifact_spec.type)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/importer_node_handler_test.py" startline="29" endline="62" pcid="1919">
  def setUp(self):
    super().setUp()
    pipeline_root = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self.id())

    # Makes sure multiple connections within a test always connect to the same
    # MLMD instance.
    metadata_path = os.path.join(pipeline_root, 'metadata', 'metadata.db')
    connection_config = metadata.sqlite_metadata_connection_config(
        metadata_path)
    connection_config.sqlite.SetInParent()
    self._mlmd_connection = metadata.Metadata(
        connection_config=connection_config)
    self._testdata_dir = os.path.join(os.path.dirname(__file__), 'testdata')

    # Sets up pipelines
    pipeline = pipeline_pb2.Pipeline()
    self.load_proto_from_text(
        os.path.join(
            os.path.dirname(__file__), 'testdata',
            'pipeline_for_launcher_test.pbtxt'), pipeline)
    self._pipeline_info = pipeline.pipeline_info
    self._pipeline_runtime_spec = pipeline.runtime_spec
    runtime_parameter_utils.substitute_runtime_parameter(
        pipeline, {
            constants.PIPELINE_RUN_ID_PARAMETER_NAME: 'my_pipeline_run',
        })

    # Extracts components
    self._importer = pipeline.nodes[3].pipeline_node
    # Fake tfx_version for tests.
    tfx_version.__version__ = '0.123.4.dev'

</source>
</class>

<class classid="95" nclones="2" nlines="19" similarity="80">
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/resolver_node_handler_test.py" startline="173" endline="195" pcid="1914">
  def testRun_InputResolutionError_ExecutionFailed(self, mock_resolve):
    mock_resolve.side_effect = exceptions.InputResolutionError('Meh')
    handler = resolver_node_handler.ResolverNodeHandler()

    execution_info = handler.run(
        mlmd_connection=self._mlmd_connection,
        pipeline_node=self._my_resolver,
        pipeline_info=self._pipeline_info,
        pipeline_runtime_spec=self._pipeline_runtime_spec)

    with self._mlmd_connection as m:
      self.assertTrue(execution_info.execution_id)
      [execution] = m.store.get_executions_by_id([execution_info.execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: FAILED
          """,
          execution,
          ignored_fields=['type_id', 'custom_properties',
                          'create_time_since_epoch',
                          'last_update_time_since_epoch'])

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/resolver_node_handler_test.py" startline="197" endline="223" pcid="1915">
  def testRun_MultipleInputs_ExecutionFailed(self, mock_resolve):
    mock_resolve.return_value = inputs_utils.Trigger([
        {'model': [self._create_model_artifact(uri='/tmp/model/1')]},
        {'model': [self._create_model_artifact(uri='/tmp/model/2')]},
    ])
    handler = resolver_node_handler.ResolverNodeHandler()

    execution_info = handler.run(
        mlmd_connection=self._mlmd_connection,
        pipeline_node=self._my_resolver,
        pipeline_info=self._pipeline_info,
        pipeline_runtime_spec=self._pipeline_runtime_spec)

    with self._mlmd_connection as m:
      self.assertTrue(execution_info.execution_id)
      [execution] = m.store.get_executions_by_id([execution_info.execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: FAILED
          """,
          execution,
          ignored_fields=['type_id', 'custom_properties',
                          'create_time_since_epoch',
                          'last_update_time_since_epoch'])


</source>
</class>

<class classid="96" nclones="2" nlines="40" similarity="83">
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/importer_node_handler_test.py" startline="63" endline="183" pcid="1920">
  def testLauncher_importer_mode_reimport_enabled(self):
    handler = importer_node_handler.ImporterNodeHandler()
    execution_info = handler.run(
        mlmd_connection=self._mlmd_connection,
        pipeline_node=self._importer,
        pipeline_info=self._pipeline_info,
        pipeline_runtime_spec=self._pipeline_runtime_spec)

    with self._mlmd_connection as m:
      [artifact] = m.store.get_artifacts_by_type('Schema')
      self.assertProtoPartiallyEquals(
          """
          id: 1
          uri: "my_url"
          custom_properties {
            key: "int_custom_property"
            value {
              int_value: 123
            }
          }
          custom_properties {
            key: "str_custom_property"
            value {
              string_value: "abc"
            }
          }
          custom_properties {
            key: "tfx_version"
            value {
              string_value: "0.123.4.dev"
            }
          }
          state: LIVE""",
          artifact,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])
      [execution] = m.store.get_executions_by_id([execution_info.execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: COMPLETE
          custom_properties {
            key: "artifact_uri"
            value {
              string_value: "my_url"
            }
          }
          custom_properties {
            key: "reimport"
            value {
              int_value: 1
            }
          }
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])

    execution_info = handler.run(
        mlmd_connection=self._mlmd_connection,
        pipeline_node=self._importer,
        pipeline_info=self._pipeline_info,
        pipeline_runtime_spec=self._pipeline_runtime_spec)
    with self._mlmd_connection as m:
      new_artifact = m.store.get_artifacts_by_type('Schema')[1]
      self.assertProtoPartiallyEquals(
          """
          id: 2
          uri: "my_url"
          custom_properties {
            key: "int_custom_property"
            value {
              int_value: 123
            }
          }
          custom_properties {
            key: "str_custom_property"
            value {
              string_value: "abc"
            }
          }
          custom_properties {
            key: "tfx_version"
            value {
              string_value: "0.123.4.dev"
            }
          }
          state: LIVE""",
          new_artifact,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])
      [execution] = m.store.get_executions_by_id([execution_info.execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 2
          last_known_state: COMPLETE
          custom_properties {
            key: "artifact_uri"
            value {
              string_value: "my_url"
            }
          }
          custom_properties {
            key: "reimport"
            value {
              int_value: 1
            }
          }
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/importer_node_handler_test.py" startline="184" endline="281" pcid="1921">
  def testLauncher_importer_mode_reimport_disabled(self):
    self._importer.parameters.parameters['reimport'].field_value.int_value = 0
    handler = importer_node_handler.ImporterNodeHandler()
    execution_info = handler.run(
        mlmd_connection=self._mlmd_connection,
        pipeline_node=self._importer,
        pipeline_info=self._pipeline_info,
        pipeline_runtime_spec=self._pipeline_runtime_spec)

    with self._mlmd_connection as m:
      [artifact] = m.store.get_artifacts_by_type('Schema')
      self.assertProtoPartiallyEquals(
          """
          id: 1
          uri: "my_url"
          custom_properties {
            key: "int_custom_property"
            value {
              int_value: 123
            }
          }
          custom_properties {
            key: "str_custom_property"
            value {
              string_value: "abc"
            }
          }
          custom_properties {
            key: "tfx_version"
            value {
              string_value: "0.123.4.dev"
            }
          }
          state: LIVE""",
          artifact,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])
      [execution] = m.store.get_executions_by_id([execution_info.execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 1
          last_known_state: COMPLETE
          custom_properties {
            key: "artifact_uri"
            value {
              string_value: "my_url"
            }
          }
          custom_properties {
            key: "reimport"
            value {
              int_value: 0
            }
          }
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])

    # Run the 2nd execution. Since the reimport is disabled, no new schema
    # is imported and the corresponding execution is published as CACHED.
    execution_info = handler.run(
        mlmd_connection=self._mlmd_connection,
        pipeline_node=self._importer,
        pipeline_info=self._pipeline_info,
        pipeline_runtime_spec=self._pipeline_runtime_spec)
    with self._mlmd_connection as m:
      # No new Schema is produced.
      self.assertLen(m.store.get_artifacts_by_type('Schema'), 1)
      [execution] = m.store.get_executions_by_id([execution_info.execution_id])
      self.assertProtoPartiallyEquals(
          """
          id: 2
          last_known_state: CACHED
          custom_properties {
            key: "artifact_uri"
            value {
              string_value: "my_url"
            }
          }
          custom_properties {
            key: "reimport"
            value {
              int_value: 0
            }
          }
          """,
          execution,
          ignored_fields=[
              'type_id', 'create_time_since_epoch',
              'last_update_time_since_epoch'
          ])


</source>
</class>

<class classid="97" nclones="2" nlines="20" similarity="80">
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/runtime_parameter_utils_test.py" startline="34" endline="55" pcid="1924">
  def testFullySubstituteRuntimeParameter(self):
    pipeline = pipeline_pb2.Pipeline()
    expected = pipeline_pb2.Pipeline()
    self.load_proto_from_text(
        os.path.join(self._testdata_dir,
                     'pipeline_with_runtime_parameter.pbtxt'), pipeline)
    self.load_proto_from_text(
        os.path.join(self._testdata_dir,
                     'pipeline_with_runtime_parameter_substituted.pbtxt'),
        expected)
    parameters = runtime_parameter_utils.substitute_runtime_parameter(
        pipeline, {
            'context_name_rp': 'my_context',
            'prop_one_rp': 2,
            'prop_two_rp': 'X'
        })
    self.assertProtoEquals(pipeline, expected)
    self.assertEqual(len(parameters), 3)
    self.assertEqual(parameters['context_name_rp'], 'my_context')
    self.assertEqual(parameters['prop_one_rp'], 2)
    self.assertEqual(parameters['prop_two_rp'], 'X')

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/runtime_parameter_utils_test.py" startline="56" endline="76" pcid="1925">
  def testPartiallySubstituteRuntimeParameter(self):
    pipeline = pipeline_pb2.Pipeline()
    expected = pipeline_pb2.Pipeline()
    self.load_proto_from_text(
        os.path.join(self._testdata_dir,
                     'pipeline_with_runtime_parameter.pbtxt'), pipeline)
    self.load_proto_from_text(
        os.path.join(
            self._testdata_dir,
            'pipeline_with_runtime_parameter_partially_substituted.pbtxt'),
        expected)
    parameters = runtime_parameter_utils.substitute_runtime_parameter(
        pipeline, {
            'context_name_rp': 'my_context',
        })
    self.assertProtoEquals(pipeline, expected)
    self.assertEqual(len(parameters), 3)
    self.assertEqual(parameters['context_name_rp'], 'my_context')
    self.assertEqual(parameters['prop_one_rp'], 1)
    self.assertIsNone(parameters['prop_two_rp'])

</source>
</class>

<class classid="98" nclones="3" nlines="34" similarity="81">
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/inputs_utils_test.py" startline="465" endline="503" pcid="2001">
  def testLatestUnprocessedArtifacts(self):
    pipeline = self.load_pipeline_proto(
        'pipeline_for_input_resolver_test.pbtxt')
    my_example_gen = pipeline.nodes[0].pipeline_node
    my_transform = pipeline.nodes[2].pipeline_node

    step1_pb = my_transform.inputs.resolver_config.resolver_steps.add()
    step1_pb.class_path = (
        'tfx.dsl.resolvers.unprocessed_artifacts_resolver'
        '.UnprocessedArtifactsResolver')
    step1_pb.config_json = '{"execution_type_name": "Transform"}'
    step2_pb = my_transform.inputs.resolver_config.resolver_steps.add()
    step2_pb.class_path = (
        'tfx.dsl.input_resolution.strategies.latest_artifact_strategy'
        '.LatestArtifactStrategy')
    step2_pb.config_json = '{}'

    with self.get_metadata() as m:
      ex1 = self.make_examples(uri='a')
      ex2 = self.make_examples(uri='b')
      output_artifacts = self.fake_execute(
          m,
          my_example_gen,
          input_map=None,
          output_map={'output_examples': [ex1]})
      ex1 = output_artifacts['output_examples'][0]
      output_artifacts = self.fake_execute(
          m,
          my_example_gen,
          input_map=None,
          output_map={'output_examples': [ex2]})
      ex2 = output_artifacts['output_examples'][0]

      result = inputs_utils.resolve_input_artifacts(
          metadata_handler=m,
          node_inputs=my_transform.inputs)

    self.assertArtifactMapEqual({'examples': [ex2]}, result)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/inputs_utils_test.py" startline="549" endline="593" pcid="2003">
  def testLatestUnprocessedArtifacts_NoneIfEverythingProcessed(self):
    pipeline = self.load_pipeline_proto(
        'pipeline_for_input_resolver_test.pbtxt')
    my_example_gen = pipeline.nodes[0].pipeline_node
    my_transform = pipeline.nodes[2].pipeline_node

    step1_pb = my_transform.inputs.resolver_config.resolver_steps.add()
    step1_pb.class_path = (
        'tfx.dsl.resolvers.unprocessed_artifacts_resolver'
        '.UnprocessedArtifactsResolver')
    step1_pb.config_json = '{"execution_type_name": "Transform"}'
    step2_pb = my_transform.inputs.resolver_config.resolver_steps.add()
    step2_pb.class_path = (
        'tfx.dsl.input_resolution.strategies.latest_artifact_strategy'
        '.LatestArtifactStrategy')
    step2_pb.config_json = '{}'

    with self.get_metadata() as m:
      ex1 = self.make_examples(uri='a')
      ex2 = self.make_examples(uri='b')
      output_artifacts = self.fake_execute(
          m,
          my_example_gen,
          input_map=None,
          output_map={'output_examples': [ex1]})
      ex1 = output_artifacts['output_examples'][0]
      output_artifacts = self.fake_execute(
          m,
          my_example_gen,
          input_map=None,
          output_map={'output_examples': [ex2]})
      ex2 = output_artifacts['output_examples'][0]
      self.fake_execute(m, my_transform,
                        input_map={'examples': [ex1]},
                        output_map=None)
      self.fake_execute(m, my_transform,
                        input_map={'examples': [ex2]},
                        output_map=None)

      result = inputs_utils.resolve_input_artifacts(
          metadata_handler=m,
          node_inputs=my_transform.inputs)

    self.assertIsNone(result)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/portable/inputs_utils_test.py" startline="506" endline="546" pcid="2002">
  def testLatestUnprocessedArtifacts_IgnoreAlreadyProcessed(self):
    pipeline = self.load_pipeline_proto(
        'pipeline_for_input_resolver_test.pbtxt')
    my_example_gen = pipeline.nodes[0].pipeline_node
    my_transform = pipeline.nodes[2].pipeline_node

    step1_pb = my_transform.inputs.resolver_config.resolver_steps.add()
    step1_pb.class_path = (
        'tfx.dsl.resolvers.unprocessed_artifacts_resolver'
        '.UnprocessedArtifactsResolver')
    step1_pb.config_json = '{"execution_type_name": "Transform"}'
    step2_pb = my_transform.inputs.resolver_config.resolver_steps.add()
    step2_pb.class_path = (
        'tfx.dsl.input_resolution.strategies.latest_artifact_strategy'
        '.LatestArtifactStrategy')
    step2_pb.config_json = '{}'

    with self.get_metadata() as m:
      ex1 = self.make_examples(uri='a')
      ex2 = self.make_examples(uri='b')
      output_artifacts = self.fake_execute(
          m,
          my_example_gen,
          input_map=None,
          output_map={'output_examples': [ex1]})
      ex1 = output_artifacts['output_examples'][0]
      output_artifacts = self.fake_execute(
          m,
          my_example_gen,
          input_map=None,
          output_map={'output_examples': [ex2]})
      ex2 = output_artifacts['output_examples'][0]
      self.fake_execute(
          m, my_transform, input_map={'examples': [ex2]}, output_map=None)

      result = inputs_utils.resolve_input_artifacts(
          metadata_handler=m,
          node_inputs=my_transform.inputs)

    self.assertArtifactMapEqual({'examples': [ex1]}, result)

</source>
</class>

<class classid="99" nclones="2" nlines="12" similarity="71">
<source file="systems/tfx-1.6.1/tfx/orchestration/local/runner_utils.py" startline="46" endline="64" pcid="2059">
def _build_executable_spec(
    node_id: str,
    spec: any_pb2.Any) -> local_deployment_config_pb2.ExecutableSpec:
  """Builds ExecutableSpec given the any proto from IntermediateDeploymentConfig."""
  result = local_deployment_config_pb2.ExecutableSpec()
  if spec.Is(result.python_class_executable_spec.DESCRIPTOR):
    spec.Unpack(result.python_class_executable_spec)
  elif spec.Is(result.container_executable_spec.DESCRIPTOR):
    spec.Unpack(result.container_executable_spec)
  elif spec.Is(result.beam_executable_spec.DESCRIPTOR):
    spec.Unpack(result.beam_executable_spec)
  else:
    raise ValueError(
        'Executor spec of {} is expected to be of one of the '
        'types of tfx.orchestration.deployment_config.ExecutableSpec.spec '
        'but got type {}'.format(node_id, spec.type_url))
  return result


</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/local/runner_utils.py" startline="65" endline="79" pcid="2060">
def _build_local_platform_config(
    node_id: str,
    spec: any_pb2.Any) -> local_deployment_config_pb2.LocalPlatformConfig:
  """Builds LocalPlatformConfig given the any proto from IntermediateDeploymentConfig."""
  result = local_deployment_config_pb2.LocalPlatformConfig()
  if spec.Is(result.docker_platform_config.DESCRIPTOR):
    spec.Unpack(result.docker_platform_config)
  else:
    raise ValueError(
        'Platform config of {} is expected to be of one of the types of '
        'tfx.orchestration.deployment_config.LocalPlatformConfig.config '
        'but got type {}'.format(node_id, spec.type_url))
  return result


</source>
</class>

<class classid="100" nclones="2" nlines="20" similarity="90">
<source file="systems/tfx-1.6.1/tfx/orchestration/airflow/airflow_dag_runner_test.py" startline="199" endline="221" pcid="2128">
  def testRuntimeParam(self):
    param = RuntimeParameter('name', str, 'tf"x')
    component_f = _FakeComponent(_FakeComponentSpecF(a=param))
    airflow_config = {
        'schedule_interval': '* * * * *',
        'start_date': datetime.datetime(2019, 1, 1)
    }
    test_pipeline = pipeline.Pipeline(
        pipeline_name='x',
        pipeline_root='y',
        metadata_connection_config=None,
        components=[component_f])

    runner = airflow_dag_runner.AirflowDagRunner(
        airflow_dag_runner.AirflowPipelineConfig(
            airflow_dag_config=airflow_config))
    dag = runner.run(test_pipeline)
    task = dag.tasks[0]
    self.assertDictEqual(
        {'exec_properties': {
            'a': '{{ dag_run.conf.get("name", "tf\\"x") }}'
        }}, task.op_kwargs)

</source>
<source file="systems/tfx-1.6.1/tfx/orchestration/airflow/airflow_dag_runner_test.py" startline="222" endline="246" pcid="2129">
  def testRuntimeParamTemplated(self):
    param = RuntimeParameter('a', str, '{{execution_date}}')
    component_f = _FakeComponent(_FakeComponentSpecF(a=param))
    airflow_config = {
        'schedule_interval': '* * * * *',
        'start_date': datetime.datetime(2019, 1, 1)
    }
    test_pipeline = pipeline.Pipeline(
        pipeline_name='x',
        pipeline_root='y',
        metadata_connection_config=None,
        components=[component_f])

    runner = airflow_dag_runner.AirflowDagRunner(
        airflow_dag_runner.AirflowPipelineConfig(
            airflow_dag_config=airflow_config))
    dag = runner.run(test_pipeline)
    task = dag.tasks[0]
    self.assertDictEqual(
        {
            'exec_properties': {
                'a': '{{ dag_run.conf.get("a", execution_date) }}'
            }
        }, task.op_kwargs)

</source>
</class>

<class classid="101" nclones="2" nlines="15" similarity="100">
<source file="systems/tfx-1.6.1/tfx/orchestration/test_pipelines/download_grep_print_pipeline.py" startline="118" endline="136" pcid="2138">
def create_pipeline_component_instances(text_url: str, pattern: str):
  """Creates tasks for the download_grep_print pipeline."""

  downloader_task = downloader_component(url=text_url)
  grep_task = grep_component(
      text=downloader_task.outputs['data'],
      pattern=pattern,
  )
  print_task = print_component(
      text=grep_task.outputs['filtered_text'],
  )

  component_instances = [
      downloader_task,
      grep_task,
      print_task,
  ]

  return component_instances
</source>
<source file="systems/tfx-1.6.1/tfx/examples/custom_components/container_components/download_grep_print_pipeline.py" startline="118" endline="136" pcid="3028">
def create_pipeline_component_instances(text_url: str, pattern: str):
  """Creates tasks for the download_grep_print pipeline."""

  downloader_task = downloader_component(url=text_url)
  grep_task = grep_component(
      text=downloader_task.outputs['data'],
      pattern=pattern,
  )
  print_task = print_component(
      text=grep_task.outputs['filtered_text'],
  )

  component_instances = [
      downloader_task,
      grep_task,
      print_task,
  ]

  return component_instances
</source>
</class>

<class classid="102" nclones="2" nlines="19" similarity="80">
<source file="systems/tfx-1.6.1/tfx/utils/deprecation_utils_test.py" startline="44" endline="69" pcid="2339">
  def testDeprecated(self):
    # By default, we warn once across all calls.
    my_function_1 = self._mock_function(name='my_function_1')
    deprecated_func_1 = deprecation_utils.deprecated(
        '2099-01-02', 'Please change to new_my_function_1')(
            my_function_1)
    deprecated_func_1()
    deprecated_func_1()
    self._assertDeprecatedWarningRegex(
        r'From .*: my_function_1 \(from .*\) is deprecated and will be '
        r'removed after 2099-01-02. Instructions for updating:\n'
        r'Please change to new_my_function_1')
    self.assertEqual(my_function_1.call_count, 2)
    self._mock_warn.reset_mock()

    # If `warn_once=False`, we warn once for each call.
    my_function_2 = self._mock_function()
    deprecated_func_2 = deprecation_utils.deprecated(
        '2099-01-02', 'Please change to new_my_function_2', warn_once=False)(
            my_function_2)
    deprecated_func_2()
    deprecated_func_2()
    deprecated_func_2()
    self.assertEqual(self._mock_warn.call_count, 3)
    self.assertEqual(my_function_2.call_count, 3)

</source>
<source file="systems/tfx-1.6.1/tfx/utils/deprecation_utils_test.py" startline="70" endline="92" pcid="2340">
  def testDeprecationAliasFunction(self):
    # By default, we warn once across all calls.
    my_function_1 = self._mock_function(name='my_function_1')
    deprecation_alias_1 = deprecation_utils.deprecated_alias(
        'deprecation_alias_1', 'my_function_1', my_function_1)
    deprecation_alias_1()
    deprecation_alias_1()
    self._assertDeprecatedWarningRegex(
        'From .*: The name deprecation_alias_1 is deprecated. Please use '
        'my_function_1 instead.')
    self.assertEqual(my_function_1.call_count, 2)
    self._mock_warn.reset_mock()

    # If `warn_once=False`, we warn once for each call.
    my_function_2 = self._mock_function()
    deprecation_alias_2 = deprecation_utils.deprecated_alias(
        'deprecation_alias_2', 'my_function_2', my_function_2, warn_once=False)
    deprecation_alias_2()
    deprecation_alias_2()
    deprecation_alias_2()
    self.assertEqual(self._mock_warn.call_count, 3)
    self.assertEqual(my_function_2.call_count, 3)

</source>
</class>

<class classid="103" nclones="3" nlines="17" similarity="72">
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/kubeflow_dag_runner_patcher_test.py" startline="32" endline="52" pcid="2371">
  def testPatcher(self):
    given_image_name = 'foo/bar'
    built_image_name = 'foo/bar@sha256:1234567890'

    mock_build_image_fn = mock.MagicMock(return_value=built_image_name)
    patcher = kubeflow_dag_runner_patcher.KubeflowDagRunnerPatcher(
        call_real_run=True,
        build_image_fn=mock_build_image_fn,
        use_temporary_output_file=True)
    runner_config = kubeflow_dag_runner.KubeflowDagRunnerConfig(
        tfx_image=given_image_name)
    runner = kubeflow_dag_runner.KubeflowDagRunner(config=runner_config)
    pipeline = tfx_pipeline.Pipeline('dummy', 'dummy_root')
    with patcher.patch() as context:
      runner.run(pipeline)
    self.assertTrue(context[patcher.USE_TEMPORARY_OUTPUT_FILE])
    self.assertIn(patcher.OUTPUT_FILE_PATH, context)

    mock_build_image_fn.assert_called_once_with(given_image_name)
    self.assertEqual(runner_config.tfx_image, built_image_name)

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/kubeflow_dag_runner_patcher_test.py" startline="53" endline="69" pcid="2372">
  def testPatcherWithOutputFile(self):
    output_filename = 'foo.tar.gz'
    patcher = kubeflow_dag_runner_patcher.KubeflowDagRunnerPatcher(
        call_real_run=False,
        build_image_fn=None,
        use_temporary_output_file=True)
    runner = kubeflow_dag_runner.KubeflowDagRunner(
        output_filename=output_filename)
    pipeline = tfx_pipeline.Pipeline('dummy', 'dummy_root')
    with patcher.patch() as context:
      runner.run(pipeline)
    self.assertFalse(context[patcher.USE_TEMPORARY_OUTPUT_FILE])
    self.assertEqual(
        os.path.basename(context[patcher.OUTPUT_FILE_PATH]), output_filename)
    self.assertEqual(runner._output_filename, output_filename)


</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/kubeflow_v2_dag_runner_patcher_test.py" startline="32" endline="49" pcid="2422">
  def testPatcherBuildImageFn(self):
    given_image_name = 'foo/bar'
    built_image_name = 'foo/bar@sha256:1234567890'

    mock_build_image_fn = mock.MagicMock(return_value=built_image_name)
    patcher = kubeflow_v2_dag_runner_patcher.KubeflowV2DagRunnerPatcher(
        call_real_run=True, build_image_fn=mock_build_image_fn)
    runner_config = kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(
        default_image=given_image_name)
    runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(config=runner_config)
    pipeline = tfx_pipeline.Pipeline('dummy', 'dummy_root')
    with patcher.patch() as context:
      runner.run(pipeline)
    self.assertIn(patcher.OUTPUT_FILE_PATH, context)

    mock_build_image_fn.assert_called_once_with(given_image_name)
    self.assertEqual(runner_config.default_image, built_image_name)

</source>
</class>

<class classid="104" nclones="3" nlines="21" similarity="71">
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/local_handler_test.py" startline="32" endline="56" pcid="2373">
  def setUp(self):
    super().setUp()
    self.chicago_taxi_pipeline_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'testdata')
    self._home = self.tmp_dir
    self.enter_context(test_case_utils.change_working_dir(self.tmp_dir))
    self.enter_context(test_case_utils.override_env_var('HOME', self._home))
    self._local_home = os.path.join(os.environ['HOME'], 'local')
    self.enter_context(
        test_case_utils.override_env_var('LOCAL_HOME', self._local_home))

    # Flags for handler.
    self.engine = 'local'
    self.pipeline_path = os.path.join(self.chicago_taxi_pipeline_dir,
                                      'test_pipeline_local_1.py')
    self.pipeline_name = 'chicago_taxi_local'
    self.pipeline_root = os.path.join(self._home, 'tfx', 'pipelines',
                                      self.pipeline_name)
    self.run_id = 'dummyID'

    self.pipeline_args = {
        labels.PIPELINE_NAME: self.pipeline_name,
        labels.PIPELINE_DSL_PATH: self.pipeline_path,
    }

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/beam_handler_test.py" startline="32" endline="56" pcid="2529">
  def setUp(self):
    super().setUp()
    self.chicago_taxi_pipeline_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'testdata')
    self._home = self.tmp_dir
    self.enter_context(test_case_utils.change_working_dir(self.tmp_dir))
    self.enter_context(test_case_utils.override_env_var('HOME', self._home))
    self._beam_home = os.path.join(os.environ['HOME'], 'beam')
    self.enter_context(
        test_case_utils.override_env_var('BEAM_HOME', self._beam_home))

    # Flags for handler.
    self.engine = 'beam'
    self.pipeline_path = os.path.join(self.chicago_taxi_pipeline_dir,
                                      'test_pipeline_beam_1.py')
    self.pipeline_name = 'chicago_taxi_beam'
    self.pipeline_root = os.path.join(self._home, 'tfx', 'pipelines',
                                      self.pipeline_name)
    self.run_id = 'dummyID'

    self.pipeline_args = {
        labels.PIPELINE_NAME: self.pipeline_name,
        labels.PIPELINE_DSL_PATH: self.pipeline_path,
    }

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/vertex_handler_test.py" startline="36" endline="64" pcid="2552">
  def setUp(self):
    super().setUp()
    self.chicago_taxi_pipeline_dir = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'testdata')

    self._home = self.tmp_dir
    self.enter_context(test_case_utils.change_working_dir(self.tmp_dir))
    self.enter_context(test_case_utils.override_env_var('HOME', self._home))
    self._vertex_home = os.path.join(self._home, 'vertex')
    self.enter_context(
        test_case_utils.override_env_var('VERTEX_HOME', self._vertex_home))

    # Flags for handler.
    self.engine = 'vertex'
    self.pipeline_path = os.path.join(self.chicago_taxi_pipeline_dir,
                                      'test_pipeline_kubeflow_v2_1.py')
    self.pipeline_name = _TEST_PIPELINE_NAME
    self.pipeline_root = os.path.join(self._home, 'tfx', 'pipelines',
                                      self.pipeline_name)
    self.run_id = 'dummyID'
    self.project = 'gcp_project_1'
    self.region = 'us-central1'

    self.runtime_parameter = {'a': '1', 'b': '2'}

    # Setting up Mock for API client, so that this Python test is hermetic.
    # subprocess Mock will be setup per-test.
    self.addCleanup(mock.patch.stopall)

</source>
</class>

<class classid="105" nclones="16" nlines="11" similarity="70">
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/local_handler_test.py" startline="90" endline="103" pcid="2377">
  def testCreatePipelineExistentPipeline(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = local_handler.LocalHandler(flags_dict)
    handler.create_pipeline()
    # Run create_pipeline again to test.
    with self.assertRaises(SystemExit) as err:
      handler.create_pipeline()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" already exists.'.format(
            self.pipeline_args[labels.PIPELINE_NAME]))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/local_handler_test.py" startline="127" endline="139" pcid="2379">
  def testUpdatePipelineNoPipeline(self):
    # Update pipeline without creating one.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = local_handler.LocalHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.update_pipeline()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            self.pipeline_args[labels.PIPELINE_NAME]))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/beam_handler_test.py" startline="78" endline="91" pcid="2532">
  def testCreatePipelineExistentPipeline(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = beam_handler.BeamHandler(flags_dict)
    handler.create_pipeline()
    # Run create_pipeline again to test.
    with self.assertRaises(SystemExit) as err:
      handler.create_pipeline()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" already exists.'.format(
            self.pipeline_args[labels.PIPELINE_NAME]))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/vertex_handler_test.py" startline="183" endline="194" pcid="2561">
  def testDeletePipelineNonExistentPipeline(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = vertex_handler.VertexHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.delete_pipeline()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/airflow_handler_test.py" startline="419" endline="432" pcid="2525">
  def testGetRunWrongPipeline(self):
    # Run pipeline without creating one.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.RUN_ID: self.run_id,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.get_run()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/vertex_handler_test.py" startline="114" endline="126" pcid="2556">
  def testUpdatePipelineNoPipeline(self):
    # Update pipeline without creating one.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = vertex_handler.VertexHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.update_pipeline()
    self.assertEqual(
        str(err.exception),
        'Pipeline "{}" does not exist.'.format(self.pipeline_name))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/beam_handler_test.py" startline="298" endline="310" pcid="2545">
  def testCreateRunNoPipeline(self):
    # Run pipeline without creating one.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = beam_handler.BeamHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.create_run()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/airflow_handler_test.py" startline="345" endline="356" pcid="2521">
  def testCreateRunNoPipeline(self):
    # Run pipeline without creating one.
    flags_dict = {labels.ENGINE_FLAG: self.engine,
                  labels.PIPELINE_NAME: self.pipeline_name,
                  labels.RUNTIME_PARAMETER: self.runtime_parameter}
    handler = airflow_handler.AirflowHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.create_run()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/beam_handler_test.py" startline="166" endline="177" pcid="2538">
  def testDeletePipelineNonExistentPipeline(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = beam_handler.BeamHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.delete_pipeline()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/local_handler_test.py" startline="178" endline="189" pcid="2383">
  def testDeletePipelineNonExistentPipeline(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = local_handler.LocalHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.delete_pipeline()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/vertex_handler_test.py" startline="76" endline="89" pcid="2554">
  def testCreatePipelineExistentPipeline(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = vertex_handler.VertexHandler(flags_dict)
    handler.create_pipeline()
    # Run create_pipeline again to test.
    with self.assertRaises(SystemExit) as err:
      handler.create_pipeline()
    self.assertEqual(
        str(err.exception),
        'Pipeline "{}" already exists.'.format(self.pipeline_name))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/airflow_handler_test.py" startline="382" endline="394" pcid="2523">
  def testListRunsWrongPipeline(self):
    # Run pipeline without creating one.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: 'chicago_taxi'
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.list_runs()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/beam_handler_test.py" startline="115" endline="127" pcid="2534">
  def testUpdatePipelineNoPipeline(self):
    # Update pipeline without creating one.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = beam_handler.BeamHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.update_pipeline()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            self.pipeline_args[labels.PIPELINE_NAME]))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/local_handler_test.py" startline="310" endline="322" pcid="2390">
  def testCreateRunNoPipeline(self):
    # Run pipeline without creating one.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = local_handler.LocalHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.create_run()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/airflow_handler_test.py" startline="131" endline="142" pcid="2508">
  def testCreatePipelineExistentPipeline(self):
    flags_dict = {labels.ENGINE_FLAG: self.engine,
                  labels.PIPELINE_DSL_PATH: self.pipeline_path}
    handler = airflow_handler.AirflowHandler(flags_dict)
    handler.create_pipeline()
    # Run create_pipeline again to test.
    with self.assertRaises(SystemExit) as err:
      handler.create_pipeline()
    self.assertEqual(
        str(err.exception), 'Pipeline "{}" already exists.'.format(
            self.pipeline_args[labels.PIPELINE_NAME]))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/base_handler_test.py" startline="109" endline="120" pcid="2588">
  def testCheckPipelineExistenceRequired(self):
    flags_dict = {
        labels.ENGINE_FLAG: 'beam',
        labels.PIPELINE_NAME: 'chicago_taxi_beam'
    }
    handler = FakeHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler._check_pipeline_existence(flags_dict[labels.PIPELINE_NAME])
    self.assertTrue(
        str(err.exception), 'Pipeline "{}" does not exist.'.format(
            flags_dict[labels.PIPELINE_NAME]))

</source>
</class>

<class classid="106" nclones="3" nlines="17" similarity="88">
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/local_handler_test.py" startline="104" endline="126" pcid="2378">
  def testUpdatePipeline(self):
    # First create pipeline with test_pipeline.py
    pipeline_path_1 = os.path.join(self.chicago_taxi_pipeline_dir,
                                   'test_pipeline_local_1.py')
    flags_dict_1 = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: pipeline_path_1
    }
    handler = local_handler.LocalHandler(flags_dict_1)
    handler.create_pipeline()

    # Update test_pipeline and run update_pipeline
    pipeline_path_2 = os.path.join(self.chicago_taxi_pipeline_dir,
                                   'test_pipeline_local_2.py')
    flags_dict_2 = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: pipeline_path_2
    }
    handler = local_handler.LocalHandler(flags_dict_2)
    handler.update_pipeline()
    self.assertTrue(
        fileio.exists(handler._get_pipeline_args_path(self.pipeline_name)))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/beam_handler_test.py" startline="92" endline="114" pcid="2533">
  def testUpdatePipeline(self):
    # First create pipeline with test_pipeline.py
    pipeline_path_1 = os.path.join(self.chicago_taxi_pipeline_dir,
                                   'test_pipeline_beam_1.py')
    flags_dict_1 = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: pipeline_path_1
    }
    handler = beam_handler.BeamHandler(flags_dict_1)
    handler.create_pipeline()

    # Update test_pipeline and run update_pipeline
    pipeline_path_2 = os.path.join(self.chicago_taxi_pipeline_dir,
                                   'test_pipeline_beam_2.py')
    flags_dict_2 = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: pipeline_path_2
    }
    handler = beam_handler.BeamHandler(flags_dict_2)
    handler.update_pipeline()
    self.assertTrue(
        fileio.exists(handler._get_pipeline_args_path(self.pipeline_name)))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/vertex_handler_test.py" startline="90" endline="113" pcid="2555">
  def testUpdatePipeline(self):
    # First create pipeline with test_pipeline.py
    pipeline_path_1 = os.path.join(self.chicago_taxi_pipeline_dir,
                                   'test_pipeline_kubeflow_v2_1.py')
    flags_dict_1 = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: pipeline_path_1
    }
    handler = vertex_handler.VertexHandler(flags_dict_1)
    handler.create_pipeline()

    # Update test_pipeline and run update_pipeline
    pipeline_path_2 = os.path.join(self.chicago_taxi_pipeline_dir,
                                   'test_pipeline_kubeflow_v2_2.py')
    flags_dict_2 = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: pipeline_path_2
    }
    handler = vertex_handler.VertexHandler(flags_dict_2)
    handler.update_pipeline()
    self.assertTrue(
        fileio.exists(
            handler._get_pipeline_definition_path(self.pipeline_name)))

</source>
</class>

<class classid="107" nclones="9" nlines="15" similarity="73">
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/local_handler_test.py" startline="159" endline="177" pcid="2382">
  def testDeletePipeline(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = local_handler.LocalHandler(flags_dict)
    handler.create_pipeline()

    # Now delete the pipeline created aand check if pipeline folder is deleted.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = local_handler.LocalHandler(flags_dict)
    handler.delete_pipeline()
    self.assertFalse(
        fileio.exists(handler._get_pipeline_info_path(self.pipeline_name)))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/airflow_handler_test.py" startline="248" endline="267" pcid="2517">
  def testPipelineSchemaNoPipelineRoot(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.get_schema()
    self.assertEqual(
        str(err.exception),
        'Create a run before inferring schema. If pipeline is already running, then wait for it to successfully finish.'
    )

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/beam_handler_test.py" startline="147" endline="165" pcid="2537">
  def testDeletePipeline(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = beam_handler.BeamHandler(flags_dict)
    handler.create_pipeline()

    # Now delete the pipeline created aand check if pipeline folder is deleted.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = beam_handler.BeamHandler(flags_dict)
    handler.delete_pipeline()
    self.assertFalse(
        fileio.exists(handler._get_pipeline_info_path(self.pipeline_name)))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/beam_handler_test.py" startline="203" endline="222" pcid="2541">
  def testPipelineSchemaNoPipelineRoot(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = beam_handler.BeamHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = beam_handler.BeamHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.get_schema()
    self.assertEqual(
        str(err.exception),
        'Create a run before inferring schema. If pipeline is already running, then wait for it to successfully finish.'
    )

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/vertex_handler_test.py" startline="163" endline="182" pcid="2560">
  def testDeletePipeline(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = vertex_handler.VertexHandler(flags_dict)
    handler.create_pipeline()

    # Now delete the pipeline created aand check if pipeline folder is deleted.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = vertex_handler.VertexHandler(flags_dict)
    handler.delete_pipeline()
    handler_pipeline_path = os.path.join(handler._handler_home_dir,
                                         self.pipeline_name)
    self.assertFalse(fileio.exists(handler_pipeline_path))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/local_handler_test.py" startline="215" endline="234" pcid="2386">
  def testPipelineSchemaNoPipelineRoot(self):
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = local_handler.LocalHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = local_handler.LocalHandler(flags_dict)
    with self.assertRaises(SystemExit) as err:
      handler.get_schema()
    self.assertEqual(
        str(err.exception),
        'Create a run before inferring schema. If pipeline is already running, then wait for it to successfully finish.'
    )

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/beam_handler_test.py" startline="223" endline="244" pcid="2542">
  def testPipelineSchemaNoSchemaGenOutput(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = beam_handler.BeamHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = beam_handler.BeamHandler(flags_dict)
    fileio.makedirs(self.pipeline_root)
    with self.assertRaises(SystemExit) as err:
      handler.get_schema()
    self.assertEqual(
        str(err.exception),
        'Either SchemaGen component does not exist or pipeline is still running. If pipeline is running, then wait for it to successfully finish.'
    )

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/local_handler_test.py" startline="235" endline="256" pcid="2387">
  def testPipelineSchemaNoSchemaGenOutput(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = local_handler.LocalHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = local_handler.LocalHandler(flags_dict)
    fileio.makedirs(self.pipeline_root)
    with self.assertRaises(SystemExit) as err:
      handler.get_schema()
    self.assertEqual(
        str(err.exception),
        'Either SchemaGen component does not exist or pipeline is still running. If pipeline is running, then wait for it to successfully finish.'
    )

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/airflow_handler_test.py" startline="268" endline="289" pcid="2518">
  def testPipelineSchemaNoSchemaGenOutput(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    fileio.makedirs(self.pipeline_root)
    with self.assertRaises(SystemExit) as err:
      handler.get_schema()
    self.assertEqual(
        str(err.exception),
        'Either SchemaGen component does not exist or pipeline is still running. If pipeline is running, then wait for it to successfully finish.'
    )

</source>
</class>

<class classid="108" nclones="4" nlines="13" similarity="86">
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/local_handler_test.py" startline="190" endline="207" pcid="2384">
  def testListPipelinesNonEmpty(self):
    # First create two pipelines in the dags folder.
    handler_pipeline_path_1 = os.path.join(os.environ['LOCAL_HOME'],
                                           'pipeline_1')
    handler_pipeline_path_2 = os.path.join(os.environ['LOCAL_HOME'],
                                           'pipeline_2')
    fileio.makedirs(handler_pipeline_path_1)
    fileio.makedirs(handler_pipeline_path_2)

    # Now, list the pipelines
    flags_dict = {labels.ENGINE_FLAG: self.engine}
    handler = local_handler.LocalHandler(flags_dict)

    with self.captureWritesToStream(sys.stdout) as captured:
      handler.list_pipelines()
    self.assertIn('pipeline_1', captured.contents())
    self.assertIn('pipeline_2', captured.contents())

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/vertex_handler_test.py" startline="138" endline="155" pcid="2558">
  def testListPipelinesNonEmpty(self):
    # First create two pipelines in the dags folder.
    handler_pipeline_path_1 = os.path.join(os.environ['VERTEX_HOME'],
                                           'pipeline_1')
    handler_pipeline_path_2 = os.path.join(os.environ['VERTEX_HOME'],
                                           'pipeline_2')
    fileio.makedirs(handler_pipeline_path_1)
    fileio.makedirs(handler_pipeline_path_2)

    # Now, list the pipelines
    flags_dict = {labels.ENGINE_FLAG: labels.VERTEX_ENGINE}
    handler = vertex_handler.VertexHandler(flags_dict)

    with self.captureWritesToStream(sys.stdout) as captured:
      handler.list_pipelines()
    self.assertIn('pipeline_1', captured.contents())
    self.assertIn('pipeline_2', captured.contents())

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/beam_handler_test.py" startline="178" endline="195" pcid="2539">
  def testListPipelinesNonEmpty(self):
    # First create two pipelines in the dags folder.
    handler_pipeline_path_1 = os.path.join(os.environ['BEAM_HOME'],
                                           'pipeline_1')
    handler_pipeline_path_2 = os.path.join(os.environ['BEAM_HOME'],
                                           'pipeline_2')
    fileio.makedirs(handler_pipeline_path_1)
    fileio.makedirs(handler_pipeline_path_2)

    # Now, list the pipelines
    flags_dict = {labels.ENGINE_FLAG: self.engine}
    handler = beam_handler.BeamHandler(flags_dict)

    with self.captureWritesToStream(sys.stdout) as captured:
      handler.list_pipelines()
    self.assertIn('pipeline_1', captured.contents())
    self.assertIn('pipeline_2', captured.contents())

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/airflow_handler_test.py" startline="202" endline="221" pcid="2513">
  def testListPipelinesNonEmpty(self):
    # First create two pipelines in the dags folder.
    handler_pipeline_path_1 = os.path.join(os.environ['AIRFLOW_HOME'],
                                           'dags',
                                           'pipeline_1')
    handler_pipeline_path_2 = os.path.join(os.environ['AIRFLOW_HOME'],
                                           'dags',
                                           'pipeline_2')
    fileio.makedirs(handler_pipeline_path_1)
    fileio.makedirs(handler_pipeline_path_2)

    # Now, list the pipelines
    flags_dict = {labels.ENGINE_FLAG: self.engine}
    handler = airflow_handler.AirflowHandler(flags_dict)

    with self.captureWritesToStream(sys.stdout) as captured:
      handler.list_pipelines()
    self.assertIn('pipeline_1', captured.contents())
    self.assertIn('pipeline_2', captured.contents())

</source>
</class>

<class classid="109" nclones="3" nlines="26" similarity="100">
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/local_handler_test.py" startline="257" endline="288" pcid="2388">
  def testPipelineSchemaSuccessfulRun(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = local_handler.LocalHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = local_handler.LocalHandler(flags_dict)
    # Create fake schema in pipeline root.
    component_output_dir = os.path.join(self.pipeline_root, 'SchemaGen')
    schema_path = base_driver._generate_output_uri(  # pylint: disable=protected-access
        component_output_dir, 'schema', 3)

    fileio.makedirs(schema_path)
    with open(os.path.join(schema_path, 'schema.pbtxt'), 'w') as f:
      f.write('SCHEMA')
    with self.captureWritesToStream(sys.stdout) as captured:
      handler.get_schema()
      curr_dir_path = os.path.abspath('schema.pbtxt')
      self.assertIn('Path to schema: {}'.format(curr_dir_path),
                    captured.contents())
      self.assertIn(
          '*********SCHEMA FOR {}**********'.format(self.pipeline_name.upper()),
          captured.contents())
      self.assertTrue(fileio.exists(curr_dir_path))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/beam_handler_test.py" startline="245" endline="276" pcid="2543">
  def testPipelineSchemaSuccessfulRun(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = beam_handler.BeamHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = beam_handler.BeamHandler(flags_dict)
    # Create fake schema in pipeline root.
    component_output_dir = os.path.join(self.pipeline_root, 'SchemaGen')
    schema_path = base_driver._generate_output_uri(  # pylint: disable=protected-access
        component_output_dir, 'schema', 3)

    fileio.makedirs(schema_path)
    with open(os.path.join(schema_path, 'schema.pbtxt'), 'w') as f:
      f.write('SCHEMA')
    with self.captureWritesToStream(sys.stdout) as captured:
      handler.get_schema()
      curr_dir_path = os.path.abspath('schema.pbtxt')
      self.assertIn('Path to schema: {}'.format(curr_dir_path),
                    captured.contents())
      self.assertIn(
          '*********SCHEMA FOR {}**********'.format(self.pipeline_name.upper()),
          captured.contents())
      self.assertTrue(fileio.exists(curr_dir_path))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/airflow_handler_test.py" startline="290" endline="320" pcid="2519">
  def testPipelineSchemaSuccessfulRun(self):
    # First create a pipeline.
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_DSL_PATH: self.pipeline_path
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    handler.create_pipeline()

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    # Create fake schema in pipeline root.
    component_output_dir = os.path.join(self.pipeline_root, 'SchemaGen')
    schema_path = base_driver._generate_output_uri(  # pylint: disable=protected-access
        component_output_dir, 'schema', 3)
    fileio.makedirs(schema_path)
    with open(os.path.join(schema_path, 'schema.pbtxt'), 'w') as f:
      f.write('SCHEMA')
    with self.captureWritesToStream(sys.stdout) as captured:
      handler.get_schema()
      curr_dir_path = os.path.abspath('schema.pbtxt')
      self.assertIn('Path to schema: {}'.format(curr_dir_path),
                    captured.contents())
      self.assertIn(
          '*********SCHEMA FOR {}**********'.format(self.pipeline_name.upper()),
          captured.contents())
      self.assertTrue(fileio.exists(curr_dir_path))

</source>
</class>

<class classid="110" nclones="2" nlines="13" similarity="100">
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/local_handler_test.py" startline="290" endline="309" pcid="2389">
  def testCreateRun(self, mock_call):
    # Create a pipeline in dags folder.
    handler_pipeline_path = os.path.join(
        os.environ['LOCAL_HOME'], self.pipeline_args[labels.PIPELINE_NAME])
    fileio.makedirs(handler_pipeline_path)

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = local_handler.LocalHandler(flags_dict)
    with open(handler._get_pipeline_args_path(self.pipeline_name), 'w') as f:
      json.dump(self.pipeline_args, f)

    # Now run the pipeline
    handler.create_run()

    mock_call.assert_called_once()
    self.assertIn(self.pipeline_path, mock_call.call_args[0][0])

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/beam_handler_test.py" startline="278" endline="297" pcid="2544">
  def testCreateRun(self, mock_call):
    # Create a pipeline in dags folder.
    handler_pipeline_path = os.path.join(
        os.environ['BEAM_HOME'], self.pipeline_args[labels.PIPELINE_NAME])
    fileio.makedirs(handler_pipeline_path)

    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = beam_handler.BeamHandler(flags_dict)
    with open(handler._get_pipeline_args_path(self.pipeline_name), 'w') as f:
      json.dump(self.pipeline_args, f)

    # Now run the pipeline
    handler.create_run()

    mock_call.assert_called_once()
    self.assertIn(self.pipeline_path, mock_call.call_args[0][0])

</source>
</class>

<class classid="111" nclones="2" nlines="14" similarity="85">
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/airflow_handler_test.py" startline="362" endline="381" pcid="2522">
  def testListRuns(self, mock_check_output):
    # Create a pipeline in dags folder.
    handler_pipeline_path = os.path.join(
        os.environ['AIRFLOW_HOME'], 'dags',
        self.pipeline_args[labels.PIPELINE_NAME])
    fileio.makedirs(handler_pipeline_path)

    # Now run the pipeline
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.PIPELINE_NAME: self.pipeline_name,
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    with self.captureWritesToStream(sys.stdout) as captured:
      handler.list_runs()
    mock_check_output.assert_called_once()

    # Just check the run_id is succesfully parsed.
    self.assertIn(self.run_id, captured.contents())

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/handler/airflow_handler_test.py" startline="400" endline="418" pcid="2524">
  def testGetRun(self, mock_check_output):
    # Create a pipeline in dags folder.
    handler_pipeline_path = os.path.join(
        os.environ['AIRFLOW_HOME'], 'dags',
        self.pipeline_args[labels.PIPELINE_NAME])
    fileio.makedirs(handler_pipeline_path)

    # Now run the pipeline
    flags_dict = {
        labels.ENGINE_FLAG: self.engine,
        labels.RUN_ID: self.run_id,
        labels.PIPELINE_NAME: self.pipeline_name
    }
    handler = airflow_handler.AirflowHandler(flags_dict)
    with self.captureWritesToStream(sys.stdout) as captured:
      handler.get_run()
    self.assertIn(self.run_id, captured.contents())
    self.assertIn('running', captured.contents())

</source>
</class>

<class classid="112" nclones="3" nlines="30" similarity="75">
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_local_e2e_test.py" startline="32" endline="72" pcid="2594">
  def setUp(self):
    super().setUp()

    # Change the encoding for Click since Python 3 is configured to use ASCII as
    # encoding for the environment.
    if codecs.lookup(locale.getpreferredencoding()).name == 'ascii':
      os.environ['LANG'] = 'en_US.utf-8'

    # Setup local_home in a temp directory
    self._home = self.tmp_dir
    self._local_home = os.path.join(self._home, 'local')
    self.enter_context(
        test_case_utils.override_env_var('LOCAL_HOME', self._local_home))
    self.enter_context(
        test_case_utils.override_env_var('HOME', self._home))

    # Testdata path.
    self._testdata_dir = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), 'testdata')

    # Copy data.
    chicago_taxi_pipeline_dir = os.path.join(
        os.path.dirname(
            os.path.dirname(
                os.path.dirname(os.path.dirname(os.path.abspath(__file__))))),
        'examples', 'chicago_taxi_pipeline', '')
    data_dir = os.path.join(chicago_taxi_pipeline_dir, 'data', 'simple')
    content = fileio.listdir(data_dir)
    assert content, 'content in {} is empty'.format(data_dir)
    target_data_dir = os.path.join(self._home, 'taxi', 'data', 'simple')
    io_utils.copy_dir(data_dir, target_data_dir)
    assert fileio.isdir(target_data_dir)
    content = fileio.listdir(target_data_dir)
    assert content, 'content in {} is {}'.format(target_data_dir, content)
    io_utils.copy_file(
        os.path.join(chicago_taxi_pipeline_dir, 'taxi_utils.py'),
        os.path.join(self._home, 'taxi', 'taxi_utils.py'))

    # Initialize CLI runner.
    self.runner = click_testing.CliRunner()

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_beam_e2e_test.py" startline="31" endline="71" pcid="2608">
  def setUp(self):
    super().setUp()

    # Change the encoding for Click since Python 3 is configured to use ASCII as
    # encoding for the environment.
    if codecs.lookup(locale.getpreferredencoding()).name == 'ascii':
      os.environ['LANG'] = 'en_US.utf-8'

    # Setup beam_home in a temp directory
    self._home = self.tmp_dir
    self._beam_home = os.path.join(self._home, 'beam')
    self.enter_context(
        test_case_utils.override_env_var('BEAM_HOME', self._beam_home))
    self.enter_context(
        test_case_utils.override_env_var('HOME', self._home))

    # Testdata path.
    self._testdata_dir = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), 'testdata')

    # Copy data.
    chicago_taxi_pipeline_dir = os.path.join(
        os.path.dirname(
            os.path.dirname(
                os.path.dirname(os.path.dirname(os.path.abspath(__file__))))),
        'examples', 'chicago_taxi_pipeline', '')
    data_dir = os.path.join(chicago_taxi_pipeline_dir, 'data', 'simple')
    content = fileio.listdir(data_dir)
    assert content, 'content in {} is empty'.format(data_dir)
    target_data_dir = os.path.join(self._home, 'taxi', 'data', 'simple')
    io_utils.copy_dir(data_dir, target_data_dir)
    assert fileio.isdir(target_data_dir)
    content = fileio.listdir(target_data_dir)
    assert content, 'content in {} is {}'.format(target_data_dir, content)
    io_utils.copy_file(
        os.path.join(chicago_taxi_pipeline_dir, 'taxi_utils.py'),
        os.path.join(self._home, 'taxi', 'taxi_utils.py'))

    # Initialize CLI runner.
    self.runner = click_testing.CliRunner()

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_airflow_e2e_test.py" startline="39" endline="92" pcid="2642">
  def setUp(self):
    super().setUp()

    # List of packages installed.
    self._pip_list = pip_utils.get_package_names()

    # Check if Apache Airflow is installed before running E2E tests.
    if labels.AIRFLOW_PACKAGE_NAME not in self._pip_list:
      sys.exit('Apache Airflow not installed.')

    # Change the encoding for Click since Python 3 is configured to use ASCII as
    # encoding for the environment.
    if codecs.lookup(locale.getpreferredencoding()).name == 'ascii':
      os.environ['LANG'] = 'en_US.utf-8'

    # Setup airflow_home in a temp directory
    self._airflow_home = os.path.join(self.tmp_dir, 'airflow')
    self.enter_context(
        test_case_utils.override_env_var('AIRFLOW_HOME', self._airflow_home))
    self.enter_context(
        test_case_utils.override_env_var('HOME', self._airflow_home))

    absl.logging.info('Using %s as AIRFLOW_HOME and HOME in this e2e test',
                      self._airflow_home)

    # Testdata path.
    self._testdata_dir = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), 'testdata')

    self._pipeline_name = 'chicago_taxi_simple'
    self._pipeline_path = os.path.join(self._testdata_dir,
                                       'test_pipeline_airflow_1.py')

    # Copy data.
    chicago_taxi_pipeline_dir = os.path.join(
        os.path.dirname(
            os.path.dirname(
                os.path.dirname(os.path.dirname(os.path.abspath(__file__))))),
        'examples', 'chicago_taxi_pipeline')
    data_dir = os.path.join(chicago_taxi_pipeline_dir, 'data', 'simple')
    content = fileio.listdir(data_dir)
    assert content, 'content in {} is empty'.format(data_dir)
    target_data_dir = os.path.join(self._airflow_home, 'taxi', 'data', 'simple')
    io_utils.copy_dir(data_dir, target_data_dir)
    assert fileio.isdir(target_data_dir)
    content = fileio.listdir(target_data_dir)
    assert content, 'content in {} is {}'.format(target_data_dir, content)
    io_utils.copy_file(
        os.path.join(chicago_taxi_pipeline_dir, 'taxi_utils.py'),
        os.path.join(self._airflow_home, 'taxi', 'taxi_utils.py'))

    # Initialize CLI runner.
    self.runner = click_testing.CliRunner()

</source>
</class>

<class classid="113" nclones="2" nlines="13" similarity="92">
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_local_e2e_test.py" startline="73" endline="89" pcid="2595">
  def _valid_create_and_check(self, pipeline_path, pipeline_name):
    handler_pipeline_path = os.path.join(self._local_home, pipeline_name)

    # Create a pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'create', '--engine', 'local', '--pipeline_path',
        pipeline_path
    ])
    logging.info('[CLI] %s', result.output)
    self.assertIn('CLI', result.output)
    self.assertIn('Creating pipeline', result.output)
    self.assertTrue(
        fileio.exists(
            os.path.join(handler_pipeline_path, 'pipeline_args.json')))
    self.assertIn('Pipeline "{}" created successfully.'.format(pipeline_name),
                  result.output)

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_beam_e2e_test.py" startline="72" endline="87" pcid="2609">
  def _valid_create_and_check(self, pipeline_path, pipeline_name):
    handler_pipeline_path = os.path.join(self._beam_home, pipeline_name)

    # Create a pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'create', '--engine', 'beam', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Creating pipeline', result.output)
    self.assertTrue(
        fileio.exists(
            os.path.join(handler_pipeline_path, 'pipeline_args.json')))
    self.assertIn('Pipeline "{}" created successfully.'.format(pipeline_name),
                  result.output)

</source>
</class>

<class classid="114" nclones="2" nlines="11" similarity="100">
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_local_e2e_test.py" startline="90" endline="105" pcid="2596">
  def testPipelineCreate(self):
    # Create a pipeline.
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_local_1.py')
    pipeline_name = 'chicago_taxi_local'
    self._valid_create_and_check(pipeline_path, pipeline_name)

    # Test pipeline create when pipeline already exists.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'create', '--engine', 'local', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Creating pipeline', result.output)
    self.assertTrue('Pipeline "{}" already exists.'.format(pipeline_name),
                    result.output)

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_beam_e2e_test.py" startline="88" endline="103" pcid="2610">
  def testPipelineCreate(self):
    # Create a pipeline.
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_beam_1.py')
    pipeline_name = 'chicago_taxi_beam'
    self._valid_create_and_check(pipeline_path, pipeline_name)

    # Test pipeline create when pipeline already exists.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'create', '--engine', 'beam', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Creating pipeline', result.output)
    self.assertTrue('Pipeline "{}" already exists.'.format(pipeline_name),
                    result.output)

</source>
</class>

<class classid="115" nclones="4" nlines="24" similarity="73">
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_local_e2e_test.py" startline="106" endline="137" pcid="2597">
  def testPipelineUpdate(self):
    pipeline_name = 'chicago_taxi_local'
    handler_pipeline_path = os.path.join(self._local_home, pipeline_name)
    pipeline_path_1 = os.path.join(self._testdata_dir,
                                   'test_pipeline_local_1.py')
    # Try pipeline update when pipeline does not exist.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'update', '--engine', 'local', '--pipeline_path',
        pipeline_path_1
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Updating pipeline', result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(pipeline_name),
                  result.output)
    self.assertFalse(fileio.exists(handler_pipeline_path))

    # Now update an existing pipeline.
    self._valid_create_and_check(pipeline_path_1, pipeline_name)
    pipeline_path_2 = os.path.join(self._testdata_dir,
                                   'test_pipeline_local_2.py')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'update', '--engine', 'local', '--pipeline_path',
        pipeline_path_2
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Updating pipeline', result.output)
    self.assertIn('Pipeline "{}" updated successfully.'.format(pipeline_name),
                  result.output)
    self.assertTrue(
        fileio.exists(
            os.path.join(handler_pipeline_path, 'pipeline_args.json')))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_local_e2e_test.py" startline="171" endline="200" pcid="2599">
  def testPipelineDelete(self):
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_local_1.py')
    pipeline_name = 'chicago_taxi_local'
    handler_pipeline_path = os.path.join(self._local_home, pipeline_name)

    # Try deleting a non existent pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'delete', '--engine', 'local', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Deleting pipeline', result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(pipeline_name),
                  result.output)
    self.assertFalse(fileio.exists(handler_pipeline_path))

    # Create a pipeline.
    self._valid_create_and_check(pipeline_path, pipeline_name)

    # Now delete the pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'delete', '--engine', 'local', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Deleting pipeline', result.output)
    self.assertFalse(fileio.exists(handler_pipeline_path))
    self.assertIn('Pipeline "{}" deleted successfully.'.format(pipeline_name),
                  result.output)

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_beam_e2e_test.py" startline="169" endline="198" pcid="2613">
  def testPipelineDelete(self):
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_beam_1.py')
    pipeline_name = 'chicago_taxi_beam'
    handler_pipeline_path = os.path.join(self._beam_home, pipeline_name)

    # Try deleting a non existent pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'delete', '--engine', 'beam', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Deleting pipeline', result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(pipeline_name),
                  result.output)
    self.assertFalse(fileio.exists(handler_pipeline_path))

    # Create a pipeline.
    self._valid_create_and_check(pipeline_path, pipeline_name)

    # Now delete the pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'delete', '--engine', 'beam', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Deleting pipeline', result.output)
    self.assertFalse(fileio.exists(handler_pipeline_path))
    self.assertIn('Pipeline "{}" deleted successfully.'.format(pipeline_name),
                  result.output)

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_beam_e2e_test.py" startline="104" endline="135" pcid="2611">
  def testPipelineUpdate(self):
    pipeline_name = 'chicago_taxi_beam'
    handler_pipeline_path = os.path.join(self._beam_home, pipeline_name)
    pipeline_path_1 = os.path.join(self._testdata_dir,
                                   'test_pipeline_beam_1.py')
    # Try pipeline update when pipeline does not exist.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'update', '--engine', 'beam', '--pipeline_path',
        pipeline_path_1
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Updating pipeline', result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(pipeline_name),
                  result.output)
    self.assertFalse(fileio.exists(handler_pipeline_path))

    # Now update an existing pipeline.
    self._valid_create_and_check(pipeline_path_1, pipeline_name)
    pipeline_path_2 = os.path.join(self._testdata_dir,
                                   'test_pipeline_beam_2.py')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'update', '--engine', 'beam', '--pipeline_path',
        pipeline_path_2
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Updating pipeline', result.output)
    self.assertIn('Pipeline "{}" updated successfully.'.format(pipeline_name),
                  result.output)
    self.assertTrue(
        fileio.exists(
            os.path.join(handler_pipeline_path, 'pipeline_args.json')))

</source>
</class>

<class classid="116" nclones="3" nlines="23" similarity="79">
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_local_e2e_test.py" startline="138" endline="170" pcid="2598">
  def testPipelineCompile(self):
    # Invalid DSL path
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_flink.py')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'local', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Invalid pipeline path: {}'.format(pipeline_path),
                  result.output)

    # Wrong Runner.
    pipeline_path = os.path.join(self.tmp_dir, 'empty_file.py')
    io_utils.write_string_file(pipeline_path, '')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'local', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Cannot find LocalDagRunner.run()', result.output)

    # Successful compilation.
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_local_2.py')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'local', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Pipeline compiled successfully', result.output)

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_beam_e2e_test.py" startline="136" endline="168" pcid="2612">
  def testPipelineCompile(self):
    # Invalid DSL path
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_flink.py')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'beam', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Invalid pipeline path: {}'.format(pipeline_path),
                  result.output)

    # Wrong Runner.
    pipeline_path = os.path.join(self.tmp_dir, 'empty_file.py')
    io_utils.write_string_file(pipeline_path, '')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'beam', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Cannot find BeamDagRunner.run()', result.output)

    # Successful compilation.
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_beam_2.py')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'beam', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Pipeline compiled successfully', result.output)

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_airflow_e2e_test.py" startline="196" endline="224" pcid="2652">
  def testPipelineCompile(self):
    # Invalid DSL path
    pipeline_path = os.path.join(self._testdata_dir, 'non_existing.py')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'airflow', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Invalid pipeline path: {}'.format(pipeline_path),
                  result.output)

    # Wrong Runner.
    pipeline_path = os.path.join(self.tmp_dir, 'empty_file.py')
    io_utils.write_string_file(pipeline_path, '')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'airflow', '--pipeline_path',
        pipeline_path
    ])
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Cannot find AirflowDagRunner.run()', result.output)

    # Successful compilation.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'compile', '--engine', 'airflow', '--pipeline_path',
        self._pipeline_path
    ])
    self.assertIn('Compiling pipeline', result.output)
    self.assertIn('Pipeline compiled successfully', result.output)

</source>
</class>

<class classid="117" nclones="2" nlines="20" similarity="100">
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_local_e2e_test.py" startline="201" endline="228" pcid="2600">
  def testPipelineList(self):

    # Try listing pipelines when there are none.
    result = self.runner.invoke(cli_group,
                                ['pipeline', 'list', '--engine', 'local'])
    self.assertIn('CLI', result.output)
    self.assertIn('Listing all pipelines', result.output)
    self.assertIn('No pipelines to display.', result.output)

    # Create pipelines.
    pipeline_name_1 = 'chicago_taxi_local'
    pipeline_path_1 = os.path.join(self._testdata_dir,
                                   'test_pipeline_local_1.py')
    self._valid_create_and_check(pipeline_path_1, pipeline_name_1)

    pipeline_name_2 = 'chicago_taxi_local_v2'
    pipeline_path_2 = os.path.join(self._testdata_dir,
                                   'test_pipeline_local_3.py')
    self._valid_create_and_check(pipeline_path_2, pipeline_name_2)

    # List pipelines.
    result = self.runner.invoke(cli_group,
                                ['pipeline', 'list', '--engine', 'local'])
    self.assertIn('CLI', result.output)
    self.assertIn('Listing all pipelines', result.output)
    self.assertIn(pipeline_name_1, result.output)
    self.assertIn(pipeline_name_2, result.output)

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_beam_e2e_test.py" startline="199" endline="226" pcid="2614">
  def testPipelineList(self):

    # Try listing pipelines when there are none.
    result = self.runner.invoke(cli_group,
                                ['pipeline', 'list', '--engine', 'beam'])
    self.assertIn('CLI', result.output)
    self.assertIn('Listing all pipelines', result.output)
    self.assertIn('No pipelines to display.', result.output)

    # Create pipelines.
    pipeline_name_1 = 'chicago_taxi_beam'
    pipeline_path_1 = os.path.join(self._testdata_dir,
                                   'test_pipeline_beam_1.py')
    self._valid_create_and_check(pipeline_path_1, pipeline_name_1)

    pipeline_name_2 = 'chicago_taxi_beam_v2'
    pipeline_path_2 = os.path.join(self._testdata_dir,
                                   'test_pipeline_beam_3.py')
    self._valid_create_and_check(pipeline_path_2, pipeline_name_2)

    # List pipelines.
    result = self.runner.invoke(cli_group,
                                ['pipeline', 'list', '--engine', 'beam'])
    self.assertIn('CLI', result.output)
    self.assertIn('Listing all pipelines', result.output)
    self.assertIn(pipeline_name_1, result.output)
    self.assertIn(pipeline_name_2, result.output)

</source>
</class>

<class classid="118" nclones="2" nlines="43" similarity="100">
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_local_e2e_test.py" startline="229" endline="292" pcid="2601">
  def testPipelineSchema(self):
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_local_2.py')
    pipeline_name = 'chicago_taxi_local'

    # Try getting schema without creating pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'schema', '--engine', 'local', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Getting latest schema.', result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(pipeline_name),
                  result.output)

    # Create a pipeline.
    self._valid_create_and_check(pipeline_path, pipeline_name)

    # Try getting schema without creating a pipeline run.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'schema', '--engine', 'local', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Getting latest schema.', result.output)
    self.assertIn(
        'Create a run before inferring schema. If pipeline is already running, then wait for it to successfully finish.',
        result.output)

    # Run pipeline.
    self._valid_run_and_check(pipeline_name)

    # Try inferring schema without SchemaGen component.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'schema', '--engine', 'local', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Getting latest schema.', result.output)
    self.assertIn(
        'Either SchemaGen component does not exist or pipeline is still running. If pipeline is running, then wait for it to successfully finish.',
        result.output)

    # Create a pipeline.
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_local_3.py')
    pipeline_name = 'chicago_taxi_local_v2'
    self._valid_create_and_check(pipeline_path, pipeline_name)

    # Run pipeline
    self._valid_run_and_check(pipeline_name)

    # Infer Schema when pipeline runs for the first time.
    schema_path = os.path.join(os.getcwd(), 'schema.pbtxt')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'schema', '--engine', 'local', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Getting latest schema.', result.output)
    self.assertTrue(fileio.exists(schema_path))
    self.assertIn('Path to schema: {}'.format(schema_path), result.output)
    self.assertIn(
        '*********SCHEMA FOR {}**********'.format(pipeline_name.upper()),
        result.output)

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_beam_e2e_test.py" startline="227" endline="290" pcid="2615">
  def testPipelineSchema(self):
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_beam_2.py')
    pipeline_name = 'chicago_taxi_beam'

    # Try getting schema without creating pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'schema', '--engine', 'beam', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Getting latest schema.', result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(pipeline_name),
                  result.output)

    # Create a pipeline.
    self._valid_create_and_check(pipeline_path, pipeline_name)

    # Try getting schema without creating a pipeline run.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'schema', '--engine', 'beam', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Getting latest schema.', result.output)
    self.assertIn(
        'Create a run before inferring schema. If pipeline is already running, then wait for it to successfully finish.',
        result.output)

    # Run pipeline.
    self._valid_run_and_check(pipeline_name)

    # Try inferring schema without SchemaGen component.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'schema', '--engine', 'beam', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Getting latest schema.', result.output)
    self.assertIn(
        'Either SchemaGen component does not exist or pipeline is still running. If pipeline is running, then wait for it to successfully finish.',
        result.output)

    # Create a pipeline.
    pipeline_path = os.path.join(self._testdata_dir, 'test_pipeline_beam_3.py')
    pipeline_name = 'chicago_taxi_beam_v2'
    self._valid_create_and_check(pipeline_path, pipeline_name)

    # Run pipeline
    self._valid_run_and_check(pipeline_name)

    # Infer Schema when pipeline runs for the first time.
    schema_path = os.path.join(os.getcwd(), 'schema.pbtxt')
    result = self.runner.invoke(cli_group, [
        'pipeline', 'schema', '--engine', 'beam', '--pipeline_name',
        pipeline_name
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Getting latest schema.', result.output)
    self.assertTrue(fileio.exists(schema_path))
    self.assertIn('Path to schema: {}'.format(schema_path), result.output)
    self.assertIn(
        '*********SCHEMA FOR {}**********'.format(pipeline_name.upper()),
        result.output)

</source>
</class>

<class classid="119" nclones="2" nlines="14" similarity="100">
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_local_e2e_test.py" startline="303" endline="324" pcid="2603">
  def testRunCreate(self):
    # Create a pipeline first.
    pipeline_name_1 = 'chicago_taxi_local'
    pipeline_path_1 = os.path.join(self._testdata_dir,
                                   'test_pipeline_local_2.py')
    self._valid_create_and_check(pipeline_path_1, pipeline_name_1)

    # Now run a different pipeline
    pipeline_name_2 = 'chicago_taxi_local_v2'
    result = self.runner.invoke(cli_group, [
        'run', 'create', '--engine', 'local', '--pipeline_name', pipeline_name_2
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Creating a run for pipeline: {}'.format(pipeline_name_2),
                  result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(pipeline_name_2),
                  result.output)

    # Now run the pipeline
    self._valid_run_and_check(pipeline_name_1)


</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_beam_e2e_test.py" startline="301" endline="322" pcid="2617">
  def testRunCreate(self):
    # Create a pipeline first.
    pipeline_name_1 = 'chicago_taxi_beam'
    pipeline_path_1 = os.path.join(self._testdata_dir,
                                   'test_pipeline_beam_2.py')
    self._valid_create_and_check(pipeline_path_1, pipeline_name_1)

    # Now run a different pipeline
    pipeline_name_2 = 'chicago_taxi_beam_v2'
    result = self.runner.invoke(cli_group, [
        'run', 'create', '--engine', 'beam', '--pipeline_name', pipeline_name_2
    ])
    self.assertIn('CLI', result.output)
    self.assertIn('Creating a run for pipeline: {}'.format(pipeline_name_2),
                  result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(pipeline_name_2),
                  result.output)

    # Now run the pipeline
    self._valid_run_and_check(pipeline_name_1)


</source>
</class>

<class classid="120" nclones="3" nlines="15" similarity="76">
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_kubeflow_e2e_test.py" startline="201" endline="222" pcid="2631">
  def testPipelineUpdate(self):
    # Try pipeline update when pipeline does not exist.
    with self.assertRaises(subprocess.CalledProcessError) as cm:
      test_utils.run_cli([
          'pipeline', 'update', '--engine', 'kubeflow', '--pipeline_path',
          self._pipeline_path, '--endpoint', self._endpoint
      ])
    self.assertIn('Cannot find pipeline "{}".'.format(self._pipeline_name),
                  cm.exception.output)

    # Now update an existing pipeline.
    self._valid_create_and_check(self._pipeline_path, self._pipeline_name)

    result = test_utils.run_cli([
        'pipeline', 'update', '--engine', 'kubeflow', '--pipeline_path',
        self._pipeline_path, '--endpoint', self._endpoint
    ])
    self.assertIn('Updating pipeline', result)
    self.assertIn(
        'Pipeline "{}" updated successfully.'.format(self._pipeline_name),
        result)

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_kubeflow_e2e_test.py" startline="252" endline="273" pcid="2633">
  def testPipelineDelete(self):
    # Try deleting a non existent pipeline.
    with self.assertRaises(subprocess.CalledProcessError) as cm:
      test_utils.run_cli([
          'pipeline', 'delete', '--engine', 'kubeflow', '--pipeline_name',
          self._pipeline_name, '--endpoint', self._endpoint
      ])
    self.assertIn('Cannot find pipeline "{}".'.format(self._pipeline_name),
                  cm.exception.output)

    # Create a pipeline.
    self._valid_create_and_check(self._pipeline_path, self._pipeline_name)

    # Now delete the pipeline.
    result = test_utils.run_cli([
        'pipeline', 'delete', '--engine', 'kubeflow', '--pipeline_name',
        self._pipeline_name, '--endpoint', self._endpoint
    ])
    self.assertIn('Deleting pipeline', result)
    self.assertIn(
        'Pipeline {} deleted successfully.'.format(self._pipeline_name), result)

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_kubeflow_e2e_test.py" startline="313" endline="337" pcid="2637">
  def testRunCreate(self):
    with self.assertRaises(subprocess.CalledProcessError) as cm:
      test_utils.run_cli([
          'run', 'create', '--engine', 'kubeflow', '--pipeline_name',
          self._pipeline_name, '--endpoint', self._endpoint
      ])
    self.assertIn('Cannot find pipeline "{}".'.format(self._pipeline_name),
                  cm.exception.output)

    # Now create a pipeline.
    self._valid_create_and_check(self._pipeline_path, self._pipeline_name)

    # Run pipeline.
    result = test_utils.run_cli([
        'run', 'create', '--engine', 'kubeflow', '--pipeline_name',
        self._pipeline_name, '--endpoint', self._endpoint
    ])

    self.assertIn('Creating a run for pipeline: {}'.format(self._pipeline_name),
                  result)
    self.assertNotIn(
        'Pipeline "{}" does not exist.'.format(self._pipeline_name), result)
    self.assertIn('Run created for pipeline: {}'.format(self._pipeline_name),
                  result)

</source>
</class>

<class classid="121" nclones="2" nlines="17" similarity="94">
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_airflow_e2e_test.py" startline="172" endline="195" pcid="2651">
  def testPipelineUpdate(self):
    # Try pipeline update when pipeline does not exist.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'update', '--engine', 'airflow', '--pipeline_path',
        self._pipeline_path
    ])
    self.assertIn('Updating pipeline', result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(self._pipeline_name),
                  result.output)
    self.assertFalse(self._does_pipeline_args_file_exist(self._pipeline_name))

    # Now update an existing pipeline.
    self._valid_create_and_check(self._pipeline_path, self._pipeline_name)

    result = self.runner.invoke(cli_group, [
        'pipeline', 'update', '--engine', 'airflow', '--pipeline_path',
        self._pipeline_path
    ])
    self.assertIn('Updating pipeline', result.output)
    self.assertIn(
        'Pipeline "{}" updated successfully.'.format(self._pipeline_name),
        result.output)
    self.assertTrue(self._does_pipeline_args_file_exist(self._pipeline_name))

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/e2e/cli_airflow_e2e_test.py" startline="225" endline="249" pcid="2653">
  def testPipelineDelete(self):
    # Try deleting a non existent pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'delete', '--engine', 'airflow', '--pipeline_name',
        self._pipeline_name
    ])
    self.assertIn('Deleting pipeline', result.output)
    self.assertIn('Pipeline "{}" does not exist.'.format(self._pipeline_name),
                  result.output)
    self.assertFalse(self._does_pipeline_args_file_exist(self._pipeline_name))

    # Create a pipeline.
    self._valid_create_and_check(self._pipeline_path, self._pipeline_name)

    # Now delete the pipeline.
    result = self.runner.invoke(cli_group, [
        'pipeline', 'delete', '--engine', 'airflow', '--pipeline_name',
        self._pipeline_name
    ])
    self.assertIn('Deleting pipeline', result.output)
    self.assertFalse(self._does_pipeline_args_file_exist(self._pipeline_name))
    self.assertIn(
        'Pipeline "{}" deleted successfully.'.format(self._pipeline_name),
        result.output)

</source>
</class>

<class classid="122" nclones="9" nlines="13" similarity="71">
<source file="systems/tfx-1.6.1/tfx/tools/cli/testdata/test_pipeline_local_2.py" startline="36" endline="53" pcid="2679">
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     metadata_path: str) -> pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      additional_pipeline_args={},
  )


</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/testdata/test_pipeline_beam_1.py" startline="37" endline="60" pcid="2683">
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     metadata_path: str) -> pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  infer_schema = SchemaGen(statistics=statistics_gen.outputs['statistics'])

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen, statistics_gen, infer_schema],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      additional_pipeline_args={},
  )


</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/testdata/test_pipeline_local_1.py" startline="39" endline="63" pcid="2691">
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     metadata_path: str) -> pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  infer_schema = SchemaGen(statistics=statistics_gen.outputs['statistics'])

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen, statistics_gen, infer_schema],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      additional_pipeline_args={},
  )

# We need to guard this in this conditional because this file is loaded multiple
# times in a single test run of local_handler_test.py.
</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/testdata/test_pipeline_beam_2.py" startline="36" endline="53" pcid="2690">
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     metadata_path: str) -> pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      additional_pipeline_args={},
  )


</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/testdata/test_pipeline_kubeflow_v2_2.py" startline="32" endline="47" pcid="2685">
def _create_pipeline(pipeline_name: str, pipeline_root: str,
                     data_root: str) -> pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen],
      enable_cache=True,
      additional_pipeline_args={},
  )


</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/testdata/test_pipeline_kubeflow_v2_1.py" startline="34" endline="55" pcid="2688">
def _create_pipeline(pipeline_name: str, pipeline_root: str,
                     data_root: str) -> pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  infer_schema = SchemaGen(statistics=statistics_gen.outputs['statistics'])

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen, statistics_gen, infer_schema],
      enable_cache=True,
      additional_pipeline_args={},
  )


</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/testdata/test_pipeline_beam_3.py" startline="51" endline="79" pcid="2687">
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     metadata_path: str) -> pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  infer_schema = SchemaGen(statistics=statistics_gen.outputs['statistics'])

  # Performs anomaly detection based on statistics and data schema.
  validate_stats = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=infer_schema.outputs['schema'])

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen, statistics_gen, infer_schema, validate_stats],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      additional_pipeline_args={},
  )


</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/testdata/test_pipeline_local_3.py" startline="51" endline="79" pcid="2684">
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     metadata_path: str) -> pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  infer_schema = SchemaGen(statistics=statistics_gen.outputs['statistics'])

  # Performs anomaly detection based on statistics and data schema.
  validate_stats = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=infer_schema.outputs['schema'])

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen, statistics_gen, infer_schema, validate_stats],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
      additional_pipeline_args={},
  )


</source>
<source file="systems/tfx-1.6.1/tfx/examples/custom_components/hello_world/example/taxi_pipeline_hello.py" startline="43" endline="65" pcid="3035">
def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,
                     metadata_path: str) -> pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""
  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=data_root)

  hello = component.HelloComponent(
      input_data=example_gen.outputs['examples'], name=u'HelloWorld')

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=hello.outputs['output_data'])

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[example_gen, hello, statistics_gen],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path))


# To run this pipeline from the python CLI:
#   $python taxi_pipeline_hello.py
</source>
</class>

<class classid="123" nclones="2" nlines="12" similarity="84">
<source file="systems/tfx-1.6.1/tfx/tools/cli/commands/pipeline_test.py" startline="49" endline="62" pcid="2704">
  def testPipelineUpdate(self):
    result = self.runner.invoke(pipeline_group, [
        'update', '--pipeline_path', 'chicago.py', '--engine', 'kubeflow',
        '--iap_client_id', 'fake_id', '--namespace', 'kubeflow', '--endpoint',
        'endpoint_url'
    ])
    self.assertIn('Updating pipeline', result.output)
    result = self.runner.invoke(pipeline_group, [
        'update', '--pipeline-path', 'chicago.py', '--engine', 'kubeflow',
        '--iap-client-id', 'fake_id', '--namespace', 'kubeflow', '--endpoint',
        'endpoint_url'
    ])
    self.assertIn('Updating pipeline', result.output)

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/commands/run_test.py" startline="155" endline="171" pcid="2731">
  def testRunDelete(self):
    result = self.runner.invoke(run_group, [
        'delete', '--run_id', 'kubeflow_run_id', '--engine', 'kubeflow',
        '--iap_client_id', 'fake_id', '--namespace', 'kubeflow', '--endpoint',
        'endpoint_url'
    ])
    self.assertIn('Deleting run', result.output)
    self.assertSucceeded(result)
    result = self.runner.invoke(run_group, [
        'delete', '--run-id', 'kubeflow_run_id', '--engine', 'kubeflow',
        '--iap-client-id', 'fake_id', '--namespace', 'kubeflow', '--endpoint',
        'endpoint_url'
    ])
    self.assertIn('Deleting run', result.output)
    self.assertSucceeded(result)


</source>
</class>

<class classid="124" nclones="2" nlines="11" similarity="100">
<source file="systems/tfx-1.6.1/tfx/tools/cli/commands/run_test.py" startline="101" endline="112" pcid="2727">
  def testRunList(self):
    result = self.runner.invoke(
        run_group,
        ['list', '--pipeline_name', 'chicago', '--engine', 'airflow'])
    self.assertIn('Listing all runs of pipeline', result.output)
    self.assertSucceeded(result)
    result = self.runner.invoke(
        run_group,
        ['list', '--pipeline-name', 'chicago', '--engine', 'airflow'])
    self.assertIn('Listing all runs of pipeline', result.output)
    self.assertSucceeded(result)

</source>
<source file="systems/tfx-1.6.1/tfx/tools/cli/commands/run_test.py" startline="143" endline="154" pcid="2730">
  def testRunTerminate(self):
    result = self.runner.invoke(
        run_group,
        ['terminate', '--run_id', 'airflow_run_id', '--engine', 'airflow'])
    self.assertIn('Terminating run.', result.output)
    self.assertSucceeded(result)
    result = self.runner.invoke(
        run_group,
        ['terminate', '--run-id', 'airflow_run_id', '--engine', 'airflow'])
    self.assertIn('Terminating run.', result.output)
    self.assertSucceeded(result)

</source>
</class>

<class classid="125" nclones="2" nlines="22" similarity="100">
<source file="systems/tfx-1.6.1/tfx/experimental/templates/taxi/kubeflow_runner.py" startline="50" endline="97" pcid="2754">
def run():
  """Define a kubeflow pipeline."""

  # Metadata config. The defaults works work with the installation of
  # KF Pipelines using Kubeflow. If installing KF Pipelines using the
  # lightweight deployment option, you may need to override the defaults.
  # If you use Kubeflow, metadata will be written to MySQL database inside
  # Kubeflow cluster.
  metadata_config = tfx.orchestration.experimental.get_default_kubeflow_metadata_config(
  )

  runner_config = tfx.orchestration.experimental.KubeflowDagRunnerConfig(
      kubeflow_metadata_config=metadata_config,
      tfx_image=configs.PIPELINE_IMAGE)
  pod_labels = {
      'add-pod-env': 'true',
      tfx.orchestration.experimental.LABEL_KFP_SDK_ENV: 'tfx-template'
  }
  tfx.orchestration.experimental.KubeflowDagRunner(
      config=runner_config, pod_labels_to_attach=pod_labels
  ).run(
      pipeline.create_pipeline(
          pipeline_name=configs.PIPELINE_NAME,
          pipeline_root=PIPELINE_ROOT,
          data_path=DATA_PATH,
          # TODO(step 7): (Optional) Uncomment below to use BigQueryExampleGen.
          # query=configs.BIG_QUERY_QUERY,
          # TODO(step 5): (Optional) Set the path of the customized schema.
          # schema_path=generated_schema_path,
          preprocessing_fn=configs.PREPROCESSING_FN,
          run_fn=configs.RUN_FN,
          train_args=tfx.proto.TrainArgs(num_steps=configs.TRAIN_NUM_STEPS),
          eval_args=tfx.proto.EvalArgs(num_steps=configs.EVAL_NUM_STEPS),
          eval_accuracy_threshold=configs.EVAL_ACCURACY_THRESHOLD,
          serving_model_dir=SERVING_MODEL_DIR,
          # TODO(step 7): (Optional) Uncomment below to use provide GCP related
          #               config for BigQuery with Beam DirectRunner.
          # beam_pipeline_args=configs
          # .BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS,
          # TODO(step 8): (Optional) Uncomment below to use Dataflow.
          # beam_pipeline_args=configs.DATAFLOW_BEAM_PIPELINE_ARGS,
          # TODO(step 9): (Optional) Uncomment below to use Cloud AI Platform.
          # ai_platform_training_args=configs.GCP_AI_PLATFORM_TRAINING_ARGS,
          # TODO(step 9): (Optional) Uncomment below to use Cloud AI Platform.
          # ai_platform_serving_args=configs.GCP_AI_PLATFORM_SERVING_ARGS,
      ))


</source>
<source file="systems/tfx-1.6.1/tfx/experimental/templates/penguin/kubeflow_runner.py" startline="50" endline="90" pcid="2797">
def run():
  """Define a kubeflow pipeline."""

  # Metadata config. The defaults works work with the installation of
  # KF Pipelines using Kubeflow. If installing KF Pipelines using the
  # lightweight deployment option, you may need to override the defaults.
  # If you use Kubeflow, metadata will be written to MySQL database inside
  # Kubeflow cluster.
  metadata_config = tfx.orchestration.experimental.get_default_kubeflow_metadata_config(
  )

  runner_config = tfx.orchestration.experimental.KubeflowDagRunnerConfig(
      kubeflow_metadata_config=metadata_config,
      tfx_image=configs.PIPELINE_IMAGE)
  pod_labels = {
      'add-pod-env': 'true',
      tfx.orchestration.experimental.LABEL_KFP_SDK_ENV: 'tfx-template'
  }
  tfx.orchestration.experimental.KubeflowDagRunner(
      config=runner_config, pod_labels_to_attach=pod_labels
  ).run(
      pipeline.create_pipeline(
          pipeline_name=configs.PIPELINE_NAME,
          pipeline_root=PIPELINE_ROOT,
          data_path=DATA_PATH,
          # NOTE: Use `query` instead of `data_path` to use BigQueryExampleGen.
          # query=configs.BIG_QUERY_QUERY,
          # NOTE: Set the path of the customized schema if any.
          # schema_path=generated_schema_path,
          preprocessing_fn=configs.PREPROCESSING_FN,
          run_fn=configs.RUN_FN,
          train_args=tfx.proto.TrainArgs(num_steps=configs.TRAIN_NUM_STEPS),
          eval_args=tfx.proto.EvalArgs(num_steps=configs.EVAL_NUM_STEPS),
          eval_accuracy_threshold=configs.EVAL_ACCURACY_THRESHOLD,
          serving_model_dir=SERVING_MODEL_DIR,
          # NOTE: Provide GCP configs to use BigQuery with Beam DirectRunner.
          # beam_pipeline_args=configs.
          # BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS,
      ))


</source>
</class>

<class classid="126" nclones="2" nlines="14" similarity="100">
<source file="systems/tfx-1.6.1/tfx/experimental/templates/taxi/local_runner.py" startline="52" endline="77" pcid="2773">
def run():
  """Define a local pipeline."""

  tfx.orchestration.LocalDagRunner().run(
      pipeline.create_pipeline(
          pipeline_name=configs.PIPELINE_NAME,
          pipeline_root=PIPELINE_ROOT,
          data_path=DATA_PATH,
          # TODO(step 7): (Optional) Uncomment here to use BigQueryExampleGen.
          # query=configs.BIG_QUERY_QUERY,
          # TODO(step 5): (Optional) Set the path of the customized schema.
          # schema_path=generated_schema_path,
          preprocessing_fn=configs.PREPROCESSING_FN,
          run_fn=configs.RUN_FN,
          train_args=tfx.proto.TrainArgs(num_steps=configs.TRAIN_NUM_STEPS),
          eval_args=tfx.proto.EvalArgs(num_steps=configs.EVAL_NUM_STEPS),
          eval_accuracy_threshold=configs.EVAL_ACCURACY_THRESHOLD,
          serving_model_dir=SERVING_MODEL_DIR,
          # TODO(step 7): (Optional) Uncomment here to use provide GCP related
          #               config for BigQuery with Beam DirectRunner.
          # beam_pipeline_args=configs.
          # BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS,
          metadata_connection_config=tfx.orchestration.metadata
          .sqlite_metadata_connection_config(METADATA_PATH)))


</source>
<source file="systems/tfx-1.6.1/tfx/experimental/templates/penguin/local_runner.py" startline="53" endline="77" pcid="2812">
def run():
  """Define a pipeline."""

  tfx.orchestration.LocalDagRunner().run(
      pipeline.create_pipeline(
          pipeline_name=configs.PIPELINE_NAME,
          pipeline_root=PIPELINE_ROOT,
          data_path=DATA_PATH,
          # NOTE: Use `query` instead of `data_path` to use BigQueryExampleGen.
          # query=configs.BIG_QUERY_QUERY,
          # NOTE: Set the path of the customized schema if any.
          # schema_path=generated_schema_path,
          preprocessing_fn=configs.PREPROCESSING_FN,
          run_fn=configs.RUN_FN,
          train_args=tfx.proto.TrainArgs(num_steps=configs.TRAIN_NUM_STEPS),
          eval_args=tfx.proto.EvalArgs(num_steps=configs.EVAL_NUM_STEPS),
          eval_accuracy_threshold=configs.EVAL_ACCURACY_THRESHOLD,
          serving_model_dir=SERVING_MODEL_DIR,
          # NOTE: Provide GCP configs to use BigQuery with Beam DirectRunner.
          # beam_pipeline_args=configs.
          # BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS,
          metadata_connection_config=tfx.orchestration.metadata
          .sqlite_metadata_connection_config(METADATA_PATH)))


</source>
</class>

<class classid="127" nclones="2" nlines="12" similarity="100">
<source file="systems/tfx-1.6.1/tfx/experimental/templates/test_utils.py" startline="152" endline="164" pcid="2793">
  def _create_pipeline(self):
    result = self._runCli([
        'pipeline',
        'create',
        '--engine',
        'local',
        '--pipeline_path',
        'local_runner.py',
    ])
    self.assertIn(
        'Pipeline "{}" created successfully.'.format(self._pipeline_name),
        result)

</source>
<source file="systems/tfx-1.6.1/tfx/experimental/templates/test_utils.py" startline="165" endline="177" pcid="2794">
  def _update_pipeline(self):
    result = self._runCli([
        'pipeline',
        'update',
        '--engine',
        'local',
        '--pipeline_path',
        'local_runner.py',
    ])
    self.assertIn(
        'Pipeline "{}" updated successfully.'.format(self._pipeline_name),
        result)

</source>
</class>

<class classid="128" nclones="2" nlines="25" similarity="77">
<source file="systems/tfx-1.6.1/tfx/experimental/templates/penguin/models/model.py" startline="35" endline="83" pcid="2798">
def _get_tf_examples_serving_signature(model, schema, tf_transform_output):
  """Returns a serving signature that accepts `tensorflow.Example`."""

  if tf_transform_output is None:  # Transform component is not used.

    @tf.function(input_signature=[
        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')
    ])
    def serve_tf_examples_fn(serialized_tf_example):
      """Returns the output to be used in the serving signature."""
      raw_feature_spec = schema_utils.schema_as_feature_spec(
          schema).feature_spec
      # Remove label feature since these will not be present at serving time.
      raw_feature_spec.pop(features.LABEL_KEY)
      raw_features = tf.io.parse_example(serialized_tf_example,
                                         raw_feature_spec)
      logging.info('serve_features = %s', raw_features)

      outputs = model(raw_features)
      # TODO(b/154085620): Convert the predicted labels from the model using a
      # reverse-lookup (opposite of transform.py).
      return {'outputs': outputs}

  else:  # Transform component exists.
    # We need to track the layers in the model in order to save it.
    # TODO(b/162357359): Revise once the bug is resolved.
    model.tft_layer_inference = tf_transform_output.transform_features_layer()

    @tf.function(input_signature=[
        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')
    ])
    def serve_tf_examples_fn(serialized_tf_example):
      """Returns the output to be used in the serving signature."""
      raw_feature_spec = tf_transform_output.raw_feature_spec()
      # Remove label feature since these will not be present at serving time.
      raw_feature_spec.pop(features.LABEL_KEY)
      raw_features = tf.io.parse_example(serialized_tf_example,
                                         raw_feature_spec)
      transformed_features = model.tft_layer_inference(raw_features)
      logging.info('serve_transformed_features = %s', transformed_features)

      outputs = model(transformed_features)
      # TODO(b/154085620): Convert the predicted labels from the model using a
      # reverse-lookup (opposite of transform.py).
      return {'outputs': outputs}

  return serve_tf_examples_fn


</source>
<source file="systems/tfx-1.6.1/tfx/experimental/templates/penguin/models/model.py" startline="84" endline="118" pcid="2801">
def _get_transform_features_signature(model, schema, tf_transform_output):
  """Returns a serving signature that applies tf.Transform to features."""

  if tf_transform_output is None:  # Transform component is not used.
    @tf.function(input_signature=[
        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')
    ])
    def transform_features_fn(serialized_tf_example):
      """Returns the transformed_features to be fed as input to evaluator."""
      raw_feature_spec = schema_utils.schema_as_feature_spec(
          schema).feature_spec
      raw_features = tf.io.parse_example(serialized_tf_example,
                                         raw_feature_spec)
      logging.info('eval_features = %s', raw_features)
      return raw_features
  else:  # Transform component exists.
    # We need to track the layers in the model in order to save it.
    # TODO(b/162357359): Revise once the bug is resolved.
    model.tft_layer_eval = tf_transform_output.transform_features_layer()

    @tf.function(input_signature=[
        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')
    ])
    def transform_features_fn(serialized_tf_example):
      """Returns the transformed_features to be fed as input to evaluator."""
      raw_feature_spec = tf_transform_output.raw_feature_spec()
      raw_features = tf.io.parse_example(serialized_tf_example,
                                         raw_feature_spec)
      transformed_features = model.tft_layer_eval(raw_features)
      logging.info('eval_transformed_features = %s', transformed_features)
      return transformed_features

  return transform_features_fn


</source>
</class>

<class classid="129" nclones="6" nlines="13" similarity="71">
<source file="systems/tfx-1.6.1/tfx/experimental/templates/container_based_test_case.py" startline="181" endline="195" pcid="2834">
  def _create_pipeline(self):
    self._runCli([
        'pipeline',
        'create',
        '--engine',
        'kubeflow',
        '--pipeline_path',
        'kubeflow_runner.py',
        '--endpoint',
        self._endpoint,
        '--build-image',
        '--build-base-image',
        self._base_container_image,
    ])

</source>
<source file="systems/tfx-1.6.1/tfx/experimental/templates/container_based_test_case.py" startline="301" endline="315" pcid="2844">
  def _run_pipeline(self):
    result = self._runCli([
        'run',
        'create',
        '--engine',
        'vertex',
        '--pipeline_name',
        self._pipeline_name,
        '--project',
        self._GCP_PROJECT_ID,
        '--region',
        self._GCP_REGION,
    ])
    run_id = self._parse_run_id(result)
    self._wait_until_completed(run_id)
</source>
<source file="systems/tfx-1.6.1/tfx/experimental/templates/container_based_test_case.py" startline="206" endline="218" pcid="2836">
  def _update_pipeline(self):
    self._runCli([
        'pipeline',
        'update',
        '--engine',
        'kubeflow',
        '--pipeline_path',
        'kubeflow_runner.py',
        '--endpoint',
        self._endpoint,
        '--build-image',
    ])

</source>
<source file="systems/tfx-1.6.1/tfx/experimental/templates/container_based_test_case.py" startline="290" endline="300" pcid="2843">
  def _update_pipeline(self):
    self._runCli([
        'pipeline',
        'update',
        '--engine',
        'vertex',
        '--pipeline_path',
        'kubeflow_v2_runner.py',
        '--build-image',
    ])

</source>
<source file="systems/tfx-1.6.1/tfx/experimental/templates/container_based_test_case.py" startline="277" endline="289" pcid="2842">
  def _create_pipeline(self):
    self._runCli([
        'pipeline',
        'create',
        '--engine',
        'vertex',
        '--pipeline-path',
        'kubeflow_v2_runner.py',
        '--build-image',
        '--build-base-image',
        self._base_container_image,
    ])

</source>
<source file="systems/tfx-1.6.1/tfx/experimental/templates/container_based_test_case.py" startline="219" endline="234" pcid="2837">
  def _run_pipeline(self):
    result = self._runCli([
        'run',
        'create',
        '--engine',
        'kubeflow',
        '--pipeline_name',
        self._pipeline_name,
        '--endpoint',
        self._endpoint,
    ])
    run_id = self._parse_run_id(result)
    self._wait_until_completed(run_id)
    kubeflow_test_utils.print_failure_log_for_run(self._endpoint, run_id,
                                                  self._namespace)

</source>
</class>

<class classid="130" nclones="2" nlines="38" similarity="89">
<source file="systems/tfx-1.6.1/tfx/experimental/pipeline_testing/examples/chicago_taxi_pipeline/taxi_pipeline_regression_e2e_test.py" startline="34" endline="82" pcid="2851">
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    self._pipeline_name = 'beam_stub_test'
    # This example assumes that the taxi data and taxi utility function are
    # stored in tfx/examples/chicago_taxi_pipeline. Feel free to customize this
    # as needed.
    taxi_root = os.path.dirname(taxi_pipeline_local.__file__)
    self._data_root = os.path.join(taxi_root, 'data', 'simple')
    self._module_file = os.path.join(taxi_root, 'taxi_utils.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'tfx', 'pipelines',
                                       self._pipeline_name)
    # Metadata path for recording successful pipeline run.
    self._recorded_mlmd_path = os.path.join(self._test_dir, 'tfx', 'record',
                                            'metadata.db')
    # Metadata path for stub pipeline runs.
    self._metadata_path = os.path.join(self._test_dir, 'tfx', 'metadata',
                                       self._pipeline_name, 'metadata.db')
    self._recorded_output_dir = os.path.join(self._test_dir, 'testdata')

    # Runs the pipeline and record to self._recorded_output_dir
    record_taxi_pipeline = taxi_pipeline_local._create_pipeline(  # pylint:disable=protected-access
        pipeline_name=self._pipeline_name,
        data_root=self._data_root,
        module_file=self._module_file,
        serving_model_dir=self._serving_model_dir,
        pipeline_root=self._pipeline_root,
        metadata_path=self._recorded_mlmd_path,
        beam_pipeline_args=[])

    BeamDagRunner().run(record_taxi_pipeline)

    pipeline_recorder_utils.record_pipeline(
        output_dir=self._recorded_output_dir,
        metadata_db_uri=self._recorded_mlmd_path,
        pipeline_name=self._pipeline_name)

    self.taxi_pipeline = taxi_pipeline_local._create_pipeline(  # pylint:disable=protected-access
        pipeline_name=self._pipeline_name,
        data_root=self._data_root,
        module_file=self._module_file,
        serving_model_dir=self._serving_model_dir,
        pipeline_root=self._pipeline_root,
        metadata_path=self._metadata_path,
        beam_pipeline_args=[])

</source>
<source file="systems/tfx-1.6.1/tfx/experimental/pipeline_testing/examples/imdb_pipeline/imdb_stub_pipeline_regression_e2e_test.py" startline="34" endline="81" pcid="2861">
  def setUp(self):
    super().setUp()
    self._test_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)
    self._pipeline_name = 'imdb_stub_test'
    # This example assumes that the imdb data and imdb utility function are
    # stored in tfx/examples/imdb. Feel free to customize this as needed.
    imdb_root = os.path.dirname(imdb_pipeline_native_keras.__file__)
    self._data_root = os.path.join(imdb_root, 'data')
    self._module_file = os.path.join(imdb_root, 'imdb_utils_native_keras.py')
    self._serving_model_dir = os.path.join(self._test_dir, 'serving_model')
    self._pipeline_root = os.path.join(self._test_dir, 'pipelines',
                                       self._pipeline_name)
    # Metadata path for recording successful pipeline run.
    self._recorded_mlmd_path = os.path.join(self._test_dir, 'record',
                                            'metadata.db')
    # Metadata path for stub pipeline
    self._metadata_path = os.path.join(self._test_dir, 'metadata',
                                       self._pipeline_name, 'metadata.db')
    self._recorded_output_dir = os.path.join(self._test_dir, 'testdata')

    record_imdb_pipeline = imdb_pipeline_native_keras._create_pipeline(  # pylint:disable=protected-access
        pipeline_name=self._pipeline_name,
        data_root=self._data_root,
        module_file=self._module_file,
        serving_model_dir=self._serving_model_dir,
        pipeline_root=self._pipeline_root,
        metadata_path=self._recorded_mlmd_path,
        beam_pipeline_args=[])

    BeamDagRunner().run(record_imdb_pipeline)

    pipeline_recorder_utils.record_pipeline(
        output_dir=self._recorded_output_dir,
        metadata_db_uri=self._recorded_mlmd_path,
        pipeline_name=self._pipeline_name)

    # Run pipeline with stub executors.
    self.imdb_pipeline = imdb_pipeline_native_keras._create_pipeline(  # pylint:disable=protected-access
        pipeline_name=self._pipeline_name,
        data_root=self._data_root,
        module_file=self._module_file,
        serving_model_dir=self._serving_model_dir,
        pipeline_root=self._pipeline_root,
        metadata_path=self._metadata_path,
        beam_pipeline_args=[])

</source>
</class>

<class classid="131" nclones="2" nlines="57" similarity="81">
<source file="systems/tfx-1.6.1/tfx/experimental/pipeline_testing/examples/chicago_taxi_pipeline/taxi_pipeline_regression_e2e_test.py" startline="117" endline="200" pcid="2860">
  def testStubbedTaxiPipelineBeam(self):
    pipeline_ir = compiler.Compiler().compile(self.taxi_pipeline)

    logging.info('Replacing with test_data_dir:%s', self._recorded_output_dir)
    pipeline_mock.replace_executor_with_stub(pipeline_ir,
                                             self._recorded_output_dir, [])

    BeamDagRunner().run_with_ir(pipeline_ir)

    self.assertTrue(fileio.exists(self._metadata_path))

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)

    # Verify that recorded files are successfully copied to the output uris.
    with metadata.Metadata(metadata_config) as m:
      artifacts = m.store.get_artifacts()
      artifact_count = len(artifacts)
      executions = m.store.get_executions()
      execution_count = len(executions)
      # Artifact count is greater by 7 due to extra artifacts produced by
      # Evaluator(blessing and evaluation), Trainer(model and model_run) and
      # Transform(example, graph, cache, pre_transform_statistics,
      # pre_transform_schema, post_transform_statistics, post_transform_schema,
      # post_transform_anomalies) minus Resolver which doesn't generate
      # new artifact.
      self.assertEqual(artifact_count, execution_count + 7)
      self.assertLen(self.taxi_pipeline.components, execution_count)

      for execution in executions:
        component_id = pipeline_recorder_utils.get_component_id_from_execution(
            m, execution)
        if component_id.startswith('Resolver'):
          continue
        eid = [execution.id]
        events = m.store.get_events_by_execution_ids(eid)
        output_events = [
            x for x in events if x.type == metadata_store_pb2.Event.OUTPUT
        ]
        for event in output_events:
          steps = event.path.steps
          self.assertTrue(steps[0].HasField('key'))
          name = steps[0].key
          artifacts = m.store.get_artifacts_by_id([event.artifact_id])
          for idx, artifact in enumerate(artifacts):
            self.assertDirectoryEqual(
                artifact.uri,
                os.path.join(self._recorded_output_dir, component_id, name,
                             str(idx)))

    # Calls verifier for pipeline output artifacts, excluding the resolver node.
    BeamDagRunner().run(self.taxi_pipeline)
    pipeline_outputs = executor_verifier_utils.get_pipeline_outputs(
        self.taxi_pipeline.metadata_connection_config, self._pipeline_name)

    verifier_map = {
        'model': self._verify_model,
        'model_run': self._verify_model,
        'examples': self._verify_examples,
        'schema': self._verify_schema,
        'anomalies': self._verify_anomalies,
        'evaluation': self._verify_evaluation,
        # A subdirectory of updated_analyzer_cache has changing name.
        'updated_analyzer_cache': self._veryify_root_dir,
    }

    # List of components to verify. Resolver is ignored because it
    # doesn't have an executor.
    verify_component_ids = [
        component.id
        for component in self.taxi_pipeline.components
        if not component.id.startswith('Resolver')
    ]

    for component_id in verify_component_ids:
      logging.info('Verifying %s', component_id)
      for key, artifact_dict in pipeline_outputs[component_id].items():
        for idx, artifact in artifact_dict.items():
          recorded_uri = os.path.join(self._recorded_output_dir, component_id,
                                      key, str(idx))
          verifier_map.get(key, self._verify_file_path)(artifact.uri,
                                                        recorded_uri)


</source>
<source file="systems/tfx-1.6.1/tfx/experimental/pipeline_testing/examples/imdb_pipeline/imdb_stub_pipeline_regression_e2e_test.py" startline="116" endline="186" pcid="2870">
  def testStubbedImdbPipelineBeam(self):
    pipeline_ir = compiler.Compiler().compile(self.imdb_pipeline)

    pipeline_mock.replace_executor_with_stub(pipeline_ir,
                                             self._recorded_output_dir, [])

    BeamDagRunner().run_with_ir(pipeline_ir)

    self.assertTrue(fileio.exists(self._metadata_path))

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)

    # Verify that recorded files are successfully copied to the output uris.
    with metadata.Metadata(metadata_config) as m:
      for execution in m.store.get_executions():
        component_id = pipeline_recorder_utils.get_component_id_from_execution(
            m, execution)
        if component_id.startswith('Resolver'):
          continue
        eid = [execution.id]
        events = m.store.get_events_by_execution_ids(eid)
        output_events = [
            x for x in events if x.type == metadata_store_pb2.Event.OUTPUT
        ]
        for event in output_events:
          steps = event.path.steps
          assert steps[0].HasField('key')
          name = steps[0].key
          artifacts = m.store.get_artifacts_by_id([event.artifact_id])
          for idx, artifact in enumerate(artifacts):
            self.assertDirectoryEqual(
                artifact.uri,
                os.path.join(self._recorded_output_dir, component_id, name,
                             str(idx)))

    # Calls verifier for pipeline output artifacts, excluding the resolver node.
    BeamDagRunner().run(self.imdb_pipeline)
    pipeline_outputs = executor_verifier_utils.get_pipeline_outputs(
        self.imdb_pipeline.metadata_connection_config,
        self._pipeline_name)

    verifier_map = {
        'model': self._verify_model,
        'model_run': self._verify_model,
        'examples': self._verify_examples,
        'schema': self._verify_schema,
        'anomalies': self._verify_anomalies,
        'evaluation': self._verify_evaluation,
        # A subdirectory of updated_analyzer_cache has changing name.
        'updated_analyzer_cache': self._veryify_root_dir,
    }

    # List of components to verify. Resolver is ignored because it
    # doesn't have an executor.
    verify_component_ids = [
        component.id
        for component in self.imdb_pipeline.components
        if not component.id.startswith('Resolver')
    ]

    for component_id in verify_component_ids:
      for key, artifact_dict in pipeline_outputs[component_id].items():
        for idx, artifact in artifact_dict.items():
          logging.info('Verifying %s', component_id)
          recorded_uri = os.path.join(self._recorded_output_dir, component_id,
                                      key, str(idx))
          verifier_map.get(key, self._verify_file_path)(artifact.uri,
                                                        recorded_uri)


</source>
</class>

<class classid="132" nclones="6" nlines="21" similarity="76">
<source file="systems/tfx-1.6.1/tfx/examples/bert/cola/bert_cola_pipeline_e2e_test.py" startline="66" endline="91" pcid="2885">
  def testColaPipelineNativeKeras(self):
    pipeline = bert_cola_pipeline._create_pipeline(
        pipeline_name=self._pipeline_name,
        data_root=self._data_root,
        module_file=self._module_file,
        serving_model_dir=self._serving_model_dir,
        pipeline_root=self._pipeline_root,
        metadata_path=self._metadata_path,
        beam_pipeline_args=[])

    LocalDagRunner().run(pipeline)

    self.assertTrue(fileio.exists(self._serving_model_dir))
    self.assertTrue(fileio.exists(self._metadata_path))
    expected_execution_count = 9  # 8 components + 1 resolver
    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(expected_execution_count, execution_count)

    self.assertPipelineExecution()


</source>
<source file="systems/tfx-1.6.1/tfx/examples/bert/mrpc/bert_mrpc_pipeline_e2e_test.py" startline="66" endline="91" pcid="2905">
  def testMrpcPipelineNativeKeras(self):
    pipeline = bert_mrpc_pipeline._create_pipeline(
        pipeline_name=self._pipeline_name,
        data_root=self._data_root,
        module_file=self._module_file,
        serving_model_dir=self._serving_model_dir,
        pipeline_root=self._pipeline_root,
        metadata_path=self._metadata_path,
        beam_pipeline_args=[])

    LocalDagRunner().run(pipeline)

    self.assertTrue(fileio.exists(self._serving_model_dir))
    self.assertTrue(fileio.exists(self._metadata_path))
    expected_execution_count = 9  # 8 components + 1 resolver
    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(expected_execution_count, execution_count)

    self.assertPipelineExecution()


</source>
<source file="systems/tfx-1.6.1/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_local_e2e_test.py" startline="73" endline="96" pcid="3088">
  def testTaxiPipelineBeam(self):
    LocalDagRunner().run(
        taxi_pipeline_local._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            serving_model_dir=self._serving_model_dir,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            beam_pipeline_args=[]))

    self.assertTrue(fileio.exists(self._serving_model_dir))
    self.assertTrue(fileio.exists(self._metadata_path))
    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(10, execution_count)

    self.assertPipelineExecution()


</source>
<source file="systems/tfx-1.6.1/tfx/examples/cifar10/cifar10_pipeline_native_keras_e2e_test.py" startline="75" endline="101" pcid="2925">
  def testCIFAR10PipelineNativeKeras(self):
    pipeline = cifar10_pipeline_native_keras._create_pipeline(
        pipeline_name=self._pipeline_name,
        data_root=self._data_root,
        module_file=self._module_file,
        serving_model_dir_lite=self._serving_model_dir_lite,
        pipeline_root=self._pipeline_root,
        metadata_path=self._metadata_path,
        labels_path=self._labels_path,
        beam_pipeline_args=[])

    LocalDagRunner().run(pipeline)

    self.assertTrue(fileio.exists(self._serving_model_dir_lite))
    self.assertTrue(fileio.exists(self._metadata_path))
    expected_execution_count = 9  # 8 components + 1 resolver
    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(expected_execution_count, execution_count)

    self.assertPipelineExecution()


</source>
<source file="systems/tfx-1.6.1/tfx/examples/tfjs_next_page_prediction/tfjs_next_page_prediction_e2e_test.py" startline="73" endline="100" pcid="3017">
  def testTFJSPagePredictionPipeline(self):
    if not tf.executing_eagerly():
      self.skipTest('The test requires TF2.')
    pipeline = tfjs_next_page_prediction_pipeline._create_pipeline(
        pipeline_name=self._pipeline_name,
        data_root=self._data_root,
        module_file=self._module_file,
        serving_model_dir=self._serving_model_dir,
        pipeline_root=self._pipeline_root,
        metadata_path=self._metadata_path,
        beam_pipeline_args=[])

    LocalDagRunner().run(pipeline)

    self.assertTrue(fileio.exists(self._serving_model_dir))
    self.assertTrue(fileio.exists(self._metadata_path))
    expected_execution_count = 9  # 8 components + 1 resolver
    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(expected_execution_count, execution_count)

    self.assertPipelineExecution()


</source>
<source file="systems/tfx-1.6.1/tfx/examples/ranking/ranking_pipeline_e2e_test.py" startline="51" endline="72" pcid="2910">
  def testPipeline(self):
    BeamDagRunner().run(
        ranking_pipeline._create_pipeline(
            pipeline_name=self._pipeline_name,
            pipeline_root=self._tfx_root,
            data_root=self._data_root,
            module_file=self._module_file,
            serving_model_dir=self._serving_model_dir,
            metadata_path=self._metadata_path,
            beam_pipeline_args=['--direct_num_workers=1']))
    self.assertTrue(tf.io.gfile.exists(self._serving_model_dir))
    self.assertTrue(tf.io.gfile.exists(self._metadata_path))

    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(9, execution_count)


</source>
</class>

<class classid="133" nclones="2" nlines="14" similarity="85">
<source file="systems/tfx-1.6.1/tfx/examples/bert/cola/bert_cola_utils.py" startline="78" endline="157" pcid="2889">
def stats_options_updater_fn(
    stats_type: stats_options_util.StatsType,
    stats_options: tfdv.StatsOptions) -> tfdv.StatsOptions:
  """Update transform stats.

  This function is called by the Transform component before it computes
  pre-transform or post-transform statistics. It takes as input a stats_type,
  which indicates whether this call is intended for pre-transform or
  post-transform statistics. It also takes as argument the StatsOptions that
  are to be (optionally) modified before being passed onto TDFV.

  Args:
    stats_type: The type of statistics that are to be computed (pre-transform or
      post-transform).
    stats_options: The configuration to pass to TFDV for computing the desired
      statistics.

  Returns:
    An updated StatsOptions object.
  """
  if stats_type == stats_options_util.StatsType.POST_TRANSFORM:
    for f in stats_options.schema.feature:
      if f.name == _INPUT_WORD_IDS:
        # Here we extend the schema for the input_word_ids feature to enable
        # NLP statistics to be computed. We pass the vocabulary (_BERT_VOCAB)
        # that was used in tokenizing this feature, key tokens of interest
        # (e.g. "[CLS]",  "[PAD]", "[SEP]", "[UNK]") and key thresholds to
        # validate. For more information on the field descriptions, see here:
        # https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/schema.proto
        text_format.Parse(
            """
            vocabulary: "{vocab}"
            coverage: {{
              min_coverage: 1.0
              min_avg_token_length: 3.0
              excluded_string_tokens: ["[CLS]", "[PAD]", "[SEP]"]
              oov_string_tokens: ["[UNK]"]
             }}
             token_constraints {{
               string_value: "[CLS]"
               min_per_sequence: 1
               max_per_sequence: 1
               min_fraction_of_sequences: 1
               max_fraction_of_sequences: 1
             }}
             token_constraints {{
               string_value: "[PAD]"
               min_per_sequence: 0
               max_per_sequence: {max_pad_per_seq}
               min_fraction_of_sequences: 0
               max_fraction_of_sequences: 1
             }}
             token_constraints {{
               string_value: "[SEP]"
               min_per_sequence: 1
               max_per_sequence: 1
               min_fraction_of_sequences: 1
               max_fraction_of_sequences: 1
             }}
             token_constraints {{
               string_value: "[UNK]"
               min_per_sequence: 0
               max_per_sequence: {max_unk_per_seq}
               min_fraction_of_sequences: 0
               max_fraction_of_sequences: 1
             }}
             sequence_length_constraints {{
               excluded_string_value: ["[PAD]"]
               min_sequence_length: 3
               max_sequence_length: {max_seq_len}
             }}
            """.format(
                vocab=_BERT_VOCAB,
                max_pad_per_seq=_MAX_LEN - 3,  # [CLS], [SEP], Token
                max_unk_per_seq=_MAX_LEN - 2,  # [CLS], [SEP]
                max_seq_len=_MAX_LEN),
            f.natural_language_domain)
  return stats_options


</source>
<source file="systems/tfx-1.6.1/tfx/examples/bert/mrpc/bert_mrpc_utils.py" startline="81" endline="156" pcid="2897">
def stats_options_updater_fn(
    stats_type: stats_options_util.StatsType,
    stats_options: tfdv.StatsOptions) -> tfdv.StatsOptions:
  """Update transform stats.

  This function is called by the Transform component before it computes
  pre-transform or post-transform statistics. It takes as input a stats_type,
  which indicates whether this call is intended for pre-transform or
  post-transform statistics. It also takes as argument the StatsOptions that
  are to be (optionally) modified before being passed onto TDFV.

  Args:
    stats_type: The type of statistics that are to be computed (pre-transform or
      post-transform).
    stats_options: The configuration to pass to TFDV for computing the desired
      statistics.

  Returns:
    An updated StatsOptions object.
  """
  if stats_type == stats_options_util.StatsType.POST_TRANSFORM:
    for f in stats_options.schema.feature:
      if f.name == _INPUT_WORD_IDS:
        # Here we extend the schema for the input_word_ids feature to enable
        # NLP statistics to be computed. We pass the vocabulary (_BERT_VOCAB)
        # that was used in tokenizing this feature, key tokens of interest
        # (e.g. "[CLS]",  "[PAD]", "[SEP]", "[UNK]") and key thresholds to
        # validate. For more information on the field descriptions, see here:
        # https://github.com/tensorflow/metadata/blob/master/
        # tensorflow_metadata/proto/v0/schema.proto
        text_format.Parse(
            """
            vocabulary: "{vocab}"
            coverage: {{
              min_coverage: 1.0
              min_avg_token_length: 3.0
              excluded_string_tokens: ["[CLS]", "[PAD]", "[SEP]"]
              oov_string_tokens: ["[UNK]"]
             }}
             token_constraints {{
               string_value: "[CLS]"
               min_per_sequence: 1
               max_per_sequence: 1
               min_fraction_of_sequences: 1
               max_fraction_of_sequences: 1
             }}
             token_constraints {{
               string_value: "[PAD]"
               min_per_sequence: 0
               max_per_sequence: {max_pad_per_seq}
               min_fraction_of_sequences: 0
               max_fraction_of_sequences: 1
             }}
             token_constraints {{
               string_value: "[SEP]"
               min_per_sequence: 2
               max_per_sequence: 2
               min_fraction_of_sequences: 1
               max_fraction_of_sequences: 1
             }}
             token_constraints {{
               string_value: "[UNK]"
               min_per_sequence: 0
               max_per_sequence: {max_unk_per_seq}
               min_fraction_of_sequences: 0
               max_fraction_of_sequences: 1
             }}
            """.format(
                vocab=_BERT_VOCAB,
                max_pad_per_seq=_MAX_LEN - 3,  # [CLS], 2x[SEP], Token
                max_unk_per_seq=_MAX_LEN - 4  # [CLS], 2x[SEP]
            ),
            f.natural_language_domain)
  return stats_options


</source>
</class>

<class classid="134" nclones="4" nlines="10" similarity="80">
<source file="systems/tfx-1.6.1/tfx/examples/bert/cola/bert_cola_utils.py" startline="158" endline="184" pcid="2890">
def _input_fn(file_pattern: List[str],
              data_accessor: tfx.components.DataAccessor,
              tf_transform_output: tft.TFTransformOutput,
              batch_size: int = 200) -> tf.data.Dataset:
  """Generates features and label for tuning/training.

  Args:
    file_pattern: List of paths or patterns of materialized transformed input
      tfrecord files.
    data_accessor: DataAccessor for converting input to RecordBatch.
    tf_transform_output: A TFTransformOutput.
    batch_size: representing the number of consecutive elements of returned
      dataset to combine in a single batch

  Returns:
    A dataset that contains (features, indices) tuple where features is a
      dictionary of Tensors, and indices is a single Tensor of label indices.
  """
  dataset = data_accessor.tf_dataset_factory(
      file_pattern,
      tfxio.TensorFlowDatasetOptions(
          batch_size=batch_size, label_key=_LABEL_KEY),
      tf_transform_output.transformed_metadata.schema)
  dataset = dataset.repeat()
  return dataset.prefetch(tf.data.AUTOTUNE)


</source>
<source file="systems/tfx-1.6.1/tfx/examples/bert/mrpc/bert_mrpc_utils.py" startline="157" endline="183" pcid="2898">
def _input_fn(file_pattern: List[str],
              data_accessor: tfx.components.DataAccessor,
              tf_transform_output: tft.TFTransformOutput,
              batch_size: int = 200) -> tf.data.Dataset:
  """Generates features and label for tuning/training.

  Args:
    file_pattern: List of paths or patterns of input tfrecord files.
    data_accessor: DataAccessor for converting input to RecordBatch.
    tf_transform_output: A TFTransformOutput.
    batch_size: representing the number of consecutive elements of returned
      dataset to combine in a single batch

  Returns:
    A dataset that contains (features, indices) tuple where features is a
      dictionary of Tensors, and indices is a single Tensor of label indices.
  """
  dataset = data_accessor.tf_dataset_factory(
      file_pattern,
      tfxio.TensorFlowDatasetOptions(
          batch_size=batch_size, label_key=_LABEL_KEY),
      tf_transform_output.transformed_metadata.schema)
  dataset = dataset.repeat()

  return dataset.prefetch(tf.data.AUTOTUNE)


</source>
<source file="systems/tfx-1.6.1/tfx/examples/tfjs_next_page_prediction/tfjs_next_page_prediction_util.py" startline="73" endline="98" pcid="3009">
def _input_fn(file_pattern: List[str],
              data_accessor: tfx.components.DataAccessor,
              tf_transform_output: tft.TFTransformOutput,
              batch_size: int = 200) -> tf.data.Dataset:
  """Generates features and label for tuning/training.

  Args:
    file_pattern: List of paths or patterns of input tfrecord files.
    data_accessor: DataAccessor for converting input to RecordBatch.
    tf_transform_output: A TFTransformOutput.
    batch_size: representing the number of consecutive elements of returned
      dataset to combine in a single batch.

  Returns:
    A dataset that contains (features, indices) tuple where features is a
      dictionary of Tensors, and indices is a single Tensor of label indices.
  """
  dataset = data_accessor.tf_dataset_factory(
      file_pattern,
      tfxio.TensorFlowDatasetOptions(
          batch_size=batch_size, label_key=_LABEL_KEY),
      tf_transform_output.transformed_metadata.schema)

  return dataset.repeat()


</source>
<source file="systems/tfx-1.6.1/tfx/examples/imdb/imdb_utils_native_keras.py" startline="99" endline="124" pcid="3002">
def _input_fn(file_pattern: List[str],
              data_accessor: DataAccessor,
              tf_transform_output: tft.TFTransformOutput,
              batch_size: int = 200) -> tf.data.Dataset:
  """Generates features and label for tuning/training.

  Args:
    file_pattern: List of paths or patterns of input tfrecord files.
    data_accessor: DataAccessor for converting input to RecordBatch.
    tf_transform_output: A TFTransformOutput.
    batch_size: representing the number of consecutive elements of returned
      dataset to combine in a single batch.

  Returns:
    A dataset that contains (features, indices) tuple where features is a
      dictionary of Tensors, and indices is a single Tensor of label indices.
  """
  dataset = data_accessor.tf_dataset_factory(
      file_pattern,
      dataset_options.TensorFlowDatasetOptions(
          batch_size=batch_size, label_key=_transformed_name(_LABEL_KEY)),
      tf_transform_output.transformed_metadata.schema)

  return dataset.repeat()


</source>
</class>

<class classid="135" nclones="4" nlines="10" similarity="90">
<source file="systems/tfx-1.6.1/tfx/examples/bert/cola/bert_cola_utils.py" startline="185" endline="204" pcid="2891">
def _get_serve_tf_examples_fn(model, tf_transform_output):
  """Returns a function that parses a serialized tf.Example."""

  model.tft_layer = tf_transform_output.transform_features_layer()

  @tf.function
  def serve_tf_examples_fn(serialized_tf_examples):
    """Returns the output to be used in the serving signature."""
    feature_spec = tf_transform_output.raw_feature_spec()
    feature_spec.pop(_LABEL_KEY)
    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)

    transformed_features = model.tft_layer(parsed_features)

    return model(transformed_features)

  return serve_tf_examples_fn


# TFX Trainer will call this function.
</source>
<source file="systems/tfx-1.6.1/tfx/examples/imdb/imdb_utils_native_keras.py" startline="155" endline="171" pcid="3004">
def _get_serve_tf_examples_fn(model, tf_transform_output):
  """Returns a function that parses a serialized tf.Example."""
  model.tft_layer = tf_transform_output.transform_features_layer()

  @tf.function
  def serve_tf_examples_fn(serialized_tf_examples):
    """Returns the output to be used in the serving signature."""
    feature_spec = tf_transform_output.raw_feature_spec()
    feature_spec.pop(_LABEL_KEY)
    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
    transformed_features = model.tft_layer(parsed_features)
    return model(transformed_features)

  return serve_tf_examples_fn


# TFX Trainer will call this function.
</source>
<source file="systems/tfx-1.6.1/tfx/examples/mnist/mnist_utils_native_keras.py" startline="27" endline="44" pcid="2933">
def _get_serve_tf_examples_fn(model, tf_transform_output):
  """Returns a function that parses a serialized tf.Example."""

  model.tft_layer = tf_transform_output.transform_features_layer()

  @tf.function
  def serve_tf_examples_fn(serialized_tf_examples):
    """Returns the output to be used in the serving signature."""
    feature_spec = tf_transform_output.raw_feature_spec()
    feature_spec.pop(base.LABEL_KEY)
    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
    transformed_features = model.tft_layer(parsed_features)
    return model(transformed_features)

  return serve_tf_examples_fn


# TFX Transform will call this function.
</source>
<source file="systems/tfx-1.6.1/tfx/examples/bert/mrpc/bert_mrpc_utils.py" startline="184" endline="205" pcid="2899">
def _get_serve_tf_examples_fn(model, tf_transform_output):
  """Returns a function that parses a serialized tf.Example."""

  model.tft_layer = tf_transform_output.transform_features_layer()

  @tf.function
  def serve_tf_examples_fn(serialized_tf_examples):
    """Returns the output to be used in the serving signature."""
    feature_spec = tf_transform_output.raw_feature_spec()
    feature_spec.pop(_LABEL_KEY)
    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)

    transformed_features = model.tft_layer(parsed_features)

    return model(transformed_features)

  return serve_tf_examples_fn


# TFX Trainer will call this function.


</source>
</class>

<class classid="136" nclones="3" nlines="31" similarity="87">
<source file="systems/tfx-1.6.1/tfx/examples/bert/cola/bert_cola_utils.py" startline="205" endline="246" pcid="2893">
def run_fn(fn_args: tfx.components.FnArgs):
  """Train the model based on given args.

  Args:
    fn_args: Holds args used to train the model as name/value pairs.
  """
  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

  train_dataset = _input_fn(
      fn_args.train_files,
      fn_args.data_accessor,
      tf_transform_output,
      batch_size=_TRAIN_BATCH_SIZE)

  eval_dataset = _input_fn(
      fn_args.eval_files,
      fn_args.data_accessor,
      tf_transform_output,
      batch_size=_EVAL_BATCH_SIZE)

  mirrored_strategy = tf.distribute.MirroredStrategy()
  with mirrored_strategy.scope():
    bert_layer = hub.KerasLayer(_BERT_LINK, trainable=True)
    model = build_and_compile_bert_classifier(bert_layer, _MAX_LEN, 2)

  model.fit(
      train_dataset,
      epochs=_EPOCHS,
      steps_per_epoch=fn_args.train_steps,
      validation_data=eval_dataset,
      validation_steps=fn_args.eval_steps)

  signatures = {
      'serving_default':
          _get_serve_tf_examples_fn(model,
                                    tf_transform_output).get_concrete_function(
                                        tf.TensorSpec(
                                            shape=[None],
                                            dtype=tf.string,
                                            name='examples')),
  }
  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)
</source>
<source file="systems/tfx-1.6.1/tfx/examples/imdb/imdb_utils_native_keras.py" startline="172" endline="217" pcid="3006">
def run_fn(fn_args: FnArgs):
  """Train the model based on given args.

  Args:
    fn_args: Holds args used to train the model as name/value pairs.
  """
  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

  train_dataset = _input_fn(
      fn_args.train_files,
      fn_args.data_accessor,
      tf_transform_output,
      batch_size=_TRAIN_BATCH_SIZE)

  eval_dataset = _input_fn(
      fn_args.eval_files,
      fn_args.data_accessor,
      tf_transform_output,
      batch_size=_EVAL_BATCH_SIZE)

  mirrored_strategy = tf.distribute.MirroredStrategy()
  with mirrored_strategy.scope():
    model = _build_keras_model()

  # In distributed training, it is common to use num_steps instead of num_epochs
  # to control training.
  # Reference: https://stackoverflow.com/questions/45989971/
  # /distributed-training-with-tf-estimator-resulting-in-more-training-steps

  model.fit(
      train_dataset,
      steps_per_epoch=fn_args.train_steps,
      validation_data=eval_dataset,
      validation_steps=fn_args.eval_steps)

  signatures = {
      'serving_default':
          _get_serve_tf_examples_fn(model,
                                    tf_transform_output).get_concrete_function(
                                        tf.TensorSpec(
                                            shape=[None],
                                            dtype=tf.string,
                                            name='examples')),
  }

  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)
</source>
<source file="systems/tfx-1.6.1/tfx/examples/bert/mrpc/bert_mrpc_utils.py" startline="206" endline="247" pcid="2901">
def run_fn(fn_args: tfx.components.FnArgs):
  """Train the model based on given args.

  Args:
    fn_args: Holds args used to train the model as name/value pairs.
  """
  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

  train_dataset = _input_fn(
      fn_args.train_files,
      fn_args.data_accessor,
      tf_transform_output,
      batch_size=_TRAIN_BATCH_SIZE)

  eval_dataset = _input_fn(
      fn_args.eval_files,
      fn_args.data_accessor,
      tf_transform_output,
      batch_size=_EVAL_BATCH_SIZE)

  mirrored_strategy = tf.distribute.MirroredStrategy()
  with mirrored_strategy.scope():
    bert_layer = hub.KerasLayer(_BERT_LINK, trainable=True)
    model = build_and_compile_bert_classifier(bert_layer, _MAX_LEN, 2, 2e-5)

  model.fit(
      train_dataset,
      epochs=_EPOCHS,
      steps_per_epoch=fn_args.train_steps,
      validation_data=eval_dataset,
      validation_steps=fn_args.eval_steps)

  signatures = {
      'serving_default':
          _get_serve_tf_examples_fn(model,
                                    tf_transform_output).get_concrete_function(
                                        tf.TensorSpec(
                                            shape=[None],
                                            dtype=tf.string,
                                            name='examples')),
  }
  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)
</source>
</class>

<class classid="137" nclones="5" nlines="15" similarity="93">
<source file="systems/tfx-1.6.1/tfx/examples/cifar10/cifar10_pipeline_native_keras_e2e_test.py" startline="46" endline="64" pcid="2923">
  def assertExecutedOnce(self, component: str) -> None:
    """Check the component is executed exactly once."""
    component_path = os.path.join(self._pipeline_root, component)
    self.assertTrue(fileio.exists(component_path))
    outputs = fileio.listdir(component_path)

    self.assertIn('.system', outputs)
    outputs.remove('.system')
    system_paths = [
        os.path.join('.system', path)
        for path in fileio.listdir(os.path.join(component_path, '.system'))
    ]
    self.assertNotEmpty(system_paths)
    self.assertIn('.system/executor_execution', system_paths)
    outputs.extend(system_paths)
    for output in outputs:
      execution = fileio.listdir(os.path.join(component_path, output))
      self.assertLen(execution, 1)

</source>
<source file="systems/tfx-1.6.1/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_local_e2e_test.py" startline="43" endline="62" pcid="3086">
  def assertExecutedOnce(self, component: str) -> None:
    """Check the component is executed exactly once."""
    component_path = os.path.join(self._pipeline_root, component)
    self.assertTrue(fileio.exists(component_path))
    outputs = fileio.listdir(component_path)

    self.assertIn('.system', outputs)
    outputs.remove('.system')
    system_paths = [
        os.path.join('.system', path)
        for path in fileio.listdir(os.path.join(component_path, '.system'))
    ]
    self.assertNotEmpty(system_paths)
    self.assertIn('.system/executor_execution', system_paths)
    outputs.extend(system_paths)
    self.assertNotEmpty(outputs)
    for output in outputs:
      execution = fileio.listdir(os.path.join(component_path, output))
      self.assertLen(execution, 1)

</source>
<source file="systems/tfx-1.6.1/tfx/examples/mnist/mnist_pipeline_native_keras_e2e_test.py" startline="51" endline="69" pcid="2938">
  def assertExecutedOnce(self, component: str) -> None:
    """Check the component is executed exactly once."""
    component_path = os.path.join(self._pipeline_root, component)
    self.assertTrue(fileio.exists(component_path))
    outputs = fileio.listdir(component_path)
    self.assertIn('.system', outputs)
    outputs.remove('.system')
    system_paths = [
        os.path.join('.system', path)
        for path in fileio.listdir(os.path.join(component_path, '.system'))
    ]
    self.assertNotEmpty(system_paths)
    self.assertIn('.system/executor_execution', system_paths)
    outputs.extend(system_paths)
    self.assertNotEmpty(outputs)
    for output in outputs:
      execution = fileio.listdir(os.path.join(component_path, output))
      self.assertLen(execution, 1)

</source>
<source file="systems/tfx-1.6.1/tfx/examples/tfjs_next_page_prediction/tfjs_next_page_prediction_e2e_test.py" startline="44" endline="62" pcid="3015">
  def assertExecutedOnce(self, component: str) -> None:
    """Check the component is executed exactly once."""
    component_path = os.path.join(self._pipeline_root, component)
    self.assertTrue(fileio.exists(component_path))
    outputs = fileio.listdir(component_path)

    self.assertIn('.system', outputs)
    outputs.remove('.system')
    system_paths = [
        os.path.join('.system', path)
        for path in fileio.listdir(os.path.join(component_path, '.system'))
    ]
    self.assertNotEmpty(system_paths)
    self.assertIn('.system/executor_execution', system_paths)
    outputs.extend(system_paths)
    for output in outputs:
      execution = fileio.listdir(os.path.join(component_path, output))
      self.assertLen(execution, 1)

</source>
<source file="systems/tfx-1.6.1/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_native_keras_e2e_test.py" startline="49" endline="67" pcid="3070">
  def assertExecutedOnce(self, component: str) -> None:
    """Check the component is executed exactly once."""
    component_path = os.path.join(self._pipeline_root, component)
    self.assertTrue(fileio.exists(component_path))
    outputs = fileio.listdir(component_path)
    self.assertIn('.system', outputs)
    outputs.remove('.system')
    system_paths = [
        os.path.join('.system', path)
        for path in fileio.listdir(os.path.join(component_path, '.system'))
    ]
    self.assertNotEmpty(system_paths)
    self.assertIn('.system/executor_execution', system_paths)
    outputs.extend(system_paths)
    self.assertNotEmpty(outputs)
    for output in outputs:
      execution = fileio.listdir(os.path.join(component_path, output))
      self.assertLen(execution, 1)

</source>
</class>

<class classid="138" nclones="2" nlines="11" similarity="75">
<source file="systems/tfx-1.6.1/tfx/examples/mnist/mnist_pipeline_native_keras_e2e_test.py" startline="70" endline="82" pcid="2939">
  def assertPipelineExecution(self) -> None:
    self.assertExecutedOnce('ImportExampleGen')
    self.assertExecutedOnce('Evaluator.mnist')
    self.assertExecutedOnce('Evaluator.mnist_lite')
    self.assertExecutedOnce('ExampleValidator')
    self.assertExecutedOnce('Pusher.mnist')
    self.assertExecutedOnce('Pusher.mnist_lite')
    self.assertExecutedOnce('SchemaGen')
    self.assertExecutedOnce('StatisticsGen')
    self.assertExecutedOnce('Trainer.mnist')
    self.assertExecutedOnce('Trainer.mnist_lite')
    self.assertExecutedOnce('Transform')

</source>
<source file="systems/tfx-1.6.1/tfx/examples/penguin/penguin_pipeline_local_infraval_e2e_test.py" startline="94" endline="104" pcid="3124">
  def _assertPipelineExecution(self):
    self._assertExecutedOnce('CsvExampleGen')
    self._assertExecutedOnce('Evaluator')
    self._assertExecutedOnce('ExampleValidator')
    self._assertExecutedOnce('ImportSchemaGen')
    self._assertExecutedOnce('InfraValidator')
    self._assertExecutedOnce('Pusher')
    self._assertExecutedOnce('StatisticsGen')
    self._assertExecutedOnce('Trainer')
    self._assertExecutedOnce('Transform')

</source>
</class>

<class classid="139" nclones="3" nlines="45" similarity="73">
<source file="systems/tfx-1.6.1/tfx/examples/mnist/mnist_pipeline_native_keras_e2e_test.py" startline="83" endline="131" pcid="2940">
  def testMNISTPipelineNativeKeras(self):
    if not tf.executing_eagerly():
      self.skipTest('The test requires TF2.')
    BeamDagRunner().run(
        mnist_pipeline_native_keras._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            module_file_lite=self._module_file_lite,
            serving_model_dir=self._serving_model_dir,
            serving_model_dir_lite=self._serving_model_dir_lite,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            beam_pipeline_args=[]))

    self.assertTrue(fileio.exists(self._serving_model_dir))
    self.assertTrue(fileio.exists(self._serving_model_dir_lite))
    self.assertTrue(fileio.exists(self._metadata_path))
    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    expected_execution_count = 11
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(execution_count, expected_execution_count)

    self.assertPipelineExecution()

    # Runs pipeline the second time.
    BeamDagRunner().run(
        mnist_pipeline_native_keras._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            module_file_lite=self._module_file_lite,
            serving_model_dir=self._serving_model_dir,
            serving_model_dir_lite=self._serving_model_dir_lite,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            beam_pipeline_args=[]))

    # Asserts cache execution.
    with metadata.Metadata(metadata_config) as m:
      # Artifact count is unchanged.
      self.assertLen(m.store.get_artifacts(), artifact_count)
      self.assertLen(m.store.get_executions(), expected_execution_count * 2)


</source>
<source file="systems/tfx-1.6.1/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_native_keras_e2e_test.py" startline="78" endline="138" pcid="3072">
  def testTaxiPipelineNativeKeras(self):
    BeamDagRunner().run(
        taxi_pipeline_native_keras._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            serving_model_dir=self._serving_model_dir,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            beam_pipeline_args=[]))

    self.assertTrue(fileio.exists(self._serving_model_dir))
    self.assertTrue(fileio.exists(self._metadata_path))
    expected_execution_count = 9  # 8 components + 1 resolver
    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(expected_execution_count, execution_count)

    self.assertPipelineExecution()

    # Runs pipeline the second time.
    BeamDagRunner().run(
        taxi_pipeline_native_keras._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            serving_model_dir=self._serving_model_dir,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            beam_pipeline_args=[]))

    # All executions but Evaluator and Pusher are cached.
    # Note that Resolver will always execute.
    with metadata.Metadata(metadata_config) as m:
      # Artifact count is increased by 3 caused by Evaluator and Pusher.
      self.assertLen(m.store.get_artifacts(), artifact_count + 3)
      artifact_count = len(m.store.get_artifacts())
      self.assertLen(m.store.get_executions(), expected_execution_count * 2)

    # Runs pipeline the third time.
    BeamDagRunner().run(
        taxi_pipeline_native_keras._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            serving_model_dir=self._serving_model_dir,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            beam_pipeline_args=[]))

    # Asserts cache execution.
    with metadata.Metadata(metadata_config) as m:
      # Artifact count is unchanged.
      self.assertLen(m.store.get_artifacts(), artifact_count)
      self.assertLen(m.store.get_executions(), expected_execution_count * 3)


</source>
<source file="systems/tfx-1.6.1/tfx/examples/penguin/penguin_pipeline_local_infraval_e2e_test.py" startline="108" endline="177" pcid="3125">
  def testPenguinPipelineLocal(self, make_warmup):
    LocalDagRunner().run(
        penguin_pipeline_local_infraval._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            accuracy_threshold=0.1,
            serving_model_dir=self._serving_model_dir,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            user_provided_schema_path=self._schema_path,
            beam_pipeline_args=[],
            make_warmup=make_warmup))

    self.assertTrue(fileio.exists(self._serving_model_dir))
    self.assertTrue(fileio.exists(self._metadata_path))
    expected_execution_count = 10  # 9 components + 1 resolver
    metadata_config = metadata.sqlite_metadata_connection_config(
        self._metadata_path)
    with metadata.Metadata(metadata_config) as m:
      artifact_count = len(m.store.get_artifacts())
      execution_count = len(m.store.get_executions())
      self.assertGreaterEqual(artifact_count, execution_count)
      self.assertEqual(expected_execution_count, execution_count)

    self._assertPipelineExecution()
    self._assertInfraValidatorPassed()

    # Runs pipeline the second time.
    LocalDagRunner().run(
        penguin_pipeline_local_infraval._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            accuracy_threshold=0.1,
            serving_model_dir=self._serving_model_dir,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            user_provided_schema_path=self._schema_path,
            beam_pipeline_args=[],
            make_warmup=make_warmup))

    # All executions but Evaluator and Pusher are cached.
    with metadata.Metadata(metadata_config) as m:
      # Artifact count is increased by 3 caused by Evaluator and Pusher.
      self.assertLen(m.store.get_artifacts(), artifact_count + 3)
      artifact_count = len(m.store.get_artifacts())
      self.assertLen(m.store.get_executions(), expected_execution_count * 2)

    # Runs pipeline the third time.
    LocalDagRunner().run(
        penguin_pipeline_local_infraval._create_pipeline(
            pipeline_name=self._pipeline_name,
            data_root=self._data_root,
            module_file=self._module_file,
            accuracy_threshold=0.1,
            serving_model_dir=self._serving_model_dir,
            pipeline_root=self._pipeline_root,
            metadata_path=self._metadata_path,
            user_provided_schema_path=self._schema_path,
            beam_pipeline_args=[],
            make_warmup=make_warmup))

    # Asserts cache execution.
    with metadata.Metadata(metadata_config) as m:
      # Artifact count is unchanged.
      self.assertLen(m.store.get_artifacts(), artifact_count)
      self.assertLen(m.store.get_executions(), expected_execution_count * 3)


</source>
</class>

<class classid="140" nclones="4" nlines="46" similarity="70">
<source file="systems/tfx-1.6.1/tfx/examples/bigquery_ml/taxi_pipeline_kubeflow_gcp_bqml.py" startline="155" endline="227" pcid="2945">
def _create_pipeline(
    pipeline_name: str, pipeline_root: str, query: str, module_file: str,
    beam_pipeline_args: List[str], ai_platform_training_args: Dict[str, str],
    bigquery_serving_args: Dict[str, str]) -> pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX and Kubeflow Pipelines."""

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = big_query_example_gen_component.BigQueryExampleGen(query=query)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that implements a model.
  # to train a model on Google Cloud AI Platform.
  trainer = Trainer(
      custom_executor_spec=executor_spec.ExecutorClassSpec(
          ai_platform_trainer_executor.Executor),
      module_file=module_file,
      transformed_examples=transform.outputs['transformed_examples'],
      schema=schema_gen.outputs['schema'],
      transform_graph=transform.outputs['transform_graph'],
      train_args=trainer_pb2.TrainArgs(num_steps=10000),
      eval_args=trainer_pb2.EvalArgs(num_steps=5000),
      custom_config={'ai_platform_training_args': ai_platform_training_args})

  # Uses TFMA to compute a evaluation statistics over features of a model.
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[
          evaluator_pb2.SingleSlicingSpec(
              column_for_slicing=['trip_start_hour'])
      ]))

  # Performs quality validation of a candidate model (compared to a baseline).
  model_validator = ModelValidator(
      examples=example_gen.outputs['examples'], model=trainer.outputs['model'])

  # Checks whether the model passed the validation steps and pushes the model
  # to  Google Cloud BigQuery ML if check passed.
  pusher = Pusher(
      custom_executor_spec=executor_spec.ExecutorClassSpec(
          bigquery_pusher_executor.Executor),
      model=trainer.outputs['model'],
      model_blessing=model_validator.outputs['blessing'],
      custom_config={'bigquery_serving_args': bigquery_serving_args})

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen, statistics_gen, schema_gen, example_validator, transform,
          trainer, evaluator, model_validator, pusher
      ],
      beam_pipeline_args=beam_pipeline_args,
  )


</source>
<source file="systems/tfx-1.6.1/tfx/examples/custom_components/presto_example_gen/example/taxi_pipeline_presto.py" startline="64" endline="134" pcid="3034">
def _create_pipeline(pipeline_name: str, pipeline_root: str, module_file: str,
                     presto_config: presto_config_pb2.PrestoConnConfig,
                     query: str, serving_model_dir: str,
                     metadata_path: str) -> pipeline.Pipeline:
  """Implements the chicago taxi pipeline with TFX."""
  # Brings data into the pipeline or otherwise joins/converts training data
  example_gen = PrestoExampleGen(presto_config, query=query)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=module_file)

  # Uses user-provided Python function that implements a model.
  trainer = Trainer(
      module_file=module_file,
      transformed_examples=transform.outputs['transformed_examples'],
      schema=schema_gen.outputs['schema'],
      transform_graph=transform.outputs['transform_graph'],
      train_args=trainer_pb2.TrainArgs(num_steps=10000),
      eval_args=trainer_pb2.EvalArgs(num_steps=5000))

  # Uses TFMA to compute a evaluation statistics over features of a model.
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[
          evaluator_pb2.SingleSlicingSpec(
              column_for_slicing=['trip_start_hour'])
      ]))

  # Performs quality validation of a candidate model (compared to a baseline).
  model_validator = ModelValidator(
      examples=example_gen.outputs['examples'], model=trainer.outputs['model'])

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=model_validator.outputs['blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen, statistics_gen, schema_gen, example_validator, transform,
          trainer, evaluator, model_validator, pusher
      ],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          metadata_path),
  )


# To run this pipeline from the python CLI:
#   $python taxi_pipeline_presto.py
</source>
<source file="systems/tfx-1.6.1/tfx/examples/custom_components/slack/example/taxi_pipeline_slack_kubeflow.py" startline="81" endline="161" pcid="3058">
def _create_pipeline():
  """Implements the chicago taxi pipeline with TFX."""
  examples = csv_input(_data_root)

  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input=examples)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      preprocessing_fn=_taxi_transformer_func)

  # Uses user-provided Python function that implements a model.
  trainer = Trainer(
      trainer_fn=_taxi_trainer_func,
      examples=transform.outputs['transformed_examples'],
      schema=schema_gen.outputs['schema'],
      transform_graph=transform.outputs['transform_graph'],
      train_args=trainer_pb2.TrainArgs(num_steps=10000),
      eval_args=trainer_pb2.EvalArgs(num_steps=5000))

  # Uses TFMA to compute a evaluation statistics over features of a model.
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[
          evaluator_pb2.SingleSlicingSpec(
              column_for_slicing=['trip_start_hour'])
      ]))

  # Performs quality validation of a candidate model (compared to a baseline).
  model_validator = ModelValidator(
      examples=example_gen.outputs['examples'], model=trainer.outputs['model'])

  # This custom component serves as a bridge between pipeline and human model
  # reviewers to enable review-and-push workflow in model development cycle. It
  # utilizes Slack API to send message to user-defined Slack channel with model
  # URI info and wait for go / no-go decision from the same Slack channel:
  #   * To approve the model, users need to reply the thread sent out by the bot
  #     started by SlackComponent with 'lgtm' or 'approve'.
  #   * To reject the model, users need to reply the thread sent out by the bot
  #     started by SlackComponent with 'decline' or 'reject'.
  slack_validator = SlackComponent(
      model=trainer.outputs['model'],
      model_blessing=model_validator.outputs['blessing'],
      slack_token=_slack_token,
      slack_channel_id=_slack_channel_id,
      timeout_sec=3600,
  )
  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=slack_validator.outputs['slack_blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=_serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=_pipeline_name,
      pipeline_root=_pipeline_root,
      components=[
          example_gen, statistics_gen, schema_gen, example_validator, transform,
          trainer, evaluator, model_validator, slack_validator, pusher
      ],
      enable_cache=True,
  )


</source>
<source file="systems/tfx-1.6.1/tfx/examples/custom_components/slack/example/taxi_pipeline_slack.py" startline="74" endline="155" pcid="3047">
def _create_pipeline():
  """Implements the chicago taxi pipeline with TFX."""
  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = CsvExampleGen(input_base=_data_root)

  # Computes statistics over data for visualization and example validation.
  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])

  # Performs anomaly detection based on statistics and data schema.
  example_validator = ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # Performs transformations and feature engineering in training and serving.
  transform = Transform(
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      module_file=_taxi_module_file)

  # Uses user-provided Python function that implements a model.
  trainer = Trainer(
      module_file=_taxi_module_file,
      examples=transform.outputs['transformed_examples'],
      schema=schema_gen.outputs['schema'],
      transform_graph=transform.outputs['transform_graph'],
      train_args=trainer_pb2.TrainArgs(num_steps=10000),
      eval_args=trainer_pb2.EvalArgs(num_steps=5000))

  # Uses TFMA to compute a evaluation statistics over features of a model.
  evaluator = Evaluator(
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[
          evaluator_pb2.SingleSlicingSpec(
              column_for_slicing=['trip_start_hour'])
      ]))

  # Performs quality validation of a candidate model (compared to a baseline).
  model_validator = ModelValidator(
      examples=example_gen.outputs['examples'], model=trainer.outputs['model'])

  # This custom component serves as a bridge between pipeline and human model
  # reviewers to enable review-and-push workflow in model development cycle. It
  # utilizes Slack API to send message to user-defined Slack channel with model
  # URI info and wait for go / no-go decision from the same Slack channel:
  #   * To approve the model, users need to reply the thread sent out by the bot
  #     started by SlackComponent with 'lgtm' or 'approve'.
  #   * To reject the model, users need to reply the thread sent out by the bot
  #     started by SlackComponent with 'decline' or 'reject'.
  slack_validator = SlackComponent(
      model=trainer.outputs['model'],
      model_blessing=model_validator.outputs['blessing'],
      slack_token=_slack_token,
      slack_channel_id=_slack_channel_id,
      timeout_sec=3600,
  )

  # Checks whether the model passed the validation steps and pushes the model
  # to a file destination if check passed.
  pusher = Pusher(
      model=trainer.outputs['model'],
      model_blessing=slack_validator.outputs['slack_blessing'],
      push_destination=pusher_pb2.PushDestination(
          filesystem=pusher_pb2.PushDestination.Filesystem(
              base_directory=_serving_model_dir)))

  return pipeline.Pipeline(
      pipeline_name=_pipeline_name,
      pipeline_root=_pipeline_root,
      components=[
          example_gen, statistics_gen, schema_gen, example_validator, transform,
          trainer, evaluator, model_validator, slack_validator, pusher
      ],
      enable_cache=True,
      metadata_connection_config=metadata.sqlite_metadata_connection_config(
          _metadata_db_root),
  )


</source>
</class>

<class classid="141" nclones="2" nlines="45" similarity="88">
<source file="systems/tfx-1.6.1/tfx/examples/bigquery_ml/taxi_utils_bqml_test.py" startline="50" endline="111" pcid="2958">
  def testPreprocessingFn(self):
    schema_file = os.path.join(self._testdata_path, 'schema_gen/schema.pbtxt')
    schema = io_utils.parse_pbtxt_file(schema_file, schema_pb2.Schema())
    feature_spec = taxi_utils_bqml._get_raw_feature_spec(schema)
    working_dir = self.get_temp_dir()
    transform_output_path = os.path.join(working_dir, 'transform_output')
    transformed_examples_path = os.path.join(
        working_dir, 'transformed_examples')

    # Run very simplified version of executor logic.
    # TODO(kestert): Replace with tft_unit.assertAnalyzeAndTransformResults.
    # Generate legacy `DatasetMetadata` object.  Future version of Transform
    # will accept the `Schema` proto directly.
    legacy_metadata = dataset_metadata.DatasetMetadata(
        schema_utils.schema_from_feature_spec(feature_spec))
    tfxio = tf_example_record.TFExampleRecord(
        file_pattern=os.path.join(self._testdata_path,
                                  'csv_example_gen/train/*'),
        telemetry_descriptors=['Tests'],
        schema=legacy_metadata.schema)
    with beam.Pipeline() as p:
      with tft_beam.Context(temp_dir=os.path.join(working_dir, 'tmp')):
        examples = p | 'ReadTrainData' >> tfxio.BeamSource()
        (transformed_examples, transformed_metadata), transform_fn = (
            (examples, tfxio.TensorAdapterConfig())
            | 'AnalyzeAndTransform' >> tft_beam.AnalyzeAndTransformDataset(
                taxi_utils_bqml.preprocessing_fn))

        # WriteTransformFn writes transform_fn and metadata to subdirectories
        # tensorflow_transform.SAVED_MODEL_DIR and
        # tensorflow_transform.TRANSFORMED_METADATA_DIR respectively.
        # pylint: disable=expression-not-assigned
        (transform_fn
         | 'WriteTransformFn' >> tft_beam.WriteTransformFn(
             transform_output_path))

        encoder = tft.coders.ExampleProtoCoder(transformed_metadata.schema)
        (transformed_examples
         | 'EncodeTrainData' >> beam.Map(encoder.encode)
         | 'WriteTrainData' >> beam.io.WriteToTFRecord(
             os.path.join(transformed_examples_path,
                          'train/transformed_examples.gz'),
             coder=beam.coders.BytesCoder()))
        # pylint: enable=expression-not-assigned

    # Verify the output matches golden output.
    # NOTE: we don't verify that transformed examples match golden output.
    expected_transformed_schema = io_utils.parse_pbtxt_file(
        os.path.join(
            self._testdata_path,
            'transform/transform_output/transformed_metadata/schema.pbtxt'),
        schema_pb2.Schema())
    transformed_schema = io_utils.parse_pbtxt_file(
        os.path.join(transform_output_path,
                     'transformed_metadata/schema.pbtxt'),
        schema_pb2.Schema())
    # Clear annotations so we only have to test main schema.
    for feature in transformed_schema.feature:
      feature.ClearField('annotation')
    transformed_schema.ClearField('annotation')
    self.assertEqual(transformed_schema, expected_transformed_schema)

</source>
<source file="systems/tfx-1.6.1/tfx/examples/chicago_taxi_pipeline/taxi_utils_test.py" startline="52" endline="112" pcid="3091">
  def testPreprocessingFn(self):
    schema_file = os.path.join(self._testdata_path, 'schema_gen/schema.pbtxt')
    schema = io_utils.parse_pbtxt_file(schema_file, schema_pb2.Schema())
    feature_spec = taxi_utils._get_raw_feature_spec(schema)
    working_dir = self.get_temp_dir()
    transform_graph_path = os.path.join(working_dir, 'transform_graph')
    transformed_examples_path = os.path.join(
        working_dir, 'transformed_examples')

    # Run very simplified version of executor logic.
    # TODO(kestert): Replace with tft_unit.assertAnalyzeAndTransformResults.
    # Generate legacy `DatasetMetadata` object.  Future version of Transform
    # will accept the `Schema` proto directly.
    legacy_metadata = dataset_metadata.DatasetMetadata(
        schema_utils.schema_from_feature_spec(feature_spec))
    tfxio = tf_example_record.TFExampleRecord(
        file_pattern=os.path.join(self._testdata_path,
                                  'csv_example_gen/Split-train/*'),
        telemetry_descriptors=['Tests'],
        schema=legacy_metadata.schema)
    with beam.Pipeline() as p:
      with tft_beam.Context(temp_dir=os.path.join(working_dir, 'tmp')):
        examples = p | 'ReadTrainData' >> tfxio.BeamSource()
        (transformed_examples, transformed_metadata), transform_fn = (
            (examples, tfxio.TensorAdapterConfig())
            | 'AnalyzeAndTransform' >> tft_beam.AnalyzeAndTransformDataset(
                taxi_utils.preprocessing_fn))

        # WriteTransformFn writes transform_fn and metadata to subdirectories
        # tensorflow_transform.SAVED_MODEL_DIR and
        # tensorflow_transform.TRANSFORMED_METADATA_DIR respectively.
        # pylint: disable=expression-not-assigned
        (transform_fn
         |
         'WriteTransformFn' >> tft_beam.WriteTransformFn(transform_graph_path))

        encoder = tft.coders.ExampleProtoCoder(transformed_metadata.schema)
        (transformed_examples
         | 'EncodeTrainData' >> beam.Map(encoder.encode)
         | 'WriteTrainData' >> beam.io.WriteToTFRecord(
             os.path.join(transformed_examples_path,
                          'Split-train/transformed_examples.gz'),
             coder=beam.coders.BytesCoder()))
        # pylint: enable=expression-not-assigned

    # Verify the output matches golden output.
    # NOTE: we don't verify that transformed examples match golden output.
    expected_transformed_schema = io_utils.parse_pbtxt_file(
        os.path.join(
            self._testdata_path,
            'transform/transform_graph/transformed_metadata/schema.pbtxt'),
        schema_pb2.Schema())
    transformed_schema = io_utils.parse_pbtxt_file(
        os.path.join(transform_graph_path, 'transformed_metadata/schema.pbtxt'),
        schema_pb2.Schema())
    # Clear annotations so we only have to test main schema.
    transformed_schema.ClearField('annotation')
    for feature in transformed_schema.feature:
      feature.ClearField('annotation')
    self.assertEqual(transformed_schema, expected_transformed_schema)

</source>
</class>

<class classid="142" nclones="2" nlines="49" similarity="74">
<source file="systems/tfx-1.6.1/tfx/examples/bigquery_ml/taxi_utils_bqml_test.py" startline="112" endline="169" pcid="2959">
  def testTrainerFn(self):
    temp_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    schema_file = os.path.join(self._testdata_path, 'schema_gen/schema.pbtxt')
    trainer_fn_args = trainer_executor.TrainerFnArgs(
        train_files=os.path.join(self._testdata_path,
                                 'transform/transformed_examples/train/*.gz'),
        transform_output=os.path.join(self._testdata_path,
                                      'transform/transform_output/'),
        serving_model_dir=os.path.join(temp_dir, 'serving_model_dir'),
        eval_files=os.path.join(self._testdata_path,
                                'transform/transformed_examples/eval/*.gz'),
        schema_file=schema_file,
        train_steps=1,
        eval_steps=1,
        base_model=os.path.join(self._testdata_path,
                                'trainer/current/serving_model_dir'),
        data_accessor=DataAccessor(
            tf_dataset_factory=tfxio_utils.get_tf_dataset_factory_from_artifact(
                [standard_artifacts.Examples()], []),
            record_batch_factory=None,
            data_view_decode_fn=None))
    schema = io_utils.parse_pbtxt_file(schema_file, schema_pb2.Schema())
    training_spec = taxi_utils_bqml.trainer_fn(trainer_fn_args, schema)

    estimator = training_spec['estimator']
    train_spec = training_spec['train_spec']
    eval_spec = training_spec['eval_spec']
    eval_input_receiver_fn = training_spec['eval_input_receiver_fn']

    self.assertIsInstance(estimator, tf.estimator.Estimator)
    self.assertIsInstance(train_spec, tf.estimator.TrainSpec)
    self.assertIsInstance(eval_spec, tf.estimator.EvalSpec)
    self.assertIsInstance(eval_input_receiver_fn, types.FunctionType)

    # Train for one step, then eval for one step.
    eval_result, exports = tf.estimator.train_and_evaluate(
        estimator, train_spec, eval_spec)
    self.assertGreater(eval_result['loss'], 0.0)
    self.assertEqual(len(exports), 1)
    self.assertGreaterEqual(len(fileio.listdir(exports[0])), 1)

    # Export the eval saved model.
    eval_savedmodel_path = tfma.export.export_eval_savedmodel(
        estimator=estimator,
        export_dir_base=path_utils.eval_model_dir(temp_dir),
        eval_input_receiver_fn=eval_input_receiver_fn)
    self.assertGreaterEqual(len(fileio.listdir(eval_savedmodel_path)), 1)

    # Test exported serving graph.
    with tf.compat.v1.Session() as sess:
      metagraph_def = tf.compat.v1.saved_model.loader.load(
          sess, [tf.saved_model.SERVING], exports[0])
      self.assertIsInstance(metagraph_def, tf.compat.v1.MetaGraphDef)


</source>
<source file="systems/tfx-1.6.1/tfx/examples/chicago_taxi_pipeline/taxi_utils_test.py" startline="113" endline="176" pcid="3092">
  def testTrainerFn(self):
    temp_dir = os.path.join(
        os.environ.get('TEST_UNDECLARED_OUTPUTS_DIR', self.get_temp_dir()),
        self._testMethodName)

    schema_file = os.path.join(self._testdata_path, 'schema_gen/schema.pbtxt')
    data_accessor = DataAccessor(
        tf_dataset_factory=tfxio_utils.get_tf_dataset_factory_from_artifact(
            [standard_artifacts.Examples()], []),
        record_batch_factory=None,
        data_view_decode_fn=None)
    trainer_fn_args = trainer_executor.TrainerFnArgs(
        train_files=os.path.join(
            self._testdata_path,
            'transform/transformed_examples/Split-train/*.gz'),
        transform_output=os.path.join(self._testdata_path,
                                      'transform/transform_graph'),
        serving_model_dir=os.path.join(temp_dir, 'serving_model_dir'),
        eval_files=os.path.join(
            self._testdata_path,
            'transform/transformed_examples/Split-eval/*.gz'),
        schema_file=schema_file,
        train_steps=1,
        eval_steps=1,
        base_model=None,
        data_accessor=data_accessor)
    schema = io_utils.parse_pbtxt_file(schema_file, schema_pb2.Schema())
    training_spec = taxi_utils.trainer_fn(trainer_fn_args, schema)

    estimator = training_spec['estimator']
    train_spec = training_spec['train_spec']
    eval_spec = training_spec['eval_spec']
    eval_input_receiver_fn = training_spec['eval_input_receiver_fn']

    self.assertIsInstance(estimator,
                          tf.estimator.DNNLinearCombinedClassifier)
    self.assertIsInstance(train_spec, tf.estimator.TrainSpec)
    self.assertIsInstance(eval_spec, tf.estimator.EvalSpec)
    self.assertIsInstance(eval_input_receiver_fn, types.FunctionType)

    # Test keep_max_checkpoint in RunConfig
    self.assertGreater(estimator._config.keep_checkpoint_max, 1)

    # Train for one step, then eval for one step.
    eval_result, exports = tf.estimator.train_and_evaluate(
        estimator, train_spec, eval_spec)
    self.assertGreater(eval_result['loss'], 0.0)
    self.assertEqual(len(exports), 1)
    self.assertGreaterEqual(len(fileio.listdir(exports[0])), 1)

    # Export the eval saved model.
    eval_savedmodel_path = tfma.export.export_eval_savedmodel(
        estimator=estimator,
        export_dir_base=path_utils.eval_model_dir(temp_dir),
        eval_input_receiver_fn=eval_input_receiver_fn)
    self.assertGreaterEqual(len(fileio.listdir(eval_savedmodel_path)), 1)

    # Test exported serving graph.
    with tf.compat.v1.Session() as sess:
      metagraph_def = tf.compat.v1.saved_model.loader.load(
          sess, [tf.saved_model.SERVING], exports[0])
      self.assertIsInstance(metagraph_def, tf.compat.v1.MetaGraphDef)


</source>
</class>

<class classid="143" nclones="2" nlines="22" similarity="90">
<source file="systems/tfx-1.6.1/tfx/examples/airflow_workshop/notebooks/utils.py" startline="347" endline="384" pcid="2979">
  def get_source_artifact_of_type(self, artifact_id, source_type_name):
    """Returns the source artifact of `source_type_name` for `artifact_id`.

    This method recursively traverses the events and associated executions that
    led to generating `artifact_id` to find an artifact of type
    `source_type_name` that was an input for these events.

    Args:
      artifact_id: A `int` indicating the id of an artifact.
      source_type_name: A `str` indicating the type of an artifact that is
          a direct or indirect input for generating `artifact_id`.

    Returns:
      A `metadata_store_pb2.Artifact` of type `source_type_name` that is a
      direct/indirect input for generating `artifact_id` or `None` if no such
      artifact exists.
    """
    a_events = self.metadata_store.get_events_by_artifact_ids([artifact_id])
    for a_event in a_events:
      if _is_input_event(a_event):
        continue
      [execution] = self.metadata_store.get_executions_by_id(
          [a_event.execution_id])
      e_events = self.metadata_store.get_events_by_execution_ids([execution.id])
      for e_event in e_events:
        if _is_output_event(e_event):
          continue
        [artifact] = self.metadata_store.get_artifacts_by_id(
            [e_event.artifact_id])
        [artifact_type] = self.metadata_store.get_artifact_types_by_id(
            [artifact.type_id])
        if artifact_type.name == source_type_name:
          return artifact
        input_artifact = self.get_source_artifact_of_type(
            artifact.id, source_type_name)
        if input_artifact:
          return input_artifact

</source>
<source file="systems/tfx-1.6.1/tfx/examples/airflow_workshop/notebooks/utils.py" startline="385" endline="423" pcid="2980">
  def get_dest_artifact_of_type(self, artifact_id, dest_type_name):
    """Returns the destination artifact of `dest_type_name` from `artifact_id`.

    This method recursively traverses the events and associated executions that
    consumed `artifact_id` to find an artifact of type `dest_type_name` that was
    an output for these events.

    Args:
      artifact_id: A `int` indicating the id of an artifact.
      dest_type_name: A `str` indicating the type of an artifact that is
          a output of an event that directly/indirectly consumed `artifact_id`.

    Returns:
      A `metadata_store_pb2.Artifact` of type `dest_type_name` that is a
      direct/indirect output from `artifact_id` or `None` if no such artifact
      exists.
    """
    a_events = self.metadata_store.get_events_by_artifact_ids([artifact_id])
    for a_event in a_events:
      if _is_output_event(a_event):
        continue
      [execution] = self.metadata_store.get_executions_by_id(
          [a_event.execution_id])
      e_events = self.metadata_store.get_events_by_execution_ids(
          [execution.id])
      for e_event in e_events:
        if _is_input_event(e_event):
          continue
        [artifact] = self.metadata_store.get_artifacts_by_id(
            [e_event.artifact_id])
        [artifact_type] = self.metadata_store.get_artifact_types_by_id(
            [artifact.type_id])
        if artifact_type.name == dest_type_name:
          return artifact
        dest_artifact = self.get_dest_artifact_of_type(
            artifact.id, dest_type_name)
        if dest_artifact:
          return dest_artifact

</source>
</class>

<class classid="144" nclones="2" nlines="33" similarity="70">
<source file="systems/tfx-1.6.1/tfx/examples/penguin/penguin_utils_keras.py" startline="130" endline="175" pcid="3106">
def run_fn(fn_args: tfx.components.FnArgs):
  """Train the model based on given args.

  Args:
    fn_args: Holds args used to train the model as name/value pairs.
  """
  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

  train_dataset = base.input_fn(
      fn_args.train_files,
      fn_args.data_accessor,
      tf_transform_output,
      base.TRAIN_BATCH_SIZE)

  eval_dataset = base.input_fn(
      fn_args.eval_files,
      fn_args.data_accessor,
      tf_transform_output,
      base.EVAL_BATCH_SIZE)

  if fn_args.hyperparameters:
    hparams = keras_tuner.HyperParameters.from_config(fn_args.hyperparameters)
  else:
    # This is a shown case when hyperparameters is decided and Tuner is removed
    # from the pipeline. User can also inline the hyperparameters directly in
    # _build_keras_model.
    hparams = _get_hyperparameters()
  absl.logging.info('HyperParameters for training: %s' % hparams.get_config())

  mirrored_strategy = tf.distribute.MirroredStrategy()
  with mirrored_strategy.scope():
    model = _make_keras_model(hparams)

  # Write logs to path
  tensorboard_callback = tf.keras.callbacks.TensorBoard(
      log_dir=fn_args.model_run_dir, update_freq='batch')

  model.fit(
      train_dataset,
      steps_per_epoch=fn_args.train_steps,
      validation_data=eval_dataset,
      validation_steps=fn_args.eval_steps,
      callbacks=[tensorboard_callback])

  signatures = base.make_serving_signatures(model, tf_transform_output)
  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)
</source>
<source file="systems/tfx-1.6.1/tfx/examples/penguin/penguin_utils_cloud_tuner.py" startline="274" endline="338" pcid="3117">
def run_fn(fn_args: tfx.components.FnArgs):
  """Train the model based on given args.

  Args:
    fn_args: Holds args as name/value pairs. See
      https://www.tensorflow.org/tfx/api_docs/python/tfx/components/trainer/fn_args_utils/FnArgs.
      - train_files: List of file paths containing training tf.Example data.
      - eval_files: List of file paths containing eval tf.Example data.
      - data_accessor: Contains factories that can create tf.data.Datasets or
        other means to access the train/eval data. They provide a uniform way of
        accessing data, regardless of how the data is stored on disk.
      - train_steps: number of train steps.
      - eval_steps: number of eval steps.
      - transform_output: A uri to a path containing statistics and metadata
        from TFTransform component. produced by TFT. Will be None if not
        specified.
      - model_run_dir: A single uri for the output directory of model training
        related files.
      - hyperparameters: An optional keras_tuner.HyperParameters config.
  """
  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)

  train_dataset = _input_fn(
      fn_args.train_files,
      fn_args.data_accessor,
      tf_transform_output,
      batch_size=_TRAIN_BATCH_SIZE)

  eval_dataset = _input_fn(
      fn_args.eval_files,
      fn_args.data_accessor,
      tf_transform_output,
      batch_size=_EVAL_BATCH_SIZE)

  if fn_args.hyperparameters:
    hparams = keras_tuner.HyperParameters.from_config(fn_args.hyperparameters)
  else:
    # This is a shown case when hyperparameters is decided and Tuner is removed
    # from the pipeline. User can also inline the hyperparameters directly in
    # _build_keras_model.
    hparams = _get_hyperparameters()
  logging.info('HyperParameters for training: %s', hparams.get_config())

  mirrored_strategy = tf.distribute.MirroredStrategy()
  with mirrored_strategy.scope():
    model = _build_keras_model(hparams)

  # Write logs to path
  tensorboard_callback = tf.keras.callbacks.TensorBoard(
      log_dir=fn_args.model_run_dir, update_freq='batch')

  model.fit(
      train_dataset,
      steps_per_epoch=fn_args.train_steps,
      validation_data=eval_dataset,
      validation_steps=fn_args.eval_steps,
      callbacks=[tensorboard_callback])

  signatures = {
      'serving_default':
          _get_tf_examples_serving_signature(model, tf_transform_output),
      'transform_features':
          _get_transform_features_signature(model, tf_transform_output),
  }
  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)
</source>
</class>

<class classid="145" nclones="2" nlines="12" similarity="76">
<source file="systems/tfx-1.6.1/tfx/examples/penguin/penguin_utils_cloud_tuner.py" startline="64" endline="90" pcid="3108">
def _get_tf_examples_serving_signature(model, tf_transform_output):
  """Returns a serving signature that accepts `tensorflow.Example`."""

  # We need to track the layers in the model in order to save it.
  # TODO(b/162357359): Revise once the bug is resolved.
  model.tft_layer_inference = tf_transform_output.transform_features_layer()

  @tf.function(input_signature=[
      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')
  ])
  def serve_tf_examples_fn(serialized_tf_example):
    """Returns the output to be used in the serving signature."""
    raw_feature_spec = tf_transform_output.raw_feature_spec()
    # Remove label feature since these will not be present at serving time.
    raw_feature_spec.pop(_LABEL_KEY)
    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)
    transformed_features = model.tft_layer_inference(raw_features)
    logging.info('serve_transformed_features = %s', transformed_features)

    outputs = model(transformed_features)
    # TODO(b/154085620): Convert the predicted labels from the model using a
    # reverse-lookup (opposite of transform.py).
    return {'outputs': outputs}

  return serve_tf_examples_fn


</source>
<source file="systems/tfx-1.6.1/tfx/examples/penguin/penguin_utils_cloud_tuner.py" startline="91" endline="111" pcid="3110">
def _get_transform_features_signature(model, tf_transform_output):
  """Returns a serving signature that applies tf.Transform to features."""

  # We need to track the layers in the model in order to save it.
  # TODO(b/162357359): Revise once the bug is resolved.
  model.tft_layer_eval = tf_transform_output.transform_features_layer()

  @tf.function(input_signature=[
      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')
  ])
  def transform_features_fn(serialized_tf_example):
    """Returns the transformed_features to be fed as input to evaluator."""
    raw_feature_spec = tf_transform_output.raw_feature_spec()
    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)
    transformed_features = model.tft_layer_eval(raw_features)
    logging.info('eval_transformed_features = %s', transformed_features)
    return transformed_features

  return transform_features_fn


</source>
</class>

<class classid="146" nclones="2" nlines="76" similarity="83">
<source file="systems/tfx-1.6.1/tfx/examples/penguin/experimental/penguin_pipeline_sklearn_gcp.py" startline="112" endline="223" pcid="3135">
def _create_pipeline(
    pipeline_name: str,
    pipeline_root: str,
    data_root: str,
    trainer_module_file: str,
    evaluator_module_file: str,
    ai_platform_training_args: Optional[Dict[str, str]],
    ai_platform_serving_args: Optional[Dict[str, str]],
    beam_pipeline_args: List[str],
) -> tfx.dsl.Pipeline:
  """Implements the Penguin pipeline with TFX."""
  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = tfx.components.CsvExampleGen(
      input_base=os.path.join(data_root, 'labelled'))

  # Computes statistics over data for visualization and example validation.
  statistics_gen = tfx.components.StatisticsGen(
      examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = tfx.components.SchemaGen(
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = tfx.components.ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # TODO(humichael): Handle applying transformation component in Milestone 3.

  # Uses user-provided Python function that trains a model.
  # Num_steps is not provided during evaluation because the scikit-learn model
  # loads and evaluates the entire test set at once.
  trainer = tfx.extensions.google_cloud_ai_platform.Trainer(
      module_file=trainer_module_file,
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      train_args=tfx.proto.TrainArgs(num_steps=2000),
      eval_args=tfx.proto.EvalArgs(),
      custom_config={
          tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY:
          ai_platform_training_args,
      })

  # Get the latest blessed model for model validation.
  model_resolver = tfx.dsl.Resolver(
      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,
      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),
      model_blessing=tfx.dsl.Channel(
          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(
              'latest_blessed_model_resolver')

  # Uses TFMA to compute evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(label_key='species')],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='Accuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          lower_bound={'value': 0.6}),
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-10})))
          ])
      ])

  evaluator = tfx.components.Evaluator(
      module_file=evaluator_module_file,
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  pusher = tfx.extensions.google_cloud_ai_platform.Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      custom_config={
          tfx.extensions.google_cloud_ai_platform.experimental
          .PUSHER_SERVING_ARGS_KEY: ai_platform_serving_args,
      })

  return tfx.dsl.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen,
          statistics_gen,
          schema_gen,
          example_validator,
          trainer,
          model_resolver,
          evaluator,
          pusher,
      ],
      enable_cache=True,
      beam_pipeline_args=beam_pipeline_args,
  )


# To run this pipeline from the python CLI:
# $ tfx pipeline create \
#   --engine kubeflow \
#   --pipeline-path penguin_pipeline_sklearn_gcp.py \
#   --endpoint my-gcp-endpoint.pipelines.googleusercontent.com
# See TFX CLI guide for creating TFX pipelines:
# https://github.com/tensorflow/tfx/blob/master/docs/guide/cli.md#create
# For endpoint, see guide on connecting to hosted AI Platform Pipelines:
# https://cloud.google.com/ai-platform/pipelines/docs/connecting-with-sdk
</source>
<source file="systems/tfx-1.6.1/tfx/examples/penguin/experimental/penguin_pipeline_sklearn_local.py" startline="66" endline="166" pcid="3148">
def _create_pipeline(
    pipeline_name: str,
    pipeline_root: str,
    data_root: str,
    trainer_module_file: str,
    evaluator_module_file: str,
    serving_model_dir: str,
    metadata_path: str,
    beam_pipeline_args: List[str],
) -> tfx.dsl.Pipeline:
  """Implements the Penguin pipeline with TFX."""
  # Brings data into the pipeline or otherwise joins/converts training data.
  example_gen = tfx.components.CsvExampleGen(
      input_base=os.path.join(data_root, 'labelled'))

  # Computes statistics over data for visualization and example validation.
  statistics_gen = tfx.components.StatisticsGen(
      examples=example_gen.outputs['examples'])

  # Generates schema based on statistics files.
  schema_gen = tfx.components.SchemaGen(
      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)

  # Performs anomaly detection based on statistics and data schema.
  example_validator = tfx.components.ExampleValidator(
      statistics=statistics_gen.outputs['statistics'],
      schema=schema_gen.outputs['schema'])

  # TODO(humichael): Handle applying transformation component in Milestone 3.

  # Uses user-provided Python function that trains a model.
  # Num_steps is not provided during evaluation because the scikit-learn model
  # loads and evaluates the entire test set at once.
  trainer = tfx.components.Trainer(
      module_file=trainer_module_file,
      examples=example_gen.outputs['examples'],
      schema=schema_gen.outputs['schema'],
      train_args=tfx.proto.TrainArgs(num_steps=2000),
      eval_args=tfx.proto.EvalArgs())

  # Get the latest blessed model for model validation.
  model_resolver = tfx.dsl.Resolver(
      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,
      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),
      model_blessing=tfx.dsl.Channel(
          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(
              'latest_blessed_model_resolver')

  # Uses TFMA to compute evaluation statistics over features of a model and
  # perform quality validation of a candidate model (compared to a baseline).
  eval_config = tfma.EvalConfig(
      model_specs=[tfma.ModelSpec(label_key='species')],
      slicing_specs=[tfma.SlicingSpec()],
      metrics_specs=[
          tfma.MetricsSpec(metrics=[
              tfma.MetricConfig(
                  class_name='Accuracy',
                  threshold=tfma.MetricThreshold(
                      value_threshold=tfma.GenericValueThreshold(
                          lower_bound={'value': 0.6}),
                      change_threshold=tfma.GenericChangeThreshold(
                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,
                          absolute={'value': -1e-10})))
          ])
      ])
  evaluator = tfx.components.Evaluator(
      module_file=evaluator_module_file,
      examples=example_gen.outputs['examples'],
      model=trainer.outputs['model'],
      baseline_model=model_resolver.outputs['model'],
      eval_config=eval_config)

  pusher = tfx.components.Pusher(
      model=trainer.outputs['model'],
      model_blessing=evaluator.outputs['blessing'],
      push_destination=tfx.proto.PushDestination(
          filesystem=tfx.proto.PushDestination.Filesystem(
              base_directory=serving_model_dir)))

  return tfx.dsl.Pipeline(
      pipeline_name=pipeline_name,
      pipeline_root=pipeline_root,
      components=[
          example_gen,
          statistics_gen,
          schema_gen,
          example_validator,
          trainer,
          model_resolver,
          evaluator,
          pusher,
      ],
      enable_cache=True,
      metadata_connection_config=tfx.orchestration.metadata
      .sqlite_metadata_connection_config(metadata_path),
      beam_pipeline_args=beam_pipeline_args,
  )


# To run this pipeline from the python CLI:
#   $python penguin_pipeline_sklearn_local.py
</source>
</class>

</clones>

<clones>
<systeminfo processor="nicad6" system="addons-0.12.0" granularity="functions-blind" threshold="30%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="1031" npairs="135"/>
<runinfo ncompares="41674" cputime="66606"/>
<classinfo nclasses="58"/>

<class classid="1" nclones="4" nlines="13" similarity="71">
<source file="systems/addons-0.12.0/tools/testing/source_code_test.py" startline="88" endline="115" pcid="9">
def test_no_private_tf_api():
    # TODO: remove all elements of the list and remove the allowlist
    # This allowlist should not grow. Do not add elements to this list.
    allowlist = [
        "tensorflow_addons/metrics/r_square.py",
        "tensorflow_addons/utils/test_utils.py",
        "tensorflow_addons/seq2seq/decoder.py",
        "tensorflow_addons/seq2seq/attention_wrapper.py",
        "tensorflow_addons/utils/types.py",
    ]

    for file_path, line_idx, line in get_lines_of_source_code(allowlist):

        if "import tensorflow.python" in line or "from tensorflow.python" in line:
            raise ImportError(
                "A private tensorflow API import was found in {} at line {}.\n"
                "tensorflow.python refers to TensorFlow's internal source "
                "code and private functions/classes.\n"
                "The use of those is forbidden in Addons for stability reasons."
                "\nYou should find a public alternative or ask the "
                "TensorFlow team to expose publicly the function/class "
                "that you are using.\n"
                "If you're trying to do `import tensorflow.python.keras` "
                "it can be replaced with `import tensorflow.keras`."
                "".format(file_path, line_idx + 1)
            )


</source>
<source file="systems/addons-0.12.0/tools/testing/source_code_test.py" startline="116" endline="145" pcid="10">
def test_no_tf_cond():
    # TODO: remove all elements of the list and remove the allowlist
    # This allowlist should not grow. Do not add elements to this list.
    allowlist = [
        "tensorflow_addons/text/crf.py",
        "tensorflow_addons/layers/wrappers.py",
        "tensorflow_addons/image/connected_components.py",
        "tensorflow_addons/optimizers/novograd.py",
        "tensorflow_addons/metrics/cohens_kappa.py",
        "tensorflow_addons/seq2seq/sampler.py",
        "tensorflow_addons/seq2seq/beam_search_decoder.py",
    ]
    for file_path, line_idx, line in get_lines_of_source_code(allowlist):

        if "tf.cond(" in line:
            raise NameError(
                "The usage of a tf.cond() function call was found in "
                "file {} at line {}:\n\n"
                "   {}\n"
                "In TensorFlow 2.x, using a simple `if` in a function decorated "
                "with `@tf.function` is equivalent to a tf.cond() thanks to Autograph. \n"
                "TensorFlow Addons aims to be written with idiomatic TF 2.x code. \n"
                "As such, using tf.cond() is not allowed in the codebase. \n"
                "Use a `if` and decorate your function with @tf.function instead. \n"
                "You can take a look at "
                "https://www.tensorflow.org/guide/function#use_python_control_flow"
                "".format(file_path, line_idx, line)
            )


</source>
<source file="systems/addons-0.12.0/tools/testing/source_code_test.py" startline="211" endline="230" pcid="13">
def test_no_deprecated_v1():
    # TODO: remove all elements of the list and remove the allowlist
    # This allowlist should not grow. Do not add elements to this list.
    allowlist = [
        "tensorflow_addons/text/skip_gram_ops.py",
        "tensorflow_addons/seq2seq/decoder.py",
        "tensorflow_addons/seq2seq/tests/attention_wrapper_test.py",
    ]
    for file_path, line_idx, line in get_lines_of_source_code(allowlist):

        if "tf.compat.v1" in line:
            raise NameError(
                "The usage of a tf.compat.v1 API was found in file {} at line {}:\n\n"
                "   {}\n"
                "TensorFlow Addons doesn't support running programs with "
                "`tf.compat.v1.disable_v2_behavior()`.\n"
                "As such, there should be no need for the compatibility module "
                "tf.compat. Please find an alternative using only the TF2.x API."
                "".format(file_path, line_idx, line)
            )
</source>
<source file="systems/addons-0.12.0/tools/testing/source_code_test.py" startline="174" endline="210" pcid="12">
def test_no_tf_control_dependencies():
    # TODO: remove all elements of the list and remove the allowlist
    # This allowlist should not grow. Do not add elements to this list.
    allowlist = [
        "tensorflow_addons/layers/wrappers.py",
        "tensorflow_addons/image/utils.py",
        "tensorflow_addons/image/dense_image_warp.py",
        "tensorflow_addons/optimizers/average_wrapper.py",
        "tensorflow_addons/optimizers/yogi.py",
        "tensorflow_addons/optimizers/lookahead.py",
        "tensorflow_addons/optimizers/weight_decay_optimizers.py",
        "tensorflow_addons/optimizers/rectified_adam.py",
        "tensorflow_addons/optimizers/lamb.py",
        "tensorflow_addons/seq2seq/sampler.py",
        "tensorflow_addons/seq2seq/beam_search_decoder.py",
        "tensorflow_addons/seq2seq/attention_wrapper.py",
    ]
    for file_path, line_idx, line in get_lines_of_source_code(allowlist):

        if "tf.control_dependencies(" in line:

            raise NameError(
                "The usage of a tf.control_dependencies() function call was found in "
                "file {} at line {}:\n\n"
                "   {}\n"
                "In TensorFlow 2.x, in a function decorated "
                "with `@tf.function` the dependencies are controlled automatically"
                " thanks to Autograph. \n"
                "TensorFlow Addons aims to be written with idiomatic TF 2.x code. \n"
                "As such, using tf.control_dependencies() is not allowed in the codebase. \n"
                "Decorate your function with @tf.function instead. \n"
                "You can take a look at \n"
                "https://github.com/tensorflow/community/blob/master/rfcs/20180918-functions-not-sessions-20.md#program-order-semantics--control-dependencies"
                "".format(file_path, line_idx, line)
            )


</source>
</class>

<class classid="2" nclones="2" nlines="11" similarity="90">
<source file="systems/addons-0.12.0/build_deps/toolchains/gpu/find_cuda_config.py" startline="175" endline="187" pcid="34">
def _header_paths():
    """Returns hard-coded set of relative paths to look for header files."""
    return [
        "",
        "include",
        "include/cuda",
        "include/*-linux-gnu",
        "extras/CUPTI/include",
        "include/cuda/CUPTI",
        "local/cuda/extras/CUPTI/include",
    ]


</source>
<source file="systems/addons-0.12.0/build_deps/toolchains/gpu/find_cuda_config.py" startline="188" endline="201" pcid="35">
def _library_paths():
    """Returns hard-coded set of relative paths to look for library files."""
    return [
        "",
        "lib64",
        "lib",
        "lib/*-linux-gnu",
        "lib/x64",
        "extras/CUPTI/*",
        "local/cuda/lib64",
        "local/cuda/extras/CUPTI/lib64",
    ]


</source>
</class>

<class classid="3" nclones="5" nlines="21" similarity="78">
<source file="systems/addons-0.12.0/build_deps/toolchains/gpu/find_cuda_config.py" startline="335" endline="367" pcid="44">
def _find_cublas_config(base_paths, required_version, cuda_version):

    if _at_least_version(cuda_version, "10.1"):

        def get_header_version(path):
            version = (
                _get_header_version(path, name)
                for name in ("CUBLAS_VER_MAJOR", "CUBLAS_VER_MINOR", "CUBLAS_VER_PATCH")
            )
            return ".".join(version)

        header_path, header_version = _find_header(
            base_paths, "cublas_api.h", required_version, get_header_version
        )
        # cuBLAS uses the major version only.
        cublas_version = header_version.split(".")[0]

    else:
        # There is no version info available before CUDA 10.1, just find the file.
        header_version = cuda_version
        header_path = _find_file(base_paths, _header_paths(), "cublas_api.h")
        # cuBLAS version is the same as CUDA version (x.y).
        cublas_version = required_version

    library_path = _find_library(base_paths, "cublas", cublas_version)

    return {
        "cublas_version": header_version,
        "cublas_include_dir": os.path.dirname(header_path),
        "cublas_library_dir": os.path.dirname(library_path),
    }


</source>
<source file="systems/addons-0.12.0/build_deps/toolchains/gpu/find_cuda_config.py" startline="484" endline="517" pcid="54">
def _find_cusparse_config(base_paths, required_version, cuda_version):

    if _at_least_version(cuda_version, "11.0"):

        def get_header_version(path):
            version = (
                _get_header_version(path, name)
                for name in (
                    "CUSPARSE_VER_MAJOR",
                    "CUSPARSE_VER_MINOR",
                    "CUSPARSE_VER_PATCH",
                )
            )
            return ".".join(version)

        header_path, header_version = _find_header(
            base_paths, "cusparse.h", required_version, get_header_version
        )
        cusparse_version = header_version.split(".")[0]

    else:
        header_version = cuda_version
        header_path = _find_file(base_paths, _header_paths(), "cusparse.h")
        cusparse_version = required_version

    library_path = _find_library(base_paths, "cusparse", cusparse_version)

    return {
        "cusparse_version": header_version,
        "cusparse_include_dir": os.path.dirname(header_path),
        "cusparse_library_dir": os.path.dirname(library_path),
    }


</source>
<source file="systems/addons-0.12.0/build_deps/toolchains/gpu/find_cuda_config.py" startline="368" endline="401" pcid="46">
def _find_cusolver_config(base_paths, required_version, cuda_version):

    if _at_least_version(cuda_version, "11.0"):

        def get_header_version(path):
            version = (
                _get_header_version(path, name)
                for name in (
                    "CUSOLVER_VER_MAJOR",
                    "CUSOLVER_VER_MINOR",
                    "CUSOLVER_VER_PATCH",
                )
            )
            return ".".join(version)

        header_path, header_version = _find_header(
            base_paths, "cusolver_common.h", required_version, get_header_version
        )
        cusolver_version = header_version.split(".")[0]

    else:
        header_version = cuda_version
        header_path = _find_file(base_paths, _header_paths(), "cusolver_common.h")
        cusolver_version = required_version

    library_path = _find_library(base_paths, "cusolver", cusolver_version)

    return {
        "cusolver_version": header_version,
        "cusolver_include_dir": os.path.dirname(header_path),
        "cusolver_library_dir": os.path.dirname(library_path),
    }


</source>
<source file="systems/addons-0.12.0/build_deps/toolchains/gpu/find_cuda_config.py" startline="432" endline="461" pcid="50">
def _find_cufft_config(base_paths, required_version, cuda_version):

    if _at_least_version(cuda_version, "11.0"):

        def get_header_version(path):
            version = (
                _get_header_version(path, name)
                for name in ("CUFFT_VER_MAJOR", "CUFFT_VER_MINOR", "CUFFT_VER_PATCH")
            )
            return ".".join(version)

        header_path, header_version = _find_header(
            base_paths, "cufft.h", required_version, get_header_version
        )
        cufft_version = header_version.split(".")[0]

    else:
        header_version = cuda_version
        header_path = _find_file(base_paths, _header_paths(), "cufft.h")
        cufft_version = required_version

    library_path = _find_library(base_paths, "cufft", cufft_version)

    return {
        "cufft_version": header_version,
        "cufft_include_dir": os.path.dirname(header_path),
        "cufft_library_dir": os.path.dirname(library_path),
    }


</source>
<source file="systems/addons-0.12.0/build_deps/toolchains/gpu/find_cuda_config.py" startline="402" endline="431" pcid="48">
def _find_curand_config(base_paths, required_version, cuda_version):

    if _at_least_version(cuda_version, "11.0"):

        def get_header_version(path):
            version = (
                _get_header_version(path, name)
                for name in ("CURAND_VER_MAJOR", "CURAND_VER_MINOR", "CURAND_VER_PATCH")
            )
            return ".".join(version)

        header_path, header_version = _find_header(
            base_paths, "curand.h", required_version, get_header_version
        )
        curand_version = header_version.split(".")[0]

    else:
        header_version = cuda_version
        header_path = _find_file(base_paths, _header_paths(), "curand.h")
        curand_version = required_version

    library_path = _find_library(base_paths, "curand", curand_version)

    return {
        "curand_version": header_version,
        "curand_include_dir": os.path.dirname(header_path),
        "curand_library_dir": os.path.dirname(library_path),
    }


</source>
</class>

<class classid="4" nclones="2" nlines="15" similarity="71">
<source file="systems/addons-0.12.0/build_deps/toolchains/gpu/find_cuda_config.py" startline="462" endline="483" pcid="52">
def _find_cudnn_config(base_paths, required_version):
    def get_header_version(path):
        version = [
            _get_header_version(path, name)
            for name in ("CUDNN_MAJOR", "CUDNN_MINOR", "CUDNN_PATCHLEVEL")
        ]
        return ".".join(version) if version[0] else None

    header_path, header_version = _find_header(
        base_paths, ("cudnn.h", "cudnn_version.h"), required_version, get_header_version
    )
    cudnn_version = header_version.split(".")[0]

    library_path = _find_library(base_paths, "cudnn", cudnn_version)

    return {
        "cudnn_version": cudnn_version,
        "cudnn_include_dir": os.path.dirname(header_path),
        "cudnn_library_dir": os.path.dirname(library_path),
    }


</source>
<source file="systems/addons-0.12.0/build_deps/toolchains/gpu/find_cuda_config.py" startline="518" endline="539" pcid="56">
def _find_nccl_config(base_paths, required_version):
    def get_header_version(path):
        version = (
            _get_header_version(path, name)
            for name in ("NCCL_MAJOR", "NCCL_MINOR", "NCCL_PATCH")
        )
        return ".".join(version)

    header_path, header_version = _find_header(
        base_paths, "nccl.h", required_version, get_header_version
    )
    nccl_version = header_version.split(".")[0]

    library_path = _find_library(base_paths, "nccl", nccl_version)

    return {
        "nccl_version": nccl_version,
        "nccl_include_dir": os.path.dirname(header_path),
        "nccl_library_dir": os.path.dirname(library_path),
    }


</source>
</class>

<class classid="5" nclones="3" nlines="15" similarity="73">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/moving_average_test.py" startline="85" endline="104" pcid="110">
def test_model_weights_update():
    grad = tf.Variable([[0.1]])
    model = tf.keras.Sequential(
        [
            tf.keras.layers.Dense(
                1,
                kernel_initializer=tf.keras.initializers.Constant([[1.0]]),
                use_bias=False,
            )
        ]
    )
    model.build(input_shape=[1, 1])

    opt = MovingAverage(tf.keras.optimizers.SGD(lr=2.0), average_decay=0.5)
    _ = opt.apply_gradients(list(zip([grad], model.variables)))
    np.testing.assert_allclose(model.variables[0].read_value(), [[0.8]])
    _ = opt.assign_average_vars(model.variables)
    np.testing.assert_allclose(model.variables[0].read_value(), [[0.9]])


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/moving_average_test.py" startline="106" endline="125" pcid="111">
def test_model_dynamic_lr():
    grad = tf.Variable([[0.1]])
    model = tf.keras.Sequential(
        [
            tf.keras.layers.Dense(
                1,
                kernel_initializer=tf.keras.initializers.Constant([[1.0]]),
                use_bias=False,
            )
        ]
    )
    model.build(input_shape=[1, 1])

    opt = MovingAverage(tf.keras.optimizers.SGD(lr=1e-3), average_decay=0.5)
    _ = opt.apply_gradients(list(zip([grad], model.variables)))
    np.testing.assert_allclose(opt.lr.read_value(), 1e-3)
    opt.lr = 1e-4
    np.testing.assert_allclose(opt.lr.read_value(), 1e-4)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/lookahead_test.py" startline="146" endline="167" pcid="252">
def test_model_dynamic_lr():
    grad = tf.Variable([[0.1]])
    model = tf.keras.Sequential(
        [
            tf.keras.layers.Dense(
                1,
                kernel_initializer=tf.keras.initializers.Constant([[1.0]]),
                use_bias=False,
            )
        ]
    )
    model.build(input_shape=[1, 1])

    opt = Lookahead("adam", sync_period=10, slow_step_size=0.4)
    _ = opt.apply_gradients(list(zip([grad], model.variables)))

    np.testing.assert_allclose(opt.lr.read_value(), 1e-3)

    opt.lr = 1e-4
    np.testing.assert_allclose(opt.lr.read_value(), 1e-4)


</source>
</class>

<class classid="6" nclones="2" nlines="19" similarity="94">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/moving_average_test.py" startline="152" endline="178" pcid="114">
def test_fit_simple_linear_model():
    seed = 0x2019
    np.random.seed(seed)
    tf.random.set_seed(seed)
    num_examples = 5000
    x = np.random.standard_normal((num_examples, 3))
    w = np.random.standard_normal((3, 1))
    y = np.dot(x, w) + np.random.standard_normal((num_examples, 1)) * 1e-4

    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Dense(input_shape=(3,), units=1))

    opt = MovingAverage("sgd")
    model.compile(opt, loss="mse")

    model.fit(x, y, epochs=5)
    opt.assign_average_vars(model.variables)

    x = np.random.standard_normal((100, 3))
    y = np.dot(x, w)

    predicted = model.predict(x)

    max_abs_diff = np.max(np.abs(predicted - y))
    assert max_abs_diff < 5e-3


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/stochastic_weight_averaging_test.py" startline="92" endline="117" pcid="260">
def test_fit_simple_linear_model():
    seed = 0x2019
    np.random.seed(seed)
    tf.random.set_seed(seed)
    num_examples = 100000
    x = np.random.standard_normal((num_examples, 3))
    w = np.random.standard_normal((3, 1))
    y = np.dot(x, w) + np.random.standard_normal((num_examples, 1)) * 1e-4

    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Dense(input_shape=(3,), units=1))
    # using num_examples - 1 since steps starts from 0.
    optimizer = SWA("sgd", start_averaging=num_examples // 32 - 1, average_period=100)
    model.compile(optimizer, loss="mse")
    model.fit(x, y, epochs=2)
    optimizer.assign_average_vars(model.variables)

    x = np.random.standard_normal((100, 3))
    y = np.dot(x, w)

    predicted = model.predict(x)

    max_abs_diff = np.max(np.abs(predicted - y))
    assert max_abs_diff < 1e-3


</source>
</class>

<class classid="7" nclones="2" nlines="12" similarity="75">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/moving_average_test.py" startline="190" endline="211" pcid="116">
def test_start_step():
    var0 = tf.Variable([1.0, 2.0])
    grads0 = tf.constant([0.1, 0.1])
    grads_and_vars = [(grads0, var0)]

    opt = MovingAverage(
        tf.keras.optimizers.SGD(lr=1.0), average_decay=0.5, start_step=1
    )

    opt.apply_gradients(grads_and_vars)

    np.testing.assert_allclose(var0.read_value(), [0.9, 1.9])

    ema_var0 = opt.get_slot(var0, "average")

    opt.apply_gradients(grads_and_vars)

    np.testing.assert_allclose(var0.read_value(), [0.8, 1.8])

    np.testing.assert_allclose(ema_var0.read_value(), [0.85, 1.85])


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/moving_average_test.py" startline="213" endline="230" pcid="117">
def test_dynamic_decay():
    var0 = tf.Variable([1.0, 2.0])
    grads0 = tf.constant([0.1, 0.1])
    grads_and_vars = [(grads0, var0)]

    opt = MovingAverage(
        tf.keras.optimizers.SGD(lr=2.0), average_decay=0.5, dynamic_decay=True
    )

    opt.apply_gradients(grads_and_vars)
    opt.apply_gradients(grads_and_vars)

    np.testing.assert_allclose(var0.read_value(), [0.6, 1.6])

    ema_var0 = opt.get_slot(var0, "average")
    np.testing.assert_allclose(ema_var0.read_value(), [0.64, 1.64])


</source>
</class>

<class classid="8" nclones="2" nlines="21" similarity="95">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/moving_average_test.py" startline="233" endline="262" pcid="118">
def test_swap_weight_no_shadow_copy(device):
    with device.scope():
        var = tf.Variable([1.0, 2.0])
        grads = tf.constant([0.1, 0.1])

        opt = MovingAverage(tf.keras.optimizers.SGD(lr=2.0), average_decay=0.5)

    @tf.function
    def apply_gradients():
        opt.apply_gradients([(grads, var)])

    device.run(apply_gradients)

    np.testing.assert_allclose(var.read_value(), [0.8, 1.8])
    ema_var = opt.get_slot(var, "average")
    np.testing.assert_allclose(ema_var.read_value(), [0.9, 1.9])

    with device.scope():
        opt.swap_weights()

    np.testing.assert_allclose(ema_var.read_value(), [0.8, 1.8])
    np.testing.assert_allclose(var.read_value(), [0.9, 1.9])

    with device.scope():
        opt.swap_weights()

    np.testing.assert_allclose(var.read_value(), [0.8, 1.8])
    np.testing.assert_allclose(ema_var.read_value(), [0.9, 1.9])


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/moving_average_test.py" startline="265" endline="295" pcid="120">
def test_swap_weights(device):
    with device.scope():
        var = tf.Variable([1.0, 2.0])
        grads = tf.constant([0.1, 0.1])

        opt = MovingAverage(tf.keras.optimizers.SGD(lr=2.0), average_decay=0.5)

    @tf.function
    def apply_gradients():
        opt.apply_gradients([(grads, var)])

    device.run(apply_gradients)

    np.testing.assert_allclose(var.read_value(), [0.8, 1.8])
    ema_var = opt.get_slot(var, "average")
    np.testing.assert_allclose(ema_var.read_value(), [0.9, 1.9])

    with device.scope():
        opt.shadow_copy([var])
        opt.swap_weights()

    np.testing.assert_allclose(ema_var.read_value(), [0.8, 1.8])
    np.testing.assert_allclose(var.read_value(), [0.9, 1.9])

    with device.scope():
        opt.swap_weights()

    np.testing.assert_allclose(var.read_value(), [0.8, 1.8])
    np.testing.assert_allclose(ema_var.read_value(), [0.9, 1.9])


</source>
</class>

<class classid="9" nclones="2" nlines="33" similarity="87">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/lazy_adam_test.py" startline="56" endline="98" pcid="127">
def _test_sparse(dtype):
    # Initialize tf for numpy implementation.
    m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0
    var0_np = np.array([1.0, 1.0, 2.0], dtype=dtype.as_numpy_dtype)
    grads0_np = np.array([0.1, 0.0, 0.1], dtype=dtype.as_numpy_dtype)
    var1_np = np.array([3.0, 3.0, 4.0], dtype=dtype.as_numpy_dtype)
    grads1_np = np.array([0.01, 0.0, 0.01], dtype=dtype.as_numpy_dtype)

    var0 = tf.Variable(var0_np)
    var1 = tf.Variable(var1_np)
    grads0_np_indices = np.array([0, 2], dtype=np.int32)
    grads0 = tf.IndexedSlices(
        tf.constant(grads0_np[grads0_np_indices]),
        tf.constant(grads0_np_indices),
        tf.constant([3]),
    )
    grads1_np_indices = np.array([0, 2], dtype=np.int32)
    grads1 = tf.IndexedSlices(
        tf.constant(grads1_np[grads1_np_indices]),
        tf.constant(grads1_np_indices),
        tf.constant([3]),
    )
    opt = lazy_adam.LazyAdam()

    # Fetch params to validate initial values
    np.testing.assert_allclose([1.0, 1.0, 2.0], var0.numpy(), 1e-6, 1e-6)
    np.testing.assert_allclose([3.0, 3.0, 4.0], var1.numpy(), 1e-6, 1e-6)

    # Run 3 steps of Adam
    for t in range(3):
        beta_1_power, beta_2_power = get_beta_accumulators(opt, dtype)
        test_utils.assert_allclose_according_to_type(0.9 ** (t + 1), beta_1_power)
        test_utils.assert_allclose_according_to_type(0.999 ** (t + 1), beta_2_power)
        opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

        var0_np, m0, v0 = adam_update_numpy(var0_np, grads0_np, t, m0, v0)
        var1_np, m1, v1 = adam_update_numpy(var1_np, grads1_np, t, m1, v1)

        # Validate updated params
        test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())
        test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/lamb_test.py" startline="69" endline="113" pcid="217">
def test_sparse():
    for dtype in _dtypes_to_test(use_gpu=test_utils.is_gpu_available()):
        # Initialize tf for numpy implementation.
        m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0
        var0_np = np.array([1.0, 1.0, 2.0], dtype=dtype.as_numpy_dtype)
        grads0_np = np.array([0.1, 0.0, 0.1], dtype=dtype.as_numpy_dtype)
        var1_np = np.array([3.0, 3.0, 4.0], dtype=dtype.as_numpy_dtype)
        grads1_np = np.array([0.01, 0.0, 0.01], dtype=dtype.as_numpy_dtype)

        var0 = tf.Variable(var0_np)
        var1 = tf.Variable(var1_np)
        grads0_np_indices = np.array([0, 2], dtype=np.int32)
        grads0 = tf.IndexedSlices(
            tf.constant(grads0_np[grads0_np_indices]),
            tf.constant(grads0_np_indices),
            tf.constant([3]),
        )
        grads1_np_indices = np.array([0, 2], dtype=np.int32)
        grads1 = tf.IndexedSlices(
            tf.constant(grads1_np[grads1_np_indices]),
            tf.constant(grads1_np_indices),
            tf.constant([3]),
        )
        opt = lamb.LAMB()

        # Fetch params to validate initial values
        np.testing.assert_allclose(np.asanyarray([1.0, 1.0, 2.0]), var0.numpy())
        np.testing.assert_allclose(np.asanyarray([3.0, 3.0, 4.0]), var1.numpy())

        # Run 3 steps of LAMB
        for t in range(3):
            beta_1_power, beta_2_power = get_beta_accumulators(opt, dtype)
            test_utils.assert_allclose_according_to_type(0.9 ** (t + 1), beta_1_power)
            test_utils.assert_allclose_according_to_type(0.999 ** (t + 1), beta_2_power)

            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

            var0_np, m0, v0 = lamb_update_numpy(var0_np, grads0_np, t, m0, v0)
            var1_np, m1, v1 = lamb_update_numpy(var1_np, grads1_np, t, m1, v1)

            # Validate updated params
            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())
            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())


</source>
</class>

<class classid="10" nclones="8" nlines="23" similarity="75">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/lazy_adam_test.py" startline="147" endline="182" pcid="131">
def test_basic(use_callable_params, dtype):
    # Initialize tf for numpy implementation.
    m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0
    var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
    grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
    var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
    grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)

    var0 = tf.Variable(var0_np)
    var1 = tf.Variable(var1_np)
    grads0 = tf.constant(grads0_np)
    grads1 = tf.constant(grads1_np)

    def learning_rate():
        return 0.001

    if not use_callable_params:
        learning_rate = learning_rate()

    opt = lazy_adam.LazyAdam(learning_rate=learning_rate)

    # Run 3 steps of Adam
    for t in range(3):
        beta_1_power, beta_2_power = get_beta_accumulators(opt, dtype)
        test_utils.assert_allclose_according_to_type(0.9 ** (t + 1), beta_1_power)
        test_utils.assert_allclose_according_to_type(0.999 ** (t + 1), beta_2_power)
        opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

        var0_np, m0, v0 = adam_update_numpy(var0_np, grads0_np, t, m0, v0)
        var1_np, m1, v1 = adam_update_numpy(var1_np, grads1_np, t, m1, v1)

        # Validate updated params
        test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())
        test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/lazy_adam_test.py" startline="184" endline="212" pcid="133">
def test_tensor_learning_rate(dtype):
    # Initialize tf for numpy implementation.
    m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0
    var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
    grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
    var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
    grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)

    var0 = tf.Variable(var0_np)
    var1 = tf.Variable(var1_np)
    grads0 = tf.constant(grads0_np)
    grads1 = tf.constant(grads1_np)
    opt = lazy_adam.LazyAdam(tf.constant(0.001))

    # Run 3 steps of Adam
    for t in range(3):
        beta_1_power, beta_2_power = get_beta_accumulators(opt, dtype)
        test_utils.assert_allclose_according_to_type(0.9 ** (t + 1), beta_1_power)
        test_utils.assert_allclose_according_to_type(0.999 ** (t + 1), beta_2_power)
        opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

        var0_np, m0, v0 = adam_update_numpy(var0_np, grads0_np, t, m0, v0)
        var1_np, m1, v1 = adam_update_numpy(var1_np, grads1_np, t, m1, v1)

        # Validate updated params
        test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())
        test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/lamb_test.py" startline="241" endline="275" pcid="221">
def test_sharing():
    for dtype in _dtypes_to_test(use_gpu=test_utils.is_gpu_available()):
        # Initialize variables for numpy implementation.
        m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0
        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)

        var0 = tf.Variable(var0_np)
        var1 = tf.Variable(var1_np)
        grads0 = tf.constant(grads0_np)
        grads1 = tf.constant(grads1_np)
        opt = lamb.LAMB()

        # Fetch params to validate initial values
        np.testing.assert_allclose(np.asanyarray([1.0, 2.0]), var0.numpy())
        np.testing.assert_allclose(np.asanyarray([3.0, 4.0]), var1.numpy())

        # Run 3 steps of intertwined LAMB1 and LAMB2.
        for t in range(3):
            beta_1_power, beta_2_power = get_beta_accumulators(opt, dtype)
            test_utils.assert_allclose_according_to_type(0.9 ** (t + 1), beta_1_power)
            test_utils.assert_allclose_according_to_type(0.999 ** (t + 1), beta_2_power)

            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

            var0_np, m0, v0 = lamb_update_numpy(var0_np, grads0_np, t, m0, v0)
            var1_np, m1, v1 = lamb_update_numpy(var1_np, grads1_np, t, m1, v1)

            # Validate updated params
            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())
            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/lamb_test.py" startline="295" endline="329" pcid="224">
def test_resource():
    for i, dtype in enumerate(_dtypes_to_test(use_gpu=test_utils.is_gpu_available())):
        # Initialize variables for numpy implementation.
        m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0
        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)

        var0 = tf.Variable(var0_np, name="var0_%d" % i)
        var1 = tf.Variable(var1_np, name="var1_%d" % i)
        grads0 = tf.constant(grads0_np)
        grads1 = tf.constant(grads1_np)

        def learning_rate():
            return 0.001

        opt = lamb.LAMB(learning_rate=learning_rate)

        # Run 3 steps of LAMB
        for t in range(3):
            beta_1_power, beta_2_power = get_beta_accumulators(opt, dtype)
            test_utils.assert_allclose_according_to_type(0.9 ** (t + 1), beta_1_power)
            test_utils.assert_allclose_according_to_type(0.999 ** (t + 1), beta_2_power)

            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

            var0_np, m0, v0 = lamb_update_numpy(var0_np, grads0_np, t, m0, v0)
            var1_np, m1, v1 = lamb_update_numpy(var1_np, grads1_np, t, m1, v1)

            # Validate updated params
            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())
            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/lazy_adam_test.py" startline="215" endline="247" pcid="134">
def test_sharing(dtype):
    # Initialize tf for numpy implementation.
    m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0
    var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
    grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
    var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
    grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)

    var0 = tf.Variable(var0_np)
    var1 = tf.Variable(var1_np)
    grads0 = tf.constant(grads0_np)
    grads1 = tf.constant(grads1_np)
    opt = lazy_adam.LazyAdam()

    # Fetch params to validate initial values
    np.testing.assert_allclose([1.0, 2.0], var0.numpy())
    np.testing.assert_allclose([3.0, 4.0], var1.numpy())

    # Run 3 steps of intertwined Adam1 and Adam2.
    for t in range(3):
        beta_1_power, beta_2_power = get_beta_accumulators(opt, dtype)
        test_utils.assert_allclose_according_to_type(0.9 ** (t + 1), beta_1_power)
        test_utils.assert_allclose_according_to_type(0.999 ** (t + 1), beta_2_power)
        opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

        var0_np, m0, v0 = adam_update_numpy(var0_np, grads0_np, t, m0, v0)
        var1_np, m1, v1 = adam_update_numpy(var1_np, grads1_np, t, m1, v1)

        # Validate updated params
        test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())
        test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/lamb_test.py" startline="206" endline="239" pcid="220">
def test_tensor_learning_rate():
    for dtype in _dtypes_to_test(use_gpu=test_utils.is_gpu_available()):
        # Initialize variables for numpy implementation.
        m0, v0, m1, v1 = 0.0, 0.0, 0.0, 0.0
        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)

        var0 = tf.Variable(var0_np)
        var1 = tf.Variable(var1_np)
        grads0 = tf.constant(grads0_np)
        grads1 = tf.constant(grads1_np)
        opt = lamb.LAMB(tf.constant(0.001))

        # Fetch params to validate initial values
        np.testing.assert_allclose(np.asanyarray([1.0, 2.0]), var0.numpy())
        np.testing.assert_allclose(np.asanyarray([3.0, 4.0]), var1.numpy())

        # Run 3 steps of LAMB
        for t in range(3):
            beta_1_power, beta_2_power = get_beta_accumulators(opt, dtype)
            test_utils.assert_allclose_according_to_type(0.9 ** (t + 1), beta_1_power)
            test_utils.assert_allclose_according_to_type(0.999 ** (t + 1), beta_2_power)
            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

            var0_np, m0, v0 = lamb_update_numpy(var0_np, grads0_np, t, m0, v0)
            var1_np, m1, v1 = lamb_update_numpy(var1_np, grads1_np, t, m1, v1)

            # Validate updated params
            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())
            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/yogi_test.py" startline="265" endline="299" pcid="210">
def test_tensor_learning_rate():
    for dtype in _dtypes_to_test(use_gpu=test_utils.is_gpu_available()):
        # Initialize variables for numpy implementation.
        m0, v0, m1, v1 = 0.0, 1.0, 0.0, 1.0
        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)

        var0 = tf.Variable(var0_np)
        var1 = tf.Variable(var1_np)
        grads0 = tf.constant(grads0_np)
        grads1 = tf.constant(grads1_np)
        opt = yogi.Yogi(tf.constant(0.01), initial_accumulator_value=1.0)

        # Fetch params to validate initial values.
        np.testing.assert_allclose(np.asanyarray([1.0, 2.0]), var0.numpy())
        np.testing.assert_allclose(np.asanyarray([3.0, 4.0]), var1.numpy())

        # Run 3 steps of Yogi.
        for t in range(1, 4):
            beta1_power, beta2_power = get_beta_accumulators(opt, dtype)
            test_utils.assert_allclose_according_to_type(0.9 ** t, beta1_power)
            test_utils.assert_allclose_according_to_type(0.999 ** t, beta2_power)

            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

            var0_np, m0, v0 = yogi_update_numpy(var0_np, grads0_np, t, m0, v0)
            var1_np, m1, v1 = yogi_update_numpy(var1_np, grads1_np, t, m1, v1)

            # Validate updated params.
            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())
            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/yogi_test.py" startline="301" endline="333" pcid="211">
def test_sharing():
    for dtype in _dtypes_to_test(use_gpu=test_utils.is_gpu_available()):
        # Initialize variables for numpy implementation.
        m0, v0, m1, v1 = 0.0, 1.0, 0.0, 1.0
        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)

        var0 = tf.Variable(var0_np)
        var1 = tf.Variable(var1_np)
        grads0 = tf.constant(grads0_np)
        grads1 = tf.constant(grads1_np)
        opt = yogi.Yogi(initial_accumulator_value=1.0)

        # Fetch params to validate initial values.
        np.testing.assert_allclose(np.asanyarray([1.0, 2.0]), var0.numpy())
        np.testing.assert_allclose(np.asanyarray([3.0, 4.0]), var1.numpy())

        # Run 3 steps of intertwined Yogi1 and Yogi2.
        for t in range(1, 4):
            beta1_power, beta2_power = get_beta_accumulators(opt, dtype)
            test_utils.assert_allclose_according_to_type(0.9 ** t, beta1_power)
            test_utils.assert_allclose_according_to_type(0.999 ** t, beta2_power)
            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))
            var0_np, m0, v0 = yogi_update_numpy(var0_np, grads0_np, t, m0, v0)
            var1_np, m1, v1 = yogi_update_numpy(var1_np, grads1_np, t, m1, v1)

            # Validate updated params.
            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())
            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())


</source>
</class>

<class classid="11" nclones="2" nlines="12" similarity="83">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/proximal_adagrad_test.py" startline="65" endline="77" pcid="140">
def test_with_l1_l2_regularization():
    run_sample(
        iterations=10,
        expected=[[-0.0495, -0.0995], [-0.0045, -0.0095]],
        optimizer=ProximalAdagrad(
            lr=3.0,
            initial_accumulator_value=0.1,
            l1_regularization_strength=0.001,
            l2_regularization_strength=2.0,
        ),
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/proximal_adagrad_test.py" startline="100" endline="113" pcid="143">
def test_sparse_with_l1_l2_regularization():
    run_sample(
        iterations=10,
        expected=[[-0.0495, 2.0], [4.0, -0.0095]],
        optimizer=ProximalAdagrad(
            lr=3.0,
            initial_accumulator_value=0.1,
            l1_regularization_strength=0.001,
            l2_regularization_strength=2.0,
        ),
        sparse=True,
    )


</source>
</class>

<class classid="12" nclones="2" nlines="10" similarity="100">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/novograd_test.py" startline="24" endline="39" pcid="145">
def run_dense_sample(iterations, expected, optimizer):
    var_0 = tf.Variable([1.0, 2.0], dtype=tf.dtypes.float32)
    var_1 = tf.Variable([3.0, 4.0], dtype=tf.dtypes.float32)

    grad_0 = tf.constant([0.1, 0.2], dtype=tf.dtypes.float32)
    grad_1 = tf.constant([0.3, 0.4], dtype=tf.dtypes.float32)

    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))

    for _ in range(iterations):
        optimizer.apply_gradients(grads_and_vars)

    np.testing.assert_allclose(var_0.read_value(), expected[0], atol=2e-4)
    np.testing.assert_allclose(var_1.read_value(), expected[1], atol=2e-4)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/rectified_adam_test.py" startline="24" endline="39" pcid="230">
def run_dense_sample(iterations, expected, optimizer):
    var_0 = tf.Variable([1.0, 2.0], dtype=tf.dtypes.float32)
    var_1 = tf.Variable([3.0, 4.0], dtype=tf.dtypes.float32)

    grad_0 = tf.constant([0.1, 0.2], dtype=tf.dtypes.float32)
    grad_1 = tf.constant([0.03, 0.04], dtype=tf.dtypes.float32)

    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))

    for _ in range(iterations):
        optimizer.apply_gradients(grads_and_vars)

    np.testing.assert_allclose(var_0.read_value(), expected[0], atol=2e-4)
    np.testing.assert_allclose(var_1.read_value(), expected[1], atol=2e-4)


</source>
</class>

<class classid="13" nclones="2" nlines="10" similarity="80">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/novograd_test.py" startline="67" endline="82" pcid="149">
def run_sparse_sample(iterations, expected, optimizer):
    var_0 = tf.Variable([1.0, 2.0])
    var_1 = tf.Variable([3.0, 4.0])

    grad_0 = tf.IndexedSlices(tf.constant([0.1]), tf.constant([0]), tf.constant([2]))
    grad_1 = tf.IndexedSlices(tf.constant([0.4]), tf.constant([1]), tf.constant([2]))

    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))

    for _ in range(iterations):
        optimizer.apply_gradients(grads_and_vars)

    np.testing.assert_allclose(var_0.read_value(), expected[0])
    np.testing.assert_allclose(var_1.read_value(), expected[1])


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/rectified_adam_test.py" startline="40" endline="55" pcid="231">
def run_sparse_sample(iterations, expected, optimizer):
    var_0 = tf.Variable([1.0, 2.0])
    var_1 = tf.Variable([3.0, 4.0])

    grad_0 = tf.IndexedSlices(tf.constant([0.1]), tf.constant([0]), tf.constant([2]))
    grad_1 = tf.IndexedSlices(tf.constant([0.04]), tf.constant([1]), tf.constant([2]))

    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))

    for _ in range(iterations):
        optimizer.apply_gradients(grads_and_vars)

    np.testing.assert_allclose(var_0.read_value(), expected[0], atol=2e-4)
    np.testing.assert_allclose(var_1.read_value(), expected[1], atol=2e-4)


</source>
</class>

<class classid="14" nclones="3" nlines="15" similarity="93">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/novograd_test.py" startline="110" endline="131" pcid="153">
def test_fit_simple_linear_model():
    np.random.seed(0x2020)
    tf.random.set_seed(0x2020)

    x = np.random.standard_normal((100000, 3))
    w = np.random.standard_normal((3, 1))
    y = np.dot(x, w) + np.random.standard_normal((100000, 1)) * 1e-5

    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Dense(input_shape=(3,), units=1))
    model.compile(NovoGrad(), loss="mse")

    model.fit(x, y, epochs=2)

    x = np.random.standard_normal((100, 3))
    y = np.dot(x, w)
    predicted = model.predict(x)

    max_abs_diff = np.max(np.abs(predicted - y))
    assert max_abs_diff < 1e-2


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/lookahead_test.py" startline="124" endline="144" pcid="251">
def test_fit_simple_linear_model_mixed_precision():
    np.random.seed(0x2019)
    tf.random.set_seed(0x2019)

    x = np.random.standard_normal((10000, 3))
    w = np.random.standard_normal((3, 1))
    y = np.dot(x, w) + np.random.standard_normal((10000, 1)) * 1e-4

    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Dense(input_shape=(3,), units=1))
    model.compile(Lookahead("sgd"), loss="mse")
    model.fit(x, y, epochs=3)

    x = np.random.standard_normal((100, 3))
    y = np.dot(x, w)
    predicted = model.predict(x)

    max_abs_diff = np.max(np.abs(predicted - y))
    assert max_abs_diff < 2.3e-3


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/lookahead_test.py" startline="101" endline="122" pcid="250">
def test_fit_simple_linear_model():
    np.random.seed(0x2019)
    tf.random.set_seed(0x2019)

    x = np.random.standard_normal((10000, 3))
    w = np.random.standard_normal((3, 1))
    y = np.dot(x, w) + np.random.standard_normal((10000, 1)) * 1e-4

    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Dense(input_shape=(3,), units=1))
    model.compile(Lookahead("sgd"), loss="mse")

    model.fit(x, y, epochs=3)

    x = np.random.standard_normal((100, 3))
    y = np.dot(x, w)
    predicted = model.predict(x)

    max_abs_diff = np.max(np.abs(predicted - y))
    assert max_abs_diff < 1e-3


</source>
</class>

<class classid="15" nclones="2" nlines="19" similarity="75">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/cyclical_learning_rate_test.py" startline="32" endline="55" pcid="158">
def test_triangular_cyclical_learning_rate(serialize):
    initial_learning_rate = 0.1
    max_learning_rate = 1
    step_size = 40
    triangular_cyclical_lr = cyclical_learning_rate.TriangularCyclicalLearningRate(
        initial_learning_rate=initial_learning_rate,
        maximal_learning_rate=max_learning_rate,
        step_size=step_size,
    )
    triangular_cyclical_lr = _maybe_serialized(triangular_cyclical_lr, serialize)

    expected = np.concatenate(
        [
            np.linspace(initial_learning_rate, max_learning_rate, num=step_size + 1),
            np.linspace(max_learning_rate, initial_learning_rate, num=step_size + 1)[
                1:
            ],
        ]
    )

    for step, expected_value in enumerate(expected):
        np.testing.assert_allclose(triangular_cyclical_lr(step), expected_value, 1e-6)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/cyclical_learning_rate_test.py" startline="57" endline="81" pcid="159">
def test_triangular2_cyclical_learning_rate(serialize):
    initial_lr = 0.1
    maximal_lr = 1
    step_size = 30
    triangular2_lr = cyclical_learning_rate.Triangular2CyclicalLearningRate(
        initial_learning_rate=initial_lr,
        maximal_learning_rate=maximal_lr,
        step_size=step_size,
    )
    triangular2_lr = _maybe_serialized(triangular2_lr, serialize)

    middle_lr = (maximal_lr + initial_lr) / 2
    expected = np.concatenate(
        [
            np.linspace(initial_lr, maximal_lr, num=step_size + 1),
            np.linspace(maximal_lr, initial_lr, num=step_size + 1)[1:],
            np.linspace(initial_lr, middle_lr, num=step_size + 1)[1:],
            np.linspace(middle_lr, initial_lr, num=step_size + 1)[1:],
        ]
    )

    for step, expected_value in enumerate(expected):
        np.testing.assert_allclose(triangular2_lr(step).numpy(), expected_value, 1e-6)


</source>
</class>

<class classid="16" nclones="2" nlines="21" similarity="90">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/cyclical_learning_rate_test.py" startline="109" endline="135" pcid="161">
def test_custom_cyclical_learning_rate(serialize):
    initial_learning_rate = 0.1
    maximal_learning_rate = 1
    step_size = 4000

    def scale_fn(x):
        return 1 / (5 ** (x * 0.0001))

    custom_cyclical_lr = cyclical_learning_rate.CyclicalLearningRate(
        initial_learning_rate=initial_learning_rate,
        maximal_learning_rate=maximal_learning_rate,
        step_size=step_size,
        scale_fn=scale_fn,
    )
    custom_cyclical_lr = _maybe_serialized(custom_cyclical_lr, serialize)

    for step in range(1, 8001):
        cycle = np.floor(1 + step / (2 * step_size))
        non_bounded_value = np.abs(step / step_size - 2 * cycle + 1)
        expected = initial_learning_rate + (
            maximal_learning_rate - initial_learning_rate
        ) * np.maximum(0, 1 - non_bounded_value) * scale_fn(cycle)
        np.testing.assert_allclose(
            custom_cyclical_lr(step).numpy(), expected, 1e-6, 1e-6
        )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/cyclical_learning_rate_test.py" startline="137" endline="163" pcid="163">
def test_custom_cyclical_learning_rate_with_scale_mode(serialize):
    initial_learning_rate = 0.1
    maximal_learning_rate = 1
    step_size = 4000
    scale_mode = "iterations"

    def scale_fn(x):
        return 1 / (5 ** (x * 0.0001))

    custom_cyclical_lr = cyclical_learning_rate.CyclicalLearningRate(
        initial_learning_rate=initial_learning_rate,
        maximal_learning_rate=maximal_learning_rate,
        step_size=step_size,
        scale_fn=scale_fn,
        scale_mode=scale_mode,
    )
    custom_cyclical_lr = _maybe_serialized(custom_cyclical_lr, serialize)

    for step in range(1, 8001):
        cycle = np.floor(1 + step / (2 * step_size))
        non_bounded_value = np.abs(step / step_size - 2 * cycle + 1)
        expected = initial_learning_rate + (
            maximal_learning_rate - initial_learning_rate
        ) * np.maximum(0, 1 - non_bounded_value) * scale_fn(step)
        np.testing.assert_allclose(
            custom_cyclical_lr(step).numpy(), expected, 1e-6, 1e-6
        )
</source>
</class>

<class classid="17" nclones="2" nlines="12" similarity="100">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="56" endline="71" pcid="167">
def test_like_dist_belief_nuclear_cg01():
    db_grad, db_out = _db_params_nuclear_cg01()
    num_samples = len(db_grad)
    var0 = tf.Variable([0.0] * num_samples)
    grads0 = tf.constant([0.0] * num_samples)
    ord = "nuclear"
    cg_opt = cg_lib.ConditionalGradient(learning_rate=0.1, lambda_=0.1, ord=ord)

    for i in range(num_samples):
        grads0 = tf.constant(db_grad[i])
        cg_opt.apply_gradients(zip([grads0], [var0]))
        np.testing.assert_allclose(
            np.array(db_out[i]), var0.numpy(), rtol=1e-6, atol=1e-6
        )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="830" endline="845" pcid="189">
def test_like_dist_belief_frobenius_cg01():
    db_grad, db_out = _db_params_frobenius_cg01()
    num_samples = len(db_grad)
    var0 = tf.Variable([0.0] * num_samples)
    grads0 = tf.constant([0.0] * num_samples)
    ord = "fro"
    cg_opt = cg_lib.ConditionalGradient(learning_rate=0.1, lambda_=0.1, ord=ord)

    for i in range(num_samples):
        grads0 = tf.constant(db_grad[i])
        cg_opt.apply_gradients(zip([grads0], [var0]))
        np.testing.assert_allclose(
            np.array(db_out[i]), var0.numpy(), rtol=1e-06, atol=1e-06
        )


</source>
</class>

<class classid="18" nclones="2" nlines="27" similarity="71">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="74" endline="107" pcid="168">
def test_minimize_sparse_resource_variable_frobenius(dtype, device):
    if "gpu" in device and dtype == tf.float16:
        pytest.xfail("See https://github.com/tensorflow/addons/issues/347")
    var0 = tf.Variable([[1.0, 2.0]], dtype=dtype)

    def loss():
        x = tf.constant([[4.0], [5.0]], dtype=dtype)
        pred = tf.matmul(tf.nn.embedding_lookup([var0], [0]), x)
        return pred * pred

    # the gradient based on the current loss function
    grads0_0 = 32 * 1.0 + 40 * 2.0
    grads0_1 = 40 * 1.0 + 50 * 2.0
    grads0 = tf.constant([[grads0_0, grads0_1]], dtype=dtype)
    norm0 = tf.math.reduce_sum(grads0 ** 2) ** 0.5

    learning_rate = 0.1
    lambda_ = 0.1
    ord = "fro"
    opt = cg_lib.ConditionalGradient(
        learning_rate=learning_rate, lambda_=lambda_, ord=ord
    )
    _ = opt.minimize(loss, var_list=[var0])
    test_utils.assert_allclose_according_to_type(
        [
            [
                1.0 * learning_rate - (1 - learning_rate) * lambda_ * grads0_0 / norm0,
                2.0 * learning_rate - (1 - learning_rate) * lambda_ * grads0_1 / norm0,
            ]
        ],
        var0.numpy(),
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="277" endline="317" pcid="176">
def test_minimize_sparse_resource_variable_nuclear():
    # TODO:
    #       to address issue #347 and #36764.
    for dtype in _dtypes_with_checking_system(
        use_gpu=test_utils.is_gpu_available(), system=platform.system()
    ):
        var0 = tf.Variable([[1.0, 2.0]], dtype=dtype)

        def loss():
            x = tf.constant([[4.0], [5.0]], dtype=dtype)
            pred = tf.matmul(tf.nn.embedding_lookup([var0], [0]), x)
            return pred * pred

        # the gradient based on the current loss function
        grads0_0 = 32 * 1.0 + 40 * 2.0
        grads0_1 = 40 * 1.0 + 50 * 2.0
        grads0 = tf.constant([[grads0_0, grads0_1]], dtype=dtype)
        top_singular_vector0 = cg_lib.ConditionalGradient._top_singular_vector(grads0)

        learning_rate = 0.1
        lambda_ = 0.1
        ord = "nuclear"
        opt = cg_lib.ConditionalGradient(
            learning_rate=learning_rate, lambda_=lambda_, ord=ord
        )
        _ = opt.minimize(loss, var_list=[var0])

        # Validate updated params
        test_utils.assert_allclose_according_to_type(
            [
                [
                    1.0 * learning_rate
                    - (1 - learning_rate) * lambda_ * top_singular_vector0[0][0],
                    2.0 * learning_rate
                    - (1 - learning_rate) * lambda_ * top_singular_vector0[0][1],
                ]
            ],
            var0.numpy(),
        )


</source>
</class>

<class classid="19" nclones="2" nlines="57" similarity="77">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="111" endline="187" pcid="170">
def test_basic_frobenius(dtype, use_resource):
    if use_resource:
        var0 = tf.Variable([1.0, 2.0], dtype=dtype[0], name="var0_%d" % dtype[1])
        var1 = tf.Variable([3.0, 4.0], dtype=dtype[0], name="var0_%d" % dtype[1])
    else:
        var0 = tf.Variable([1.0, 2.0], dtype=dtype[0])
        var1 = tf.Variable([3.0, 4.0], dtype=dtype[0])
    grads0 = tf.constant([0.1, 0.1], dtype=dtype[0])
    grads1 = tf.constant([0.01, 0.01], dtype=dtype[0])
    norm0 = tf.math.reduce_sum(grads0 ** 2) ** 0.5
    norm1 = tf.math.reduce_sum(grads1 ** 2) ** 0.5

    def learning_rate():
        return 0.5

    def lambda_():
        return 0.01

    ord = "fro"

    cg_opt = cg_lib.ConditionalGradient(
        learning_rate=learning_rate, lambda_=lambda_, ord=ord
    )
    _ = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

    # Check we have slots
    assert ["conditional_gradient"] == cg_opt.get_slot_names()
    slot0 = cg_opt.get_slot(var0, "conditional_gradient")
    assert slot0.get_shape() == var0.get_shape()
    slot1 = cg_opt.get_slot(var1, "conditional_gradient")
    assert slot1.get_shape() == var1.get_shape()

    test_utils.assert_allclose_according_to_type(
        np.array(
            [
                1.0 * 0.5 - (1 - 0.5) * 0.01 * 0.1 / norm0,
                2.0 * 0.5 - (1 - 0.5) * 0.01 * 0.1 / norm0,
            ]
        ),
        var0.numpy(),
    )
    test_utils.assert_allclose_according_to_type(
        np.array(
            [
                3.0 * 0.5 - (1 - 0.5) * 0.01 * 0.01 / norm1,
                4.0 * 0.5 - (1 - 0.5) * 0.01 * 0.01 / norm1,
            ]
        ),
        var1.numpy(),
    )

    # Step 2: the conditional_gradient contain the previous update.
    cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))
    test_utils.assert_allclose_according_to_type(
        np.array(
            [
                (1.0 * 0.5 - (1 - 0.5) * 0.01 * 0.1 / norm0) * 0.5
                - (1 - 0.5) * 0.01 * 0.1 / norm0,
                (2.0 * 0.5 - (1 - 0.5) * 0.01 * 0.1 / norm0) * 0.5
                - (1 - 0.5) * 0.01 * 0.1 / norm0,
            ]
        ),
        var0.numpy(),
    )
    test_utils.assert_allclose_according_to_type(
        np.array(
            [
                (3.0 * 0.5 - (1 - 0.5) * 0.01 * 0.01 / norm1) * 0.5
                - (1 - 0.5) * 0.01 * 0.01 / norm1,
                (4.0 * 0.5 - (1 - 0.5) * 0.01 * 0.01 / norm1) * 0.5
                - (1 - 0.5) * 0.01 * 0.01 / norm1,
            ]
        ),
        var1.numpy(),
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="501" endline="567" pcid="187">
def test_tensor_learning_rate_and_conditional_gradient_frobenius(dtype):
    var0 = tf.Variable([1.0, 2.0], dtype=dtype)
    var1 = tf.Variable([3.0, 4.0], dtype=dtype)
    grads0 = tf.constant([0.1, 0.1], dtype=dtype)
    grads1 = tf.constant([0.01, 0.01], dtype=dtype)
    norm0 = tf.math.reduce_sum(grads0 ** 2) ** 0.5
    norm1 = tf.math.reduce_sum(grads1 ** 2) ** 0.5
    ord = "fro"
    cg_opt = cg_lib.ConditionalGradient(
        learning_rate=tf.constant(0.5), lambda_=tf.constant(0.01), ord=ord
    )
    _ = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

    # Check we have slots
    assert ["conditional_gradient"] == cg_opt.get_slot_names()
    slot0 = cg_opt.get_slot(var0, "conditional_gradient")
    assert slot0.get_shape() == var0.get_shape()
    slot1 = cg_opt.get_slot(var1, "conditional_gradient")
    assert slot1.get_shape() == var1.get_shape()

    # Check that the parameters have been updated.
    test_utils.assert_allclose_according_to_type(
        np.array(
            [
                1.0 * 0.5 - (1 - 0.5) * 0.01 * 0.1 / norm0,
                2.0 * 0.5 - (1 - 0.5) * 0.01 * 0.1 / norm0,
            ]
        ),
        var0.numpy(),
    )
    test_utils.assert_allclose_according_to_type(
        np.array(
            [
                3.0 * 0.5 - (1 - 0.5) * 0.01 * 0.01 / norm1,
                4.0 * 0.5 - (1 - 0.5) * 0.01 * 0.01 / norm1,
            ]
        ),
        var1.numpy(),
    )
    # Step 2: the conditional_gradient contain the
    # previous update.
    cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))
    # Check that the parameters have been updated.
    test_utils.assert_allclose_according_to_type(
        np.array(
            [
                (1.0 * 0.5 - (1 - 0.5) * 0.01 * 0.1 / norm0) * 0.5
                - (1 - 0.5) * 0.01 * 0.1 / norm0,
                (2.0 * 0.5 - (1 - 0.5) * 0.01 * 0.1 / norm0) * 0.5
                - (1 - 0.5) * 0.01 * 0.1 / norm0,
            ]
        ),
        var0.numpy(),
    )
    test_utils.assert_allclose_according_to_type(
        np.array(
            [
                (3.0 * 0.5 - (1 - 0.5) * 0.01 * 0.01 / norm1) * 0.5
                - (1 - 0.5) * 0.01 * 0.01 / norm1,
                (4.0 * 0.5 - (1 - 0.5) * 0.01 * 0.01 / norm1) * 0.5
                - (1 - 0.5) * 0.01 * 0.01 / norm1,
            ]
        ),
        var1.numpy(),
    )


</source>
</class>

<class classid="20" nclones="2" nlines="60" similarity="79">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="190" endline="275" pcid="173">
def test_basic_nuclear(use_resource):
    # TODO:
    #       to address issue #36764
    for i, dtype in enumerate(
        _dtypes_with_checking_system(
            use_gpu=test_utils.is_gpu_available(), system=platform.system()
        )
    ):

        if use_resource:
            var0 = tf.Variable([1.0, 2.0], dtype=dtype, name="var0_%d" % i)
            var1 = tf.Variable([3.0, 4.0], dtype=dtype, name="var1_%d" % i)
        else:
            var0 = tf.Variable([1.0, 2.0], dtype=dtype)
            var1 = tf.Variable([3.0, 4.0], dtype=dtype)

        grads0 = tf.constant([0.1, 0.1], dtype=dtype)
        grads1 = tf.constant([0.01, 0.01], dtype=dtype)
        top_singular_vector0 = cg_lib.ConditionalGradient._top_singular_vector(grads0)
        top_singular_vector1 = cg_lib.ConditionalGradient._top_singular_vector(grads1)

        def learning_rate():
            return 0.5

        def lambda_():
            return 0.01

        ord = "nuclear"

        cg_opt = cg_lib.ConditionalGradient(
            learning_rate=learning_rate, lambda_=lambda_, ord=ord
        )
        _ = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

        # Check we have slots
        assert ["conditional_gradient"] == cg_opt.get_slot_names()
        slot0 = cg_opt.get_slot(var0, "conditional_gradient")
        assert slot0.get_shape() == var0.get_shape()
        slot1 = cg_opt.get_slot(var1, "conditional_gradient")
        assert slot1.get_shape() == var1.get_shape()

        test_utils.assert_allclose_according_to_type(
            np.array(
                [
                    1.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[0],
                    2.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[1],
                ]
            ),
            var0.numpy(),
        )
        test_utils.assert_allclose_according_to_type(
            np.array(
                [
                    3.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0],
                    4.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[1],
                ]
            ),
            var1.numpy(),
        )

        # Step 2: the conditional_gradient contain the previous update.
        cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))
        test_utils.assert_allclose_according_to_type(
            np.array(
                [
                    (1.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[0]) * 0.5
                    - (1 - 0.5) * 0.01 * top_singular_vector0[0],
                    (2.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[1]) * 0.5
                    - (1 - 0.5) * 0.01 * top_singular_vector0[1],
                ]
            ),
            var0.numpy(),
        )
        test_utils.assert_allclose_according_to_type(
            np.array(
                [
                    (3.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0]) * 0.5
                    - (1 - 0.5) * 0.01 * top_singular_vector1[1],
                    (4.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0]) * 0.5
                    - (1 - 0.5) * 0.01 * top_singular_vector1[1],
                ]
            ),
            var1.numpy(),
        )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="319" endline="395" pcid="178">
def test_tensor_learning_rate_and_conditional_gradient_nuclear():
    for dtype in _dtypes_with_checking_system(
        use_gpu=test_utils.is_gpu_available(), system=platform.system()
    ):
        # TODO:
        # Based on issue #36764 in the following link,
        #        "https://github.com/tensorflow/tensorflow/issues/36764"
        # tf.half is not registered for tf.linalg.svd function on Windows
        # CPU version.
        # So we have to remove tf.half when testing with Windows CPU version.
        var0 = tf.Variable([1.0, 2.0], dtype=dtype)
        var1 = tf.Variable([3.0, 4.0], dtype=dtype)
        grads0 = tf.constant([0.1, 0.1], dtype=dtype)
        grads1 = tf.constant([0.01, 0.01], dtype=dtype)
        top_singular_vector0 = cg_lib.ConditionalGradient._top_singular_vector(grads0)
        top_singular_vector1 = cg_lib.ConditionalGradient._top_singular_vector(grads1)
        ord = "nuclear"
        cg_opt = cg_lib.ConditionalGradient(
            learning_rate=tf.constant(0.5), lambda_=tf.constant(0.01), ord=ord
        )
        _ = cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

        # Check we have slots
        assert ["conditional_gradient"] == cg_opt.get_slot_names()
        slot0 = cg_opt.get_slot(var0, "conditional_gradient")
        assert slot0.get_shape() == var0.get_shape()
        slot1 = cg_opt.get_slot(var1, "conditional_gradient")
        assert slot1.get_shape() == var1.get_shape()

        # Check that the parameters have been updated.
        test_utils.assert_allclose_according_to_type(
            np.array(
                [
                    1.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[0],
                    2.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[1],
                ]
            ),
            var0.numpy(),
        )
        test_utils.assert_allclose_according_to_type(
            np.array(
                [
                    3.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0],
                    4.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[1],
                ]
            ),
            var1.numpy(),
        )
        # Step 2: the conditional_gradient contain the
        # previous update.
        cg_opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

        # Check that the parameters have been updated.
        test_utils.assert_allclose_according_to_type(
            np.array(
                [
                    (1.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[0]) * 0.5
                    - (1 - 0.5) * 0.01 * top_singular_vector0[0],
                    (2.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector0[1]) * 0.5
                    - (1 - 0.5) * 0.01 * top_singular_vector0[1],
                ]
            ),
            var0.numpy(),
        )
        test_utils.assert_allclose_according_to_type(
            np.array(
                [
                    (3.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[0]) * 0.5
                    - (1 - 0.5) * 0.01 * top_singular_vector1[0],
                    (4.0 * 0.5 - (1 - 0.5) * 0.01 * top_singular_vector1[1]) * 0.5
                    - (1 - 0.5) * 0.01 * top_singular_vector1[1],
                ]
            ),
            var1.numpy(),
        )


</source>
</class>

<class classid="21" nclones="2" nlines="11" similarity="100">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="397" endline="413" pcid="179">
def test_variables_across_graphs_frobenius():
    optimizer = cg_lib.ConditionalGradient(0.01, 0.5, ord="fro")
    var0 = tf.Variable([1.0, 2.0], dtype=tf.float32, name="var0")
    var1 = tf.Variable([3.0, 4.0], dtype=tf.float32, name="var1")

    def loss():
        return tf.math.reduce_sum(var0 + var1)

    optimizer.minimize(loss, var_list=[var0, var1])
    optimizer_variables = optimizer.variables()
    # There should be three items. The first item is iteration,
    # and one item for each variable.
    assert optimizer_variables[1].name.startswith("ConditionalGradient/var0")
    assert optimizer_variables[2].name.startswith("ConditionalGradient/var1")
    assert 3 == len(optimizer_variables)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="415" endline="431" pcid="181">
def test_variables_across_graphs_nuclear():
    optimizer = cg_lib.ConditionalGradient(0.01, 0.5, ord="nuclear")
    var0 = tf.Variable([1.0, 2.0], dtype=tf.float32, name="var0")
    var1 = tf.Variable([3.0, 4.0], dtype=tf.float32, name="var1")

    def loss():
        return tf.math.reduce_sum(var0 + var1)

    optimizer.minimize(loss, var_list=[var0, var1])
    optimizer_variables = optimizer.variables()
    # There should be three items. The first item is iteration,
    # and one item for each variable.
    assert optimizer_variables[1].name.startswith("ConditionalGradient/var0")
    assert optimizer_variables[2].name.startswith("ConditionalGradient/var1")
    assert 3 == len(optimizer_variables)


</source>
</class>

<class classid="22" nclones="2" nlines="244" similarity="100">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="568" endline="828" pcid="188">
def _db_params_frobenius_cg01():
    """Return dist-belief conditional_gradient values.

    Return values been generated from the dist-belief
    conditional_gradient unittest, running with a learning rate of 0.1
    and a lambda_ of 0.1.

    These values record how a parameter vector of size 10, initialized
    with 0.0, gets updated with 10 consecutive conditional_gradient
    steps.
    It uses random gradients.

    Returns:
        db_grad: The gradients to apply
        db_out: The parameters after the conditional_gradient update.
    """
    db_grad = [[]] * 10
    db_out = [[]] * 10
    db_grad[0] = [
        0.00096264342,
        0.17914793,
        0.93945462,
        0.41396621,
        0.53037018,
        0.93197989,
        0.78648776,
        0.50036013,
        0.55345792,
        0.96722615,
    ]
    db_out[0] = [
        -4.1555551e-05,
        -7.7334875e-03,
        -4.0554531e-02,
        -1.7870162e-02,
        -2.2895107e-02,
        -4.0231861e-02,
        -3.3951234e-02,
        -2.1599628e-02,
        -2.3891762e-02,
        -4.1753378e-02,
    ]
    db_grad[1] = [
        0.17075552,
        0.88821375,
        0.20873757,
        0.25236958,
        0.57578111,
        0.15312378,
        0.5513742,
        0.94687688,
        0.16012503,
        0.22159521,
    ]
    db_out[1] = [
        -0.00961733,
        -0.0507779,
        -0.01580694,
        -0.01599489,
        -0.03470477,
        -0.01264373,
        -0.03443632,
        -0.05546713,
        -0.01140388,
        -0.01665068,
    ]
    db_grad[2] = [
        0.35077485,
        0.47304362,
        0.44412705,
        0.44368884,
        0.078527533,
        0.81223965,
        0.31168157,
        0.43203235,
        0.16792089,
        0.24644311,
    ]
    db_out[2] = [
        -0.02462724,
        -0.03699233,
        -0.03154434,
        -0.03153357,
        -0.00876844,
        -0.05606323,
        -0.02447166,
        -0.03469437,
        -0.0124694,
        -0.01829169,
    ]
    db_grad[3] = [
        0.9694621,
        0.75035888,
        0.28171822,
        0.83813518,
        0.53807181,
        0.3728098,
        0.81454384,
        0.03848977,
        0.89759839,
        0.93665648,
    ]
    db_out[3] = [
        -0.04124615,
        -0.03371741,
        -0.0144246,
        -0.03668303,
        -0.02240246,
        -0.02052062,
        -0.03503307,
        -0.00500922,
        -0.03715545,
        -0.0393002,
    ]
    db_grad[4] = [
        0.38578293,
        0.8536852,
        0.88722926,
        0.66276771,
        0.13678469,
        0.94036359,
        0.69107032,
        0.81897682,
        0.5433259,
        0.67860287,
    ]
    db_out[4] = [
        -0.01979208,
        -0.0380417,
        -0.03747472,
        -0.0305847,
        -0.00779536,
        -0.04024222,
        -0.03156913,
        -0.0337613,
        -0.02578116,
        -0.03148952,
    ]
    db_grad[5] = [
        0.27885768,
        0.76100707,
        0.24625534,
        0.81354135,
        0.18959245,
        0.48038563,
        0.84163809,
        0.41172323,
        0.83259648,
        0.44941229,
    ]
    db_out[5] = [
        -0.01555188,
        -0.04084422,
        -0.01573331,
        -0.04265549,
        -0.01000746,
        -0.02740575,
        -0.04412147,
        -0.02341569,
        -0.0431026,
        -0.02502293,
    ]
    db_grad[6] = [
        0.27233034,
        0.056316052,
        0.5039115,
        0.24105175,
        0.35697976,
        0.75913221,
        0.73577434,
        0.16014607,
        0.57500273,
        0.071136251,
    ]
    db_out[6] = [
        -0.01890448,
        -0.00767214,
        -0.03367592,
        -0.01962219,
        -0.02374279,
        -0.05110247,
        -0.05128598,
        -0.01254396,
        -0.04094185,
        -0.00703416,
    ]
    db_grad[7] = [
        0.58697265,
        0.2494842,
        0.08106143,
        0.39954534,
        0.15892942,
        0.12683646,
        0.74053431,
        0.16033,
        0.66625422,
        0.73515922,
    ]
    db_out[7] = [
        -0.03772914,
        -0.01599993,
        -0.00831695,
        -0.02635719,
        -0.01207801,
        -0.01285448,
        -0.05034328,
        -0.01104364,
        -0.04477356,
        -0.04558991,
    ]
    db_grad[8] = [
        0.8215279,
        0.41994119,
        0.95172721,
        0.68000203,
        0.79439718,
        0.43384039,
        0.55561525,
        0.22567581,
        0.93331909,
        0.29438227,
    ]
    db_out[8] = [
        -0.03919835,
        -0.01970845,
        -0.04187151,
        -0.03195836,
        -0.03546333,
        -0.01999326,
        -0.02899324,
        -0.01083582,
        -0.04472339,
        -0.01725317,
    ]
    db_grad[9] = [
        0.68297005,
        0.67758518,
        0.1748755,
        0.13266537,
        0.70697063,
        0.055731893,
        0.68593478,
        0.50580865,
        0.12602448,
        0.093537711,
    ]
    db_out[9] = [
        -0.04510314,
        -0.04282944,
        -0.0147322,
        -0.0111956,
        -0.04617687,
        -0.00535998,
        -0.0442614,
        -0.03158399,
        -0.01207165,
        -0.00736567,
    ]
    return db_grad, db_out


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/conditional_gradient_test.py" startline="1079" endline="1339" pcid="193">
def _db_params_nuclear_cg01():
    """Return dist-belief conditional_gradient values.

    Return values been generated from the dist-belief
    conditional_gradient unittest, running with a learning rate of 0.1
    and a lambda_ of 0.1.

    These values record how a parameter vector of size 10, initialized
    with 0.0, gets updated with 10 consecutive conditional_gradient
    steps.
    It uses random gradients.

    Returns:
        db_grad: The gradients to apply
        db_out: The parameters after the conditional_gradient update.
    """
    db_grad = [[]] * 10
    db_out = [[]] * 10
    db_grad[0] = [
        0.00096264342,
        0.17914793,
        0.93945462,
        0.41396621,
        0.53037018,
        0.93197989,
        0.78648776,
        0.50036013,
        0.55345792,
        0.96722615,
    ]
    db_out[0] = [
        -4.1552783e-05,
        -7.7334875e-03,
        -4.0554535e-02,
        -1.7870164e-02,
        -2.2895109e-02,
        -4.0231861e-02,
        -3.3951234e-02,
        -2.1599628e-02,
        -2.3891764e-02,
        -4.1753381e-02,
    ]
    db_grad[1] = [
        0.17075552,
        0.88821375,
        0.20873757,
        0.25236958,
        0.57578111,
        0.15312378,
        0.5513742,
        0.94687688,
        0.16012503,
        0.22159521,
    ]
    db_out[1] = [
        -0.00961733,
        -0.0507779,
        -0.01580694,
        -0.01599489,
        -0.03470477,
        -0.01264373,
        -0.03443632,
        -0.05546713,
        -0.01140388,
        -0.01665068,
    ]
    db_grad[2] = [
        0.35077485,
        0.47304362,
        0.44412705,
        0.44368884,
        0.078527533,
        0.81223965,
        0.31168157,
        0.43203235,
        0.16792089,
        0.24644311,
    ]
    db_out[2] = [
        -0.02462724,
        -0.03699233,
        -0.03154433,
        -0.03153357,
        -0.00876844,
        -0.05606324,
        -0.02447166,
        -0.03469437,
        -0.0124694,
        -0.01829169,
    ]
    db_grad[3] = [
        0.9694621,
        0.75035888,
        0.28171822,
        0.83813518,
        0.53807181,
        0.3728098,
        0.81454384,
        0.03848977,
        0.89759839,
        0.93665648,
    ]
    db_out[3] = [
        -0.04124615,
        -0.03371741,
        -0.0144246,
        -0.03668303,
        -0.02240246,
        -0.02052062,
        -0.03503307,
        -0.00500922,
        -0.03715545,
        -0.0393002,
    ]
    db_grad[4] = [
        0.38578293,
        0.8536852,
        0.88722926,
        0.66276771,
        0.13678469,
        0.94036359,
        0.69107032,
        0.81897682,
        0.5433259,
        0.67860287,
    ]
    db_out[4] = [
        -0.01979207,
        -0.0380417,
        -0.03747472,
        -0.0305847,
        -0.00779536,
        -0.04024221,
        -0.03156913,
        -0.0337613,
        -0.02578116,
        -0.03148951,
    ]
    db_grad[5] = [
        0.27885768,
        0.76100707,
        0.24625534,
        0.81354135,
        0.18959245,
        0.48038563,
        0.84163809,
        0.41172323,
        0.83259648,
        0.44941229,
    ]
    db_out[5] = [
        -0.01555188,
        -0.04084422,
        -0.01573331,
        -0.04265549,
        -0.01000746,
        -0.02740575,
        -0.04412147,
        -0.02341569,
        -0.0431026,
        -0.02502293,
    ]
    db_grad[6] = [
        0.27233034,
        0.056316052,
        0.5039115,
        0.24105175,
        0.35697976,
        0.75913221,
        0.73577434,
        0.16014607,
        0.57500273,
        0.071136251,
    ]
    db_out[6] = [
        -0.01890448,
        -0.00767214,
        -0.03367592,
        -0.01962219,
        -0.02374278,
        -0.05110246,
        -0.05128598,
        -0.01254396,
        -0.04094184,
        -0.00703416,
    ]
    db_grad[7] = [
        0.58697265,
        0.2494842,
        0.08106143,
        0.39954534,
        0.15892942,
        0.12683646,
        0.74053431,
        0.16033,
        0.66625422,
        0.73515922,
    ]
    db_out[7] = [
        -0.03772915,
        -0.01599993,
        -0.00831695,
        -0.0263572,
        -0.01207801,
        -0.01285448,
        -0.05034329,
        -0.01104364,
        -0.04477356,
        -0.04558992,
    ]
    db_grad[8] = [
        0.8215279,
        0.41994119,
        0.95172721,
        0.68000203,
        0.79439718,
        0.43384039,
        0.55561525,
        0.22567581,
        0.93331909,
        0.29438227,
    ]
    db_out[8] = [
        -0.03919835,
        -0.01970845,
        -0.04187151,
        -0.03195836,
        -0.03546333,
        -0.01999326,
        -0.02899324,
        -0.01083582,
        -0.04472339,
        -0.01725317,
    ]
    db_grad[9] = [
        0.68297005,
        0.67758518,
        0.1748755,
        0.13266537,
        0.70697063,
        0.055731893,
        0.68593478,
        0.50580865,
        0.12602448,
        0.093537711,
    ]
    db_out[9] = [
        -0.04510314,
        -0.04282944,
        -0.0147322,
        -0.0111956,
        -0.04617687,
        -0.00535998,
        -0.0442614,
        -0.031584,
        -0.01207165,
        -0.00736567,
    ]
    return db_grad, db_out


</source>
</class>

<class classid="23" nclones="2" nlines="32" similarity="82">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/yogi_test.py" startline="98" endline="146" pcid="199">
def do_test_sparse(beta1=0.0, l1reg=0.0, l2reg=0.0):
    for dtype in _dtypes_to_test(use_gpu=test_utils.is_gpu_available()):
        # Initialize variables for numpy implementation.
        m0, v0, m1, v1 = 0.0, 1.0, 0.0, 1.0
        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)

        var0 = tf.Variable(var0_np)
        var1 = tf.Variable(var1_np)
        grads0_np_indices = np.array([0, 1], dtype=np.int32)
        grads0 = tf.IndexedSlices(
            tf.constant(grads0_np), tf.constant(grads0_np_indices), tf.constant([2])
        )
        grads1_np_indices = np.array([0, 1], dtype=np.int32)
        grads1 = tf.IndexedSlices(
            tf.constant(grads1_np), tf.constant(grads1_np_indices), tf.constant([2])
        )
        opt = yogi.Yogi(
            beta1=beta1,
            l1_regularization_strength=l1reg,
            l2_regularization_strength=l2reg,
            initial_accumulator_value=1.0,
        )

        # Fetch params to validate initial values.
        np.testing.assert_allclose(np.asanyarray([1.0, 2.0]), var0.numpy())
        np.testing.assert_allclose(np.asanyarray([3.0, 4.0]), var1.numpy())

        # Run 3 steps of Yogi.
        for t in range(1, 4):
            beta1_power, beta2_power = get_beta_accumulators(opt, dtype)
            test_utils.assert_allclose_according_to_type(beta1 ** t, beta1_power)
            test_utils.assert_allclose_according_to_type(0.999 ** t, beta2_power)
            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

            var0_np, m0, v0 = yogi_update_numpy(
                var0_np, grads0_np, t, m0, v0, beta1=beta1, l1reg=l1reg, l2reg=l2reg
            )
            var1_np, m1, v1 = yogi_update_numpy(
                var1_np, grads1_np, t, m1, v1, beta1=beta1, l1reg=l1reg, l2reg=l2reg
            )

            # Validate updated params.
            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())
            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/yogi_test.py" startline="199" endline="243" pcid="205">
def do_test_basic(beta1=0.0, l1reg=0.0, l2reg=0.0):
    for dtype in _dtypes_to_test(use_gpu=test_utils.is_gpu_available()):
        # Initialize variables for numpy implementation.
        m0, v0, m1, v1 = 0.0, 1.0, 0.0, 1.0
        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)
        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)
        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)
        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)

        var0 = tf.Variable(var0_np)
        var1 = tf.Variable(var1_np)
        grads0 = tf.constant(grads0_np)
        grads1 = tf.constant(grads1_np)

        opt = yogi.Yogi(
            beta1=beta1,
            l1_regularization_strength=l1reg,
            l2_regularization_strength=l2reg,
            initial_accumulator_value=1.0,
        )

        # Fetch params to validate initial values.
        np.testing.assert_allclose(np.asanyarray([1.0, 2.0]), var0.numpy())
        np.testing.assert_allclose(np.asanyarray([3.0, 4.0]), var1.numpy())

        # Run 3 steps of Yogi.
        for t in range(1, 4):
            beta1_power, beta2_power = get_beta_accumulators(opt, dtype)
            test_utils.assert_allclose_according_to_type(beta1 ** t, beta1_power)
            test_utils.assert_allclose_according_to_type(0.999 ** t, beta2_power)

            opt.apply_gradients(zip([grads0, grads1], [var0, var1]))

            var0_np, m0, v0 = yogi_update_numpy(
                var0_np, grads0_np, t, m0, v0, beta1=beta1, l1reg=l1reg, l2reg=l2reg
            )
            var1_np, m1, v1 = yogi_update_numpy(
                var1_np, grads1_np, t, m1, v1, beta1=beta1, l1reg=l1reg, l2reg=l2reg
            )

            # Validate updated params.
            test_utils.assert_allclose_according_to_type(var0_np, var0.numpy())
            test_utils.assert_allclose_according_to_type(var1_np, var1.numpy())


</source>
</class>

<class classid="24" nclones="2" nlines="14" similarity="73">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/lookahead_test.py" startline="24" endline="44" pcid="246">
def run_dense_sample(iterations, optimizer, seed=0x2019):
    np.random.seed(seed)
    tf.random.set_seed(seed)

    val_0 = np.random.random((2,))
    val_1 = np.random.random((2,))

    var_0 = tf.Variable(val_0, dtype=tf.dtypes.float32)
    var_1 = tf.Variable(val_1, dtype=tf.dtypes.float32)

    grad_0 = tf.constant(np.random.standard_normal((2,)), dtype=tf.dtypes.float32)
    grad_1 = tf.constant(np.random.standard_normal((2,)), dtype=tf.dtypes.float32)

    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))

    for _ in range(iterations):
        optimizer.apply_gradients(grads_and_vars)

    return [val_0, val_1], [var_0, var_1]


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/lookahead_test.py" startline="45" endline="69" pcid="247">
def run_sparse_sample(iterations, optimizer, seed=0x2019):
    np.random.seed(seed)
    tf.random.set_seed(seed)

    val_0 = np.random.random((2,))
    val_1 = np.random.random((2,))

    var_0 = tf.Variable(val_0, dtype=tf.dtypes.float32)
    var_1 = tf.Variable(val_1, dtype=tf.dtypes.float32)

    grad_0 = tf.IndexedSlices(
        tf.constant([np.random.standard_normal()]), tf.constant([0]), tf.constant([2])
    )
    grad_1 = tf.IndexedSlices(
        tf.constant([np.random.standard_normal()]), tf.constant([1]), tf.constant([2])
    )

    grads_and_vars = list(zip([grad_0, grad_1], [var_0, var_1]))

    for _ in range(iterations):
        optimizer.apply_gradients(grads_and_vars)

    return [val_0, val_1], [var_0, var_1]


</source>
</class>

<class classid="25" nclones="2" nlines="11" similarity="100">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/lookahead_test.py" startline="71" endline="84" pcid="248">
def test_dense_exact_ratio():
    for k in [5, 10, 100]:
        for alpha in [0.3, 0.7]:
            optimizer = tf.keras.optimizers.get("adam")
            vals, quick_vars = run_dense_sample(k, optimizer)
            optimizer = Lookahead("adam", sync_period=k, slow_step_size=alpha)
            _, slow_vars = run_dense_sample(k, optimizer)
            for val, quick, slow in zip(vals, quick_vars, slow_vars):
                expected = val + (quick - val) * alpha
                np.testing.assert_allclose(
                    expected.numpy(), slow.numpy(), rtol=1e-06, atol=1e-06
                )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/tests/lookahead_test.py" startline="86" endline="99" pcid="249">
def test_sparse_exact_ratio():
    for k in [5, 10, 100]:
        for alpha in [0.3, 0.7]:
            optimizer = tf.keras.optimizers.get("adam")
            vals, quick_vars = run_sparse_sample(k, optimizer)
            optimizer = Lookahead("adam", sync_period=k, slow_step_size=alpha)
            _, slow_vars = run_sparse_sample(k, optimizer)
            for val, quick, slow in zip(vals, quick_vars, slow_vars):
                expected = val + (quick - val) * alpha
                np.testing.assert_allclose(
                    expected.numpy(), slow.numpy(), rtol=1e-06, atol=1e-06
                )


</source>
</class>

<class classid="26" nclones="3" nlines="16" similarity="83">
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/cyclical_learning_rate.py" startline="117" endline="173" pcid="279">
    def __init__(
        self,
        initial_learning_rate: Union[FloatTensorLike, Callable],
        maximal_learning_rate: Union[FloatTensorLike, Callable],
        step_size: FloatTensorLike,
        scale_mode: str = "cycle",
        name: str = "TriangularCyclicalLearningRate",
    ):
        """Applies triangular cyclical schedule to the learning rate.

        See Cyclical Learning Rates for Training Neural Networks. https://arxiv.org/abs/1506.01186


        ```python
        from tf.keras.optimizers import schedules

        lr_schedule = schedules.TriangularCyclicalLearningRate(
            initial_learning_rate=1e-4,
            maximal_learning_rate=1e-2,
            step_size=2000,
            scale_mode="cycle",
            name="MyCyclicScheduler")

        model.compile(optimizer=tf.keras.optimizers.SGD(
                                                    learning_rate=lr_schedule),
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])

        model.fit(data, labels, epochs=5)
        ```

        You can pass this schedule directly into a
        `tf.keras.optimizers.Optimizer` as the learning rate.

        Args:
            initial_learning_rate: A scalar `float32` or `float64` `Tensor` or
                a Python number.  The initial learning rate.
            maximal_learning_rate: A scalar `float32` or `float64` `Tensor` or
                a Python number.  The maximum learning rate.
            step_size: A scalar `float32` or `float64` `Tensor` or a
                Python number. Step size.
            scale_mode: ['cycle', 'iterations']. Mode to apply during cyclic
                schedule
            name: (Optional) Name for the operation.

        Returns:
            Updated learning rate value.
        """
        super().__init__(
            initial_learning_rate=initial_learning_rate,
            maximal_learning_rate=maximal_learning_rate,
            step_size=step_size,
            scale_fn=lambda x: 1.0,
            scale_mode=scale_mode,
            name=name,
        )

</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/cyclical_learning_rate.py" startline="255" endline="316" pcid="283">
    def __init__(
        self,
        initial_learning_rate: Union[FloatTensorLike, Callable],
        maximal_learning_rate: Union[FloatTensorLike, Callable],
        step_size: FloatTensorLike,
        scale_mode: str = "iterations",
        gamma: FloatTensorLike = 1.0,
        name: str = "ExponentialCyclicalLearningRate",
    ):
        """Applies exponential cyclical schedule to the learning rate.

        See Cyclical Learning Rates for Training Neural Networks. https://arxiv.org/abs/1506.01186


        ```python
        from tf.keras.optimizers import schedules

        lr_schedule = ExponentialCyclicalLearningRate(
            initial_learning_rate=1e-4,
            maximal_learning_rate=1e-2,
            step_size=2000,
            scale_mode="cycle",
            gamma=0.96,
            name="MyCyclicScheduler")

        model.compile(optimizer=tf.keras.optimizers.SGD(
                                                    learning_rate=lr_schedule),
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])

        model.fit(data, labels, epochs=5)
        ```

        You can pass this schedule directly into a
        `tf.keras.optimizers.Optimizer` as the learning rate.

        Args:
            initial_learning_rate: A scalar `float32` or `float64` `Tensor` or
                a Python number.  The initial learning rate.
            maximal_learning_rate: A scalar `float32` or `float64` `Tensor` or
                a Python number.  The maximum learning rate.
            step_size: A scalar `float32` or `float64` `Tensor` or a
                Python number. Step size.
            scale_mode: ['cycle', 'iterations']. Mode to apply during cyclic
                schedule
            gamma: A scalar `float32` or `float64` `Tensor` or a
                Python number.  Gamma value.
            name: (Optional) Name for the operation.

        Returns:
            Updated learning rate value.
        """
        self.gamma = gamma
        super().__init__(
            initial_learning_rate=initial_learning_rate,
            maximal_learning_rate=maximal_learning_rate,
            step_size=step_size,
            scale_fn=lambda x: gamma ** x,
            scale_mode=scale_mode,
            name=name,
        )

</source>
<source file="systems/addons-0.12.0/tensorflow_addons/optimizers/cyclical_learning_rate.py" startline="186" endline="242" pcid="281">
    def __init__(
        self,
        initial_learning_rate: Union[FloatTensorLike, Callable],
        maximal_learning_rate: Union[FloatTensorLike, Callable],
        step_size: FloatTensorLike,
        scale_mode: str = "cycle",
        name: str = "Triangular2CyclicalLearningRate",
    ):
        """Applies triangular2 cyclical schedule to the learning rate.

        See Cyclical Learning Rates for Training Neural Networks. https://arxiv.org/abs/1506.01186


        ```python
        from tf.keras.optimizers import schedules

        lr_schedule = schedules.Triangular2CyclicalLearningRate(
            initial_learning_rate=1e-4,
            maximal_learning_rate=1e-2,
            step_size=2000,
            scale_mode="cycle",
            name="MyCyclicScheduler")

        model.compile(optimizer=tf.keras.optimizers.SGD(
                                                    learning_rate=lr_schedule),
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])

        model.fit(data, labels, epochs=5)
        ```

        You can pass this schedule directly into a
        `tf.keras.optimizers.Optimizer` as the learning rate.

        Args:
            initial_learning_rate: A scalar `float32` or `float64` `Tensor` or
                a Python number.  The initial learning rate.
            maximal_learning_rate: A scalar `float32` or `float64` `Tensor` or
                a Python number.  The maximum learning rate.
            step_size: A scalar `float32` or `float64` `Tensor` or a
                Python number. Step size.
            scale_mode: ['cycle', 'iterations']. Mode to apply during cyclic
                schedule
            name: (Optional) Name for the operation.

        Returns:
            Updated learning rate value.
        """
        super().__init__(
            initial_learning_rate=initial_learning_rate,
            maximal_learning_rate=maximal_learning_rate,
            step_size=step_size,
            scale_fn=lambda x: 1 / (2.0 ** (x - 1)),
            scale_mode=scale_mode,
            name=name,
        )

</source>
</class>

<class classid="27" nclones="4" nlines="21" similarity="81">
<source file="systems/addons-0.12.0/tensorflow_addons/callbacks/tests/avg_model_checkpoint_test.py" startline="58" endline="81" pcid="288">
def test_mode_auto(tmp_path):
    test_model_filepath = str(tmp_path / "test_model.h5")
    x, y, model = get_data_and_model()
    monitor = "val_loss"
    save_best_only = False
    mode = "auto"
    avg_model_ckpt = AverageModelCheckpoint(
        update_weights=True,
        filepath=test_model_filepath,
        monitor=monitor,
        save_best_only=save_best_only,
        mode=mode,
    )
    model.fit(
        x,
        y,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(x, y),
        callbacks=[avg_model_ckpt],
    )
    assert os.path.exists(test_model_filepath)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/callbacks/tests/avg_model_checkpoint_test.py" startline="82" endline="105" pcid="289">
def test_mode_min(tmp_path):
    test_model_filepath = str(tmp_path / "test_model.h5")
    x, y, model = get_data_and_model()
    monitor = "val_loss"
    save_best_only = False
    mode = "min"
    avg_model_ckpt = AverageModelCheckpoint(
        update_weights=True,
        filepath=test_model_filepath,
        monitor=monitor,
        save_best_only=save_best_only,
        mode=mode,
    )
    model.fit(
        x,
        y,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(x, y),
        callbacks=[avg_model_ckpt],
    )
    assert os.path.exists(test_model_filepath)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/callbacks/tests/avg_model_checkpoint_test.py" startline="106" endline="129" pcid="290">
def test_mode_max(tmp_path):
    test_model_filepath = str(tmp_path / "test_model.h5")
    x, y, model = get_data_and_model()
    mode = "max"
    monitor = "val_acc"
    save_best_only = False
    avg_model_ckpt = AverageModelCheckpoint(
        update_weights=True,
        filepath=test_model_filepath,
        monitor=monitor,
        save_best_only=save_best_only,
        mode=mode,
    )
    model.fit(
        x,
        y,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(x, y),
        callbacks=[avg_model_ckpt],
    )
    assert os.path.exists(test_model_filepath)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/callbacks/tests/avg_model_checkpoint_test.py" startline="148" endline="168" pcid="292">
def test_metric_unavailable(tmp_path):
    test_model_filepath = str(tmp_path / "test_model.h5")
    x, y, model = get_data_and_model()
    monitor = "unknown"
    avg_model_ckpt = AverageModelCheckpoint(
        update_weights=False,
        filepath=test_model_filepath,
        monitor=monitor,
        save_best_only=True,
    )
    model.fit(
        x,
        y,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(x, y),
        callbacks=[avg_model_ckpt],
    )
    assert not os.path.exists(test_model_filepath)


</source>
</class>

<class classid="28" nclones="2" nlines="83" similarity="90">
<source file="systems/addons-0.12.0/tensorflow_addons/rnn/tests/nas_cell_test.py" startline="24" endline="116" pcid="330">
def test_base():
    units = 6
    batch_size = 3
    expected_output = np.array(
        [
            [0.576751, 0.576751, 0.576751, 0.576751, 0.576751, 0.576751],
            [0.618936, 0.618936, 0.618936, 0.618936, 0.618936, 0.618936],
            [0.627393, 0.627393, 0.627393, 0.627393, 0.627393, 0.627393],
        ]
    )
    expected_state = np.array(
        [
            [
                0.7157977,
                0.7157977,
                0.7157977,
                0.7157977,
                0.7157977,
                0.7157977,
                0.5767508,
                0.5767508,
                0.5767508,
                0.5767508,
                0.5767508,
                0.5767508,
            ],
            [
                0.7804162,
                0.7804162,
                0.7804162,
                0.7804162,
                0.7804162,
                0.7804162,
                0.6189357,
                0.6189357,
                0.6189357,
                0.6189357,
                0.6189357,
                0.6189357,
            ],
            [
                0.7945764,
                0.7945764,
                0.7945764,
                0.7945764,
                0.7945765,
                0.7945765,
                0.6273934,
                0.6273934,
                0.6273934,
                0.6273934,
                0.6273934,
                0.6273934,
            ],
        ]
    )
    const_initializer = tf.constant_initializer(0.5)
    cell = NASCell(
        units=units,
        kernel_initializer=const_initializer,
        recurrent_initializer=const_initializer,
    )

    inputs = tf.constant(
        np.array(
            [[1.0, 1.0, 1.0, 1.0], [2.0, 2.0, 2.0, 2.0], [3.0, 3.0, 3.0, 3.0]],
            dtype=np.float32,
        ),
        dtype=tf.float32,
    )
    state_value = tf.constant(
        0.1 * np.ones((batch_size, units), dtype=np.float32), dtype=tf.float32
    )
    init_state = [state_value, state_value]
    output, state = cell(inputs, init_state)
    res = [output, state]

    # This is a smoke test: Only making sure expected values not change.
    assert len(res) == 2
    np.testing.assert_allclose(res[0], expected_output, rtol=1e-6, atol=1e-6)
    # There should be 2 states in the list.
    assert len(res[1]) == 2
    # Checking the shape of each state to be batch_size * num_units
    new_c, new_h = res[1]
    assert new_c.shape[0] == batch_size
    assert new_c.shape[1] == units
    assert new_h.shape[0] == batch_size
    assert new_h.shape[1] == units
    np.testing.assert_allclose(
        np.concatenate(res[1], axis=1), expected_state, rtol=1e-6, atol=1e-6
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/rnn/tests/nas_cell_test.py" startline="117" endline="212" pcid="331">
def test_projection():
    units = 6
    batch_size = 3
    projection = 5
    expected_output = np.array(
        [
            [1.697418, 1.697418, 1.697418, 1.697418, 1.697418],
            [1.840037, 1.840037, 1.840037, 1.840037, 1.840037],
            [1.873985, 1.873985, 1.873985, 1.873985, 1.873985],
        ]
    )

    expected_state = np.array(
        [
            [
                0.69855207,
                0.69855207,
                0.69855207,
                0.69855207,
                0.69855207,
                0.69855207,
                1.69741797,
                1.69741797,
                1.69741797,
                1.69741797,
                1.69741797,
            ],
            [
                0.77073824,
                0.77073824,
                0.77073824,
                0.77073824,
                0.77073824,
                0.77073824,
                1.84003687,
                1.84003687,
                1.84003687,
                1.84003687,
                1.84003687,
            ],
            [
                0.78973997,
                0.78973997,
                0.78973997,
                0.78973997,
                0.78973997,
                0.78973997,
                1.87398517,
                1.87398517,
                1.87398517,
                1.87398517,
                1.87398517,
            ],
        ]
    )
    const_initializer = tf.constant_initializer(0.5)
    cell = NASCell(
        units=units,
        projection=projection,
        kernel_initializer=const_initializer,
        recurrent_initializer=const_initializer,
        projection_initializer=const_initializer,
    )
    inputs = tf.constant(
        np.array(
            [[1.0, 1.0, 1.0, 1.0], [2.0, 2.0, 2.0, 2.0], [3.0, 3.0, 3.0, 3.0]],
            dtype=np.float32,
        ),
        dtype=tf.float32,
    )
    state_value_c = tf.constant(
        0.1 * np.ones((batch_size, units), dtype=np.float32), dtype=tf.float32
    )
    state_value_h = tf.constant(
        0.1 * np.ones((batch_size, projection), dtype=np.float32), dtype=tf.float32
    )
    init_state = [state_value_c, state_value_h]
    output, state = cell(inputs, init_state)
    res = [output, state]

    # This is a smoke test: Only making sure expected values not change.
    assert len(res) == 2
    np.testing.assert_allclose(res[0], expected_output, rtol=1e-6, atol=1e-6)
    # There should be 2 states in the tuple.
    assert len(res[1]) == 2
    # Checking the shape of each state to be batch_size * num_units
    new_c, new_h = res[1]
    assert new_c.shape[0] == batch_size
    assert new_c.shape[1] == units
    assert new_h.shape[0] == batch_size
    assert new_h.shape[1] == projection
    np.testing.assert_allclose(
        np.concatenate(res[1], axis=1), expected_state, rtol=1e-6, atol=1e-6
    )


</source>
</class>

<class classid="29" nclones="2" nlines="14" similarity="100">
<source file="systems/addons-0.12.0/tensorflow_addons/losses/tests/npairs_test.py" startline="61" endline="95" pcid="363">
def test_single_label():
    """Test single label, which is the same with `NpairsLoss`."""
    nml_obj = npairs.NpairsMultilabelLoss()
    # batch size = 4, hidden size = 2
    y_true = tf.constant(
        [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]], dtype=tf.int64
    )
    # features of anchors
    f = tf.constant(
        [[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]], dtype=tf.float32
    )
    # features of positive samples
    fp = tf.constant(
        [[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]], dtype=tf.float32
    )
    # similarity matrix
    y_pred = tf.matmul(f, fp, transpose_a=False, transpose_b=True)
    loss = nml_obj(y_true, y_pred)

    # Loss = 1/4 * \sum_i log(1 + \sum_{j != i} exp(f_i*fp_j^T-f_i*f_i^T))
    # Compute loss for i = 0, 1, 2, 3 without multiplier 1/4
    # i = 0 => log(1 + sum([exp(-2), exp(-2), exp(-4)])) = 0.253846
    # i = 1 => log(1 + sum([exp(-2), exp(-4), exp(-2)])) = 0.253846
    # i = 2 => log(1 + sum([exp(-2), exp(-4), exp(-2)])) = 0.253846
    # i = 3 => log(1 + sum([exp(-4), exp(-2), exp(-2)])) = 0.253846
    # Loss = (0.253856 + 0.253856 + 0.253856 + 0.253856) / 4 = 0.253856

    np.testing.assert_allclose(loss, 0.253856, rtol=1e-06, atol=1e-06)

    # Test sparse tensor
    y_true = tf.sparse.from_dense(y_true)
    loss = nml_obj(y_true, y_pred)
    np.testing.assert_allclose(loss, 0.253856, rtol=1e-06, atol=1e-06)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/losses/tests/npairs_test.py" startline="96" endline="135" pcid="364">
def test_multilabel():
    nml_obj = npairs.NpairsMultilabelLoss()
    # batch size = 4, hidden size = 2
    y_true = tf.constant(
        [[1, 1, 0, 0], [0, 1, 1, 0], [0, 0, 1, 1], [0, 0, 0, 1]], dtype=tf.int64
    )
    # features of anchors
    f = tf.constant(
        [[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]], dtype=tf.float32
    )
    # features of positive samples
    fp = tf.constant(
        [[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]], dtype=tf.float32
    )
    # similarity matrix
    y_pred = tf.matmul(f, fp, transpose_a=False, transpose_b=True)
    loss = nml_obj(y_true, y_pred)

    # Loss = \sum_i log(1 + \sum_{j != i} exp(f_i*fp_j^T-f_i*f_i^T))
    # Because of multilabel, the label matrix is normalized so that each
    # row sums to one. That's why the multiplier before log exists.
    # Compute loss for i = 0, 1, 2, 3 without multiplier 1/4
    # i = 0 => 2/3 * log(1 + sum([exp(-2), exp(-2), exp(-4)])) +
    #          1/3 * log(1 + sum([exp(2) , exp(0) , exp(-2)])) = 0.920522
    # i = 1 => 1/4 * log(1 + sum([exp(2) , exp(-2), exp(0) ])) +
    #          1/2 * log(1 + sum([exp(-2), exp(-4), exp(-2)])) +
    #          1/4 * log(1 + sum([exp(2) , exp(4) , exp(2) ])) = 1.753856
    # i = 2 => 1/4 * log(1 + sum([exp(2) , exp(4) , exp(2) ])) +
    #          1/2 * log(1 + sum([exp(-2), exp(-4), exp(-2)])) +
    #          1/4 * log(1 + sum([exp(0) , exp(-2), exp(2) ])) = 1.753856
    # i = 4 => 1/2 * log(1 + sum([exp(-2), exp(0) , exp(2) ])) +
    #          1/2 * log(1 + sum([exp(-4), exp(-2), exp(-2)])) = 1.253856
    # Loss = (0.920522 + 1.753856 + 1.753856 + 1.253856) / 4 = 1.420522

    np.testing.assert_allclose(loss, 1.420522, rtol=1e-06, atol=1e-06)

    # Test sparse tensor
    y_true = tf.sparse.from_dense(y_true)
    loss = nml_obj(y_true, y_pred)
    np.testing.assert_allclose(loss, 1.420522, rtol=1e-06, atol=1e-06)
</source>
</class>

<class classid="30" nclones="3" nlines="10" similarity="100">
<source file="systems/addons-0.12.0/tensorflow_addons/losses/tests/sparsemax_loss_test.py" startline="28" endline="51" pcid="398">
def _np_sparsemax(z):
    z = z - np.mean(z, axis=1)[:, np.newaxis]

    # sort z
    z_sorted = np.sort(z, axis=1)[:, ::-1]

    # calculate k(z)
    z_cumsum = np.cumsum(z_sorted, axis=1)
    k = np.arange(1, z.shape[1] + 1)
    z_check = 1 + k * z_sorted > z_cumsum
    # use argmax to get the index by row as .nonzero() doesn't
    # take an axis argument. np.argmax return the first index, but the last
    # index is required here, use np.flip to get the last index and
    # `z.shape[axis]` to compensate for np.flip afterwards.
    k_z = z.shape[1] - np.argmax(z_check[:, ::-1], axis=1)

    # calculate tau(z)
    tau_sum = z_cumsum[np.arange(0, z.shape[0]), k_z - 1]
    tau_z = ((tau_sum - 1) / k_z).reshape(-1, 1)

    # calculate p
    return np.maximum(0, z - tau_z)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/activations/tests/sparsemax_test.py" startline="26" endline="49" pcid="464">
def _np_sparsemax(z):
    z = z - np.mean(z, axis=1)[:, np.newaxis]

    # sort z
    z_sorted = np.sort(z, axis=1)[:, ::-1]

    # calculate k(z)
    z_cumsum = np.cumsum(z_sorted, axis=1)
    k = np.arange(1, z.shape[1] + 1)
    z_check = 1 + k * z_sorted > z_cumsum
    # use argmax to get the index by row as .nonzero() doesn't
    # take an axis argument. np.argmax return the first index, but the last
    # index is required here, use np.flip to get the last index and
    # `z.shape[axis]` to compensate for np.flip afterwards.
    k_z = z.shape[1] - np.argmax(z_check[:, ::-1], axis=1)

    # calculate tau(z)
    tau_sum = z_cumsum[np.arange(0, z.shape[0]), k_z - 1]
    tau_z = ((tau_sum - 1) / k_z).reshape(-1, 1)

    # calculate p
    return np.maximum(0, z - tau_z)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/sparsemax_test.py" startline="26" endline="49" pcid="944">
def _np_sparsemax(z):
    z = z - np.mean(z, axis=1)[:, np.newaxis]

    # sort z
    z_sorted = np.sort(z, axis=1)[:, ::-1]

    # calculate k(z)
    z_cumsum = np.cumsum(z_sorted, axis=1)
    k = np.arange(1, z.shape[1] + 1)
    z_check = 1 + k * z_sorted > z_cumsum
    # use argmax to get the index by row as .nonzero() doesn't
    # take an axis argument. np.argmax return the first index, but the last
    # index is required here, use np.flip to get the last index and
    # `z.shape[axis]` to compensate for np.flip afterwards.
    k_z = z.shape[1] - np.argmax(z_check[:, ::-1], axis=1)

    # calculate tau(z)
    tau_sum = z_cumsum[np.arange(0, z.shape[0]), k_z - 1]
    tau_z = ((tau_sum - 1) / k_z).reshape(-1, 1)

    # calculate p
    return np.maximum(0, z - tau_z)


</source>
</class>

<class classid="31" nclones="2" nlines="10" similarity="90">
<source file="systems/addons-0.12.0/tensorflow_addons/image/tests/dense_image_warp_test.py" startline="27" endline="41" pcid="510">
def test_interpolate_small_grid_ij():
    grid = tf.constant(
        [[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0], [9.0, 10.0, 11.0]],
        shape=[1, 4, 3, 1],
    )
    query_points = tf.constant(
        [[0.0, 0.0], [1.0, 0.0], [2.0, 0.5], [1.5, 1.5], [3.0, 2.0]], shape=[1, 5, 2]
    )
    expected_results = np.reshape(np.array([0.0, 3.0, 6.5, 6.0, 11.0]), [1, 5, 1])

    interp = interpolate_bilinear(grid, query_points)

    np.testing.assert_allclose(expected_results, interp)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/image/tests/dense_image_warp_test.py" startline="42" endline="56" pcid="511">
def test_interpolate_small_grid_xy():
    grid = tf.constant(
        [[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0], [9.0, 10.0, 11.0]],
        shape=[1, 4, 3, 1],
    )
    query_points = tf.constant(
        [[0.0, 0.0], [0.0, 1.0], [0.5, 2.0], [1.5, 1.5], [2.0, 3.0]], shape=[1, 5, 2]
    )
    expected_results = np.reshape(np.array([0.0, 3.0, 6.5, 6.0, 11.0]), [1, 5, 1])

    interp = interpolate_bilinear(grid, query_points, indexing="xy")

    np.testing.assert_allclose(expected_results, interp)


</source>
</class>

<class classid="32" nclones="2" nlines="10" similarity="100">
<source file="systems/addons-0.12.0/tensorflow_addons/image/tests/interpolate_spline_test.py" startline="158" endline="175" pcid="540">
def test_1d_interpolation():
    """Regression test for interpolation with 1-D points."""

    tp = _QuadraticPlusSinProblem1D()
    (query_points, _, train_points, train_values) = tp.get_problem(dtype="float64")

    for order in (1, 2, 3):
        for reg_weight in (0, 0.01):
            interp = interpolate_spline(
                train_points, train_values, query_points, order, reg_weight
            )

            target_interpolation = tp.HARDCODED_QUERY_VALUES[(order, reg_weight)]
            target_interpolation = np.array(target_interpolation)

            np.testing.assert_allclose(interp[0, :, 0], target_interpolation)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/image/tests/interpolate_spline_test.py" startline="176" endline="193" pcid="541">
def test_nd_linear_interpolation():
    """Regression test for interpolation with N-D points."""

    tp = _QuadraticPlusSinProblemND()
    (query_points, _, train_points, train_values) = tp.get_problem(dtype="float64")

    for order in (1, 2, 3):
        for reg_weight in (0, 0.01):
            interp = interpolate_spline(
                train_points, train_values, query_points, order, reg_weight
            )

            target_interpolation = tp.HARDCODED_QUERY_VALUES[(order, reg_weight)]
            target_interpolation = np.array(target_interpolation)

            np.testing.assert_allclose(interp[0, :, 0], target_interpolation)


</source>
</class>

<class classid="33" nclones="3" nlines="35" similarity="97">
<source file="systems/addons-0.12.0/tensorflow_addons/image/tests/distort_image_ops_test.py" startline="62" endline="98" pcid="567">
def test_adjust_random_hue_in_yiq():
    x_shapes = [
        [2, 2, 3],
        [4, 2, 3],
        [2, 4, 3],
        [2, 5, 3],
        [1000, 1, 3],
    ]
    test_styles = [
        "all_random",
        "rg_same",
        "rb_same",
        "gb_same",
        "rgb_same",
    ]
    for x_shape in x_shapes:
        for test_style in test_styles:
            x_np = np.random.rand(*x_shape) * 255.0
            delta_h = (np.random.rand() * 2.0 - 1.0) * np.pi
            if test_style == "all_random":
                pass
            elif test_style == "rg_same":
                x_np[..., 1] = x_np[..., 0]
            elif test_style == "rb_same":
                x_np[..., 2] = x_np[..., 0]
            elif test_style == "gb_same":
                x_np[..., 2] = x_np[..., 1]
            elif test_style == "rgb_same":
                x_np[..., 1] = x_np[..., 0]
                x_np[..., 2] = x_np[..., 0]
            else:
                raise AssertionError("Invalid test style: %s" % (test_style))
            y_np = _adjust_hue_in_yiq_np(x_np, delta_h)
            y_tf = _adjust_hue_in_yiq_tf(x_np, delta_h)
            np.testing.assert_allclose(y_tf, y_np, rtol=2e-4, atol=1e-4)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/image/tests/distort_image_ops_test.py" startline="231" endline="267" pcid="579">
def test_adjust_random_saturation_in_yiq():
    x_shapes = [
        [2, 2, 3],
        [4, 2, 3],
        [2, 4, 3],
        [2, 5, 3],
        [1000, 1, 3],
    ]
    test_styles = [
        "all_random",
        "rg_same",
        "rb_same",
        "gb_same",
        "rgb_same",
    ]
    for x_shape in x_shapes:
        for test_style in test_styles:
            x_np = np.random.rand(*x_shape) * 255.0
            scale = np.random.rand() * 2.0 - 1.0
            if test_style == "all_random":
                pass
            elif test_style == "rg_same":
                x_np[..., 1] = x_np[..., 0]
            elif test_style == "rb_same":
                x_np[..., 2] = x_np[..., 0]
            elif test_style == "gb_same":
                x_np[..., 2] = x_np[..., 1]
            elif test_style == "rgb_same":
                x_np[..., 1] = x_np[..., 0]
                x_np[..., 2] = x_np[..., 0]
            else:
                raise AssertionError("Invalid test style: %s" % (test_style))
            y_baseline = _adjust_saturation_in_yiq_np(x_np, scale)
            y_tf = _adjust_saturation_in_yiq_tf(x_np, scale)
            np.testing.assert_allclose(y_tf, y_baseline, rtol=2e-4, atol=1e-4)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/image/tests/distort_image_ops_test.py" startline="151" endline="187" pcid="574">
def test_adjust_random_value_in_yiq():
    x_shapes = [
        [2, 2, 3],
        [4, 2, 3],
        [2, 4, 3],
        [2, 5, 3],
        [1000, 1, 3],
    ]
    test_styles = [
        "all_random",
        "rg_same",
        "rb_same",
        "gb_same",
        "rgb_same",
    ]
    for x_shape in x_shapes:
        for test_style in test_styles:
            x_np = np.random.rand(*x_shape) * 255.0
            scale = np.random.rand() * 2.0 - 1.0
            if test_style == "all_random":
                pass
            elif test_style == "rg_same":
                x_np[..., 1] = x_np[..., 0]
            elif test_style == "rb_same":
                x_np[..., 2] = x_np[..., 0]
            elif test_style == "gb_same":
                x_np[..., 2] = x_np[..., 1]
            elif test_style == "rgb_same":
                x_np[..., 1] = x_np[..., 0]
                x_np[..., 2] = x_np[..., 0]
            else:
                raise AssertionError("Invalid test style: %s" % (test_style))
            y_np = _adjust_value_in_yiq_np(x_np, scale)
            y_tf = _adjust_value_in_yiq_tf(x_np, scale)
            np.testing.assert_allclose(y_tf, y_np, rtol=2e-4, atol=1e-4)


</source>
</class>

<class classid="34" nclones="3" nlines="11" similarity="75">
<source file="systems/addons-0.12.0/tensorflow_addons/metrics/tests/f_scores_test.py" startline="113" endline="125" pcid="627">
def test_fbeta_random_score_none(avg_val, beta, result):
    preds = [
        [0.9, 0.1, 0],
        [0.2, 0.6, 0.2],
        [0, 0, 1],
        [0.4, 0.3, 0.3],
        [0, 0.9, 0.1],
        [0, 0, 1],
    ]
    actuals = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1]]
    _test_fbeta_score(actuals, preds, None, avg_val, beta, result, None)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/metrics/tests/f_scores_test.py" startline="167" endline="179" pcid="628">
def test_fbeta_weighted_random_score_none(avg_val, beta, sample_weights, result):
    preds = [
        [0.9, 0.1, 0],
        [0.2, 0.6, 0.2],
        [0, 0, 1],
        [0.4, 0.3, 0.3],
        [0, 0.9, 0.1],
        [0, 0, 1],
    ]
    actuals = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1]]
    _test_fbeta_score(actuals, preds, sample_weights, avg_val, beta, result, None)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/metrics/tests/f_scores_test.py" startline="237" endline="249" pcid="634">
def test_serialization_f1_score(average, threshold):
    f1 = F1Score(3, average, threshold)
    preds = [
        [0.9, 0.1, 0],
        [0.2, 0.6, 0.2],
        [0, 0, 1],
        [0.4, 0.3, 0.3],
        [0, 0.9, 0.1],
        [0, 0, 1],
    ]
    actuals = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1]]

    check_metric_serialization(f1, np.array(actuals), np.array(preds))
</source>
</class>

<class classid="35" nclones="2" nlines="16" similarity="81">
<source file="systems/addons-0.12.0/tensorflow_addons/metrics/tests/f_scores_test.py" startline="185" endline="203" pcid="630">
def test_eq():
    f1 = F1Score(3)
    fbeta = FBetaScore(3, beta=1.0)

    preds = [
        [0.9, 0.1, 0],
        [0.2, 0.6, 0.2],
        [0, 0, 1],
        [0.4, 0.3, 0.3],
        [0, 0.9, 0.1],
        [0, 0, 1],
    ]
    actuals = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1]]

    fbeta.update_state(actuals, preds)
    f1.update_state(actuals, preds)
    np.testing.assert_allclose(fbeta.result().numpy(), f1.result().numpy())


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/metrics/tests/f_scores_test.py" startline="204" endline="223" pcid="631">
def test_sample_eq():
    f1 = F1Score(3)
    f1_weighted = F1Score(3)

    preds = [
        [0.9, 0.1, 0],
        [0.2, 0.6, 0.2],
        [0, 0, 1],
        [0.4, 0.3, 0.3],
        [0, 0.9, 0.1],
        [0, 0, 1],
    ]
    actuals = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 0, 0], [0, 0, 1]]
    sample_weights = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

    f1.update_state(actuals, preds)
    f1_weighted(actuals, preds, sample_weights)
    np.testing.assert_allclose(f1.result().numpy(), f1_weighted.result().numpy())


</source>
</class>

<class classid="36" nclones="2" nlines="30" similarity="80">
<source file="systems/addons-0.12.0/tensorflow_addons/metrics/tests/hamming_test.py" startline="66" endline="100" pcid="678">
def test_mc_5_classes():
    actuals = tf.constant(
        [
            [1, 0, 0, 0, 0],
            [0, 0, 0, 1, 0],
            [0, 0, 0, 0, 1],
            [0, 1, 0, 0, 0],
            [0, 0, 1, 0, 0],
            [0, 0, 1, 0, 0],
            [1, 0, 0, 0, 0],
            [0, 1, 0, 0, 0],
        ],
        dtype=tf.float32,
    )

    predictions = tf.constant(
        [
            [0.85, 0, 0.15, 0, 0],
            [0, 0, 0, 1, 0],
            [0, 1, 0, 0, 0],
            [0.05, 0.90, 0.04, 0, 0.01],
            [0.10, 0, 0.81, 0.09, 0],
            [0.10, 0.045, 0, 0.81, 0.045],
            [1, 0, 0, 0, 0],
            [0, 0.85, 0, 0, 0.15],
        ],
        dtype=tf.float32,
    )
    # Initialize
    hl_obj = HammingLoss("multiclass", threshold=0.8)
    hl_obj.update_state(actuals, predictions)
    # Check results
    check_results(hl_obj, 0.25)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/metrics/tests/hamming_test.py" startline="114" endline="147" pcid="680">
def test_ml_5_classes():
    actuals = tf.constant(
        [
            [1, 0, 0, 0, 0],
            [0, 0, 1, 1, 0],
            [0, 1, 0, 1, 0],
            [0, 1, 1, 0, 0],
            [0, 0, 1, 1, 0],
            [0, 0, 1, 1, 0],
            [1, 0, 0, 0, 1],
            [0, 1, 1, 0, 0],
        ],
        dtype=tf.float32,
    )
    predictions = tf.constant(
        [
            [1, 0.75, 0.2, 0.55, 0],
            [0.65, 0.22, 0.97, 0.88, 0],
            [0, 1, 0, 1, 0],
            [0, 0.85, 0.9, 0.34, 0.5],
            [0.4, 0.65, 0.87, 0, 0.12],
            [0.66, 0.55, 1, 0.98, 0],
            [0.95, 0.34, 0.67, 0.65, 0.10],
            [0.45, 0.97, 0.89, 0.67, 0.46],
        ],
        dtype=tf.float32,
    )
    # Initialize
    hl_obj = HammingLoss("multilabel", threshold=0.7)
    hl_obj.update_state(actuals, predictions)
    # Check results
    check_results(hl_obj, 0.075)


</source>
</class>

<class classid="37" nclones="2" nlines="37" similarity="100">
<source file="systems/addons-0.12.0/tensorflow_addons/metrics/tests/multilabel_confusion_matrix_test.py" startline="49" endline="90" pcid="695">
def test_mcm_4_classes(dtype):
    actuals = tf.constant(
        [
            [1, 0, 0, 1],
            [0, 0, 1, 1],
            [1, 0, 0, 1],
            [1, 1, 0, 0],
            [0, 1, 0, 1],
            [1, 0, 0, 1],
            [0, 0, 1, 1],
            [1, 0, 0, 1],
            [0, 1, 1, 0],
            [0, 1, 0, 1],
        ],
        dtype=dtype,
    )
    preds = tf.constant(
        [
            [1, 0, 1, 0],
            [0, 0, 1, 1],
            [0, 0, 0, 1],
            [1, 1, 0, 0],
            [1, 0, 0, 0],
            [1, 0, 0, 1],
            [0, 0, 1, 1],
            [1, 0, 0, 1],
            [0, 1, 0, 0],
            [0, 0, 0, 1],
        ],
        dtype=dtype,
    )

    # Initialize
    mcm_obj = MultiLabelConfusionMatrix(num_classes=4, dtype=dtype)
    mcm_obj.update_state(actuals, preds)
    # Check results
    check_results(
        mcm_obj,
        [[[4, 1], [1, 4]], [[6, 0], [2, 2]], [[6, 1], [1, 2]], [[2, 0], [2, 6]]],
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/metrics/tests/multilabel_confusion_matrix_test.py" startline="92" endline="131" pcid="696">
def test_multiclass(dtype):
    actuals = tf.constant(
        [
            [1, 0, 0, 0],
            [0, 0, 1, 0],
            [0, 0, 0, 1],
            [0, 1, 0, 0],
            [0, 1, 0, 0],
            [1, 0, 0, 0],
            [0, 0, 1, 0],
            [1, 0, 0, 0],
            [0, 0, 1, 0],
            [0, 0, 0, 1],
        ],
        dtype=dtype,
    )
    preds = tf.constant(
        [
            [1, 0, 0, 0],
            [0, 0, 1, 0],
            [0, 0, 0, 1],
            [1, 0, 0, 0],
            [1, 0, 0, 0],
            [1, 0, 0, 0],
            [0, 0, 1, 0],
            [1, 0, 0, 0],
            [0, 1, 0, 0],
            [0, 0, 0, 1],
        ],
        dtype=dtype,
    )

    # Initialize
    mcm_obj = MultiLabelConfusionMatrix(num_classes=4, dtype=dtype)
    mcm_obj.update_state(actuals, preds)
    # Check results
    check_results(
        mcm_obj,
        [[[5, 2], [0, 3]], [[7, 1], [2, 0]], [[7, 0], [1, 2]], [[8, 0], [0, 2]]],
    )
</source>
</class>

<class classid="38" nclones="2" nlines="66" similarity="93">
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/beam_search_decoder_test.py" startline="299" endline="376" pcid="715">
def test_beam_step():
    batch_size = 2
    beam_width = 3
    vocab_size = 5
    end_token = 0
    length_penalty_weight = 0.6
    coverage_penalty_weight = 0.0
    output_all_scores = False

    dummy_cell_state = tf.zeros([batch_size, beam_width])
    beam_state = beam_search_decoder.BeamSearchDecoderState(
        cell_state=dummy_cell_state,
        log_probs=tf.nn.log_softmax(tf.ones([batch_size, beam_width])),
        lengths=tf.constant(2, shape=[batch_size, beam_width], dtype=tf.int64),
        finished=tf.zeros([batch_size, beam_width], dtype=tf.bool),
        accumulated_attention_probs=(),
    )

    logits_ = np.full([batch_size, beam_width, vocab_size], 0.0001)
    logits_[0, 0, 2] = 1.9
    logits_[0, 0, 3] = 2.1
    logits_[0, 1, 3] = 3.1
    logits_[0, 1, 4] = 0.9
    logits_[1, 0, 1] = 0.5
    logits_[1, 1, 2] = 2.7
    logits_[1, 2, 2] = 10.0
    logits_[1, 2, 3] = 0.2
    logits = tf.convert_to_tensor(logits_, dtype=tf.float32)
    log_probs = tf.nn.log_softmax(logits)

    outputs, next_beam_state = beam_search_decoder._beam_search_step(
        time=2,
        logits=logits,
        next_cell_state=dummy_cell_state,
        beam_state=beam_state,
        batch_size=tf.convert_to_tensor(batch_size),
        beam_width=beam_width,
        end_token=end_token,
        length_penalty_weight=length_penalty_weight,
        coverage_penalty_weight=coverage_penalty_weight,
        output_all_scores=output_all_scores,
    )

    outputs_, next_state_, state_, log_probs_ = [
        outputs,
        next_beam_state,
        beam_state,
        log_probs,
    ]

    np.testing.assert_equal(
        outputs_.predicted_ids.numpy(), np.asanyarray([[3, 3, 2], [2, 2, 1]])
    )
    np.testing.assert_equal(
        outputs_.parent_ids.numpy(), np.asanyarray([[1, 0, 0], [2, 1, 0]])
    )
    np.testing.assert_equal(
        next_state_.lengths.numpy(), np.asanyarray([[3, 3, 3], [3, 3, 3]])
    )
    np.testing.assert_equal(
        next_state_.finished.numpy(),
        np.asanyarray([[False, False, False], [False, False, False]]),
    )

    expected_log_probs = []
    expected_log_probs.append(state_.log_probs[0].numpy())
    expected_log_probs.append(state_.log_probs[1].numpy())
    expected_log_probs[0][0] += log_probs_[0, 1, 3]
    expected_log_probs[0][1] += log_probs_[0, 0, 3]
    expected_log_probs[0][2] += log_probs_[0, 0, 2]
    expected_log_probs[1][0] += log_probs_[1, 2, 2]
    expected_log_probs[1][1] += log_probs_[1, 1, 2]
    expected_log_probs[1][2] += log_probs_[1, 0, 1]
    np.testing.assert_equal(
        next_state_.log_probs.numpy(), np.asanyarray(expected_log_probs)
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/beam_search_decoder_test.py" startline="377" endline="454" pcid="716">
def test_step_with_eos():
    batch_size = 2
    beam_width = 3
    vocab_size = 5
    end_token = 0
    length_penalty_weight = 0.6
    coverage_penalty_weight = 0.0
    output_all_scores = False

    dummy_cell_state = tf.zeros([batch_size, beam_width])
    beam_state = beam_search_decoder.BeamSearchDecoderState(
        cell_state=dummy_cell_state,
        log_probs=tf.nn.log_softmax(tf.ones([batch_size, beam_width])),
        lengths=tf.convert_to_tensor([[2, 1, 2], [2, 2, 1]], dtype=tf.int64),
        finished=tf.convert_to_tensor(
            [[False, True, False], [False, False, True]], dtype=tf.bool
        ),
        accumulated_attention_probs=(),
    )

    logits_ = np.full([batch_size, beam_width, vocab_size], 0.0001)
    logits_[0, 0, 2] = 1.9
    logits_[0, 0, 3] = 2.1
    logits_[0, 1, 3] = 3.1
    logits_[0, 1, 4] = 0.9
    logits_[1, 0, 1] = 0.5
    logits_[1, 1, 2] = 5.7  # why does this not work when it's 2.7?
    logits_[1, 2, 2] = 1.0
    logits_[1, 2, 3] = 0.2
    logits = tf.convert_to_tensor(logits_, dtype=tf.float32)
    log_probs = tf.nn.log_softmax(logits)

    outputs, next_beam_state = beam_search_decoder._beam_search_step(
        time=2,
        logits=logits,
        next_cell_state=dummy_cell_state,
        beam_state=beam_state,
        batch_size=tf.convert_to_tensor(batch_size),
        beam_width=beam_width,
        end_token=end_token,
        length_penalty_weight=length_penalty_weight,
        coverage_penalty_weight=coverage_penalty_weight,
        output_all_scores=output_all_scores,
    )

    outputs_, next_state_, state_, log_probs_ = [
        outputs,
        next_beam_state,
        beam_state,
        log_probs,
    ]

    np.testing.assert_equal(
        outputs_.parent_ids.numpy(), np.asanyarray([[1, 0, 0], [1, 2, 0]])
    )
    np.testing.assert_equal(
        outputs_.predicted_ids.numpy(), np.asanyarray([[0, 3, 2], [2, 0, 1]])
    )
    np.testing.assert_equal(
        next_state_.lengths.numpy(), np.asanyarray([[1, 3, 3], [3, 1, 3]])
    )
    np.testing.assert_equal(
        next_state_.finished.numpy(),
        np.asanyarray([[True, False, False], [False, True, False]]),
    )

    expected_log_probs = []
    expected_log_probs.append(state_.log_probs[0].numpy())
    expected_log_probs.append(state_.log_probs[1].numpy())
    expected_log_probs[0][1] += log_probs_[0, 0, 3]
    expected_log_probs[0][2] += log_probs_[0, 0, 2]
    expected_log_probs[1][0] += log_probs_[1, 1, 2]
    expected_log_probs[1][2] += log_probs_[1, 0, 1]
    np.testing.assert_equal(
        next_state_.log_probs.numpy(), np.asanyarray(expected_log_probs)
    )


</source>
</class>

<class classid="39" nclones="2" nlines="52" similarity="87">
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/loss_test.py" startline="136" endline="189" pcid="732">
def test_sum_reduction():
    (
        batch_size,
        sequence_length,
        _,
        logits,
        targets,
        weights,
        expected_loss,
    ) = get_test_data()
    seq_loss = loss.SequenceLoss(
        average_across_timesteps=False,
        average_across_batch=False,
        sum_over_timesteps=True,
        sum_over_batch=True,
    )
    average_loss_per_example = seq_loss(targets, logits, weights)
    res = average_loss_per_example.numpy()
    np.testing.assert_allclose(expected_loss, res, atol=1e-6, rtol=1e-6)

    seq_loss = loss.SequenceLoss(
        average_across_timesteps=False,
        average_across_batch=False,
        sum_over_timesteps=False,
        sum_over_batch=True,
    )
    average_loss_per_sequence = seq_loss(targets, logits, weights)
    res = average_loss_per_sequence.numpy()
    compare_per_sequence = np.full((sequence_length), expected_loss)
    np.testing.assert_allclose(compare_per_sequence, res, atol=1e-6, rtol=1e-6)

    seq_loss = loss.SequenceLoss(
        average_across_timesteps=False,
        average_across_batch=False,
        sum_over_timesteps=True,
        sum_over_batch=False,
    )
    average_loss_per_batch = seq_loss(targets, logits, weights)
    res = average_loss_per_batch.numpy()
    compare_per_batch = np.full((batch_size), expected_loss)
    np.testing.assert_allclose(compare_per_batch, res, atol=1e-6, rtol=1e-6)

    seq_loss = loss.SequenceLoss(
        average_across_timesteps=False,
        average_across_batch=False,
        sum_over_timesteps=False,
        sum_over_batch=False,
    )
    total_loss = seq_loss(targets, logits, weights)
    res = total_loss.numpy()
    compare_total = np.full((batch_size, sequence_length), expected_loss)
    np.testing.assert_allclose(compare_total, res, atol=1e-6, rtol=1e-6)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/loss_test.py" startline="191" endline="254" pcid="733">
def test_weighted_sum_reduction():
    (
        batch_size,
        sequence_length,
        _,
        logits,
        targets,
        _,
        expected_loss,
    ) = get_test_data()
    weights = [tf.constant(1.0, shape=[batch_size]) for _ in range(sequence_length)]
    # Make the last element in the sequence to have zero weights.
    weights[-1] = tf.constant(0.0, shape=[batch_size])
    weights = tf.stack(weights, axis=1)
    seq_loss = loss.SequenceLoss(
        average_across_timesteps=False,
        average_across_batch=False,
        sum_over_timesteps=True,
        sum_over_batch=True,
    )
    average_loss_per_example = seq_loss(targets, logits, weights)
    res = average_loss_per_example.numpy()
    np.testing.assert_allclose(expected_loss, res, rtol=1e-6, atol=1e-6)

    seq_loss = loss.SequenceLoss(
        average_across_timesteps=False,
        average_across_batch=False,
        sum_over_timesteps=False,
        sum_over_batch=True,
    )
    average_loss_per_sequence = seq_loss(targets, logits, weights)
    res = average_loss_per_sequence.numpy()
    compare_per_sequence = np.full(sequence_length, expected_loss)
    # The last element in every sequence are zeros, which will be
    # filtered.
    compare_per_sequence[-1] = 0.0
    np.testing.assert_allclose(compare_per_sequence, res, rtol=1e-6, atol=1e-6)

    seq_loss = loss.SequenceLoss(
        average_across_timesteps=False,
        average_across_batch=False,
        sum_over_timesteps=True,
        sum_over_batch=False,
    )
    average_loss_per_batch = seq_loss(targets, logits, weights)
    res = average_loss_per_batch.numpy()
    compare_per_batch = np.full(batch_size, expected_loss)
    np.testing.assert_allclose(compare_per_batch, res, rtol=1e-6, atol=1e-6)

    seq_loss = loss.SequenceLoss(
        average_across_timesteps=False,
        average_across_batch=False,
        sum_over_timesteps=False,
        sum_over_batch=False,
    )
    total_loss = seq_loss(targets, logits, weights)
    res = total_loss.numpy()
    compare_total = np.full((batch_size, sequence_length), expected_loss)
    # The last element in every sequence are zeros, which will be
    # filtered.
    compare_total[:, -1] = 0
    np.testing.assert_allclose(compare_total, res, rtol=1e-6, atol=1e-6)


</source>
</class>

<class classid="40" nclones="2" nlines="43" similarity="84">
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/basic_decoder_test.py" startline="170" endline="222" pcid="739">
def test_step_with_greedy_embedding_helper():
    batch_size = 5
    vocabulary_size = 7
    cell_depth = vocabulary_size  # cell's logits must match vocabulary size
    input_depth = 10
    start_tokens = np.random.randint(0, vocabulary_size, size=batch_size)
    end_token = 1

    embeddings = np.random.randn(vocabulary_size, input_depth).astype(np.float32)
    embeddings_t = tf.constant(embeddings)
    cell = tf.keras.layers.LSTMCell(vocabulary_size)
    sampler = sampler_py.GreedyEmbeddingSampler()
    initial_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)
    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler)
    (first_finished, first_inputs, first_state) = my_decoder.initialize(
        embeddings_t,
        start_tokens=start_tokens,
        end_token=end_token,
        initial_state=initial_state,
    )
    output_size = my_decoder.output_size
    output_dtype = my_decoder.output_dtype
    assert (
        basic_decoder.BasicDecoderOutput(cell_depth, tf.TensorShape([])) == output_size
    )
    assert basic_decoder.BasicDecoderOutput(tf.float32, tf.int32) == output_dtype

    (step_outputs, step_state, step_next_inputs, step_finished) = my_decoder.step(
        tf.constant(0), first_inputs, first_state
    )

    assert len(first_state) == 2
    assert len(step_state) == 2
    assert isinstance(step_outputs, basic_decoder.BasicDecoderOutput)
    assert (batch_size, cell_depth) == step_outputs[0].shape
    assert (batch_size,) == step_outputs[1].shape
    assert (batch_size, cell_depth) == first_state[0].shape
    assert (batch_size, cell_depth) == first_state[1].shape
    assert (batch_size, cell_depth) == step_state[0].shape
    assert (batch_size, cell_depth) == step_state[1].shape

    expected_sample_ids = np.argmax(step_outputs.rnn_output, -1)
    expected_step_finished = expected_sample_ids == end_token
    expected_step_next_inputs = embeddings[expected_sample_ids]
    np.testing.assert_equal(
        np.asanyarray([False, False, False, False, False]), first_finished
    )
    np.testing.assert_equal(expected_step_finished, step_finished)
    assert output_dtype.sample_id == step_outputs.sample_id.dtype
    np.testing.assert_equal(expected_sample_ids, step_outputs.sample_id)
    np.testing.assert_equal(expected_step_next_inputs, step_next_inputs)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/basic_decoder_test.py" startline="223" endline="272" pcid="740">
def test_step_with_sample_embedding_helper():
    batch_size = 5
    vocabulary_size = 7
    cell_depth = vocabulary_size  # cell's logits must match vocabulary size
    input_depth = 10
    np.random.seed(0)
    start_tokens = np.random.randint(0, vocabulary_size, size=batch_size)
    end_token = 1

    embeddings = np.random.randn(vocabulary_size, input_depth).astype(np.float32)
    embeddings_t = tf.constant(embeddings)
    cell = tf.keras.layers.LSTMCell(vocabulary_size)
    sampler = sampler_py.SampleEmbeddingSampler(seed=0)
    initial_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)
    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler)
    (first_finished, first_inputs, first_state) = my_decoder.initialize(
        embeddings_t,
        start_tokens=start_tokens,
        end_token=end_token,
        initial_state=initial_state,
    )
    output_size = my_decoder.output_size
    output_dtype = my_decoder.output_dtype
    assert (
        basic_decoder.BasicDecoderOutput(cell_depth, tf.TensorShape([])) == output_size
    )
    assert basic_decoder.BasicDecoderOutput(tf.float32, tf.int32) == output_dtype

    (step_outputs, step_state, step_next_inputs, step_finished) = my_decoder.step(
        tf.constant(0), first_inputs, first_state
    )

    assert len(first_state) == 2
    assert len(step_state) == 2
    assert isinstance(step_outputs, basic_decoder.BasicDecoderOutput)
    assert (batch_size, cell_depth) == step_outputs[0].shape
    assert (batch_size,) == step_outputs[1].shape
    assert (batch_size, cell_depth) == first_state[0].shape
    assert (batch_size, cell_depth) == first_state[1].shape
    assert (batch_size, cell_depth) == step_state[0].shape
    assert (batch_size, cell_depth) == step_state[1].shape

    sample_ids = step_outputs.sample_id.numpy()
    assert output_dtype.sample_id == sample_ids.dtype
    expected_step_finished = sample_ids == end_token
    expected_step_next_inputs = embeddings[sample_ids, :]
    np.testing.assert_equal(expected_step_finished, step_finished)
    np.testing.assert_equal(expected_step_next_inputs, step_next_inputs)


</source>
</class>

<class classid="41" nclones="2" nlines="48" similarity="79">
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/basic_decoder_test.py" startline="458" endline="523" pcid="744">
def test_step_with_inference_helper_categorical():
    batch_size = 5
    vocabulary_size = 7
    cell_depth = vocabulary_size
    start_token = 0
    end_token = 6

    start_inputs = tf.one_hot(
        np.ones(batch_size, dtype=np.int32) * start_token, vocabulary_size
    )

    # The sample function samples categorically from the logits.
    def sample_fn(x):
        return sampler_py.categorical_sample(logits=x)

    # The next inputs are a one-hot encoding of the sampled labels.
    def next_inputs_fn(x):
        return tf.one_hot(x, vocabulary_size, dtype=tf.float32)

    def end_fn(sample_ids):
        return tf.equal(sample_ids, end_token)

    cell = tf.keras.layers.LSTMCell(vocabulary_size)
    sampler = sampler_py.InferenceSampler(
        sample_fn,
        sample_shape=(),
        sample_dtype=tf.int32,
        end_fn=end_fn,
        next_inputs_fn=next_inputs_fn,
    )
    initial_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)
    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler)
    (first_finished, first_inputs, first_state) = my_decoder.initialize(
        start_inputs, initial_state=initial_state
    )

    output_size = my_decoder.output_size
    output_dtype = my_decoder.output_dtype
    assert (
        basic_decoder.BasicDecoderOutput(cell_depth, tf.TensorShape([])) == output_size
    )
    assert basic_decoder.BasicDecoderOutput(tf.float32, tf.int32) == output_dtype

    (step_outputs, step_state, step_next_inputs, step_finished) = my_decoder.step(
        tf.constant(0), first_inputs, first_state
    )

    assert len(first_state) == 2
    assert len(step_state) == 2
    assert isinstance(step_outputs, basic_decoder.BasicDecoderOutput)
    assert (batch_size, cell_depth) == step_outputs[0].shape
    assert (batch_size,) == step_outputs[1].shape
    assert (batch_size, cell_depth) == first_state[0].shape
    assert (batch_size, cell_depth) == first_state[1].shape
    assert (batch_size, cell_depth) == step_state[0].shape
    assert (batch_size, cell_depth) == step_state[1].shape

    sample_ids = step_outputs.sample_id.numpy()
    assert output_dtype.sample_id == sample_ids.dtype
    expected_step_finished = sample_ids == end_token
    expected_step_next_inputs = np.zeros((batch_size, vocabulary_size))
    expected_step_next_inputs[np.arange(batch_size), sample_ids] = 1.0
    np.testing.assert_equal(expected_step_finished, step_finished)
    np.testing.assert_equal(expected_step_next_inputs, step_next_inputs)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/basic_decoder_test.py" startline="524" endline="585" pcid="748">
def test_step_with_inference_helper_multilabel():
    batch_size = 5
    vocabulary_size = 7
    cell_depth = vocabulary_size
    start_token = 0
    end_token = 6

    start_inputs = tf.one_hot(
        np.ones(batch_size, dtype=np.int32) * start_token, vocabulary_size
    )

    # The sample function samples independent bernoullis from the logits.
    def sample_fn(x):
        return sampler_py.bernoulli_sample(logits=x, dtype=tf.bool)

    # The next inputs are a one-hot encoding of the sampled labels.
    def next_inputs_fn(x):
        return tf.cast(x, tf.float32)

    def end_fn(sample_ids):
        return sample_ids[:, end_token]

    cell = tf.keras.layers.LSTMCell(vocabulary_size)
    sampler = sampler_py.InferenceSampler(
        sample_fn,
        sample_shape=[cell_depth],
        sample_dtype=tf.bool,
        end_fn=end_fn,
        next_inputs_fn=next_inputs_fn,
    )
    initial_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)
    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler)
    (first_finished, first_inputs, first_state) = my_decoder.initialize(
        start_inputs, initial_state=initial_state
    )
    output_size = my_decoder.output_size
    output_dtype = my_decoder.output_dtype
    assert basic_decoder.BasicDecoderOutput(cell_depth, cell_depth) == output_size
    assert basic_decoder.BasicDecoderOutput(tf.float32, tf.bool) == output_dtype

    (step_outputs, step_state, step_next_inputs, step_finished) = my_decoder.step(
        tf.constant(0), first_inputs, first_state
    )

    assert len(first_state) == 2
    assert len(step_state) == 2
    assert isinstance(step_outputs, basic_decoder.BasicDecoderOutput)
    assert (batch_size, cell_depth) == step_outputs[0].shape
    assert (batch_size, cell_depth) == step_outputs[1].shape
    assert (batch_size, cell_depth) == first_state[0].shape
    assert (batch_size, cell_depth) == first_state[1].shape
    assert (batch_size, cell_depth) == step_state[0].shape
    assert (batch_size, cell_depth) == step_state[1].shape

    sample_ids = step_outputs.sample_id.numpy()
    assert output_dtype.sample_id == sample_ids.dtype
    expected_step_finished = sample_ids[:, end_token]
    expected_step_next_inputs = sample_ids.astype(np.float32)
    np.testing.assert_equal(expected_step_finished, step_finished)
    np.testing.assert_equal(expected_step_next_inputs, step_next_inputs)


</source>
</class>

<class classid="42" nclones="2" nlines="14" similarity="73">
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/beam_search_ops_test.py" startline="32" endline="51" pcid="755">
def test_gather_tree_one():
    # (max_time = 4, batch_size = 1, beams = 3)
    end_token = 10
    step_ids = _transpose_batch_time([[[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]]])
    parent_ids = _transpose_batch_time(
        [[[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]]]
    )
    max_sequence_lengths = [3]
    expected_result = _transpose_batch_time(
        [[[2, 2, 2], [6, 5, 6], [7, 8, 9], [10, 10, 10]]]
    )
    beams = gather_tree(
        step_ids=step_ids,
        parent_ids=parent_ids,
        max_sequence_lengths=max_sequence_lengths,
        end_token=end_token,
    )
    np.testing.assert_equal(expected_result, beams.numpy())


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/beam_search_ops_test.py" startline="53" endline="71" pcid="756">
def test_bad_parent_values_on_cpu():
    # (batch_size = 1, max_time = 4, beams = 3)
    # bad parent in beam 1 time 1
    end_token = 10
    step_ids = _transpose_batch_time([[[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]]])
    parent_ids = _transpose_batch_time(
        [[[0, 0, 0], [0, -1, 1], [2, 1, 2], [-1, -1, -1]]]
    )
    max_sequence_lengths = [3]

    with pytest.raises(tf.errors.InvalidArgumentError, match="parent id"):
        _ = gather_tree(
            step_ids=step_ids,
            parent_ids=parent_ids,
            max_sequence_lengths=max_sequence_lengths,
            end_token=end_token,
        )


</source>
</class>

<class classid="43" nclones="2" nlines="24" similarity="100">
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/attention_wrapper_test.py" startline="471" endline="499" pcid="776">
def test_bahdanau_normalized_dtype(dtype):
    dummy_data = DummyData2()
    encoder_outputs = dummy_data.encoder_outputs.astype(dtype)
    decoder_inputs = dummy_data.decoder_inputs.astype(dtype)
    attention_mechanism = wrapper.BahdanauAttention(
        units=dummy_data.units,
        memory=encoder_outputs,
        memory_sequence_length=dummy_data.encoder_sequence_length,
        normalize=True,
        dtype=dtype,
    )
    cell = tf.keras.layers.LSTMCell(
        dummy_data.units, recurrent_activation="sigmoid", dtype=dtype
    )
    cell = wrapper.AttentionWrapper(cell, attention_mechanism, dtype=dtype)

    sampler = sampler_py.TrainingSampler()
    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler, dtype=dtype)

    final_outputs, final_state, _ = my_decoder(
        decoder_inputs,
        initial_state=cell.get_initial_state(batch_size=dummy_data.batch, dtype=dtype),
        sequence_length=dummy_data.decoder_sequence_length,
    )
    assert isinstance(final_outputs, basic_decoder.BasicDecoderOutput)
    assert final_outputs.rnn_output.dtype == dtype
    assert isinstance(final_state, wrapper.AttentionWrapperState)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/attention_wrapper_test.py" startline="501" endline="530" pcid="777">
def test_luong_scaled_dtype(dtype):
    dummy_data = DummyData2()
    # Test case for GitHub issue 18099
    encoder_outputs = dummy_data.encoder_outputs.astype(dtype)
    decoder_inputs = dummy_data.decoder_inputs.astype(dtype)
    attention_mechanism = wrapper.LuongAttention(
        units=dummy_data.units,
        memory=encoder_outputs,
        memory_sequence_length=dummy_data.encoder_sequence_length,
        scale=True,
        dtype=dtype,
    )
    cell = tf.keras.layers.LSTMCell(
        dummy_data.units, recurrent_activation="sigmoid", dtype=dtype
    )
    cell = wrapper.AttentionWrapper(cell, attention_mechanism, dtype=dtype)

    sampler = sampler_py.TrainingSampler()
    my_decoder = basic_decoder.BasicDecoder(cell=cell, sampler=sampler, dtype=dtype)

    final_outputs, final_state, _ = my_decoder(
        decoder_inputs,
        initial_state=cell.get_initial_state(batch_size=dummy_data.batch, dtype=dtype),
        sequence_length=dummy_data.decoder_sequence_length,
    )
    assert isinstance(final_outputs, basic_decoder.BasicDecoderOutput)
    assert final_outputs.rnn_output.dtype == dtype
    assert isinstance(final_state, wrapper.AttentionWrapperState)


</source>
</class>

<class classid="44" nclones="9" nlines="31" similarity="71">
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/attention_wrapper_test.py" startline="542" endline="581" pcid="779">
def test_bahdanau_not_normalized():
    set_random_state_for_tf_and_np()
    policy = tf.keras.mixed_precision.experimental.global_policy()
    create_attention_mechanism = wrapper.BahdanauAttention
    create_attention_kwargs = {"kernel_initializer": "ones"}
    expected_final_output = basic_decoder.BasicDecoderOutput(
        rnn_output=ResultSummary(
            shape=(5, 3, 6), dtype=policy.compute_dtype, mean=-0.003204414
        ),
        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype(np.int32), mean=3.2),
    )
    expected_final_state = wrapper.AttentionWrapperState(
        cell_state=[
            ResultSummary(shape=(5, 9), dtype=policy.compute_dtype, mean=0.40868404),
            ResultSummary(shape=(5, 9), dtype=policy.compute_dtype, mean=0.89017969),
        ],
        attention=ResultSummary(
            shape=(5, 6), dtype=policy.compute_dtype, mean=0.041453815
        ),
        alignments=ResultSummary(shape=(5, 8), dtype=policy.compute_dtype, mean=0.125),
        attention_state=ResultSummary(
            shape=(5, 8), dtype=policy.compute_dtype, mean=0.125
        ),
        alignment_history=(),
    )
    expected_final_alignment_history = ResultSummary(
        shape=(3, 5, 8), dtype=policy.compute_dtype, mean=0.125
    )

    _test_with_attention(
        create_attention_mechanism,
        expected_final_output,
        expected_final_state,
        alignment_history=True,
        create_query_layer=True,
        expected_final_alignment_history=expected_final_alignment_history,
        create_attention_kwargs=create_attention_kwargs,
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/attention_wrapper_test.py" startline="618" endline="651" pcid="781">
def test_luong_not_normalized():
    set_random_state_for_tf_and_np()
    policy = tf.keras.mixed_precision.experimental.global_policy()
    create_attention_mechanism = wrapper.LuongAttention

    expected_final_output = basic_decoder.BasicDecoderOutput(
        rnn_output=ResultSummary(
            shape=(5, 3, 6), dtype=policy.compute_dtype, mean=-0.06124732
        ),
        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype("int32"), mean=2.73333333),
    )
    expected_final_state = wrapper.AttentionWrapperState(
        cell_state=[
            ResultSummary(shape=(5, 9), dtype=policy.compute_dtype, mean=0.52021580),
            ResultSummary(shape=(5, 9), dtype=policy.compute_dtype, mean=1.0964939),
        ],
        attention=ResultSummary(
            shape=(5, 6), dtype=policy.compute_dtype, mean=-0.0318060
        ),
        alignments=ResultSummary(shape=(5, 8), dtype=policy.compute_dtype, mean=0.125),
        attention_state=ResultSummary(
            shape=(5, 8), dtype=policy.compute_dtype, mean=0.125
        ),
        alignment_history=(),
    )

    _test_with_attention(
        create_attention_mechanism,
        expected_final_output,
        expected_final_state,
        attention_mechanism_depth=9,
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/attention_wrapper_test.py" startline="723" endline="764" pcid="784">
def test_bahdanau_monotonic_not_normalized():
    set_random_state_for_tf_and_np()
    create_attention_mechanism = wrapper.BahdanauMonotonicAttention
    create_attention_kwargs = {"kernel_initializer": "ones"}

    expected_final_output = basic_decoder.BasicDecoderOutput(
        rnn_output=ResultSummary(
            shape=(5, 3, 6), dtype=np.dtype("float32"), mean=-0.009921653
        ),
        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype("int32"), mean=3.13333333),
    )
    expected_final_state = wrapper.AttentionWrapperState(
        cell_state=[
            ResultSummary(shape=(5, 9), dtype=np.dtype("float32"), mean=0.44612807),
            ResultSummary(shape=(5, 9), dtype=np.dtype("float32"), mean=0.95786464),
        ],
        attention=ResultSummary(
            shape=(5, 6), dtype=np.dtype("float32"), mean=0.038682378
        ),
        alignments=ResultSummary(
            shape=(5, 8), dtype=np.dtype("float32"), mean=0.09778417
        ),
        attention_state=ResultSummary(
            shape=(5, 8), dtype=np.dtype("float32"), mean=0.09778417
        ),
        alignment_history=(),
    )
    expected_final_alignment_history = ResultSummary(
        shape=(3, 5, 8), dtype=np.dtype("float32"), mean=0.10261579603
    )

    _test_with_attention(
        create_attention_mechanism,
        expected_final_output,
        expected_final_state,
        alignment_history=True,
        expected_final_alignment_history=expected_final_alignment_history,
        create_query_layer=True,
        create_attention_kwargs=create_attention_kwargs,
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/attention_wrapper_test.py" startline="652" endline="686" pcid="782">
def test_luong_scaled():
    set_random_state_for_tf_and_np()
    create_attention_mechanism = wrapper.LuongAttention
    create_attention_kwargs = {"scale": True}

    expected_final_output = basic_decoder.BasicDecoderOutput(
        rnn_output=ResultSummary(
            shape=(5, 3, 6), dtype=np.dtype("float32"), mean=-0.06124732
        ),
        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype("int32"), mean=2.73333333),
    )
    expected_final_state = wrapper.AttentionWrapperState(
        cell_state=[
            ResultSummary(shape=(5, 9), dtype=np.dtype("float32"), mean=0.52021580),
            ResultSummary(shape=(5, 9), dtype=np.dtype("float32"), mean=1.0964939),
        ],
        attention=ResultSummary(
            shape=(5, 6), dtype=np.dtype("float32"), mean=-0.0318060
        ),
        alignments=ResultSummary(shape=(5, 8), dtype=np.dtype("float32"), mean=0.125),
        attention_state=ResultSummary(
            shape=(5, 8), dtype=np.dtype("float32"), mean=0.125
        ),
        alignment_history=(),
    )

    _test_with_attention(
        create_attention_mechanism,
        expected_final_output,
        expected_final_state,
        attention_mechanism_depth=9,
        create_attention_kwargs=create_attention_kwargs,
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/attention_wrapper_test.py" startline="765" endline="805" pcid="785">
def test_bahdanau_monotonic_normalized():
    set_random_state_for_tf_and_np()
    create_attention_mechanism = wrapper.BahdanauMonotonicAttention
    create_attention_kwargs = {"kernel_initializer": "ones", "normalize": True}
    expected_final_output = basic_decoder.BasicDecoderOutput(
        rnn_output=ResultSummary(
            shape=(5, 3, 6), dtype=np.dtype("float32"), mean=0.007140680
        ),
        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype("int32"), mean=3.26666666),
    )
    expected_final_state = wrapper.AttentionWrapperState(
        cell_state=[
            ResultSummary(shape=(5, 9), dtype=np.dtype("float32"), mean=0.47012400),
            ResultSummary(shape=(5, 9), dtype=np.dtype("float32"), mean=1.0249618),
        ],
        attention=ResultSummary(
            shape=(5, 6), dtype=np.dtype("float32"), mean=0.068432882
        ),
        alignments=ResultSummary(
            shape=(5, 8), dtype=np.dtype("float32"), mean=0.0615656
        ),
        attention_state=ResultSummary(
            shape=(5, 8), dtype=np.dtype("float32"), mean=0.0615656
        ),
        alignment_history=(),
    )
    expected_final_alignment_history = ResultSummary(
        shape=(3, 5, 8), dtype=np.dtype("float32"), mean=0.07909643
    )

    _test_with_attention(
        create_attention_mechanism,
        expected_final_output,
        expected_final_state,
        alignment_history=True,
        expected_final_alignment_history=expected_final_alignment_history,
        create_query_layer=True,
        create_attention_kwargs=create_attention_kwargs,
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/attention_wrapper_test.py" startline="687" endline="722" pcid="783">
def test_not_use_attention_layer():
    set_random_state_for_tf_and_np()
    create_attention_mechanism = wrapper.BahdanauAttention
    create_attention_kwargs = {"kernel_initializer": "ones"}

    expected_final_output = basic_decoder.BasicDecoderOutput(
        rnn_output=ResultSummary(
            shape=(5, 3, 10), dtype=np.dtype("float32"), mean=0.078317143
        ),
        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype("int32"), mean=4.2),
    )
    expected_final_state = wrapper.AttentionWrapperState(
        cell_state=[
            ResultSummary(shape=(5, 9), dtype=np.dtype("float32"), mean=0.89382392),
            ResultSummary(shape=(5, 9), dtype=np.dtype("float32"), mean=1.722382),
        ],
        attention=ResultSummary(
            shape=(5, 10), dtype=np.dtype("float32"), mean=0.026356646
        ),
        alignments=ResultSummary(shape=(5, 8), dtype=np.dtype("float32"), mean=0.125),
        attention_state=ResultSummary(
            shape=(5, 8), dtype=np.dtype("float32"), mean=0.125
        ),
        alignment_history=(),
    )

    _test_with_attention(
        create_attention_mechanism,
        expected_final_output,
        expected_final_state,
        attention_layer_size=None,
        create_query_layer=True,
        create_attention_kwargs=create_attention_kwargs,
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/attention_wrapper_test.py" startline="582" endline="616" pcid="780">
def test_bahdanau_normalized():
    set_random_state_for_tf_and_np()
    create_attention_mechanism = wrapper.BahdanauAttention
    create_attention_kwargs = {"kernel_initializer": "ones", "normalize": True}

    expected_final_output = basic_decoder.BasicDecoderOutput(
        rnn_output=ResultSummary(
            shape=(5, 3, 6), dtype=np.dtype("float32"), mean=-0.008089137
        ),
        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype("int32"), mean=2.8),
    )
    expected_final_state = wrapper.AttentionWrapperState(
        cell_state=[
            ResultSummary(shape=(5, 9), dtype=np.dtype("float32"), mean=0.49166861),
            ResultSummary(shape=(5, 9), dtype=np.dtype("float32"), mean=1.01068615),
        ],
        attention=ResultSummary(
            shape=(5, 6), dtype=np.dtype("float32"), mean=0.042427111
        ),
        alignments=ResultSummary(shape=(5, 8), dtype=np.dtype("float32"), mean=0.125),
        attention_state=ResultSummary(
            shape=(5, 8), dtype=np.dtype("float32"), mean=0.125
        ),
        alignment_history=(),
    )

    _test_with_attention(
        create_attention_mechanism,
        expected_final_output,
        expected_final_state,
        create_query_layer=True,
        create_attention_kwargs=create_attention_kwargs,
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/attention_wrapper_test.py" startline="846" endline="887" pcid="787">
def test_luong_monotonic_scaled():
    set_random_state_for_tf_and_np()
    create_attention_mechanism = wrapper.LuongMonotonicAttention
    create_attention_kwargs = {"scale": True}

    expected_final_output = basic_decoder.BasicDecoderOutput(
        rnn_output=ResultSummary(
            shape=(5, 3, 6), dtype=np.dtype("float32"), mean=0.003664831
        ),
        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype("int32"), mean=3.06666666),
    )
    expected_final_state = wrapper.AttentionWrapperState(
        cell_state=[
            ResultSummary(shape=(5, 9), dtype=np.dtype("float32"), mean=0.54318606),
            ResultSummary(shape=(5, 9), dtype=np.dtype("float32"), mean=1.12592840),
        ],
        attention=ResultSummary(
            shape=(5, 6), dtype=np.dtype("float32"), mean=0.059128221
        ),
        alignments=ResultSummary(
            shape=(5, 8), dtype=np.dtype("float32"), mean=0.05112994
        ),
        attention_state=ResultSummary(
            shape=(5, 8), dtype=np.dtype("float32"), mean=0.05112994
        ),
        alignment_history=(),
    )
    expected_final_alignment_history = ResultSummary(
        shape=(3, 5, 8), dtype=np.dtype("float32"), mean=0.06994973868
    )

    _test_with_attention(
        create_attention_mechanism,
        expected_final_output,
        expected_final_state,
        attention_mechanism_depth=9,
        alignment_history=True,
        expected_final_alignment_history=expected_final_alignment_history,
        create_attention_kwargs=create_attention_kwargs,
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/seq2seq/tests/attention_wrapper_test.py" startline="806" endline="845" pcid="786">
def test_luong_monotonic_not_normalized():
    set_random_state_for_tf_and_np()
    create_attention_mechanism = wrapper.LuongMonotonicAttention

    expected_final_output = basic_decoder.BasicDecoderOutput(
        rnn_output=ResultSummary(
            shape=(5, 3, 6), dtype=np.dtype("float32"), mean=0.003664831
        ),
        sample_id=ResultSummary(shape=(5, 3), dtype=np.dtype("int32"), mean=3.06666666),
    )
    expected_final_state = wrapper.AttentionWrapperState(
        cell_state=[
            ResultSummary(shape=(5, 9), dtype=np.dtype("float32"), mean=0.54318606),
            ResultSummary(shape=(5, 9), dtype=np.dtype("float32"), mean=1.12592840),
        ],
        attention=ResultSummary(
            shape=(5, 6), dtype=np.dtype("float32"), mean=0.059128221
        ),
        alignments=ResultSummary(
            shape=(5, 8), dtype=np.dtype("float32"), mean=0.05112994
        ),
        attention_state=ResultSummary(
            shape=(5, 8), dtype=np.dtype("float32"), mean=0.05112994
        ),
        alignment_history=(),
    )
    expected_final_alignment_history = ResultSummary(
        shape=(3, 5, 8), dtype=np.dtype("float32"), mean=0.06994973868
    )

    _test_with_attention(
        create_attention_mechanism,
        expected_final_output,
        expected_final_state,
        attention_mechanism_depth=9,
        alignment_history=True,
        expected_final_alignment_history=expected_final_alignment_history,
    )


</source>
</class>

<class classid="45" nclones="3" nlines="24" similarity="75">
<source file="systems/addons-0.12.0/tensorflow_addons/text/tests/skip_gram_ops_test.py" startline="35" endline="60" pcid="801">
def test_skip_gram_sample_skips_2():
    """Tests skip-gram with min_skips = max_skips = 2."""
    input_tensor = tf.constant([b"the", b"quick", b"brown", b"fox", b"jumps"])
    tokens, labels = text.skip_gram_sample(input_tensor, min_skips=2, max_skips=2)
    expected_tokens, expected_labels = _split_tokens_labels(
        [
            (b"the", b"quick"),
            (b"the", b"brown"),
            (b"quick", b"the"),
            (b"quick", b"brown"),
            (b"quick", b"fox"),
            (b"brown", b"the"),
            (b"brown", b"quick"),
            (b"brown", b"fox"),
            (b"brown", b"jumps"),
            (b"fox", b"quick"),
            (b"fox", b"brown"),
            (b"fox", b"jumps"),
            (b"jumps", b"brown"),
            (b"jumps", b"fox"),
        ]
    )
    np.testing.assert_equal(np.asanyarray(expected_tokens), tokens.numpy())
    np.testing.assert_equal(np.asanyarray(expected_labels), labels.numpy())


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/text/tests/skip_gram_ops_test.py" startline="171" endline="203" pcid="807">
def test_skip_gram_sample_random_skips():
    """Tests skip-gram with min_skips != max_skips, with random output."""
    # The number of outputs is non-deterministic in this case, so set random
    # seed to help ensure the outputs remain constant for this test case.
    tf.random.set_seed(42)

    input_tensor = tf.constant([b"the", b"quick", b"brown", b"fox", b"jumps", b"over"])
    tokens, labels = text.skip_gram_sample(
        input_tensor, min_skips=1, max_skips=2, seed=9
    )
    expected_tokens, expected_labels = _split_tokens_labels(
        [
            (b"the", b"quick"),
            (b"the", b"brown"),
            (b"quick", b"the"),
            (b"quick", b"brown"),
            (b"quick", b"fox"),
            (b"brown", b"the"),
            (b"brown", b"quick"),
            (b"brown", b"fox"),
            (b"brown", b"jumps"),
            (b"fox", b"brown"),
            (b"fox", b"jumps"),
            (b"jumps", b"fox"),
            (b"jumps", b"over"),
            (b"over", b"fox"),
            (b"over", b"jumps"),
        ]
    )
    np.testing.assert_equal(np.asanyarray(expected_tokens), tokens.numpy())
    np.testing.assert_equal(np.asanyarray(expected_labels), labels.numpy())


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/text/tests/skip_gram_ops_test.py" startline="61" endline="93" pcid="802">
def test_skip_gram_sample_emit_self():
    """Tests skip-gram with emit_self_as_target = True."""
    input_tensor = tf.constant([b"the", b"quick", b"brown", b"fox", b"jumps"])
    tokens, labels = text.skip_gram_sample(
        input_tensor, min_skips=2, max_skips=2, emit_self_as_target=True
    )
    expected_tokens, expected_labels = _split_tokens_labels(
        [
            (b"the", b"the"),
            (b"the", b"quick"),
            (b"the", b"brown"),
            (b"quick", b"the"),
            (b"quick", b"quick"),
            (b"quick", b"brown"),
            (b"quick", b"fox"),
            (b"brown", b"the"),
            (b"brown", b"quick"),
            (b"brown", b"brown"),
            (b"brown", b"fox"),
            (b"brown", b"jumps"),
            (b"fox", b"quick"),
            (b"fox", b"brown"),
            (b"fox", b"fox"),
            (b"fox", b"jumps"),
            (b"jumps", b"brown"),
            (b"jumps", b"fox"),
            (b"jumps", b"jumps"),
        ]
    )
    np.testing.assert_equal(np.asanyarray(expected_tokens), tokens.numpy())
    np.testing.assert_equal(np.asanyarray(expected_labels), labels.numpy())


</source>
</class>

<class classid="46" nclones="3" nlines="14" similarity="71">
<source file="systems/addons-0.12.0/tensorflow_addons/text/tests/skip_gram_ops_test.py" startline="117" endline="134" pcid="804">
def test_skip_gram_sample_skips_exceed_length():
    """Tests skip-gram when min/max_skips exceed length of input."""
    input_tensor = tf.constant([b"the", b"quick", b"brown"])
    tokens, labels = text.skip_gram_sample(input_tensor, min_skips=100, max_skips=100)
    expected_tokens, expected_labels = _split_tokens_labels(
        [
            (b"the", b"quick"),
            (b"the", b"brown"),
            (b"quick", b"the"),
            (b"quick", b"brown"),
            (b"brown", b"the"),
            (b"brown", b"quick"),
        ]
    )
    np.testing.assert_equal(np.asanyarray(expected_tokens), tokens.numpy())
    np.testing.assert_equal(np.asanyarray(expected_labels), labels.numpy())


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/text/tests/skip_gram_ops_test.py" startline="153" endline="170" pcid="806">
def test_skip_gram_sample_limit_exceeds():
    """Tests skip-gram when limit exceeds the length of the input."""
    input_tensor = tf.constant([b"foo", b"the", b"quick", b"brown"])
    tokens, labels = text.skip_gram_sample(
        input_tensor, min_skips=1, max_skips=1, start=1, limit=100
    )
    expected_tokens, expected_labels = _split_tokens_labels(
        [
            (b"the", b"quick"),
            (b"quick", b"the"),
            (b"quick", b"brown"),
            (b"brown", b"quick"),
        ]
    )
    np.testing.assert_equal(np.asanyarray(expected_tokens), tokens.numpy())
    np.testing.assert_equal(np.asanyarray(expected_labels), labels.numpy())


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/text/tests/skip_gram_ops_test.py" startline="135" endline="152" pcid="805">
def test_skip_gram_sample_start_limit():
    """Tests skip-gram over a limited portion of the input."""
    input_tensor = tf.constant([b"foo", b"the", b"quick", b"brown", b"bar"])
    tokens, labels = text.skip_gram_sample(
        input_tensor, min_skips=1, max_skips=1, start=1, limit=3
    )
    expected_tokens, expected_labels = _split_tokens_labels(
        [
            (b"the", b"quick"),
            (b"quick", b"the"),
            (b"quick", b"brown"),
            (b"brown", b"quick"),
        ]
    )
    np.testing.assert_equal(np.asanyarray(expected_tokens), tokens.numpy())
    np.testing.assert_equal(np.asanyarray(expected_labels), labels.numpy())


</source>
</class>

<class classid="47" nclones="2" nlines="19" similarity="73">
<source file="systems/addons-0.12.0/tensorflow_addons/text/tests/skip_gram_ops_test.py" startline="501" endline="524" pcid="817">
def _skip_gram_sample_with_text_vocab_subsample_vocab(text_vocab_freq_file):
    _text_vocab_subsample_vocab_helper(
        vocab_freq_file=text_vocab_freq_file,
        vocab_min_count=3,
        vocab_freq_dtype=tf.dtypes.int64,
    )
    _text_vocab_subsample_vocab_helper(
        vocab_freq_file=text_vocab_freq_file,
        vocab_min_count=3,
        vocab_freq_dtype=tf.dtypes.int64,
        corpus_size=100,
    )

    # The user-supplied corpus_size should not be less than the sum of all
    # the frequency counts of vocab_freq_file, which is 100.
    with pytest.raises(ValueError):
        _text_vocab_subsample_vocab_helper(
            vocab_freq_file=text_vocab_freq_file,
            vocab_min_count=3,
            vocab_freq_dtype=tf.dtypes.int64,
            corpus_size=99,
        )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/text/tests/skip_gram_ops_test.py" startline="541" endline="564" pcid="819">
def _skip_gram_sample_with_text_vocab_subsample_vocab_float(text_vocab_float_file):
    _text_vocab_subsample_vocab_helper(
        vocab_freq_file=text_vocab_float_file,
        vocab_min_count=0.03,
        vocab_freq_dtype=tf.dtypes.float32,
    )
    _text_vocab_subsample_vocab_helper(
        vocab_freq_file=text_vocab_float_file,
        vocab_min_count=0.03,
        vocab_freq_dtype=tf.dtypes.float32,
        corpus_size=1.0,
    )

    # The user-supplied corpus_size should not be less than the sum of all
    # the frequency counts of vocab_freq_file, which is 1.
    with pytest.raises(ValueError):
        _text_vocab_subsample_vocab_helper(
            vocab_freq_file=text_vocab_float_file,
            vocab_min_count=0.03,
            vocab_freq_dtype=tf.dtypes.float32,
            corpus_size=0.99,
        )


</source>
</class>

<class classid="48" nclones="2" nlines="13" similarity="71">
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/multihead_attention_test.py" startline="27" endline="47" pcid="880">
def test_output_size():
    batch_size = 10
    num_heads = 8
    head_size = 12
    output_size = 20

    q = tf.random.uniform((batch_size, 5, 9), dtype=np.float32)
    k = tf.random.uniform((batch_size, 7, 11), dtype=np.float32)
    v = tf.random.uniform((batch_size, 7, 13), dtype=np.float32)

    mha = MultiHeadAttention(
        head_size=head_size, num_heads=num_heads, output_size=output_size
    )

    output = mha([q, k, v])

    assert output.shape[0] == batch_size
    assert output.shape[1] == q.shape[1]
    assert output.shape[2] == output_size


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/multihead_attention_test.py" startline="48" endline="65" pcid="881">
def test_output_shape():
    batch_size = 10
    num_heads = 8
    head_size = 12

    q = tf.random.uniform((batch_size, 5, 9), dtype=np.float32)
    k = tf.random.uniform((batch_size, 7, 11), dtype=np.float32)
    v = tf.random.uniform((batch_size, 7, 13), dtype=np.float32)

    mha = MultiHeadAttention(head_size=head_size, num_heads=num_heads)

    output = mha([q, k, v])

    assert output.shape[0] == batch_size
    assert output.shape[1] == q.shape[1]
    assert output.shape[2] == v.shape[2]


</source>
</class>

<class classid="49" nclones="3" nlines="16" similarity="78">
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/multihead_attention_test.py" startline="130" endline="153" pcid="885">
def test_attention_coefficients_shape():
    batch_size = 10
    num_heads = 8
    head_size = 12

    q = tf.random.uniform((batch_size, 5, 9), dtype=np.float32)
    k = tf.random.uniform((batch_size, 7, 11), dtype=np.float32)
    v = tf.random.uniform((batch_size, 7, 13), dtype=np.float32)

    mha = MultiHeadAttention(
        head_size=head_size, num_heads=num_heads, return_attn_coef=True
    )

    output, attn_coef = mha([q, k, v])

    assert attn_coef.shape[0] == batch_size
    assert attn_coef.shape[1] == num_heads
    assert attn_coef.shape[2] == q.shape[1]
    assert attn_coef.shape[3] == k.shape[1]

    assert output.shape[1] == q.shape[1]
    assert output.shape[2] == v.shape[2]


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/multihead_attention_test.py" startline="191" endline="213" pcid="888">
def test_no_value():
    batch_size = 10
    num_heads = 8
    head_size = 12

    q = tf.random.uniform((batch_size, 5, 9), dtype=np.float32)
    k = tf.random.uniform((batch_size, 7, 11), dtype=np.float32)

    mha = MultiHeadAttention(
        head_size=head_size, num_heads=num_heads, return_attn_coef=True
    )

    output, attn_coef = mha([q, k])

    assert attn_coef.shape[0] == batch_size
    assert attn_coef.shape[1] == num_heads
    assert attn_coef.shape[2] == q.shape[1]
    assert attn_coef.shape[3] == k.shape[1]

    assert output.shape[1] == q.shape[1]
    assert output.shape[2] == k.shape[2]


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/multihead_attention_test.py" startline="320" endline="345" pcid="893">
def test_mask():
    batch_size = 10
    num_heads = 8
    head_size = 12

    q = tf.random.uniform((batch_size, 5, 9), dtype=np.float32)
    k = tf.random.uniform((batch_size, 7, 11), dtype=np.float32)
    v = tf.random.uniform((batch_size, 7, 13), dtype=np.float32)
    mask = tf.random.uniform((batch_size, num_heads, 5, 7), dtype=np.float32) > 0.1

    mha = MultiHeadAttention(
        head_size=head_size, num_heads=num_heads, return_attn_coef=True
    )

    output, attn_coef = mha([q, k, v], mask=mask)

    assert attn_coef.shape[0] == batch_size
    assert attn_coef.shape[1] == num_heads
    assert attn_coef.shape[2] == q.shape[1]
    assert attn_coef.shape[3] == k.shape[1]

    assert output.shape[0] == batch_size
    assert output.shape[1] == q.shape[1]
    assert output.shape[2] == v.shape[2]

    np.testing.assert_array_equal((attn_coef != 0), mask)
</source>
</class>

<class classid="50" nclones="2" nlines="21" similarity="100">
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/adaptive_pooling_test.py" startline="32" endline="55" pcid="894">
def test_avg_1d():
    valid_input = np.arange(start=0.0, stop=12.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 12, 1))
    output = np.array([1.0, 4.0, 7.0, 10.0]).astype(np.float32)
    output = np.reshape(output, (1, 4, 1))
    test_utils.layer_test(
        AdaptiveAveragePooling1D,
        kwargs={"output_size": 4, "data_format": "channels_last"},
        input_data=valid_input,
        expected_output=output,
    )

    valid_input = np.arange(start=0.0, stop=12.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 1, 12))
    output = np.array([1.0, 4.0, 7.0, 10.0]).astype(np.float32)
    output = np.reshape(output, (1, 1, 4))
    test_utils.layer_test(
        AdaptiveAveragePooling1D,
        kwargs={"output_size": 4, "data_format": "channels_first"},
        input_data=valid_input,
        expected_output=output,
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/adaptive_pooling_test.py" startline="111" endline="134" pcid="897">
def test_max_1d():
    valid_input = np.arange(start=0.0, stop=12.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 12, 1))
    output = np.array([2.0, 5.0, 8.0, 11.0]).astype(np.float32)
    output = np.reshape(output, (1, 4, 1))
    test_utils.layer_test(
        AdaptiveMaxPooling1D,
        kwargs={"output_size": 4, "data_format": "channels_last"},
        input_data=valid_input,
        expected_output=output,
    )

    valid_input = np.arange(start=0.0, stop=12.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 1, 12))
    output = np.array([2.0, 5.0, 8.0, 11.0]).astype(np.float32)
    output = np.reshape(output, (1, 1, 4))
    test_utils.layer_test(
        AdaptiveMaxPooling1D,
        kwargs={"output_size": 4, "data_format": "channels_first"},
        input_data=valid_input,
        expected_output=output,
    )


</source>
</class>

<class classid="51" nclones="3" nlines="20" similarity="71">
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/adaptive_pooling_test.py" startline="57" endline="80" pcid="895">
def test_avg_2d():
    valid_input = np.arange(start=0.0, stop=40.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 4, 10, 1))
    output = np.array([[7.0, 12.0], [27.0, 32.0]]).astype(np.float32)
    output = np.reshape(output, (1, 2, 2, 1))
    test_utils.layer_test(
        AdaptiveAveragePooling2D,
        kwargs={"output_size": (2, 2), "data_format": "channels_last"},
        input_data=valid_input,
        expected_output=output,
    )

    valid_input = np.arange(start=0.0, stop=40.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 1, 4, 10))
    output = np.array([[7.0, 12.0], [27.0, 32.0]]).astype(np.float32)
    output = np.reshape(output, (1, 1, 2, 2))
    test_utils.layer_test(
        AdaptiveAveragePooling2D,
        kwargs={"output_size": (2, 2), "data_format": "channels_first"},
        input_data=valid_input,
        expected_output=output,
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/spatial_pyramid_pooling_test.py" startline="37" endline="58" pcid="941">
def test_spp_output_2d():
    inputs = np.arange(start=0.0, stop=16.0, step=1.0).astype(np.float32)
    inputs = np.reshape(inputs, (1, 4, 4, 1))
    output = np.array([[[7.5], [2.5], [4.5], [10.5], [12.5]]]).astype(np.float32)
    test_utils.layer_test(
        SpatialPyramidPooling2D,
        kwargs={"bins": [[1, 1], [2, 2]], "data_format": "channels_last"},
        input_data=inputs,
        expected_output=output,
    )

    inputs = np.arange(start=0.0, stop=16.0, step=1.0).astype(np.float32)
    inputs = np.reshape(inputs, (1, 1, 4, 4))
    output = np.array([[[7.5, 2.5, 4.5, 10.5, 12.5]]]).astype(np.float32)
    test_utils.layer_test(
        SpatialPyramidPooling2D,
        kwargs={"bins": [[1, 1], [2, 2]], "data_format": "channels_first"},
        input_data=inputs,
        expected_output=output,
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/adaptive_pooling_test.py" startline="136" endline="159" pcid="898">
def test_max_2d():
    valid_input = np.arange(start=0.0, stop=40.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 4, 10, 1))
    output = np.array([[14.0, 19.0], [34.0, 39.0]]).astype(np.float32)
    output = np.reshape(output, (1, 2, 2, 1))
    test_utils.layer_test(
        AdaptiveMaxPooling2D,
        kwargs={"output_size": (2, 2), "data_format": "channels_last"},
        input_data=valid_input,
        expected_output=output,
    )

    valid_input = np.arange(start=0.0, stop=40.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 1, 4, 10))
    output = np.array([[14.0, 19.0], [34.0, 39.0]]).astype(np.float32)
    output = np.reshape(output, (1, 1, 2, 2))
    test_utils.layer_test(
        AdaptiveMaxPooling2D,
        kwargs={"output_size": (2, 2), "data_format": "channels_first"},
        input_data=valid_input,
        expected_output=output,
    )


</source>
</class>

<class classid="52" nclones="2" nlines="23" similarity="100">
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/adaptive_pooling_test.py" startline="82" endline="109" pcid="896">
def test_avg_3d():
    valid_input = np.arange(start=0.0, stop=80.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 4, 10, 2, 1))
    output = np.array(
        [[[14.0, 15.0], [24.0, 25.0]], [[54.0, 55.0], [64.0, 65.0]]]
    ).astype(np.float32)
    output = np.reshape(output, (1, 2, 2, 2, 1))
    test_utils.layer_test(
        AdaptiveAveragePooling3D,
        kwargs={"output_size": (2, 2, 2), "data_format": "channels_last"},
        input_data=valid_input,
        expected_output=output,
    )

    valid_input = np.arange(start=0.0, stop=80.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 1, 4, 10, 2))
    output = np.array(
        [[[14.0, 15.0], [24.0, 25.0]], [[54.0, 55.0], [64.0, 65.0]]]
    ).astype(np.float32)
    output = np.reshape(output, (1, 1, 2, 2, 2))
    test_utils.layer_test(
        AdaptiveAveragePooling3D,
        kwargs={"output_size": (2, 2, 2), "data_format": "channels_first"},
        input_data=valid_input,
        expected_output=output,
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/adaptive_pooling_test.py" startline="161" endline="186" pcid="899">
def test_max_3d():
    valid_input = np.arange(start=0.0, stop=80.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 4, 10, 2, 1))
    output = np.array(
        [[[28.0, 29.0], [38.0, 39.0]], [[68.0, 69.0], [78.0, 79.0]]]
    ).astype(np.float32)
    output = np.reshape(output, (1, 2, 2, 2, 1))
    test_utils.layer_test(
        AdaptiveMaxPooling3D,
        kwargs={"output_size": (2, 2, 2), "data_format": "channels_last"},
        input_data=valid_input,
        expected_output=output,
    )

    valid_input = np.arange(start=0.0, stop=80.0, step=1.0).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 1, 4, 10, 2))
    output = np.array(
        [[[28.0, 29.0], [38.0, 39.0]], [[68.0, 69.0], [78.0, 79.0]]]
    ).astype(np.float32)
    output = np.reshape(output, (1, 1, 2, 2, 2))
    test_utils.layer_test(
        AdaptiveMaxPooling3D,
        kwargs={"output_size": (2, 2, 2), "data_format": "channels_first"},
        input_data=valid_input,
        expected_output=output,
    )
</source>
</class>

<class classid="53" nclones="2" nlines="17" similarity="88">
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/poincare_test.py" startline="38" endline="58" pcid="907">
def test_poincare_normalize():
    x_shape = [20, 7, 3]
    epsilon = 1e-5
    tol = 1e-6
    np.random.seed(1)
    inputs = np.random.random_sample(x_shape).astype(np.float32)

    for dim in range(len(x_shape)):
        outputs_expected = _poincare_normalize(inputs, dim, epsilon)

        outputs = test_utils.layer_test(
            PoincareNormalize,
            kwargs={"axis": dim, "epsilon": epsilon},
            input_data=inputs,
            expected_output=outputs_expected,
        )
        for y in outputs_expected, outputs:
            norm = np.linalg.norm(y, axis=dim)
            assert norm.max() <= 1.0 - epsilon + tol


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/poincare_test.py" startline="60" endline="78" pcid="908">
def test_poincare_normalize_dim_array():
    x_shape = [20, 7, 3]
    epsilon = 1e-5
    tol = 1e-6
    np.random.seed(1)
    inputs = np.random.random_sample(x_shape).astype(np.float32)
    dim = [1, 2]

    outputs_expected = _poincare_normalize(inputs, dim, epsilon)

    outputs = test_utils.layer_test(
        PoincareNormalize,
        kwargs={"axis": dim, "epsilon": epsilon},
        input_data=inputs,
        expected_output=outputs_expected,
    )
    for y in outputs_expected, outputs:
        norm = np.linalg.norm(y, axis=tuple(dim))
        assert norm.max() <= 1.0 - epsilon + tol
</source>
</class>

<class classid="54" nclones="2" nlines="15" similarity="80">
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/normalizations_test.py" startline="305" endline="326" pcid="965">
def test_groupnorm_convnet():
    np.random.seed(0x2020)
    model = tf.keras.models.Sequential()
    norm = GroupNormalization(axis=1, input_shape=(3, 4, 4), groups=3)
    model.add(norm)
    model.compile(loss="mse", optimizer="sgd")

    # centered = 5.0, variance  = 10.0
    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))
    model.fit(x, x, epochs=4, verbose=0)
    out = model.predict(x)
    out -= np.reshape(norm.beta.numpy(), (1, 3, 1, 1))
    out /= np.reshape(norm.gamma.numpy(), (1, 3, 1, 1))

    np.testing.assert_allclose(
        np.mean(out, axis=(0, 2, 3), dtype=np.float32), (0.0, 0.0, 0.0), atol=1e-1
    )
    np.testing.assert_allclose(
        np.std(out, axis=(0, 2, 3), dtype=np.float32), (1.0, 1.0, 1.0), atol=1e-1
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/normalizations_test.py" startline="328" endline="348" pcid="966">
def test_groupnorm_convnet_no_center_no_scale():
    np.random.seed(0x2020)
    model = tf.keras.models.Sequential()
    norm = GroupNormalization(
        axis=-1, groups=2, center=False, scale=False, input_shape=(3, 4, 4)
    )
    model.add(norm)
    model.compile(loss="mse", optimizer="sgd")
    # centered and variance are  5.0 and 10.0, respectively
    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))
    model.fit(x, x, epochs=4, verbose=0)
    out = model.predict(x)

    np.testing.assert_allclose(
        np.mean(out, axis=(0, 2, 3), dtype=np.float32), (0.0, 0.0, 0.0), atol=1e-1
    )
    np.testing.assert_allclose(
        np.std(out, axis=(0, 2, 3), dtype=np.float32), (1.0, 1.0, 1.0), atol=1e-1
    )


</source>
</class>

<class classid="55" nclones="2" nlines="10" similarity="100">
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/normalizations_test.py" startline="368" endline="380" pcid="969">
def test_with_beta(dtype):
    set_random_seed()
    inputs = np.random.rand(28, 28, 1).astype(dtype)
    inputs = np.expand_dims(inputs, axis=0)
    frn = FilterResponseNormalization(
        beta_initializer="ones", gamma_initializer="ones", dtype=dtype
    )
    frn.build((None, 28, 28, 1))
    observed = frn(inputs)
    expected = calculate_frn(inputs, beta=1, gamma=1, dtype=dtype)
    np.testing.assert_allclose(expected[0], observed[0])


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/normalizations_test.py" startline="383" endline="395" pcid="970">
def test_with_gamma(dtype):
    set_random_seed()
    inputs = np.random.rand(28, 28, 1).astype(dtype)
    inputs = np.expand_dims(inputs, axis=0)
    frn = FilterResponseNormalization(
        beta_initializer="zeros", gamma_initializer="ones", dtype=dtype
    )
    frn.build((None, 28, 28, 1))
    observed = frn(inputs)
    expected = calculate_frn(inputs, beta=0, gamma=1, dtype=dtype)
    np.testing.assert_allclose(expected[0], observed[0])


</source>
</class>

<class classid="56" nclones="2" nlines="23" similarity="80">
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/noisy_dense_test.py" startline="93" endline="115" pcid="983">
def test_noisy_dense_automatic_reset_noise():
    inputs = tf.convert_to_tensor(np.random.randint(low=0, high=7, size=(2, 2)))
    layer = NoisyDense(5, name="noise_dense_auto_reset_noise")
    layer(inputs)
    initial_eps_kernel = layer.eps_kernel
    initial_eps_bias = layer.eps_bias
    layer(inputs)
    new_eps_kernel = layer.eps_kernel
    new_eps_bias = layer.eps_bias
    np.testing.assert_raises(
        AssertionError,
        np.testing.assert_array_equal,
        initial_eps_kernel,
        new_eps_kernel,
    )
    np.testing.assert_raises(
        AssertionError,
        np.testing.assert_array_equal,
        initial_eps_bias,
        new_eps_bias,
    )


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/noisy_dense_test.py" startline="117" endline="141" pcid="984">
def test_noisy_dense_remove_noise():
    inputs = tf.convert_to_tensor(np.random.randint(low=0, high=7, size=(2, 2)))
    layer = NoisyDense(5, name="noise_dense_manual_reset_noise")
    layer(inputs)
    initial_eps_kernel = layer.eps_kernel
    initial_eps_bias = layer.eps_bias
    layer(inputs, reset_noise=False, remove_noise=True)
    new_eps_kernel = layer.eps_kernel
    new_eps_bias = layer.eps_bias
    kernel_zeros = tf.zeros(initial_eps_kernel.shape, dtype=initial_eps_kernel.dtype)
    bias_zeros = tf.zeros(initial_eps_bias.shape, dtype=initial_eps_kernel.dtype)
    np.testing.assert_raises(
        AssertionError,
        np.testing.assert_array_equal,
        initial_eps_kernel,
        new_eps_kernel,
    )
    np.testing.assert_raises(
        AssertionError,
        np.testing.assert_array_equal,
        initial_eps_bias,
        new_eps_bias,
    )
    np.testing.assert_array_equal(kernel_zeros, new_eps_kernel)
    np.testing.assert_array_equal(bias_zeros, new_eps_bias)
</source>
</class>

<class classid="57" nclones="2" nlines="29" similarity="86">
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/max_unpooling_2d_test.py" startline="75" endline="115" pcid="988">
def test_batch():
    valid_input = np.array(
        # fmt: off
        [
            1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,
            22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32
        ]
        # fmt: on
    ).astype(np.float32)
    valid_input = np.reshape(valid_input, (2, 2, 4, 2))
    indices = np.array(
        # fmt: off
        [
            2, 23, 8, 9, 12, 15, 40, 43, 44, 47, 72, 75, 80, 79, 62, 65, 0, 1, 30, 7,
            14, 35, 42, 21, 68, 69, 50, 51, 56, 5, 86, 63
        ]
        # fmt: on
    ).astype(np.float32)
    indices = np.reshape(indices, (2, 2, 4, 2))
    expected_output_shape = (2, 4, 12, 2)
    expected_output = np.array(
        # fmt: off
        [
            0, 0, 1, 0, 0, 0, 0, 0, 3, 4, 0, 0, 5, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 2, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 8, 9, 0, 0, 10, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 16, 0, 0, 0, 0, 0, 0, 11,
            0, 0, 12, 0, 0, 0, 14, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            17, 18, 0, 0, 0, 30, 0, 20, 0, 0, 0, 0, 0, 0, 21, 0, 0, 0, 0, 0, 0, 24, 0,
            0, 0, 0, 0, 0, 0, 0, 19, 0, 0, 0, 0, 22, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0,
            0, 0, 0, 27, 28, 0, 0, 0, 0, 29, 0, 0, 0, 0, 0, 0, 32, 0, 0, 0, 0, 25, 26,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 31, 0, 0, 0, 0, 0, 0, 0,
            0, 0
        ]
        # fmt: on
    ).astype(np.float32)
    expected_output = np.reshape(expected_output, expected_output_shape)

    output = MaxUnpooling2D(strides=(2, 3))(valid_input, indices)
    np.testing.assert_array_equal(expected_output, output)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/max_unpooling_2d_test.py" startline="117" endline="156" pcid="989">
def test_batch_and_padding_valid():
    valid_input = np.array(
        # fmt: off
        [
            1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,
            22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32
        ]
        # fmt: on
    ).astype(np.float32)
    valid_input = np.reshape(valid_input, (2, 2, 4, 2))
    indices = np.array(
        # fmt: off
        [
            2, 23, 8, 9, 12, 15, 40, 43, 44, 47, 72, 75, 80, 79, 62, 65, 0, 1, 30, 7,
            14, 35, 42, 21, 68, 69, 50, 51, 56, 5, 86, 63
        ]
        # fmt: on
    ).astype(np.float32)
    indices = np.reshape(indices, (2, 2, 4, 2))
    expected_output_shape = (2, 4, 11, 2)
    expected_output = np.array(
        # fmt: off
        [
            0, 0, 1, 0, 0, 0, 0, 0, 3, 4, 0, 0, 5, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 2, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 8, 9, 0, 0, 10, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 16, 0, 0, 0, 0, 0, 0, 11,
            0, 0, 12, 0, 0, 0, 14, 13, 0, 0, 0, 0, 0, 0, 0, 17, 18, 0, 0, 0, 30, 0,
            20, 0, 0, 0, 0, 0, 0, 21, 0, 0, 0, 0, 0, 0, 24, 0, 0, 0, 0, 0, 0, 0, 0,
            19, 0, 0, 0, 0, 22, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0, 0, 0, 0, 27, 28, 0,
            0, 0, 0, 29, 0, 0, 0, 0, 0, 0, 32, 0, 0, 0, 0, 25, 26, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 31, 0
        ]
        # fmt: on
    ).astype(np.float32)
    expected_output = np.reshape(expected_output, expected_output_shape)

    output = MaxUnpooling2D(strides=(2, 3), padding="VALID")(valid_input, indices)
    np.testing.assert_array_equal(expected_output, output)


</source>
</class>

<class classid="58" nclones="3" nlines="11" similarity="72">
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/max_unpooling_2d_test.py" startline="172" endline="188" pcid="991">
def test_with_pooling():
    valid_input = np.array(
        [1, 2, 4, 3, 8, 6, 7, 5, 9, 10, 12, 11, 13, 16, 15, 14]
    ).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 4, 4, 1))
    updates, indices = tf.nn.max_pool_with_argmax(
        valid_input, ksize=[2, 2], strides=[2, 2], padding="SAME"
    )
    expected_output = np.array(
        [0, 0, 0, 0, 8, 0, 7, 0, 0, 0, 0, 0, 0, 16, 15, 0]
    ).astype(np.float32)
    expected_output = np.reshape(expected_output, valid_input.shape)

    output = MaxUnpooling2D()(updates, indices).numpy()
    np.testing.assert_array_equal(expected_output, output)


</source>
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/max_unpooling_2d_test.py" startline="226" endline="240" pcid="994">
def test_padding_valid_complex():
    valid_input = np.array(
        [1, 2, 4, 3, 8, 6, 7, 5, 9, 10, 12, 11, 13, 16, 15, 14, 20, 18, 17, 19]
    ).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 4, 5, 1))
    updates, indices = tf.nn.max_pool_with_argmax(
        valid_input, ksize=[2, 3], strides=[2, 2], padding="VALID"
    )
    expected_output = np.array(
        [0, 0, 0, 0, 0, 0, 7, 0, 0, 10, 0, 0, 0, 0, 0, 0, 20, 0, 0, 19]
    ).astype(np.float32)
    expected_output = np.reshape(expected_output, valid_input.shape)

    output = MaxUnpooling2D(pool_size=(2, 3), padding="VALID")(updates, indices).numpy()
    np.testing.assert_array_equal(expected_output, output)
</source>
<source file="systems/addons-0.12.0/tensorflow_addons/layers/tests/max_unpooling_2d_test.py" startline="208" endline="224" pcid="993">
def test_padding_valid():
    valid_input = np.array(
        [1, 2, 4, 3, 8, 6, 7, 5, 9, 10, 12, 11, 13, 16, 15, 14]
    ).astype(np.float32)
    valid_input = np.reshape(valid_input, (1, 4, 4, 1))
    updates, indices = tf.nn.max_pool_with_argmax(
        valid_input, ksize=[2, 2], strides=[2, 2], padding="VALID"
    )
    expected_output = np.array(
        [0, 0, 0, 0, 8, 0, 7, 0, 0, 0, 0, 0, 0, 16, 15, 0]
    ).astype(np.float32)
    expected_output = np.reshape(expected_output, valid_input.shape)

    output = MaxUnpooling2D(padding="VALID")(updates, indices).numpy()
    np.testing.assert_array_equal(expected_output, output)


</source>
</class>

</clones>

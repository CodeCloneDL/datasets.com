<clones>
<systeminfo processor="nicad6" system="allennlp-2.9.0" granularity="functions-blind" threshold="0%" minlines="10" maxlines="2500"/>
<cloneinfo npcs="1677" npairs="54"/>
<runinfo ncompares="12207" cputime="44573"/>
<classinfo nclasses="38"/>

<class classid="1" nclones="2" nlines="19" similarity="100">
<source file="systems/allennlp-2.9.0/allennlp/modules/seq2vec_encoders/pytorch_seq2vec_wrapper.py" startline="125" endline="145" pcid="540">
    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        num_layers: int = 1,
        bias: bool = True,
        dropout: float = 0.0,
        bidirectional: bool = False,
    ):
        module = torch.nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            bias=bias,
            batch_first=True,
            dropout=dropout,
            bidirectional=bidirectional,
        )
        super().__init__(module=module)


</source>
<source file="systems/allennlp-2.9.0/allennlp/modules/seq2vec_encoders/pytorch_seq2vec_wrapper.py" startline="152" endline="172" pcid="541">
    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        num_layers: int = 1,
        bias: bool = True,
        dropout: float = 0.0,
        bidirectional: bool = False,
    ):
        module = torch.nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            bias=bias,
            batch_first=True,
            dropout=dropout,
            bidirectional=bidirectional,
        )
        super().__init__(module=module)


</source>
</class>

<class classid="2" nclones="2" nlines="20" similarity="100">
<source file="systems/allennlp-2.9.0/allennlp/modules/seq2seq_encoders/pytorch_seq2seq_wrapper.py" startline="131" endline="152" pcid="572">
    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        num_layers: int = 1,
        bias: bool = True,
        dropout: float = 0.0,
        bidirectional: bool = False,
        stateful: bool = False,
    ):
        module = torch.nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            bias=bias,
            batch_first=True,
            dropout=dropout,
            bidirectional=bidirectional,
        )
        super().__init__(module=module, stateful=stateful)


</source>
<source file="systems/allennlp-2.9.0/allennlp/modules/seq2seq_encoders/pytorch_seq2seq_wrapper.py" startline="159" endline="180" pcid="573">
    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        num_layers: int = 1,
        bias: bool = True,
        dropout: float = 0.0,
        bidirectional: bool = False,
        stateful: bool = False,
    ):
        module = torch.nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            bias=bias,
            batch_first=True,
            dropout=dropout,
            bidirectional=bidirectional,
        )
        super().__init__(module=module, stateful=stateful)


</source>
</class>

<class classid="3" nclones="2" nlines="13" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/boolean_accuracy_test.py" startline="73" endline="86" pcid="635">
    def test_distributed_accuracy(self):
        predictions = [torch.tensor([[0, 1], [2, 3]]), torch.tensor([[4, 5], [6, 7]])]
        targets = [torch.tensor([[0, 1], [2, 2]]), torch.tensor([[4, 5], [7, 7]])]
        metric_kwargs = {"predictions": predictions, "gold_labels": targets}
        desired_values = 0.5
        run_distributed_test(
            [-1, -1],
            global_distributed_metric,
            BooleanAccuracy(),
            metric_kwargs,
            desired_values,
            exact=True,
        )

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/boolean_accuracy_test.py" startline="101" endline="115" pcid="637">
    def test_multiple_distributed_runs(self):
        predictions = [torch.tensor([[0, 1], [2, 3]]), torch.tensor([[4, 5], [6, 7]])]
        targets = [torch.tensor([[0, 1], [2, 2]]), torch.tensor([[4, 5], [7, 7]])]
        metric_kwargs = {"predictions": predictions, "gold_labels": targets}
        desired_values = 0.5
        run_distributed_test(
            [-1, -1],
            multiple_runs,
            BooleanAccuracy(),
            metric_kwargs,
            desired_values,
            exact=True,
        )


</source>
</class>

<class classid="4" nclones="2" nlines="19" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/evalb_bracketing_scorer_test.py" startline="68" endline="87" pcid="645">
    def test_distributed_evalb(self):
        tree1 = Tree.fromstring("(S (VP (D the) (NP dog)) (VP (V chased) (NP (D the) (N cat))))")
        tree2 = Tree.fromstring("(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))")
        predicted_trees = [[tree1], [tree2]]
        gold_trees = [[tree2], [tree2]]
        metric_kwargs = {"predicted_trees": predicted_trees, "gold_trees": gold_trees}
        desired_values = {
            "evalb_recall": 0.875,
            "evalb_precision": 0.875,
            "evalb_f1_measure": 0.875,
        }
        run_distributed_test(
            [-1, -1],
            global_distributed_metric,
            EvalbBracketingScorer(),
            metric_kwargs,
            desired_values,
            exact=True,
        )

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/evalb_bracketing_scorer_test.py" startline="88" endline="108" pcid="646">
    def test_multiple_distributed_runs(self):
        tree1 = Tree.fromstring("(S (VP (D the) (NP dog)) (VP (V chased) (NP (D the) (N cat))))")
        tree2 = Tree.fromstring("(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))")
        predicted_trees = [[tree1], [tree2]]
        gold_trees = [[tree2], [tree2]]
        metric_kwargs = {"predicted_trees": predicted_trees, "gold_trees": gold_trees}
        desired_values = {
            "evalb_recall": 0.875,
            "evalb_precision": 0.875,
            "evalb_f1_measure": 0.875,
        }
        run_distributed_test(
            [-1, -1],
            multiple_runs,
            EvalbBracketingScorer(),
            metric_kwargs,
            desired_values,
            exact=False,
        )


</source>
</class>

<class classid="5" nclones="2" nlines="17" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/evalb_bracketing_scorer_test.py" startline="109" endline="130" pcid="647">
def multiple_runs(
    global_rank: int,
    world_size: int,
    gpu_id: Union[int, torch.device],
    metric: EvalbBracketingScorer,
    metric_kwargs: Dict[str, List[Any]],
    desired_values: Dict[str, Any],
    exact: Union[bool, Tuple[float, float]] = True,
):

    kwargs = {}
    # Use the arguments meant for the process with rank `global_rank`.
    for argname in metric_kwargs:
        kwargs[argname] = metric_kwargs[argname][global_rank]

    for i in range(200):
        metric(**kwargs)

    metric_values = metric.get_metric()

    for key in desired_values:
        assert desired_values[key] == metric_values[key]
</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/attachment_scores_test.py" startline="194" endline="215" pcid="714">
def multiple_runs(
    global_rank: int,
    world_size: int,
    gpu_id: Union[int, torch.device],
    metric: AttachmentScores,
    metric_kwargs: Dict[str, List[Any]],
    desired_values: Dict[str, Any],
    exact: Union[bool, Tuple[float, float]] = True,
):

    kwargs = {}
    # Use the arguments meant for the process with rank `global_rank`.
    for argname in metric_kwargs:
        kwargs[argname] = metric_kwargs[argname][global_rank]

    for i in range(200):
        metric(**kwargs)

    metrics = metric.get_metric()

    for key in metrics:
        assert desired_values[key] == metrics[key]
</source>
</class>

<class classid="6" nclones="2" nlines="24" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/unigram_recall_test.py" startline="69" endline="96" pcid="658">
    def test_distributed_accuracy(self):
        gold = torch.tensor([[2, 4, 8], [1, 2, 3], [7, 1, 1], [11, 14, 17]])
        predictions = torch.tensor(
            [
                [[2, 4, 8], [2, 5, 9]],  # 3/3
                [[-1, 2, 4], [3, 8, -1]],  # 2/2
                [[-1, -1, -1], [7, 2, -1]],  # 1/2
                [[12, 13, 17], [11, 13, 18]],  # 2/2
            ]
        )
        mask = torch.tensor(
            [[True, True, True], [False, True, True], [True, True, False], [True, False, True]]
        )
        gold = [gold[:2], gold[2:]]
        predictions = [predictions[:2], predictions[2:]]
        mask = [mask[:2], mask[2:]]

        metric_kwargs = {"predictions": predictions, "gold_labels": gold, "mask": mask}
        desired_values = {"unigram_recall": 7 / 8}
        run_distributed_test(
            [-1, -1],
            global_distributed_metric,
            UnigramRecall(),
            metric_kwargs,
            desired_values,
            exact=False,
        )

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/unigram_recall_test.py" startline="97" endline="125" pcid="659">
    def test_multiple_distributed_runs(self):
        gold = torch.tensor([[2, 4, 8], [1, 2, 3], [7, 1, 1], [11, 14, 17]])
        predictions = torch.tensor(
            [
                [[2, 4, 8], [2, 5, 9]],  # 3/3
                [[-1, 2, 4], [3, 8, -1]],  # 2/2
                [[-1, -1, -1], [7, 2, -1]],  # 1/2
                [[12, 13, 17], [11, 13, 18]],  # 2/2
            ]
        )
        mask = torch.tensor(
            [[True, True, True], [False, True, True], [True, True, False], [True, False, True]]
        )
        gold = [gold[:2], gold[2:]]
        predictions = [predictions[:2], predictions[2:]]
        mask = [mask[:2], mask[2:]]

        metric_kwargs = {"predictions": predictions, "gold_labels": gold, "mask": mask}
        desired_values = {"unigram_recall": 7 / 8}
        run_distributed_test(
            [-1, -1],
            multiple_runs,
            UnigramRecall(),
            metric_kwargs,
            desired_values,
            exact=True,
        )


</source>
</class>

<class classid="7" nclones="3" nlines="15" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/unigram_recall_test.py" startline="126" endline="144" pcid="660">
def multiple_runs(
    global_rank: int,
    world_size: int,
    gpu_id: Union[int, torch.device],
    metric: UnigramRecall,
    metric_kwargs: Dict[str, List[Any]],
    desired_values: Dict[str, Any],
    exact: Union[bool, Tuple[float, float]] = True,
):

    kwargs = {}
    # Use the arguments meant for the process with rank `global_rank`.
    for argname in metric_kwargs:
        kwargs[argname] = metric_kwargs[argname][global_rank]

    for i in range(200):
        metric(**kwargs)

    assert desired_values["unigram_recall"] == metric.get_metric()["unigram_recall"]
</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/sequence_accuracy_test.py" startline="126" endline="144" pcid="721">
def multiple_runs(
    global_rank: int,
    world_size: int,
    gpu_id: Union[int, torch.device],
    metric: SequenceAccuracy,
    metric_kwargs: Dict[str, List[Any]],
    desired_values: Dict[str, Any],
    exact: Union[bool, Tuple[float, float]] = True,
):

    kwargs = {}
    # Use the arguments meant for the process with rank `global_rank`.
    for argname in metric_kwargs:
        kwargs[argname] = metric_kwargs[argname][global_rank]

    for i in range(200):
        metric(**kwargs)

    assert desired_values["accuracy"] == metric.get_metric()["accuracy"]
</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/mean_absolute_error_test.py" startline="156" endline="174" pcid="693">
def multiple_runs(
    global_rank: int,
    world_size: int,
    gpu_id: Union[int, torch.device],
    metric: MeanAbsoluteError,
    metric_kwargs: Dict[str, List[Any]],
    desired_values: Dict[str, Any],
    exact: Union[bool, Tuple[float, float]] = True,
):

    kwargs = {}
    # Use the arguments meant for the process with rank `global_rank`.
    for argname in metric_kwargs:
        kwargs[argname] = metric_kwargs[argname][global_rank]

    for i in range(200):
        metric(**kwargs)

    assert desired_values["mae"] == metric.get_metric()["mae"]
</source>
</class>

<class classid="8" nclones="2" nlines="13" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/entropy_test.py" startline="53" endline="66" pcid="664">
    def test_distributed_entropy(self):
        logits = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 1]], dtype=torch.float)
        logits = [logits[0], logits[1]]
        metric_kwargs = {"logits": logits}
        desired_values = {"entropy": 1.38629436}
        run_distributed_test(
            [-1, -1],
            global_distributed_metric,
            Entropy(),
            metric_kwargs,
            desired_values,
            exact=False,
        )

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/entropy_test.py" startline="67" endline="81" pcid="665">
    def test_multiple_distributed_runs(self):
        logits = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 1]], dtype=torch.float)
        logits = [logits[0], logits[1]]
        metric_kwargs = {"logits": logits}
        desired_values = {"entropy": 1.38629436}
        run_distributed_test(
            [-1, -1],
            multiple_runs,
            Entropy(),
            metric_kwargs,
            desired_values,
            exact=False,
        )


</source>
</class>

<class classid="9" nclones="2" nlines="10" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_measure_test.py" startline="69" endline="82" pcid="670">
    def test_fbeta_multiclass_state(self, device: str):
        self.predictions = self.predictions.to(device)
        self.targets = self.targets.to(device)

        fbeta = FBetaMeasure()
        fbeta(self.predictions, self.targets)

        # check state
        assert_allclose(fbeta._pred_sum.tolist(), self.pred_sum)
        assert_allclose(fbeta._true_sum.tolist(), self.true_sum)
        assert_allclose(fbeta._true_positive_sum.tolist(), self.true_positive_sum)
        assert_allclose(fbeta._true_negative_sum.tolist(), self.true_negative_sum)
        assert_allclose(fbeta._total_sum.tolist(), self.total_sum)

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_multi_label_measure_test.py" startline="79" endline="92" pcid="742">
    def test_fbeta_multilabel_state(self, device: str):
        self.predictions = self.predictions.to(device)
        self.targets = self.targets.to(device)

        fbeta = FBetaMultiLabelMeasure()
        fbeta(self.predictions, self.targets)

        # check state
        assert_allclose(fbeta._pred_sum.tolist(), self.pred_sum)
        assert_allclose(fbeta._true_sum.tolist(), self.true_sum)
        assert_allclose(fbeta._true_positive_sum.tolist(), self.true_positive_sum)
        assert_allclose(fbeta._true_negative_sum.tolist(), self.true_negative_sum)
        assert_allclose(fbeta._total_sum.tolist(), self.total_sum)

</source>
</class>

<class classid="10" nclones="2" nlines="15" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_measure_test.py" startline="84" endline="104" pcid="671">
    def test_fbeta_multiclass_metric(self, device: str):
        self.predictions = self.predictions.to(device)
        self.targets = self.targets.to(device)

        fbeta = FBetaMeasure()
        fbeta(self.predictions, self.targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        # check value
        assert_allclose(precisions, self.desired_precisions)
        assert_allclose(recalls, self.desired_recalls)
        assert_allclose(fscores, self.desired_fscores)

        # check type
        assert isinstance(precisions, List)
        assert isinstance(recalls, List)
        assert isinstance(fscores, List)

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_multi_label_measure_test.py" startline="94" endline="114" pcid="743">
    def test_fbeta_multilabel_metric(self, device: str):
        self.predictions = self.predictions.to(device)
        self.targets = self.targets.to(device)

        fbeta = FBetaMultiLabelMeasure()
        fbeta(self.predictions, self.targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        # check value
        assert_allclose(precisions, self.desired_precisions)
        assert_allclose(recalls, self.desired_recalls)
        assert_allclose(fscores, self.desired_fscores)

        # check type
        assert isinstance(precisions, List)
        assert isinstance(recalls, List)
        assert isinstance(fscores, List)

</source>
</class>

<class classid="11" nclones="2" nlines="18" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_measure_test.py" startline="134" endline="158" pcid="673">
    def test_fbeta_multiclass_macro_average_metric(self, device: str):
        self.predictions = self.predictions.to(device)
        self.targets = self.targets.to(device)

        fbeta = FBetaMeasure(average="macro")
        fbeta(self.predictions, self.targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        # We keep the expected values in CPU because FBetaMeasure returns them in CPU.
        macro_precision = torch.tensor(self.desired_precisions).mean()
        macro_recall = torch.tensor(self.desired_recalls).mean()
        macro_fscore = torch.tensor(self.desired_fscores).mean()
        # check value
        assert_allclose(precisions, macro_precision)
        assert_allclose(recalls, macro_recall)
        assert_allclose(fscores, macro_fscore)

        # check type
        assert isinstance(precisions, float)
        assert isinstance(recalls, float)
        assert isinstance(fscores, float)

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_multi_label_measure_test.py" startline="166" endline="190" pcid="746">
    def test_fbeta_multilabel_macro_average_metric(self, device: str):
        self.predictions = self.predictions.to(device)
        self.targets = self.targets.to(device)

        fbeta = FBetaMultiLabelMeasure(average="macro")
        fbeta(self.predictions, self.targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        # We keep the expected values in CPU because FBetaMeasure returns them in CPU.
        macro_precision = torch.tensor(self.desired_precisions).mean()
        macro_recall = torch.tensor(self.desired_recalls).mean()
        macro_fscore = torch.tensor(self.desired_fscores).mean()
        # check value
        assert_allclose(precisions, macro_precision)
        assert_allclose(recalls, macro_recall)
        assert_allclose(fscores, macro_fscore)

        # check type
        assert isinstance(precisions, float)
        assert isinstance(recalls, float)
        assert isinstance(fscores, float)

</source>
</class>

<class classid="12" nclones="2" nlines="21" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_measure_test.py" startline="160" endline="186" pcid="674">
    def test_fbeta_multiclass_micro_average_metric(self, device: str):
        self.predictions = self.predictions.to(device)
        self.targets = self.targets.to(device)

        fbeta = FBetaMeasure(average="micro")
        fbeta(self.predictions, self.targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        # We keep the expected values in CPU because FBetaMeasure returns them in CPU.
        true_positives = torch.tensor([1, 1, 0, 1, 0], dtype=torch.float32)
        false_positives = torch.tensor([0, 3, 0, 0, 0], dtype=torch.float32)
        false_negatives = torch.tensor([2, 0, 0, 0, 1], dtype=torch.float32)
        mean_true_positive = true_positives.mean()
        mean_false_positive = false_positives.mean()
        mean_false_negative = false_negatives.mean()

        micro_precision = mean_true_positive / (mean_true_positive + mean_false_positive)
        micro_recall = mean_true_positive / (mean_true_positive + mean_false_negative)
        micro_fscore = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall)
        # check value
        assert_allclose(precisions, micro_precision)
        assert_allclose(recalls, micro_recall)
        assert_allclose(fscores, micro_fscore)

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_multi_label_measure_test.py" startline="192" endline="218" pcid="747">
    def test_fbeta_multilabel_micro_average_metric(self, device: str):
        self.predictions = self.predictions.to(device)
        self.targets = self.targets.to(device)

        fbeta = FBetaMultiLabelMeasure(average="micro")
        fbeta(self.predictions, self.targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        # We keep the expected values in CPU because FBetaMeasure returns them in CPU.
        true_positives = torch.tensor([3, 3, 2, 4, 0], dtype=torch.float32)
        false_positives = torch.tensor([1, 0, 1, 0, 1], dtype=torch.float32)
        false_negatives = torch.tensor([1, 2, 0, 0, 0], dtype=torch.float32)
        mean_true_positive = true_positives.mean()
        mean_false_positive = false_positives.mean()
        mean_false_negative = false_negatives.mean()

        micro_precision = mean_true_positive / (mean_true_positive + mean_false_positive)
        micro_recall = mean_true_positive / (mean_true_positive + mean_false_negative)
        micro_fscore = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall)
        # check value
        assert_allclose(precisions, micro_precision)
        assert_allclose(recalls, micro_recall)
        assert_allclose(fscores, micro_fscore)

</source>
</class>

<class classid="13" nclones="2" nlines="15" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_measure_test.py" startline="188" endline="207" pcid="675">
    def test_fbeta_multiclass_with_explicit_labels(self, device: str):
        self.predictions = self.predictions.to(device)
        self.targets = self.targets.to(device)

        # same prediction but with and explicit label ordering
        fbeta = FBetaMeasure(labels=[4, 3, 2, 1, 0])
        fbeta(self.predictions, self.targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        desired_precisions = self.desired_precisions[::-1]
        desired_recalls = self.desired_recalls[::-1]
        desired_fscores = self.desired_fscores[::-1]
        # check value
        assert_allclose(precisions, desired_precisions)
        assert_allclose(recalls, desired_recalls)
        assert_allclose(fscores, desired_fscores)

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_multi_label_measure_test.py" startline="220" endline="239" pcid="748">
    def test_fbeta_multilabel_with_explicit_labels(self, device: str):
        self.predictions = self.predictions.to(device)
        self.targets = self.targets.to(device)

        # same prediction but with and explicit label ordering
        fbeta = FBetaMultiLabelMeasure(labels=[4, 3, 2, 1, 0])
        fbeta(self.predictions, self.targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        desired_precisions = self.desired_precisions[::-1]
        desired_recalls = self.desired_recalls[::-1]
        desired_fscores = self.desired_fscores[::-1]
        # check value
        assert_allclose(precisions, desired_precisions)
        assert_allclose(recalls, desired_recalls)
        assert_allclose(fscores, desired_fscores)

</source>
</class>

<class classid="14" nclones="2" nlines="16" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_measure_test.py" startline="209" endline="230" pcid="676">
    def test_fbeta_multiclass_with_macro_average(self, device: str):
        self.predictions = self.predictions.to(device)
        self.targets = self.targets.to(device)

        labels = [0, 1]
        fbeta = FBetaMeasure(average="macro", labels=labels)
        fbeta(self.predictions, self.targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        # We keep the expected values in CPU because FBetaMeasure returns them in CPU.
        macro_precision = torch.tensor(self.desired_precisions)[labels].mean()
        macro_recall = torch.tensor(self.desired_recalls)[labels].mean()
        macro_fscore = torch.tensor(self.desired_fscores)[labels].mean()

        # check value
        assert_allclose(precisions, macro_precision)
        assert_allclose(recalls, macro_recall)
        assert_allclose(fscores, macro_fscore)

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_multi_label_measure_test.py" startline="241" endline="262" pcid="749">
    def test_fbeta_multilabel_with_macro_average(self, device: str):
        self.predictions = self.predictions.to(device)
        self.targets = self.targets.to(device)

        labels = [0, 1]
        fbeta = FBetaMultiLabelMeasure(average="macro", labels=labels)
        fbeta(self.predictions, self.targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        # We keep the expected values in CPU because FBetaMeasure returns them in CPU.
        macro_precision = torch.tensor(self.desired_precisions)[labels].mean()
        macro_recall = torch.tensor(self.desired_recalls)[labels].mean()
        macro_fscore = torch.tensor(self.desired_fscores)[labels].mean()

        # check value
        assert_allclose(precisions, macro_precision)
        assert_allclose(recalls, macro_recall)
        assert_allclose(fscores, macro_fscore)

</source>
</class>

<class classid="15" nclones="2" nlines="22" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_measure_test.py" startline="232" endline="259" pcid="677">
    def test_fbeta_multiclass_with_micro_average(self, device: str):
        self.predictions = self.predictions.to(device)
        self.targets = self.targets.to(device)

        labels = [1, 3]
        fbeta = FBetaMeasure(average="micro", labels=labels)
        fbeta(self.predictions, self.targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        # We keep the expected values in CPU because FBetaMeasure returns them in CPU.
        true_positives = torch.tensor([1, 1], dtype=torch.float32)
        false_positives = torch.tensor([3, 0], dtype=torch.float32)
        false_negatives = torch.tensor([0, 0], dtype=torch.float32)
        mean_true_positive = true_positives.mean()
        mean_false_positive = false_positives.mean()
        mean_false_negative = false_negatives.mean()

        micro_precision = mean_true_positive / (mean_true_positive + mean_false_positive)
        micro_recall = mean_true_positive / (mean_true_positive + mean_false_negative)
        micro_fscore = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall)
        # check value
        assert_allclose(precisions, micro_precision)
        assert_allclose(recalls, micro_recall)
        assert_allclose(fscores, micro_fscore)

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_multi_label_measure_test.py" startline="264" endline="291" pcid="750">
    def test_fbeta_multilabel_with_micro_average(self, device: str):
        self.predictions = self.predictions.to(device)
        self.targets = self.targets.to(device)

        labels = [1, 3]
        fbeta = FBetaMultiLabelMeasure(average="micro", labels=labels)
        fbeta(self.predictions, self.targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        # We keep the expected values in CPU because FBetaMeasure returns them in CPU.
        true_positives = torch.tensor([3, 4], dtype=torch.float32)
        false_positives = torch.tensor([0, 0], dtype=torch.float32)
        false_negatives = torch.tensor([2, 0], dtype=torch.float32)
        mean_true_positive = true_positives.mean()
        mean_false_positive = false_positives.mean()
        mean_false_negative = false_negatives.mean()

        micro_precision = mean_true_positive / (mean_true_positive + mean_false_positive)
        micro_recall = mean_true_positive / (mean_true_positive + mean_false_negative)
        micro_fscore = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall)
        # check value
        assert_allclose(precisions, micro_precision)
        assert_allclose(recalls, micro_recall)
        assert_allclose(fscores, micro_fscore)

</source>
</class>

<class classid="16" nclones="4" nlines="12" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_measure_test.py" startline="301" endline="317" pcid="680">
    def test_fbeta_handles_no_prediction_false_last_class(self, device: str):

        predictions = torch.tensor([[0.65, 0.35], [0.0, 0.0]], device=device)
        # preds = [0, NA]
        targets = torch.tensor([0, 0], device=device)

        fbeta = FBetaMeasure()
        fbeta(predictions, targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        assert_allclose(precisions, [1.0, 0.0])
        assert_allclose(recalls, [0.5, 0.0])
        assert_allclose(fscores, [0.6667, 0.0])

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_measure_test.py" startline="319" endline="335" pcid="681">
    def test_fbeta_handles_no_prediction_true_last_class(self, device: str):

        predictions = torch.tensor([[0.65, 0.35], [0.0, 0.0]], device=device)
        # preds = [0, NA]
        targets = torch.tensor([0, 1], device=device)

        fbeta = FBetaMeasure()
        fbeta(predictions, targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        assert_allclose(precisions, [1.0, 0.0])
        assert_allclose(recalls, [1.0, 0.0])
        assert_allclose(fscores, [1.0, 0.0])

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_measure_test.py" startline="337" endline="353" pcid="682">
    def test_fbeta_handles_no_prediction_true_other_class(self, device: str):

        predictions = torch.tensor([[0.65, 0.35], [0.0, 0.0]], device=device)
        # preds = [0, NA]
        targets = torch.tensor([1, 0], device=device)

        fbeta = FBetaMeasure()
        fbeta(predictions, targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        assert_allclose(precisions, [0.0, 0.0])
        assert_allclose(recalls, [0.0, 0.0])
        assert_allclose(fscores, [0.0, 0.0])

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_measure_test.py" startline="355" endline="371" pcid="683">
    def test_fbeta_handles_no_prediction_true_all_class(self, device: str):

        predictions = torch.tensor([[0.65, 0.35], [0.0, 0.0]], device=device)
        # preds = [0, NA]
        targets = torch.tensor([1, 1], device=device)

        fbeta = FBetaMeasure()
        fbeta(predictions, targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        assert_allclose(precisions, [0.0, 0.0])
        assert_allclose(recalls, [0.0, 0.0])
        assert_allclose(fscores, [0.0, 0.0])

</source>
</class>

<class classid="17" nclones="2" nlines="22" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_measure_test.py" startline="372" endline="396" pcid="684">
    def test_distributed_fbeta_measure(self):
        predictions = [
            torch.tensor(
                [[0.35, 0.25, 0.1, 0.1, 0.2], [0.1, 0.6, 0.1, 0.2, 0.0], [0.1, 0.6, 0.1, 0.2, 0.0]]
            ),
            torch.tensor(
                [[0.1, 0.5, 0.1, 0.2, 0.0], [0.1, 0.2, 0.1, 0.7, 0.0], [0.1, 0.6, 0.1, 0.2, 0.0]]
            ),
        ]
        targets = [torch.tensor([0, 4, 1]), torch.tensor([0, 3, 0])]
        metric_kwargs = {"predictions": predictions, "gold_labels": targets}
        desired_metrics = {
            "precision": self.desired_precisions,
            "recall": self.desired_recalls,
            "fscore": self.desired_fscores,
        }
        run_distributed_test(
            [-1, -1],
            global_distributed_metric,
            FBetaMeasure(),
            metric_kwargs,
            desired_metrics,
            exact=False,
        )

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_measure_test.py" startline="397" endline="422" pcid="685">
    def test_multiple_distributed_runs(self):
        predictions = [
            torch.tensor(
                [[0.35, 0.25, 0.1, 0.1, 0.2], [0.1, 0.6, 0.1, 0.2, 0.0], [0.1, 0.6, 0.1, 0.2, 0.0]]
            ),
            torch.tensor(
                [[0.1, 0.5, 0.1, 0.2, 0.0], [0.1, 0.2, 0.1, 0.7, 0.0], [0.1, 0.6, 0.1, 0.2, 0.0]]
            ),
        ]
        targets = [torch.tensor([0, 4, 1]), torch.tensor([0, 3, 0])]
        metric_kwargs = {"predictions": predictions, "gold_labels": targets}
        desired_metrics = {
            "precision": self.desired_precisions,
            "recall": self.desired_recalls,
            "fscore": self.desired_fscores,
        }
        run_distributed_test(
            [-1, -1],
            multiple_runs,
            FBetaMeasure(),
            metric_kwargs,
            desired_metrics,
            exact=False,
        )


</source>
</class>

<class classid="18" nclones="2" nlines="17" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_measure_test.py" startline="423" endline="444" pcid="686">
def multiple_runs(
    global_rank: int,
    world_size: int,
    gpu_id: Union[int, torch.device],
    metric: FBetaMeasure,
    metric_kwargs: Dict[str, List[Any]],
    desired_values: Dict[str, Any],
    exact: Union[bool, Tuple[float, float]] = True,
):

    kwargs = {}
    # Use the arguments meant for the process with rank `global_rank`.
    for argname in metric_kwargs:
        kwargs[argname] = metric_kwargs[argname][global_rank]

    for i in range(200):
        metric(**kwargs)

    metric_values = metric.get_metric()

    for key in desired_values:
        assert_allclose(desired_values[key], metric_values[key])
</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_multi_label_measure_test.py" startline="483" endline="504" pcid="759">
def multiple_runs(
    global_rank: int,
    world_size: int,
    gpu_id: Union[int, torch.device],
    metric: FBetaMultiLabelMeasure,
    metric_kwargs: Dict[str, List[Any]],
    desired_values: Dict[str, Any],
    exact: Union[bool, Tuple[float, float]] = True,
):

    kwargs = {}
    # Use the arguments meant for the process with rank `global_rank`.
    for argname in metric_kwargs:
        kwargs[argname] = metric_kwargs[argname][global_rank]

    for i in range(200):
        metric(**kwargs)

    metric_values = metric.get_metric()

    for key in desired_values:
        assert_allclose(desired_values[key], metric_values[key])
</source>
</class>

<class classid="19" nclones="2" nlines="35" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/mean_absolute_error_test.py" startline="75" endline="114" pcid="691">
    def test_distributed_accuracy(self):
        predictions = [
            torch.tensor(
                [
                    [1.0, 1.5, 1.0],
                    [2.0, 3.0, 3.5],
                ]
            ),
            torch.tensor(
                [
                    [4.0, 5.0, 5.5],
                    [6.0, 7.0, 7.5],
                ]
            ),
        ]
        targets = [
            torch.tensor(
                [
                    [0.0, 1.0, 0.0],
                    [2.0, 2.0, 0.0],
                ]
            ),
            torch.tensor(
                [
                    [4.0, 5.0, 0.0],
                    [7.0, 7.0, 0.0],
                ]
            ),
        ]
        metric_kwargs = {"predictions": predictions, "gold_labels": targets}
        desired_values = {"mae": 21.0 / 12.0}
        run_distributed_test(
            [-1, -1],
            global_distributed_metric,
            MeanAbsoluteError(),
            metric_kwargs,
            desired_values,
            exact=True,
        )

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/mean_absolute_error_test.py" startline="115" endline="155" pcid="692">
    def test_multiple_distributed_runs(self):
        predictions = [
            torch.tensor(
                [
                    [1.0, 1.5, 1.0],
                    [2.0, 3.0, 3.5],
                ]
            ),
            torch.tensor(
                [
                    [4.0, 5.0, 5.5],
                    [6.0, 7.0, 7.5],
                ]
            ),
        ]
        targets = [
            torch.tensor(
                [
                    [0.0, 1.0, 0.0],
                    [2.0, 2.0, 0.0],
                ]
            ),
            torch.tensor(
                [
                    [4.0, 5.0, 0.0],
                    [7.0, 7.0, 0.0],
                ]
            ),
        ]
        metric_kwargs = {"predictions": predictions, "gold_labels": targets}
        desired_values = {"mae": 21.0 / 12.0}
        run_distributed_test(
            [-1, -1],
            multiple_runs,
            MeanAbsoluteError(),
            metric_kwargs,
            desired_values,
            exact=True,
        )


</source>
</class>

<class classid="20" nclones="2" nlines="33" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/attachment_scores_test.py" startline="113" endline="152" pcid="712">
    def test_distributed_attachment_scores(self):
        predictions = [torch.Tensor([[0, 1, 3, 5, 2, 4]]), torch.Tensor([[0, 3, 2, 1, 0, 0]])]

        gold_indices = [torch.Tensor([[0, 1, 3, 5, 2, 4]]), torch.Tensor([[0, 3, 2, 1, 0, 0]])]

        label_predictions = [
            torch.Tensor([[0, 5, 2, 3, 3, 3]]),
            torch.Tensor([[7, 4, 8, 2, 0, 0]]),
        ]

        gold_labels = [torch.Tensor([[0, 5, 2, 1, 4, 2]]), torch.Tensor([[0, 4, 8, 2, 0, 0]])]

        mask = [
            torch.tensor([[True, True, True, True, True, True]]),
            torch.tensor([[True, True, True, True, False, False]]),
        ]

        metric_kwargs = {
            "predicted_indices": predictions,
            "gold_indices": gold_indices,
            "predicted_labels": label_predictions,
            "gold_labels": gold_labels,
            "mask": mask,
        }

        desired_metrics = {
            "UAS": 1.0,
            "LAS": 0.6,
            "UEM": 1.0,
            "LEM": 0.0,
        }
        run_distributed_test(
            [-1, -1],
            global_distributed_metric,
            AttachmentScores(),
            metric_kwargs,
            desired_metrics,
            exact=True,
        )

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/attachment_scores_test.py" startline="153" endline="193" pcid="713">
    def test_multiple_distributed_runs(self):
        predictions = [torch.Tensor([[0, 1, 3, 5, 2, 4]]), torch.Tensor([[0, 3, 2, 1, 0, 0]])]

        gold_indices = [torch.Tensor([[0, 1, 3, 5, 2, 4]]), torch.Tensor([[0, 3, 2, 1, 0, 0]])]

        label_predictions = [
            torch.Tensor([[0, 5, 2, 3, 3, 3]]),
            torch.Tensor([[7, 4, 8, 2, 0, 0]]),
        ]

        gold_labels = [torch.Tensor([[0, 5, 2, 1, 4, 2]]), torch.Tensor([[0, 4, 8, 2, 0, 0]])]

        mask = [
            torch.tensor([[True, True, True, True, True, True]]),
            torch.tensor([[True, True, True, True, False, False]]),
        ]

        metric_kwargs = {
            "predicted_indices": predictions,
            "gold_indices": gold_indices,
            "predicted_labels": label_predictions,
            "gold_labels": gold_labels,
            "mask": mask,
        }

        desired_metrics = {
            "UAS": 1.0,
            "LAS": 0.6,
            "UEM": 1.0,
            "LEM": 0.0,
        }
        run_distributed_test(
            [-1, -1],
            multiple_runs,
            AttachmentScores(),
            metric_kwargs,
            desired_metrics,
            exact=True,
        )


</source>
</class>

<class classid="21" nclones="2" nlines="25" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/sequence_accuracy_test.py" startline="69" endline="96" pcid="719">
    def test_distributed_sequence_accuracy(self):
        gold = torch.tensor([[1, 2, 3], [2, 4, 8], [0, 1, 1], [11, 13, 17]])
        predictions = torch.tensor(
            [
                [[1, 2, 3], [1, 2, -1]],
                [[2, 4, 8], [2, 5, 9]],
                [[-1, -1, -1], [0, 1, -1]],
                [[12, 13, 17], [11, 13, 18]],
            ]
        )
        mask = torch.tensor(
            [[False, True, True], [True, True, True], [True, True, False], [True, False, True]],
        )
        gold = [gold[:2], gold[2:]]
        predictions = [predictions[:2], predictions[2:]]
        mask = [mask[:2], mask[2:]]

        metric_kwargs = {"predictions": predictions, "gold_labels": gold, "mask": mask}
        desired_values = {"accuracy": 3 / 4}
        run_distributed_test(
            [-1, -1],
            global_distributed_metric,
            SequenceAccuracy(),
            metric_kwargs,
            desired_values,
            exact=False,
        )

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/sequence_accuracy_test.py" startline="97" endline="125" pcid="720">
    def test_multiple_distributed_runs(self):
        gold = torch.tensor([[1, 2, 3], [2, 4, 8], [0, 1, 1], [11, 13, 17]])
        predictions = torch.tensor(
            [
                [[1, 2, 3], [1, 2, -1]],
                [[2, 4, 8], [2, 5, 9]],
                [[-1, -1, -1], [0, 1, -1]],
                [[12, 13, 17], [11, 13, 18]],
            ]
        )
        mask = torch.tensor(
            [[False, True, True], [True, True, True], [True, True, False], [True, False, True]],
        )
        gold = [gold[:2], gold[2:]]
        predictions = [predictions[:2], predictions[2:]]
        mask = [mask[:2], mask[2:]]

        metric_kwargs = {"predictions": predictions, "gold_labels": gold, "mask": mask}
        desired_values = {"accuracy": 3 / 4}
        run_distributed_test(
            [-1, -1],
            multiple_runs,
            SequenceAccuracy(),
            metric_kwargs,
            desired_values,
            exact=True,
        )


</source>
</class>

<class classid="22" nclones="4" nlines="12" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_multi_label_measure_test.py" startline="339" endline="355" pcid="753">
    def test_fbeta_multilabel_handles_no_prediction_false_last_class(self, device: str):

        predictions = torch.tensor([[0.65, 0.35], [0.0, 0.0]], device=device)
        # preds = [0, NA]
        targets = torch.tensor([[1, 0], [1, 0]], device=device)

        fbeta = FBetaMultiLabelMeasure()
        fbeta(predictions, targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        assert_allclose(precisions, [1.0, 0.0])
        assert_allclose(recalls, [0.5, 0.0])
        assert_allclose(fscores, [0.6667, 0.0])

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_multi_label_measure_test.py" startline="375" endline="390" pcid="755">
    def test_fbeta_multilabel_handles_no_prediction_true_other_class(self, device: str):
        predictions = torch.tensor([[0.65, 0.35], [0.0, 0.0]], device=device)
        # preds = [0, NA]
        targets = torch.tensor([[0, 1], [1, 0]], device=device)

        fbeta = FBetaMultiLabelMeasure()
        fbeta(predictions, targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        assert_allclose(precisions, [0.0, 0.0])
        assert_allclose(recalls, [0.0, 0.0])
        assert_allclose(fscores, [0.0, 0.0])

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_multi_label_measure_test.py" startline="392" endline="407" pcid="756">
    def test_fbeta_multilabel_handles_no_prediction_true_all_class(self, device: str):
        predictions = torch.tensor([[0.65, 0.35], [0.0, 0.0]], device=device)
        # preds = [0, NA]
        targets = torch.tensor([[0, 1], [0, 1]], device=device)

        fbeta = FBetaMultiLabelMeasure()
        fbeta(predictions, targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        assert_allclose(precisions, [0.0, 0.0])
        assert_allclose(recalls, [0.0, 0.0])
        assert_allclose(fscores, [0.0, 0.0])

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_multi_label_measure_test.py" startline="357" endline="373" pcid="754">
    def test_fbeta_multilabel_handles_no_prediction_true_last_class(self, device: str):

        predictions = torch.tensor([[0.65, 0.35], [0.0, 0.0]], device=device)
        # preds = [0, NA]
        targets = torch.tensor([[1, 0], [0, 1]], device=device)

        fbeta = FBetaMultiLabelMeasure()
        fbeta(predictions, targets)
        metric = fbeta.get_metric()
        precisions = metric["precision"]
        recalls = metric["recall"]
        fscores = metric["fscore"]

        assert_allclose(precisions, [1.0, 0.0])
        assert_allclose(recalls, [1.0, 0.0])
        assert_allclose(fscores, [1.0, 0.0])

</source>
</class>

<class classid="23" nclones="2" nlines="33" similarity="100">
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_multi_label_measure_test.py" startline="408" endline="445" pcid="757">
    def test_distributed_fbeta_multilabel_measure(self):
        predictions = [
            torch.tensor(
                [
                    [0.55, 0.25, 0.10, 0.10, 0.20],
                    [0.10, 0.60, 0.10, 0.95, 0.00],
                    [0.90, 0.80, 0.75, 0.80, 0.00],
                ]
            ),
            torch.tensor(
                [
                    [0.49, 0.50, 0.95, 0.55, 0.00],
                    [0.60, 0.49, 0.60, 0.65, 0.85],
                    [0.85, 0.40, 0.10, 0.20, 0.00],
                ]
            ),
        ]

        targets = [
            torch.tensor([[1, 1, 0, 0, 0], [0, 1, 0, 1, 0], [1, 1, 0, 1, 0]]),
            torch.tensor([[1, 1, 1, 1, 0], [1, 1, 1, 1, 0], [0, 0, 0, 0, 0]]),
        ]

        metric_kwargs = {"predictions": predictions, "gold_labels": targets}
        desired_metrics = {
            "precision": self.desired_precisions,
            "recall": self.desired_recalls,
            "fscore": self.desired_fscores,
        }
        run_distributed_test(
            [-1, -1],
            global_distributed_metric,
            FBetaMultiLabelMeasure(),
            metric_kwargs,
            desired_metrics,
            exact=False,
        )

</source>
<source file="systems/allennlp-2.9.0/tests/training/metrics/fbeta_multi_label_measure_test.py" startline="446" endline="482" pcid="758">
    def test_multiple_distributed_runs(self):
        predictions = [
            torch.tensor(
                [
                    [0.55, 0.25, 0.10, 0.10, 0.20],
                    [0.10, 0.60, 0.10, 0.95, 0.00],
                    [0.90, 0.80, 0.75, 0.80, 0.00],
                ]
            ),
            torch.tensor(
                [
                    [0.49, 0.50, 0.95, 0.55, 0.00],
                    [0.60, 0.49, 0.60, 0.65, 0.85],
                    [0.85, 0.40, 0.10, 0.20, 0.00],
                ]
            ),
        ]
        targets = [
            torch.tensor([[1, 1, 0, 0, 0], [0, 1, 0, 1, 0], [1, 1, 0, 1, 0]]),
            torch.tensor([[1, 1, 1, 1, 0], [1, 1, 1, 1, 0], [0, 0, 0, 0, 0]]),
        ]
        metric_kwargs = {"predictions": predictions, "gold_labels": targets}
        desired_metrics = {
            "precision": self.desired_precisions,
            "recall": self.desired_recalls,
            "fscore": self.desired_fscores,
        }
        run_distributed_test(
            [-1, -1],
            multiple_runs,
            FBetaMultiLabelMeasure(),
            metric_kwargs,
            desired_metrics,
            exact=False,
        )


</source>
</class>

<class classid="24" nclones="2" nlines="56" similarity="100">
<source file="systems/allennlp-2.9.0/tests/data/token_indexers/elmo_indexer_test.py" startline="12" endline="68" pcid="869">
    def test_bos_to_char_ids(self):
        indexer = ELMoTokenCharactersIndexer()
        indices = indexer.tokens_to_indices([Token("<S>")], Vocabulary())
        expected_indices = [
            259,
            257,
            260,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
        ]
        assert indices == {"elmo_tokens": [expected_indices]}

</source>
<source file="systems/allennlp-2.9.0/tests/data/token_indexers/elmo_indexer_test.py" startline="69" endline="125" pcid="870">
    def test_eos_to_char_ids(self):
        indexer = ELMoTokenCharactersIndexer()
        indices = indexer.tokens_to_indices([Token("</S>")], Vocabulary())
        expected_indices = [
            259,
            258,
            260,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
            261,
        ]
        assert indices == {"elmo_tokens": [expected_indices]}

</source>
</class>

<class classid="25" nclones="2" nlines="17" similarity="100">
<source file="systems/allennlp-2.9.0/tests/data/tokenizers/pretrained_transformer_tokenizer_test.py" startline="10" endline="28" pcid="882">
    def test_splits_roberta(self):
        tokenizer = PretrainedTransformerTokenizer("roberta-base")

        sentence = "A, <mask> AllenNLP sentence."
        expected_tokens = [
            "<s>",
            "A",
            ",",
            "<mask>",
            "ĠAllen",
            "N",
            "LP",
            "Ġsentence",
            ".",
            "</s>",
        ]
        tokens = [t.text for t in tokenizer.tokenize(sentence)]
        assert tokens == expected_tokens

</source>
<source file="systems/allennlp-2.9.0/tests/data/tokenizers/pretrained_transformer_tokenizer_test.py" startline="29" endline="47" pcid="883">
    def test_splits_cased_bert(self):
        tokenizer = PretrainedTransformerTokenizer("bert-base-cased")

        sentence = "A, [MASK] AllenNLP sentence."
        expected_tokens = [
            "[CLS]",
            "A",
            ",",
            "[MASK]",
            "Allen",
            "##NL",
            "##P",
            "sentence",
            ".",
            "[SEP]",
        ]
        tokens = [t.text for t in tokenizer.tokenize(sentence)]
        assert tokens == expected_tokens

</source>
</class>

<class classid="26" nclones="2" nlines="22" similarity="100">
<source file="systems/allennlp-2.9.0/tests/data/tokenizers/pretrained_transformer_tokenizer_test.py" startline="94" endline="116" pcid="886">
    def test_token_idx_bert_uncased(self):
        sentence = "A, naïve [MASK] AllenNLP sentence."
        expected_tokens = [
            "[CLS]",
            "a",
            ",",
            "naive",  # BERT normalizes this away
            "[MASK]",
            "allen",
            "##nl",
            "##p",
            "sentence",
            ".",
            "[SEP]",
        ]
        expected_idxs = [None, 0, 1, 3, 9, 16, 21, 23, 25, 33, None]
        tokenizer = PretrainedTransformerTokenizer("bert-base-uncased")
        tokenized = tokenizer.tokenize(sentence)
        tokens = [t.text for t in tokenized]
        assert tokens == expected_tokens
        idxs = [t.idx for t in tokenized]
        assert idxs == expected_idxs

</source>
<source file="systems/allennlp-2.9.0/tests/data/tokenizers/pretrained_transformer_tokenizer_test.py" startline="160" endline="182" pcid="890">
    def test_token_idx_roberta(self):
        sentence = "A, naïve <mask> AllenNLP sentence."
        expected_tokens = [
            "<s>",
            "A",
            ",",
            "ĠnaÃ¯ve",  # RoBERTa mangles this. Or maybe it "encodes"?
            "<mask>",
            "ĠAllen",
            "N",
            "LP",
            "Ġsentence",
            ".",
            "</s>",
        ]
        expected_idxs = [None, 0, 1, 3, 9, 16, 21, 22, 25, 33, None]
        tokenizer = PretrainedTransformerTokenizer("roberta-base")
        tokenized = tokenizer.tokenize(sentence)
        tokens = [t.text for t in tokenized]
        assert tokens == expected_tokens
        idxs = [t.idx for t in tokenized]
        assert idxs == expected_idxs

</source>
</class>

<class classid="27" nclones="2" nlines="12" similarity="100">
<source file="systems/allennlp-2.9.0/tests/data/tokenizers/sentence_splitter_test.py" startline="24" endline="36" pcid="913">
    def test_batch_rule_based_sentence_splitting(self):
        text = [
            "This is a sentence. This is a second sentence.",
            "This isn't a sentence. This is a second sentence! This is a third sentence.",
        ]
        batch_split = self.rule_based_splitter.batch_split_sentences(text)
        separately_split = [self.rule_based_splitter.split_sentences(doc) for doc in text]
        assert len(batch_split) == len(separately_split)
        for batch_doc, separate_doc in zip(batch_split, separately_split):
            assert len(batch_doc) == len(separate_doc)
            for batch_sentence, separate_sentence in zip(batch_doc, separate_doc):
                assert batch_sentence == separate_sentence

</source>
<source file="systems/allennlp-2.9.0/tests/data/tokenizers/sentence_splitter_test.py" startline="37" endline="49" pcid="914">
    def test_batch_dep_parse_sentence_splitting(self):
        text = [
            "This is a sentence. This is a second sentence.",
            "This isn't a sentence. This is a second sentence! This is a third sentence.",
        ]
        batch_split = self.dep_parse_splitter.batch_split_sentences(text)
        separately_split = [self.dep_parse_splitter.split_sentences(doc) for doc in text]
        assert len(batch_split) == len(separately_split)
        for batch_doc, separate_doc in zip(batch_split, separately_split):
            assert len(batch_doc) == len(separate_doc)
            for batch_sentence, separate_sentence in zip(batch_doc, separate_doc):
                assert batch_sentence == separate_sentence

</source>
</class>

<class classid="28" nclones="2" nlines="14" similarity="100">
<source file="systems/allennlp-2.9.0/tests/data/samplers/max_tokens_batch_sampler_test.py" startline="10" endline="25" pcid="1011">
    def test_create_batches_groups_correctly(self):
        sampler = MaxTokensBatchSampler(max_tokens=8, padding_noise=0, sorting_keys=["text"])

        grouped_instances = []
        for indices in sampler.get_batch_indices(self.instances):
            grouped_instances.append([self.instances[idx] for idx in indices])
        expected_groups = [
            [self.instances[4], self.instances[2]],
            [self.instances[0], self.instances[1]],
            [self.instances[3]],
        ]
        for group in grouped_instances:
            assert group in expected_groups
            expected_groups.remove(group)
        assert expected_groups == []

</source>
<source file="systems/allennlp-2.9.0/tests/data/samplers/bucket_batch_sampler_test.py" startline="11" endline="26" pcid="1024">
    def test_create_batches_groups_correctly(self):
        sampler = BucketBatchSampler(batch_size=2, padding_noise=0, sorting_keys=["text"])

        grouped_instances = []
        for indices in sampler.get_batch_indices(self.instances):
            grouped_instances.append([self.instances[idx] for idx in indices])
        expected_groups = [
            [self.instances[4], self.instances[2]],
            [self.instances[0], self.instances[1]],
            [self.instances[3]],
        ]
        for group in grouped_instances:
            assert group in expected_groups
            expected_groups.remove(group)
        assert expected_groups == []

</source>
</class>

<class classid="29" nclones="2" nlines="26" similarity="100">
<source file="systems/allennlp-2.9.0/tests/data/samplers/max_tokens_batch_sampler_test.py" startline="26" endline="58" pcid="1012">
    def test_guess_sorting_key_picks_the_longest_key(self):
        sampler = MaxTokensBatchSampler(max_tokens=8, padding_noise=0)
        instances = []
        short_tokens = [Token(t) for t in ["what", "is", "this", "?"]]
        long_tokens = [Token(t) for t in ["this", "is", "a", "not", "very", "long", "passage"]]
        instances.append(
            Instance(
                {
                    "question": TextField(short_tokens, self.token_indexers),
                    "passage": TextField(long_tokens, self.token_indexers),
                }
            )
        )
        instances.append(
            Instance(
                {
                    "question": TextField(short_tokens, self.token_indexers),
                    "passage": TextField(long_tokens, self.token_indexers),
                }
            )
        )
        instances.append(
            Instance(
                {
                    "question": TextField(short_tokens, self.token_indexers),
                    "passage": TextField(long_tokens, self.token_indexers),
                }
            )
        )
        assert sampler.sorting_keys is None
        sampler._guess_sorting_keys(instances)
        assert sampler.sorting_keys == ["passage"]

</source>
<source file="systems/allennlp-2.9.0/tests/data/samplers/bucket_batch_sampler_test.py" startline="41" endline="73" pcid="1026">
    def test_guess_sorting_key_picks_the_longest_key(self):
        sampler = BucketBatchSampler(batch_size=2, padding_noise=0)
        instances = []
        short_tokens = [Token(t) for t in ["what", "is", "this", "?"]]
        long_tokens = [Token(t) for t in ["this", "is", "a", "not", "very", "long", "passage"]]
        instances.append(
            Instance(
                {
                    "question": TextField(short_tokens, self.token_indexers),
                    "passage": TextField(long_tokens, self.token_indexers),
                }
            )
        )
        instances.append(
            Instance(
                {
                    "question": TextField(short_tokens, self.token_indexers),
                    "passage": TextField(long_tokens, self.token_indexers),
                }
            )
        )
        instances.append(
            Instance(
                {
                    "question": TextField(short_tokens, self.token_indexers),
                    "passage": TextField(long_tokens, self.token_indexers),
                }
            )
        )
        assert sampler.sorting_keys is None
        sampler._guess_sorting_keys(instances)
        assert sampler.sorting_keys == ["passage"]

</source>
</class>

<class classid="30" nclones="2" nlines="21" similarity="100">
<source file="systems/allennlp-2.9.0/tests/common/file_utils_test.py" startline="350" endline="373" pcid="1096">
    def test_cached_path_extract_remote_tar(self):
        url = "http://fake.datastore.com/utf-8.tar.gz"
        byt = open(self.tar_file, "rb").read()

        responses.add(
            responses.GET,
            url,
            body=byt,
            status=200,
            content_type="application/tar+gzip",
            stream=True,
            headers={"Content-Length": str(len(byt))},
        )
        responses.add(
            responses.HEAD,
            url,
            status=200,
            headers={"ETag": "fake-etag"},
        )

        extracted = cached_path(url, cache_dir=self.TEST_DIR, extract_archive=True)
        assert extracted.endswith("-extracted")
        self.check_extracted(extracted)

</source>
<source file="systems/allennlp-2.9.0/tests/common/file_utils_test.py" startline="375" endline="399" pcid="1097">
    def test_cached_path_extract_remote_zip(self):
        url = "http://fake.datastore.com/utf-8.zip"
        byt = open(self.zip_file, "rb").read()

        responses.add(
            responses.GET,
            url,
            body=byt,
            status=200,
            content_type="application/zip",
            stream=True,
            headers={"Content-Length": str(len(byt))},
        )
        responses.add(
            responses.HEAD,
            url,
            status=200,
            headers={"ETag": "fake-etag"},
        )

        extracted = cached_path(url, cache_dir=self.TEST_DIR, extract_archive=True)
        assert extracted.endswith("-extracted")
        self.check_extracted(extracted)


</source>
</class>

<class classid="31" nclones="2" nlines="26" similarity="100">
<source file="systems/allennlp-2.9.0/tests/common/from_params_test.py" startline="356" endline="390" pcid="1153">
    def test_dict(self):

        from allennlp.common.registrable import Registrable

        class A(Registrable):
            pass

        @A.register("b")
        class B(A):
            def __init__(self, size: int) -> None:
                self.size = size

        class C(Registrable):
            pass

        @C.register("d")
        class D(C):
            def __init__(self, items: Dict[str, A]) -> None:
                self.items = items

        params = Params(
            {
                "type": "d",
                "items": {"first": {"type": "b", "size": 1}, "second": {"type": "b", "size": 2}},
            }
        )
        d = C.from_params(params)

        assert isinstance(d.items, dict)
        assert len(d.items) == 2
        assert all(isinstance(key, str) for key in d.items.keys())
        assert all(isinstance(value, B) for value in d.items.values())
        assert d.items["first"].size == 1
        assert d.items["second"].size == 2

</source>
<source file="systems/allennlp-2.9.0/tests/common/from_params_test.py" startline="814" endline="847" pcid="1198">
    def test_mapping(self):
        from allennlp.common.registrable import Registrable

        class A(Registrable):
            pass

        @A.register("b")
        class B(A):
            def __init__(self, size: int) -> None:
                self.size = size

        class C(Registrable):
            pass

        @C.register("d")
        class D(C):
            def __init__(self, items: Mapping[str, A]) -> None:
                self.items = items

        params = Params(
            {
                "type": "d",
                "items": {"first": {"type": "b", "size": 1}, "second": {"type": "b", "size": 2}},
            }
        )
        d = C.from_params(params)

        assert isinstance(d.items, Mapping)
        assert len(d.items) == 2
        assert all(isinstance(key, str) for key in d.items.keys())
        assert all(isinstance(value, B) for value in d.items.values())
        assert d.items["first"].size == 1
        assert d.items["second"].size == 2

</source>
</class>

<class classid="32" nclones="2" nlines="10" similarity="100">
<source file="systems/allennlp-2.9.0/tests/fairness/bias_mitigators_test.py" startline="241" endline="255" pcid="1298">
    def test_inlp(self, device: str):
        self.seed_embeddings1 = self.seed_embeddings1.to(device)
        self.seed_embeddings2 = self.seed_embeddings2.to(device)
        self.evaluation_embeddings = self.evaluation_embeddings.to(device)
        self.expected_bias_mitigated_embeddings = self.expected_bias_mitigated_embeddings.to(device)

        ibm = INLPBiasMitigator()
        test_bias_mitigated_embeddings = ibm(
            self.evaluation_embeddings, self.seed_embeddings1, self.seed_embeddings2
        )
        assert allclose(
            self.expected_bias_mitigated_embeddings, test_bias_mitigated_embeddings, atol=1e-6
        )


</source>
<source file="systems/allennlp-2.9.0/tests/fairness/bias_mitigators_test.py" startline="290" endline="303" pcid="1301">
    def test_oscar_without_grad(self, device: str):
        self.bias_direction1 = self.bias_direction1.to(device)
        self.bias_direction2 = self.bias_direction2.to(device)
        self.evaluation_embeddings = self.evaluation_embeddings.to(device)
        self.expected_bias_mitigated_embeddings = self.expected_bias_mitigated_embeddings.to(device)

        obm = OSCaRBiasMitigator()
        test_bias_mitigated_embeddings = obm(
            self.evaluation_embeddings, self.bias_direction1, self.bias_direction2
        )
        assert allclose(
            self.expected_bias_mitigated_embeddings, test_bias_mitigated_embeddings, atol=1e-6
        )

</source>
</class>

<class classid="33" nclones="2" nlines="16" similarity="100">
<source file="systems/allennlp-2.9.0/tests/interpret/simple_gradient_test.py" startline="17" endline="37" pcid="1348">
    def test_simple_gradient_basic_text(self):
        inputs = {"sentence": "It was the ending that I hated"}
        archive = load_archive(
            self.FIXTURES_ROOT / "basic_classifier" / "serialization" / "model.tar.gz"
        )
        predictor = Predictor.from_archive(archive, "text_classifier")

        interpreter = SimpleGradient(predictor)
        interpretation = interpreter.saliency_interpret_from_json(inputs)
        assert interpretation is not None
        assert "instance_1" in interpretation
        assert "grad_input_1" in interpretation["instance_1"]
        grad_input_1 = interpretation["instance_1"]["grad_input_1"]
        assert len(grad_input_1) == 7  # 7 words in input

        # two interpretations should be identical for gradient
        repeat_interpretation = interpreter.saliency_interpret_from_json(inputs)
        repeat_grad_input_1 = repeat_interpretation["instance_1"]["grad_input_1"]
        for grad, repeat_grad in zip(grad_input_1, repeat_grad_input_1):
            assert grad == approx(repeat_grad)

</source>
<source file="systems/allennlp-2.9.0/tests/interpret/integrated_gradient_test.py" startline="17" endline="37" pcid="1351">
    def test_integrated_gradient(self):
        inputs = {"sentence": "It was the ending that I hated"}
        archive = load_archive(
            self.FIXTURES_ROOT / "basic_classifier" / "serialization" / "model.tar.gz"
        )
        predictor = Predictor.from_archive(archive, "text_classifier")

        interpreter = IntegratedGradient(predictor)
        interpretation = interpreter.saliency_interpret_from_json(inputs)
        assert interpretation is not None
        assert "instance_1" in interpretation
        assert "grad_input_1" in interpretation["instance_1"]
        grad_input_1 = interpretation["instance_1"]["grad_input_1"]
        assert len(grad_input_1) == 7  # 7 words in input

        # two interpretations should be identical for integrated gradients
        repeat_interpretation = interpreter.saliency_interpret_from_json(inputs)
        repeat_grad_input_1 = repeat_interpretation["instance_1"]["grad_input_1"]
        for grad, repeat_grad in zip(grad_input_1, repeat_grad_input_1):
            assert grad == approx(repeat_grad)

</source>
</class>

<class classid="34" nclones="3" nlines="13" similarity="100">
<source file="systems/allennlp-2.9.0/tests/interpret/simple_gradient_test.py" startline="49" endline="63" pcid="1350">
    def test_interpret_works_with_custom_embedding_layer(self):
        inputs = {"sentence": "It was the ending that I hated"}
        vocab = Vocabulary()
        vocab.add_tokens_to_namespace([w for w in inputs["sentence"].split(" ")])
        model = FakeModelForTestingInterpret(vocab, max_tokens=len(inputs["sentence"].split(" ")))
        predictor = FakePredictorForTestingInterpret(model, TextClassificationJsonReader())
        interpreter = SimpleGradient(predictor)

        interpretation = interpreter.saliency_interpret_from_json(inputs)

        assert interpretation is not None
        assert "instance_1" in interpretation
        assert "grad_input_1" in interpretation["instance_1"]
        grad_input_1 = interpretation["instance_1"]["grad_input_1"]
        assert len(grad_input_1) == 7  # 7 words in input
</source>
<source file="systems/allennlp-2.9.0/tests/interpret/smooth_gradient_test.py" startline="42" endline="56" pcid="1361">
    def test_interpret_works_with_custom_embedding_layer(self):
        inputs = {"sentence": "It was the ending that I hated"}
        vocab = Vocabulary()
        vocab.add_tokens_to_namespace([w for w in inputs["sentence"].split(" ")])
        model = FakeModelForTestingInterpret(vocab, max_tokens=len(inputs["sentence"].split(" ")))
        predictor = FakePredictorForTestingInterpret(model, TextClassificationJsonReader())
        interpreter = SmoothGradient(predictor)

        interpretation = interpreter.saliency_interpret_from_json(inputs)

        assert interpretation is not None
        assert "instance_1" in interpretation
        assert "grad_input_1" in interpretation["instance_1"]
        grad_input_1 = interpretation["instance_1"]["grad_input_1"]
        assert len(grad_input_1) == 7  # 7 words in input
</source>
<source file="systems/allennlp-2.9.0/tests/interpret/integrated_gradient_test.py" startline="49" endline="63" pcid="1353">
    def test_interpret_works_with_custom_embedding_layer(self):
        inputs = {"sentence": "It was the ending that I hated"}
        vocab = Vocabulary()
        vocab.add_tokens_to_namespace([w for w in inputs["sentence"].split(" ")])
        model = FakeModelForTestingInterpret(vocab, max_tokens=len(inputs["sentence"].split(" ")))
        predictor = FakePredictorForTestingInterpret(model, TextClassificationJsonReader())
        interpreter = IntegratedGradient(predictor)

        interpretation = interpreter.saliency_interpret_from_json(inputs)

        assert interpretation is not None
        assert "instance_1" in interpretation
        assert "grad_input_1" in interpretation["instance_1"]
        grad_input_1 = interpretation["instance_1"]["grad_input_1"]
        assert len(grad_input_1) == 7  # 7 words in input
</source>
</class>

<class classid="35" nclones="2" nlines="12" similarity="100">
<source file="systems/allennlp-2.9.0/tests/modules/stacked_bidirectional_lstm_test.py" startline="15" endline="27" pcid="1403">
    def test_stacked_bidirectional_lstm_completes_forward_pass(self):
        input_tensor = torch.rand(4, 5, 3)
        input_tensor[1, 4:, :] = 0.0
        input_tensor[2, 2:, :] = 0.0
        input_tensor[3, 1:, :] = 0.0
        input_tensor = pack_padded_sequence(input_tensor, [5, 4, 2, 1], batch_first=True)
        lstm = StackedBidirectionalLstm(3, 7, 3)
        output, _ = lstm(input_tensor)
        output_sequence, _ = pad_packed_sequence(output, batch_first=True)
        numpy.testing.assert_array_equal(output_sequence.data[1, 4:, :].numpy(), 0.0)
        numpy.testing.assert_array_equal(output_sequence.data[2, 2:, :].numpy(), 0.0)
        numpy.testing.assert_array_equal(output_sequence.data[3, 1:, :].numpy(), 0.0)

</source>
<source file="systems/allennlp-2.9.0/tests/modules/stacked_alternating_lstm_test.py" startline="10" endline="22" pcid="1437">
    def test_stacked_alternating_lstm_completes_forward_pass(self):
        input_tensor = torch.rand(4, 5, 3)
        input_tensor[1, 4:, :] = 0.0
        input_tensor[2, 2:, :] = 0.0
        input_tensor[3, 1:, :] = 0.0
        input_tensor = pack_padded_sequence(input_tensor, [5, 4, 2, 1], batch_first=True)
        lstm = StackedAlternatingLstm(3, 7, 3)
        output, _ = lstm(input_tensor)
        output_sequence, _ = pad_packed_sequence(output, batch_first=True)
        numpy.testing.assert_array_equal(output_sequence.data[1, 4:, :].numpy(), 0.0)
        numpy.testing.assert_array_equal(output_sequence.data[2, 2:, :].numpy(), 0.0)
        numpy.testing.assert_array_equal(output_sequence.data[3, 1:, :].numpy(), 0.0)

</source>
</class>

<class classid="36" nclones="2" nlines="20" similarity="100">
<source file="systems/allennlp-2.9.0/tests/modules/text_field_embedders/basic_text_field_embedder_test.py" startline="98" endline="120" pcid="1416">
    def test_forward_runs_with_non_bijective_mapping(self):
        elmo_fixtures_path = self.FIXTURES_ROOT / "elmo"
        options_file = str(elmo_fixtures_path / "options.json")
        weight_file = str(elmo_fixtures_path / "lm_weights.hdf5")
        params = Params(
            {
                "token_embedders": {
                    "words": {"type": "embedding", "num_embeddings": 20, "embedding_dim": 2},
                    "elmo": {
                        "type": "elmo_token_embedder",
                        "options_file": options_file,
                        "weight_file": weight_file,
                    },
                }
            }
        )
        token_embedder = BasicTextFieldEmbedder.from_params(vocab=self.vocab, params=params)
        inputs = {
            "words": {"tokens": (torch.rand(3, 6) * 20).long()},
            "elmo": {"elmo_tokens": (torch.rand(3, 6, 50) * 15).long()},
        }
        token_embedder(inputs)

</source>
<source file="systems/allennlp-2.9.0/tests/modules/text_field_embedders/basic_text_field_embedder_test.py" startline="140" endline="162" pcid="1418">
    def test_forward_runs_with_non_bijective_mapping_with_dict(self):
        elmo_fixtures_path = self.FIXTURES_ROOT / "elmo"
        options_file = str(elmo_fixtures_path / "options.json")
        weight_file = str(elmo_fixtures_path / "lm_weights.hdf5")
        params = Params(
            {
                "token_embedders": {
                    "words": {"type": "embedding", "num_embeddings": 20, "embedding_dim": 2},
                    "elmo": {
                        "type": "elmo_token_embedder",
                        "options_file": options_file,
                        "weight_file": weight_file,
                    },
                }
            }
        )
        token_embedder = BasicTextFieldEmbedder.from_params(vocab=self.vocab, params=params)
        inputs = {
            "words": {"tokens": (torch.rand(3, 6) * 20).long()},
            "elmo": {"elmo_tokens": (torch.rand(3, 6, 50) * 15).long()},
        }
        token_embedder(inputs)

</source>
</class>

<class classid="37" nclones="3" nlines="11" similarity="100">
<source file="systems/allennlp-2.9.0/tests/modules/span_extractors/endpoint_span_extractor_test.py" startline="10" endline="22" pcid="1558">
    def test_endpoint_span_extractor_can_build_from_params(self):
        params = Params(
            {
                "type": "endpoint",
                "input_dim": 7,
                "num_width_embeddings": 5,
                "span_width_embedding_dim": 3,
            }
        )
        extractor = SpanExtractor.from_params(params)
        assert isinstance(extractor, EndpointSpanExtractor)
        assert extractor.get_output_dim() == 17  # 2 * input_dim + span_width_embedding_dim

</source>
<source file="systems/allennlp-2.9.0/tests/modules/span_extractors/self_attentive_span_extractor_test.py" startline="9" endline="21" pcid="1568">
    def test_locally_normalised_span_extractor_can_build_from_params(self):
        params = Params(
            {
                "type": "self_attentive",
                "input_dim": 7,
                "num_width_embeddings": 5,
                "span_width_embedding_dim": 3,
            }
        )
        extractor = SpanExtractor.from_params(params)
        assert isinstance(extractor, SelfAttentiveSpanExtractor)
        assert extractor.get_output_dim() == 10  # input_dim + span_width_embedding_dim

</source>
<source file="systems/allennlp-2.9.0/tests/modules/span_extractors/max_pooling_span_extractor_test.py" startline="10" endline="22" pcid="1571">
    def test_locally_span_extractor_can_build_from_params(self):
        params = Params(
            {
                "type": "max_pooling",
                "input_dim": 3,
                "num_width_embeddings": 5,
                "span_width_embedding_dim": 3,
            }
        )
        extractor = SpanExtractor.from_params(params)
        assert isinstance(extractor, MaxPoolingSpanExtractor)
        assert extractor.get_output_dim() == 6

</source>
</class>

<class classid="38" nclones="2" nlines="10" similarity="100">
<source file="systems/allennlp-2.9.0/tests/modules/seq2vec_encoders/pytorch_seq2vec_wrapper_test.py" startline="15" endline="26" pcid="1613">
    def test_get_dimensions_is_correct(self):
        lstm = LSTM(bidirectional=True, num_layers=3, input_size=2, hidden_size=7, batch_first=True)
        encoder = PytorchSeq2VecWrapper(lstm)
        assert encoder.get_output_dim() == 14
        assert encoder.get_input_dim() == 2
        lstm = LSTM(
            bidirectional=False, num_layers=3, input_size=2, hidden_size=7, batch_first=True
        )
        encoder = PytorchSeq2VecWrapper(lstm)
        assert encoder.get_output_dim() == 7
        assert encoder.get_input_dim() == 2

</source>
<source file="systems/allennlp-2.9.0/tests/modules/seq2seq_encoders/pytorch_seq2seq_wrapper_test.py" startline="15" endline="26" pcid="1641">
    def test_get_dimension_is_correct(self):
        lstm = LSTM(bidirectional=True, num_layers=3, input_size=2, hidden_size=7, batch_first=True)
        encoder = PytorchSeq2SeqWrapper(lstm)
        assert encoder.get_output_dim() == 14
        assert encoder.get_input_dim() == 2
        lstm = LSTM(
            bidirectional=False, num_layers=3, input_size=2, hidden_size=7, batch_first=True
        )
        encoder = PytorchSeq2SeqWrapper(lstm)
        assert encoder.get_output_dim() == 7
        assert encoder.get_input_dim() == 2

</source>
</class>

</clones>

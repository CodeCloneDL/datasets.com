<clonepair1>
<source file="systems/stellargraph-1.2.1/stellargraph/ensemble.py" startline="196" endline="303" pcid="209"></source>
    def fit(
        self,
        generator,
        steps_per_epoch=None,
        epochs=1,
        verbose=1,
        validation_data=None,
        validation_steps=None,
        class_weight=None,
        max_queue_size=10,
        workers=1,
        use_multiprocessing=False,
        shuffle=True,
        initial_epoch=0,
        use_early_stopping=False,
        early_stopping_monitor="val_loss",
    ):
        """
        This method trains the ensemble on the data specified by the generator. If validation data are given, then the
        training metrics are evaluated on these data and results printed on screen if verbose level is greater than 0.

        The method trains each model in the ensemble in series for the number of epochs specified. Training can
        also stop early with the best model as evaluated on the validation data, if use_early_stopping is set to True.

        For detail descriptions of Keras-specific parameters consult the Keras documentation
        at https://keras.io/models/sequential/

        Args:
            generator: The generator object for training data. It should be one of type
                NodeSequence, LinkSequence, SparseFullBatchSequence, or FullBatchSequence.
            steps_per_epoch (None or int): (Keras-specific parameter) If not None, it specifies the number of steps
                to yield from the generator before declaring one epoch finished and starting a new epoch.
            epochs (int): (Keras-specific parameter) The number of training epochs.
            verbose (int): (Keras-specific parameter) The verbosity mode that should be 0 , 1, or 2 meaning silent,
                progress bar, and one line per epoch respectively.
            validation_data: A generator for validation data that is optional (None). If not None then, it should
                be one of type NodeSequence, LinkSequence, SparseFullBatchSequence, or FullBatchSequence.
            validation_steps (None or int): (Keras-specific parameter) If validation_generator is not None, then it
                specifies the number of steps to yield from the generator before stopping at the end of every epoch.
            class_weight (None or dict): (Keras-specific parameter) If not None, it should be a dictionary
                mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during
                training only). This can be useful to tell the model to "pay more attention" to samples from an
                under-represented class.
            max_queue_size (int): (Keras-specific parameter) The maximum size for the generator queue.
            workers (int): (Keras-specific parameter) The maximum number of workers to use.
            use_multiprocessing (bool): (Keras-specific parameter) If True then use process based threading.
            shuffle (bool): (Keras-specific parameter) If True, then it shuffles the order of batches at the
                beginning of each training epoch.
            initial_epoch (int): (Keras-specific parameter) Epoch at which to start training (useful for resuming a
                previous training run).
            use_early_stopping (bool): If set to True, then early stopping is used when training each model
                in the ensemble. The default is False.
            early_stopping_monitor (str): The quantity to monitor for early stopping, e.g., 'val_loss',
                'val_weighted_acc'. It should be a valid Keras metric.

        Returns:
            list: It returns a list of Keras History objects each corresponding to one trained model in the ensemble.

        """
        if not isinstance(
            generator,
            (
                sg.mapper.NodeSequence,
                sg.mapper.LinkSequence,
                sg.mapper.FullBatchSequence,
                sg.mapper.SparseFullBatchSequence,
            ),
        ):
            raise ValueError(
                "({}) If train_data is None, generator must be one of type NodeSequence, LinkSequence, FullBatchSequence "
                "but received object of type {}".format(
                    type(self).__name__, type(generator).__name__
                )
            )

        self.history = []

        es_callback = None
        if use_early_stopping and validation_data is not None:
            es_callback = [
                EarlyStopping(
                    monitor=early_stopping_monitor,
                    patience=self.early_stoppping_patience,
                    restore_best_weights=True,
                )
            ]

        for model in self.models:
            self.history.append(
                model.fit(
                    generator,
                    steps_per_epoch=steps_per_epoch,
                    epochs=epochs,
                    verbose=verbose,
                    callbacks=es_callback,
                    validation_data=validation_data,
                    validation_steps=validation_steps,
                    class_weight=class_weight,
                    max_queue_size=max_queue_size,
                    workers=workers,
                    use_multiprocessing=use_multiprocessing,
                    shuffle=shuffle,
                    initial_epoch=initial_epoch,
                )
            )

        return self.history

</clonepair1>

<clonepair1>
<source file="systems/stellargraph-1.2.1/stellargraph/ensemble.py" startline="579" endline="727" pcid="216"></source>
    def fit(
        self,
        generator,
        train_data,
        train_targets,
        steps_per_epoch=None,
        epochs=1,
        verbose=1,
        validation_data=None,
        validation_steps=None,
        class_weight=None,
        max_queue_size=10,
        workers=1,
        use_multiprocessing=False,
        shuffle=True,
        initial_epoch=0,
        bag_size=None,
        use_early_stopping=False,
        early_stopping_monitor="val_loss",
    ):
        """
        This method trains the ensemble on the data given in train_data and train_targets. If validation data are
        also given, then the training metrics are evaluated on these data and results printed on screen if verbose
        level is greater than 0.

        The method trains each model in the ensemble in series for the number of epochs specified. Training can
        also stop early with the best model as evaluated on the validation data, if use_early_stopping is enabled.

        Each model in the ensemble is trained using a bootstrapped sample of the data (the train data are re-sampled
        with replacement.) The number of bootstrap samples can be specified via the bag_size parameter; by default,
        the number of bootstrap samples equals the number of training points.

        For detail descriptions of Keras-specific parameters consult the Keras documentation
        at https://keras.io/models/sequential/

        Args:
            generator: The generator object for training data. It should be one of type
                GraphSAGENodeGenerator, HinSAGENodeGenerator, FullBatchNodeGenerator, GraphSAGELinkGenerator,
                or HinSAGELinkGenerator.
            train_data (iterable): It is an iterable, e.g. list, that specifies the data
                to train the model with.
            train_targets (iterable): It is an iterable, e.g. list, that specifies the target
                values for the train data.
            steps_per_epoch (None or int): (Keras-specific parameter) If not None, it specifies the number of steps
                to yield from the generator before declaring one epoch finished and starting a new epoch.
            epochs (int): (Keras-specific parameter) The number of training epochs.
            verbose (int): (Keras-specific parameter) The verbosity mode that should be 0 , 1, or 2 meaning silent,
                progress bar, and one line per epoch respectively.
            validation_data: A generator for validation data that is optional (None). If not None then, it should
                be one of type GraphSAGENodeGenerator, HinSAGENodeGenerator, FullBatchNodeGenerator,
                GraphSAGELinkGenerator, or HinSAGELinkGenerator.
            validation_steps (None or int): (Keras-specific parameter) If validation_generator is not None, then it
                specifies the number of steps to yield from the generator before stopping at the end of every epoch.
            class_weight (None or dict): (Keras-specific parameter) If not None, it should be a dictionary
                mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during
                training only). This can be useful to tell the model to "pay more attention" to samples from an
                under-represented class.
            max_queue_size (int): (Keras-specific parameter) The maximum size for the generator queue.
            workers (int): (Keras-specific parameter) The maximum number of workers to use.
            use_multiprocessing (bool): (Keras-specific parameter) If True then use process based threading.
            shuffle (bool): (Keras-specific parameter) If True, then it shuffles the order of batches at the
                beginning of each training epoch.
            initial_epoch (int): (Keras-specific parameter) Epoch at which to start training (useful for resuming a
                previous training run).
            bag_size (None or int): The number of samples in a bootstrap sample. If None and bagging is used, then
                the number of samples is equal to the number of training points.
            use_early_stopping (bool): If set to True, then early stopping is used when training each model
                in the ensemble. The default is False.
            early_stopping_monitor (str): The quantity to monitor for early stopping, e.g., 'val_loss',
                'val_weighted_acc'. It should be a valid Keras metric.

        Returns:
            list: It returns a list of Keras History objects each corresponding to one trained model in the ensemble.

        """
        if not isinstance(
            generator,
            (
                sg.mapper.GraphSAGENodeGenerator,
                sg.mapper.HinSAGENodeGenerator,
                sg.mapper.FullBatchNodeGenerator,
                sg.mapper.GraphSAGELinkGenerator,
                sg.mapper.HinSAGELinkGenerator,
            ),
        ):
            raise ValueError(
                "({}) generator parameter must be of type GraphSAGENodeGenerator, HinSAGENodeGenerator, "
                "FullBatchNodeGenerator, GraphSAGELinkGenerator, or HinSAGELinkGenerator if you want to use Bagging. "
                "Received type {}".format(type(self).__name__, type(generator).__name__)
            )
        if bag_size is not None and (bag_size > len(train_data) or bag_size <= 0):
            raise ValueError(
                "({}) bag_size must be positive and less than or equal to the number of training points ({})".format(
                    type(self).__name__, len(train_data)
                )
            )
        if train_targets is None:
            raise ValueError(
                "({}) If train_data is given then train_targets must be given as well.".format(
                    type(self).__name__
                )
            )

        self.history = []

        num_points_per_bag = bag_size if bag_size is not None else len(train_data)

        # Prepare the training data for each model. Use sampling with replacement to create len(self.models)
        # datasets.
        for model in self.models:
            di_index = np.random.choice(
                len(train_data), size=num_points_per_bag
            )  # sample with replacement
            di_train = train_data[di_index]

            di_targets = train_targets[di_index]

            di_gen = generator.flow(di_train, di_targets)

            es_callback = None
            if use_early_stopping and validation_data is not None:
                es_callback = [
                    EarlyStopping(
                        monitor=early_stopping_monitor,
                        patience=self.early_stoppping_patience,
                        restore_best_weights=True,
                    )
                ]

            self.history.append(
                model.fit(
                    di_gen,
                    steps_per_epoch=steps_per_epoch,
                    epochs=epochs,
                    verbose=verbose,
                    callbacks=es_callback,
                    validation_data=validation_data,
                    validation_steps=validation_steps,
                    class_weight=class_weight,
                    max_queue_size=max_queue_size,
                    workers=workers,
                    use_multiprocessing=use_multiprocessing,
                    shuffle=shuffle,
                    initial_epoch=initial_epoch,
                )
            )

        return self.history

</clonepair1>
<clonepair2>
<source file="systems/stellargraph-1.2.1/stellargraph/ensemble.py" startline="315" endline="407" pcid="211"></source>
    def evaluate(
        self,
        generator,
        test_data=None,
        test_targets=None,
        max_queue_size=10,
        workers=1,
        use_multiprocessing=False,
        verbose=0,
    ):
        """
        Evaluates the ensemble on a data (node or link) generator. It makes `n_predictions` for each data point for each
        of the `n_estimators` and returns the mean and standard deviation of the predictions.

        For detailed descriptions of Keras-specific parameters consult the Keras documentation
        at https://keras.io/models/sequential/

        Args:
            generator: The generator object that, if test_data is not None, should be one of type
                GraphSAGENodeGenerator, HinSAGENodeGenerator, FullBatchNodeGenerator, GraphSAGELinkGenerator,
                or HinSAGELinkGenerator. However, if test_data is None, then generator should be one of type
                NodeSequence, LinkSequence, or FullBatchSequence.
            test_data (None or iterable): If not None, then it is an iterable, e.g. list, that specifies the node IDs
                to evaluate the model on.
            test_targets (None or iterable): If not None, then it is an iterable, e.g. list, that specifies the target
                values for the test_data.
            max_queue_size (int): (Keras-specific parameter) The maximum size for the generator queue.
            workers (int): (Keras-specific parameter) The maximum number of workers to use.
            use_multiprocessing (bool): (Keras-specific parameter) If True then use process based threading.
            verbose (int): (Keras-specific parameter) The verbosity mode that should be 0 or 1 with the former turning
                verbosity off and the latter on.

        Returns:
            tuple: The mean and standard deviation of the model metrics for the given data.

        """
        if test_data is not None and not isinstance(
            generator,
            (
                sg.mapper.GraphSAGENodeGenerator,
                sg.mapper.HinSAGENodeGenerator,
                sg.mapper.FullBatchNodeGenerator,
                sg.mapper.GraphSAGELinkGenerator,
                sg.mapper.HinSAGELinkGenerator,
            ),
        ):
            raise ValueError(
                "({}) generator parameter must be of type GraphSAGENodeGenerator, HinSAGENodeGenerator, FullBatchNodeGenerator, "
                "GraphSAGELinkGenerator, or HinSAGELinkGenerator. Received type {}".format(
                    type(self).__name__, type(generator).__name__
                )
            )
        elif not isinstance(
            generator,
            (
                sg.mapper.NodeSequence,
                sg.mapper.LinkSequence,
                sg.mapper.FullBatchSequence,
                sg.mapper.SparseFullBatchSequence,
            ),
        ):
            raise ValueError(
                "({}) If test_data is None, generator must be one of type NodeSequence, "
                "LinkSequence, FullBatchSequence, or SparseFullBatchSequence "
                "but received object of type {}".format(
                    type(self).__name__, type(generator).__name__
                )
            )
        if test_data is not None and test_targets is None:
            raise ValueError("({}) test_targets not given.".format(type(self).__name__))

        data_generator = generator
        if test_data is not None:
            data_generator = generator.flow(test_data, test_targets)

        test_metrics = []
        for model in self.models:
            tm = []
            for _ in range(self.n_predictions):
                tm.append(
                    model.evaluate(
                        data_generator,
                        max_queue_size=max_queue_size,
                        workers=workers,
                        use_multiprocessing=use_multiprocessing,
                        verbose=verbose,
                    )  # Keras evaluate_generator returns a scalar
                )
            test_metrics.append(np.mean(tm, axis=0))

        # Return the mean and standard deviation of the metrics
        return np.mean(test_metrics, axis=0), np.std(test_metrics, axis=0)

</clonepair2>

<clonepair2>
<source file="systems/stellargraph-1.2.1/stellargraph/ensemble.py" startline="419" endline="531" pcid="213"></source>
    def predict(
        self,
        generator,
        predict_data=None,
        summarise=False,
        output_layer=None,
        max_queue_size=10,
        workers=1,
        use_multiprocessing=False,
        verbose=0,
    ):
        """
        This method generates predictions for the data produced by the given generator or alternatively the data
        given in parameter predict_data.

        For detailed descriptions of Keras-specific parameters consult the Keras documentation
        at https://keras.io/models/sequential/

        Args:
            generator: The generator object that, if predict_data is None, should be one of type
                GraphSAGENodeGenerator, HinSAGENodeGenerator, FullBatchNodeGenerator, GraphSAGELinkGenerator,
                or HinSAGELinkGenerator. However, if predict_data is not None, then generator should be one of type
                NodeSequence, LinkSequence, SparseFullBatchSequence, or FullBatchSequence.
            predict_data (None or iterable): If not None, then it is an iterable, e.g. list, that specifies the node IDs
                to make predictions for. If generator is of type FullBatchNodeGenerator then predict_data should be all
                the nodes in the graph since full batch approaches such as GCN and GAT can only be used to make
                predictions for all graph nodes.
            summarise (bool): If True, then the mean of the predictions over self.n_estimators and
                self.n_predictions are returned for each query point. If False, then all predictions are returned.
            output_layer (None or int): If not None, then the predictions are the outputs of the layer specified.
                The default is the model's output layer.
            max_queue_size (int): (Keras-specific parameter) The maximum size for the generator queue.
            workers (int): (Keras-specific parameter) The maximum number of workers to use.
            use_multiprocessing (bool): (Keras-specific parameter) If True then use process based threading.
            verbose (int): (Keras-specific parameter) The verbosity mode that should be 0 or 1 with the former turning
                verbosity off and the latter on.


        Returns:
            numpy array: The predictions. It will have shape ``M × K × N × F`` if ``summarise`` is set to ``False``, or ``N × F``
            otherwise. ``M`` is the number of estimators in the ensemble; ``K`` is the number of predictions per query
            point; ``N`` is the number of query points; and ``F`` is the output dimensionality of the specified layer
            determined by the shape of the output layer.

        """
        data_generator = generator
        if predict_data is not None:
            if not isinstance(
                generator,
                (
                    sg.mapper.GraphSAGENodeGenerator,
                    sg.mapper.HinSAGENodeGenerator,
                    sg.mapper.FullBatchNodeGenerator,
                ),
            ):
                raise ValueError(
                    "({}) generator parameter must be of type GraphSAGENodeGenerator, HinSAGENodeGenerator, or FullBatchNodeGenerator. Received type {}".format(
                        type(self).__name__, type(generator).__name__
                    )
                )
            data_generator = generator.flow(predict_data)
        elif not isinstance(
            generator,
            (
                sg.mapper.NodeSequence,
                sg.mapper.LinkSequence,
                sg.mapper.FullBatchSequence,
                sg.mapper.SparseFullBatchSequence,
            ),
        ):
            raise ValueError(
                "({}) If x is None, generator must be one of type NodeSequence, "
                "LinkSequence, SparseFullBatchSequence, or FullBatchSequence.".format(
                    type(self).__name__
                )
            )

        predictions = []

        if output_layer is not None:
            predict_models = [
                K.Model(inputs=model.input, outputs=model.layers[output_layer].output)
                for model in self.models
            ]
        else:
            predict_models = self.models

        for model in predict_models:
            model_predictions = []
            for _ in range(self.n_predictions):
                model_predictions.append(
                    model.predict(
                        data_generator,
                        max_queue_size=max_queue_size,
                        workers=workers,
                        use_multiprocessing=use_multiprocessing,
                        verbose=verbose,
                    )
                )
            # add to predictions list
            predictions.append(model_predictions)

        predictions = np.array(predictions)

        if summarise is True:
            # average the predictions across models and predictions per query point
            predictions = np.mean(predictions, axis=(0, 1))

        # if len(predictions.shape) > 4:
        #     predictions = predictions.reshape(predictions.shape[0:3] + (-1,))

        return predictions

</clonepair2>
<clonepair3>
<source file="systems/stellargraph-1.2.1/tests/mapper/test_full_batch_generators.py" startline="211" endline="289" pcid="439"></source>
    def test_generator_methods(self):
        node_ids = list(self.G.nodes())
        Aadj = self.G.to_adjacency_matrix().toarray()
        Aadj_selfloops = Aadj + np.eye(*Aadj.shape) - np.diag(Aadj.diagonal())
        Dtilde = np.diag(Aadj_selfloops.sum(axis=1) ** (-0.5))
        Agcn = Dtilde.dot(Aadj_selfloops).dot(Dtilde)
        Appnp = 0.1 * np.linalg.inv(np.eye(Agcn.shape[0]) - ((1 - 0.1) * Agcn))

        A_dense, _, _ = self.generator_flow(
            self.G, node_ids, None, sparse=True, method="none"
        )
        np.testing.assert_allclose(A_dense, Aadj)
        A_dense, _, _ = self.generator_flow(
            self.G, node_ids, None, sparse=False, method="none"
        )
        np.testing.assert_allclose(A_dense, Aadj)

        A_dense, _, _ = self.generator_flow(
            self.G, node_ids, None, sparse=True, method="self_loops"
        )
        np.testing.assert_allclose(A_dense, Aadj_selfloops)
        A_dense, _, _ = self.generator_flow(
            self.G, node_ids, None, sparse=False, method="self_loops"
        )
        np.testing.assert_allclose(A_dense, Aadj_selfloops)

        A_dense, _, _ = self.generator_flow(
            self.G, node_ids, None, sparse=True, method="gat"
        )
        np.testing.assert_allclose(A_dense, Aadj_selfloops)
        A_dense, _, _ = self.generator_flow(
            self.G, node_ids, None, sparse=False, method="gat"
        )
        np.testing.assert_allclose(A_dense, Aadj_selfloops)

        A_dense, _, _ = self.generator_flow(
            self.G, node_ids, None, sparse=True, method="gcn"
        )
        np.testing.assert_allclose(A_dense, Agcn)
        A_dense, _, _ = self.generator_flow(
            self.G, node_ids, None, sparse=False, method="gcn"
        )
        np.testing.assert_allclose(A_dense, Agcn)

        # Check other preprocessing options
        A_dense, _, _ = self.generator_flow(
            self.G, node_ids, None, sparse=True, method="sgc", k=2
        )
        np.testing.assert_allclose(A_dense, Agcn.dot(Agcn))
        A_dense, _, _ = self.generator_flow(
            self.G, node_ids, None, sparse=False, method="sgc", k=2
        )
        np.testing.assert_allclose(A_dense, Agcn.dot(Agcn))

        A_dense, _, _ = self.generator_flow(
            self.G,
            node_ids,
            None,
            sparse=False,
            method="ppnp",
            teleport_probability=0.1,
        )
        np.testing.assert_allclose(A_dense, Appnp)

        ppnp_sparse_failed = False
        try:
            A_dense, _, _ = self.generator_flow(
                self.G,
                node_ids,
                None,
                sparse=True,
                method="ppnp",
                teleport_probability=0.1,
            )
        except ValueError as e:
            ppnp_sparse_failed = True

        assert ppnp_sparse_failed

</clonepair3>

<clonepair3>
<source file="systems/stellargraph-1.2.1/tests/mapper/test_full_batch_generators.py" startline="424" endline="501" pcid="451"></source>
    def test_generator_methods(self):
        link_ids = list(self.G.edges())[:10]
        Aadj = self.G.to_adjacency_matrix().toarray()
        Aadj_selfloops = Aadj + np.eye(*Aadj.shape) - np.diag(Aadj.diagonal())
        Dtilde = np.diag(Aadj_selfloops.sum(axis=1) ** (-0.5))
        Agcn = Dtilde.dot(Aadj_selfloops).dot(Dtilde)
        Appnp = 0.1 * np.linalg.inv(np.eye(Agcn.shape[0]) - ((1 - 0.1) * Agcn))

        A_dense, _, _ = self.generator_flow(
            self.G, link_ids, None, sparse=True, method="none"
        )
        np.testing.assert_allclose(A_dense, Aadj)
        A_dense, _, _ = self.generator_flow(
            self.G, link_ids, None, sparse=False, method="none"
        )
        np.testing.assert_allclose(A_dense, Aadj)

        A_dense, _, _ = self.generator_flow(
            self.G, link_ids, None, sparse=True, method="self_loops"
        )
        np.testing.assert_allclose(A_dense, Aadj_selfloops)
        A_dense, _, _ = self.generator_flow(
            self.G, link_ids, None, sparse=False, method="self_loops"
        )
        np.testing.assert_allclose(A_dense, Aadj_selfloops)

        A_dense, _, _ = self.generator_flow(
            self.G, link_ids, None, sparse=True, method="gat"
        )
        np.testing.assert_allclose(A_dense, Aadj_selfloops)
        A_dense, _, _ = self.generator_flow(
            self.G, link_ids, None, sparse=False, method="gat"
        )
        np.testing.assert_allclose(A_dense, Aadj_selfloops)

        A_dense, _, _ = self.generator_flow(
            self.G, link_ids, None, sparse=True, method="gcn"
        )
        np.testing.assert_allclose(A_dense, Agcn)
        A_dense, _, _ = self.generator_flow(
            self.G, link_ids, None, sparse=False, method="gcn"
        )
        np.testing.assert_allclose(A_dense, Agcn)

        # Check other preprocessing options
        A_dense, _, _ = self.generator_flow(
            self.G, link_ids, None, sparse=True, method="sgc", k=2
        )
        np.testing.assert_allclose(A_dense, Agcn.dot(Agcn))
        A_dense, _, _ = self.generator_flow(
            self.G, link_ids, None, sparse=False, method="sgc", k=2
        )
        np.testing.assert_allclose(A_dense, Agcn.dot(Agcn))

        A_dense, _, _ = self.generator_flow(
            self.G,
            link_ids,
            None,
            sparse=False,
            method="ppnp",
            teleport_probability=0.1,
        )
        np.testing.assert_allclose(A_dense, Appnp)

        ppnp_sparse_failed = False
        try:
            A_dense, _, _ = self.generator_flow(
                self.G,
                link_ids,
                None,
                sparse=True,
                method="ppnp",
                teleport_probability=0.1,
            )
        except ValueError as e:
            ppnp_sparse_failed = True

        assert ppnp_sparse_failed
</clonepair3>
<clonepair4>
<source file="systems/stellargraph-1.2.1/tests/mapper/test_full_batch_generators.py" startline="85" endline="127" pcid="430"></source>
    def generator_flow(
        self,
        G,
        node_ids,
        node_targets,
        sparse=False,
        method="none",
        k=1,
        teleport_probability=0.1,
    ):
        generator = FullBatchNodeGenerator(
            G,
            sparse=sparse,
            method=method,
            k=k,
            teleport_probability=teleport_probability,
        )
        n_nodes = G.number_of_nodes()

        gen = generator.flow(node_ids, node_targets)
        if sparse:
            [X, tind, A_ind, A_val], y = gen[0]
            A_sparse = sps.coo_matrix(
                (A_val[0], (A_ind[0, :, 0], A_ind[0, :, 1])), shape=(n_nodes, n_nodes)
            )
            A_dense = A_sparse.toarray()

        else:
            [X, tind, A], y = gen[0]
            A_dense = A[0]

        np.testing.assert_allclose(X, gen.features)  # X should be equal to gen.features
        assert tind.shape[1] == len(node_ids)

        if node_targets is not None:
            np.testing.assert_allclose(y.squeeze(), node_targets)

        # Check that the diagonals are one
        if method == "self_loops":
            np.testing.assert_allclose(A_dense.diagonal(), 1)

        return A_dense, tind, y

</clonepair4>

<clonepair4>
<source file="systems/stellargraph-1.2.1/tests/mapper/test_full_batch_generators.py" startline="328" endline="373" pcid="444"></source>
    def generator_flow(
        self,
        G,
        link_ids,
        link_targets,
        sparse=False,
        method="none",
        k=1,
        teleport_probability=0.1,
    ):
        generator = FullBatchLinkGenerator(
            G,
            sparse=sparse,
            method=method,
            k=k,
            teleport_probability=teleport_probability,
        )
        n_nodes = G.number_of_nodes()

        gen = generator.flow(link_ids, link_targets)
        if sparse:
            [X, tind, A_ind, A_val], y = gen[0]
            A_sparse = sps.coo_matrix(
                (A_val[0], (A_ind[0, :, 0], A_ind[0, :, 1])), shape=(n_nodes, n_nodes)
            )
            A_dense = A_sparse.toarray()

        else:
            [X, tind, A], y = gen[0]
            A_dense = A[0]

        np.testing.assert_allclose(X, gen.features)  # X should be equal to gen.features
        assert isinstance(tind, np.ndarray)
        assert tind.ndim == 3
        assert tind.shape[1] == len(link_ids)
        assert tind.shape[2] == 2

        if link_targets is not None:
            np.testing.assert_allclose(y.squeeze(), link_targets)

        # Check that the diagonals are one
        if method == "self_loops":
            np.testing.assert_allclose(A_dense.diagonal(), 1)

        return A_dense, tind, y

</clonepair4>
<clonepair5>
<source file="systems/stellargraph-1.2.1/stellargraph/mapper/sampled_link_generators.py" startline="650" endline="685" pcid="173"></source>
    def __init__(
        self,
        G,
        batch_size,
        in_samples,
        out_samples,
        seed=None,
        name=None,
        weighted=False,
    ):
        super().__init__(G, batch_size)

        self.in_samples = in_samples
        self.out_samples = out_samples
        self._name = name
        self.weighted = weighted

        # Check that there is only a single node type for GraphSAGE
        if len(self.schema.node_types) > 1:
            warnings.warn(
                "running homogeneous GraphSAGE on a graph with multiple node types",
                RuntimeWarning,
                stacklevel=2,
            )

        self.head_node_types = self.schema.node_types * 2

        self._graph = G

        self._samplers = SeededPerBatch(
            lambda s: DirectedBreadthFirstNeighbours(
                self._graph, graph_schema=self.schema, seed=s
            ),
            seed=seed,
        )

</clonepair5>

<clonepair5>
<source file="systems/stellargraph-1.2.1/stellargraph/mapper/sampled_node_generators.py" startline="345" endline="376" pcid="104"></source>
    def __init__(
        self,
        G,
        batch_size,
        in_samples,
        out_samples,
        seed=None,
        name=None,
        weighted=False,
    ):
        super().__init__(G, batch_size)

        # TODO Add checks for in- and out-nodes sizes
        self.in_samples = in_samples
        self.out_samples = out_samples
        self.head_node_types = self.schema.node_types
        self.name = name
        self.weighted = weighted

        # Check that there is only a single node type for GraphSAGE
        if len(self.head_node_types) > 1:
            warnings.warn(
                "running homogeneous GraphSAGE on a graph with multiple node types",
                RuntimeWarning,
                stacklevel=2,
            )

        # Create sampler for GraphSAGE
        self.sampler = DirectedBreadthFirstNeighbours(
            G, graph_schema=self.schema, seed=seed
        )

</clonepair5>
<clonepair6>
<source file="systems/stellargraph-1.2.1/tests/layer/test_graph_attention.py" startline="479" endline="504" pcid="604"></source>
    def test_gat_build_l2norm(self):
        G = example_graph(feature_size=self.F_in)
        gen = FullBatchNodeGenerator(G, sparse=self.sparse, method=self.method)
        gat = GAT(
            layer_sizes=self.layer_sizes,
            activations=self.activations,
            attn_heads=self.attn_heads,
            generator=gen,
            bias=True,
            normalize="l2",
            kernel_initializer="ones",
            attn_kernel_initializer="ones",
        )

        x_in, x_out = gat.in_out_tensors()

        model = keras.Model(inputs=x_in, outputs=x_out)

        ng = gen.flow(G.nodes())
        actual = model.predict(ng)
        expected = np.ones((G.number_of_nodes(), self.layer_sizes[-1])) * (
            1.0 / G.number_of_nodes()
        )

        np.testing.assert_allclose(expected, actual[0])

</clonepair6>

<clonepair6>
<source file="systems/stellargraph-1.2.1/tests/layer/test_graph_attention.py" startline="505" endline="533" pcid="605"></source>
    def test_gat_build_no_norm(self):
        G = example_graph(feature_size=self.F_in)
        gen = FullBatchNodeGenerator(G, sparse=self.sparse, method=self.method)
        gat = GAT(
            layer_sizes=self.layer_sizes,
            activations=self.activations,
            attn_heads=self.attn_heads,
            generator=gen,
            bias=True,
            normalize=None,
            kernel_initializer="ones",
            attn_kernel_initializer="ones",
        )

        x_in, x_out = gat.in_out_tensors()

        model = keras.Model(inputs=x_in, outputs=x_out)

        ng = gen.flow(G.nodes())
        actual = model.predict(ng)

        expected = np.ones((G.number_of_nodes(), self.layer_sizes[-1])) * (
            self.F_in
            * self.layer_sizes[0]
            * self.attn_heads
            * np.max(G.node_features(G.nodes()))
        )
        np.testing.assert_allclose(expected, actual[0])

</clonepair6>
<clonepair7>
<source file="systems/stellargraph-1.2.1/tests/layer/test_appnp.py" startline="268" endline="299" pcid="747"></source>
def test_APPNP_apply_propagate_model_sparse():

    G, features = create_graph_features()
    adj = G.to_adjacency_matrix()
    features, adj = GCN_Aadj_feats_op(features, adj)
    adj = adj.tocoo()
    A_indices = np.expand_dims(
        np.hstack((adj.row[:, None], adj.col[:, None])).astype(np.int64), 0
    )
    A_values = np.expand_dims(adj.data, 0)

    generator = FullBatchNodeGenerator(G, sparse=True, method="gcn")
    appnpnModel = APPNP([2], generator=generator, activations=["relu"], dropout=0.5)

    fully_connected_model = keras.Sequential()
    fully_connected_model.add(Dense(2))

    x_in, x_out = appnpnModel.propagate_model(fully_connected_model)
    model = keras.Model(inputs=x_in, outputs=x_out)

    # Check fit method
    out_indices = np.array([[0, 1]], dtype="int32")
    preds_1 = model.predict([features[None, :, :], out_indices, A_indices, A_values])
    assert preds_1.shape == (1, 2, 2)

    # Check fit method
    preds_2 = model.predict(generator.flow(["a", "b"]))
    assert preds_2.shape == (1, 2, 2)

    assert preds_1 == pytest.approx(preds_2)


</clonepair7>

<clonepair7>
<source file="systems/stellargraph-1.2.1/tests/layer/test_gcn.py" startline="169" endline="199" pcid="670"></source>
def test_GCN_apply_sparse():

    G, features = create_graph_features()
    adj = G.to_adjacency_matrix()
    features, adj = GCN_Aadj_feats_op(features, adj)
    adj = adj.tocoo()
    A_indices = np.expand_dims(
        np.hstack((adj.row[:, None], adj.col[:, None])).astype(np.int64), 0
    )
    A_values = np.expand_dims(adj.data, 0)

    generator = FullBatchNodeGenerator(G, sparse=True, method="gcn")
    gcnModel = GCN(
        layer_sizes=[2], activations=["relu"], generator=generator, dropout=0.5
    )

    x_in, x_out = gcnModel.in_out_tensors()
    model = keras.Model(inputs=x_in, outputs=x_out)

    # Check fit method
    out_indices = np.array([[0, 1]], dtype="int32")
    preds_1 = model.predict([features[None, :, :], out_indices, A_indices, A_values])
    assert preds_1.shape == (1, 2, 2)

    # Check fit method
    preds_2 = model.predict(generator.flow(["a", "b"]))
    assert preds_2.shape == (1, 2, 2)

    assert preds_1 == pytest.approx(preds_2)


</clonepair7>
<clonepair8>
<source file="systems/stellargraph-1.2.1/tests/layer/test_appnp.py" startline="118" endline="146" pcid="742"></source>
def test_APPNP_apply_sparse():

    G, features = create_graph_features()
    adj = G.to_adjacency_matrix()
    features, adj = GCN_Aadj_feats_op(features, adj)
    adj = adj.tocoo()
    A_indices = np.expand_dims(
        np.hstack((adj.row[:, None], adj.col[:, None])).astype(np.int64), 0
    )
    A_values = np.expand_dims(adj.data, 0)

    generator = FullBatchNodeGenerator(G, sparse=True, method="gcn")
    appnpnModel = APPNP([2], generator=generator, activations=["relu"], dropout=0.5)

    x_in, x_out = appnpnModel.in_out_tensors()
    model = keras.Model(inputs=x_in, outputs=x_out)

    # Check fit method
    out_indices = np.array([[0, 1]], dtype="int32")
    preds_1 = model.predict([features[None, :, :], out_indices, A_indices, A_values])
    assert preds_1.shape == (1, 2, 2)

    # Check fit method
    preds_2 = model.predict(generator.flow(["a", "b"]))
    assert preds_2.shape == (1, 2, 2)

    assert preds_1 == pytest.approx(preds_2)


</clonepair8>

<clonepair8>
<source file="systems/stellargraph-1.2.1/tests/layer/test_appnp.py" startline="268" endline="299" pcid="747"></source>
def test_APPNP_apply_propagate_model_sparse():

    G, features = create_graph_features()
    adj = G.to_adjacency_matrix()
    features, adj = GCN_Aadj_feats_op(features, adj)
    adj = adj.tocoo()
    A_indices = np.expand_dims(
        np.hstack((adj.row[:, None], adj.col[:, None])).astype(np.int64), 0
    )
    A_values = np.expand_dims(adj.data, 0)

    generator = FullBatchNodeGenerator(G, sparse=True, method="gcn")
    appnpnModel = APPNP([2], generator=generator, activations=["relu"], dropout=0.5)

    fully_connected_model = keras.Sequential()
    fully_connected_model.add(Dense(2))

    x_in, x_out = appnpnModel.propagate_model(fully_connected_model)
    model = keras.Model(inputs=x_in, outputs=x_out)

    # Check fit method
    out_indices = np.array([[0, 1]], dtype="int32")
    preds_1 = model.predict([features[None, :, :], out_indices, A_indices, A_values])
    assert preds_1.shape == (1, 2, 2)

    # Check fit method
    preds_2 = model.predict(generator.flow(["a", "b"]))
    assert preds_2.shape == (1, 2, 2)

    assert preds_1 == pytest.approx(preds_2)


</clonepair8>
<clonepair9>
<source file="systems/stellargraph-1.2.1/stellargraph/mapper/sampled_link_generators.py" startline="243" endline="269" pcid="163"></source>
    def __init__(
        self, G, batch_size, num_samples, seed=None, name=None, weighted=False
    ):
        super().__init__(G, batch_size)

        self.num_samples = num_samples
        self.name = name
        self.weighted = weighted

        # Check that there is only a single node type for GraphSAGE
        if len(self.schema.node_types) > 1:
            warnings.warn(
                "running homogeneous GraphSAGE on a graph with multiple node types",
                RuntimeWarning,
                stacklevel=2,
            )

        self.head_node_types = self.schema.node_types * 2

        self._graph = G
        self._samplers = SeededPerBatch(
            lambda s: SampledBreadthFirstWalk(
                self._graph, graph_schema=self.schema, seed=s
            ),
            seed=seed,
        )

</clonepair9>

<clonepair9>
<source file="systems/stellargraph-1.2.1/stellargraph/mapper/sampled_node_generators.py" startline="223" endline="246" pcid="100"></source>
    def __init__(
        self, G, batch_size, num_samples, seed=None, name=None, weighted=False
    ):
        super().__init__(G, batch_size)

        self.num_samples = num_samples
        self.head_node_types = self.schema.node_types
        self.name = name
        self.weighted = weighted

        # Check that there is only a single node type for GraphSAGE
        if len(self.head_node_types) > 1:
            warnings.warn(
                "running homogeneous GraphSAGE on a graph with multiple node types",
                RuntimeWarning,
                stacklevel=2,
            )

        # Create sampler for GraphSAGE
        self._samplers = SeededPerBatch(
            lambda s: SampledBreadthFirstWalk(G, graph_schema=self.schema, seed=s),
            seed=seed,
        )

</clonepair9>
<clonepair10>
<source file="systems/stellargraph-1.2.1/tests/layer/test_gcn.py" startline="169" endline="199" pcid="670"></source>
def test_GCN_apply_sparse():

    G, features = create_graph_features()
    adj = G.to_adjacency_matrix()
    features, adj = GCN_Aadj_feats_op(features, adj)
    adj = adj.tocoo()
    A_indices = np.expand_dims(
        np.hstack((adj.row[:, None], adj.col[:, None])).astype(np.int64), 0
    )
    A_values = np.expand_dims(adj.data, 0)

    generator = FullBatchNodeGenerator(G, sparse=True, method="gcn")
    gcnModel = GCN(
        layer_sizes=[2], activations=["relu"], generator=generator, dropout=0.5
    )

    x_in, x_out = gcnModel.in_out_tensors()
    model = keras.Model(inputs=x_in, outputs=x_out)

    # Check fit method
    out_indices = np.array([[0, 1]], dtype="int32")
    preds_1 = model.predict([features[None, :, :], out_indices, A_indices, A_values])
    assert preds_1.shape == (1, 2, 2)

    # Check fit method
    preds_2 = model.predict(generator.flow(["a", "b"]))
    assert preds_2.shape == (1, 2, 2)

    assert preds_1 == pytest.approx(preds_2)


</clonepair10>

<clonepair10>
<source file="systems/stellargraph-1.2.1/tests/layer/test_gcn.py" startline="223" endline="253" pcid="672"></source>
def test_GCN_linkmodel_apply_sparse():

    G, features = create_graph_features()
    adj = G.to_adjacency_matrix()
    features, adj = GCN_Aadj_feats_op(features, adj)
    adj = adj.tocoo()
    A_indices = np.expand_dims(
        np.hstack((adj.row[:, None], adj.col[:, None])).astype(np.int64), 0
    )
    A_values = np.expand_dims(adj.data, 0)

    generator = FullBatchLinkGenerator(G, sparse=True, method="gcn")
    gcnModel = GCN(
        layer_sizes=[3], activations=["relu"], generator=generator, dropout=0.5
    )

    x_in, x_out = gcnModel.in_out_tensors()
    model = keras.Model(inputs=x_in, outputs=x_out)

    # Check fit method
    out_indices = np.array([[[0, 1], [1, 2]]], dtype="int32")
    preds_1 = model.predict([features[None, :, :], out_indices, A_indices, A_values])
    assert preds_1.shape == (1, 2, 2, 3)

    # Check fit method
    preds_2 = model.predict(generator.flow([("a", "b"), ("b", "c")]))
    assert preds_2.shape == (1, 2, 2, 3)

    assert preds_1 == pytest.approx(preds_2)


</clonepair10>
<clonepair11>
<source file="systems/stellargraph-1.2.1/tests/layer/test_appnp.py" startline="170" endline="200" pcid="744"></source>
def test_APPNP_linkmodel_apply_sparse():

    G, features = create_graph_features()
    adj = G.to_adjacency_matrix()
    features, adj = GCN_Aadj_feats_op(features, adj)
    adj = adj.tocoo()
    A_indices = np.expand_dims(
        np.hstack((adj.row[:, None], adj.col[:, None])).astype(np.int64), 0
    )
    A_values = np.expand_dims(adj.data, 0)

    generator = FullBatchLinkGenerator(G, sparse=True, method="gcn")
    appnpnModel = APPNP(
        layer_sizes=[3], activations=["relu"], generator=generator, dropout=0.5
    )

    x_in, x_out = appnpnModel.in_out_tensors()
    model = keras.Model(inputs=x_in, outputs=x_out)

    # Check fit method
    out_indices = np.array([[[0, 1], [1, 2]]], dtype="int32")
    preds_1 = model.predict([features[None, :, :], out_indices, A_indices, A_values])
    assert preds_1.shape == (1, 2, 2, 3)

    # Check fit method
    preds_2 = model.predict(generator.flow([("a", "b"), ("b", "c")]))
    assert preds_2.shape == (1, 2, 2, 3)

    assert preds_1 == pytest.approx(preds_2)


</clonepair11>

<clonepair11>
<source file="systems/stellargraph-1.2.1/tests/layer/test_gcn.py" startline="169" endline="199" pcid="670"></source>
def test_GCN_apply_sparse():

    G, features = create_graph_features()
    adj = G.to_adjacency_matrix()
    features, adj = GCN_Aadj_feats_op(features, adj)
    adj = adj.tocoo()
    A_indices = np.expand_dims(
        np.hstack((adj.row[:, None], adj.col[:, None])).astype(np.int64), 0
    )
    A_values = np.expand_dims(adj.data, 0)

    generator = FullBatchNodeGenerator(G, sparse=True, method="gcn")
    gcnModel = GCN(
        layer_sizes=[2], activations=["relu"], generator=generator, dropout=0.5
    )

    x_in, x_out = gcnModel.in_out_tensors()
    model = keras.Model(inputs=x_in, outputs=x_out)

    # Check fit method
    out_indices = np.array([[0, 1]], dtype="int32")
    preds_1 = model.predict([features[None, :, :], out_indices, A_indices, A_values])
    assert preds_1.shape == (1, 2, 2)

    # Check fit method
    preds_2 = model.predict(generator.flow(["a", "b"]))
    assert preds_2.shape == (1, 2, 2)

    assert preds_1 == pytest.approx(preds_2)


</clonepair11>
<clonepair12>
<source file="systems/stellargraph-1.2.1/tests/layer/test_appnp.py" startline="118" endline="146" pcid="742"></source>
def test_APPNP_apply_sparse():

    G, features = create_graph_features()
    adj = G.to_adjacency_matrix()
    features, adj = GCN_Aadj_feats_op(features, adj)
    adj = adj.tocoo()
    A_indices = np.expand_dims(
        np.hstack((adj.row[:, None], adj.col[:, None])).astype(np.int64), 0
    )
    A_values = np.expand_dims(adj.data, 0)

    generator = FullBatchNodeGenerator(G, sparse=True, method="gcn")
    appnpnModel = APPNP([2], generator=generator, activations=["relu"], dropout=0.5)

    x_in, x_out = appnpnModel.in_out_tensors()
    model = keras.Model(inputs=x_in, outputs=x_out)

    # Check fit method
    out_indices = np.array([[0, 1]], dtype="int32")
    preds_1 = model.predict([features[None, :, :], out_indices, A_indices, A_values])
    assert preds_1.shape == (1, 2, 2)

    # Check fit method
    preds_2 = model.predict(generator.flow(["a", "b"]))
    assert preds_2.shape == (1, 2, 2)

    assert preds_1 == pytest.approx(preds_2)


</clonepair12>

<clonepair12>
<source file="systems/stellargraph-1.2.1/tests/layer/test_gcn.py" startline="169" endline="199" pcid="670"></source>
def test_GCN_apply_sparse():

    G, features = create_graph_features()
    adj = G.to_adjacency_matrix()
    features, adj = GCN_Aadj_feats_op(features, adj)
    adj = adj.tocoo()
    A_indices = np.expand_dims(
        np.hstack((adj.row[:, None], adj.col[:, None])).astype(np.int64), 0
    )
    A_values = np.expand_dims(adj.data, 0)

    generator = FullBatchNodeGenerator(G, sparse=True, method="gcn")
    gcnModel = GCN(
        layer_sizes=[2], activations=["relu"], generator=generator, dropout=0.5
    )

    x_in, x_out = gcnModel.in_out_tensors()
    model = keras.Model(inputs=x_in, outputs=x_out)

    # Check fit method
    out_indices = np.array([[0, 1]], dtype="int32")
    preds_1 = model.predict([features[None, :, :], out_indices, A_indices, A_values])
    assert preds_1.shape == (1, 2, 2)

    # Check fit method
    preds_2 = model.predict(generator.flow(["a", "b"]))
    assert preds_2.shape == (1, 2, 2)

    assert preds_1 == pytest.approx(preds_2)


</clonepair12>
<clonepair13>
<source file="systems/stellargraph-1.2.1/tests/layer/test_appnp.py" startline="170" endline="200" pcid="744"></source>
def test_APPNP_linkmodel_apply_sparse():

    G, features = create_graph_features()
    adj = G.to_adjacency_matrix()
    features, adj = GCN_Aadj_feats_op(features, adj)
    adj = adj.tocoo()
    A_indices = np.expand_dims(
        np.hstack((adj.row[:, None], adj.col[:, None])).astype(np.int64), 0
    )
    A_values = np.expand_dims(adj.data, 0)

    generator = FullBatchLinkGenerator(G, sparse=True, method="gcn")
    appnpnModel = APPNP(
        layer_sizes=[3], activations=["relu"], generator=generator, dropout=0.5
    )

    x_in, x_out = appnpnModel.in_out_tensors()
    model = keras.Model(inputs=x_in, outputs=x_out)

    # Check fit method
    out_indices = np.array([[[0, 1], [1, 2]]], dtype="int32")
    preds_1 = model.predict([features[None, :, :], out_indices, A_indices, A_values])
    assert preds_1.shape == (1, 2, 2, 3)

    # Check fit method
    preds_2 = model.predict(generator.flow([("a", "b"), ("b", "c")]))
    assert preds_2.shape == (1, 2, 2, 3)

    assert preds_1 == pytest.approx(preds_2)


</clonepair13>

<clonepair13>
<source file="systems/stellargraph-1.2.1/tests/layer/test_gcn.py" startline="223" endline="253" pcid="672"></source>
def test_GCN_linkmodel_apply_sparse():

    G, features = create_graph_features()
    adj = G.to_adjacency_matrix()
    features, adj = GCN_Aadj_feats_op(features, adj)
    adj = adj.tocoo()
    A_indices = np.expand_dims(
        np.hstack((adj.row[:, None], adj.col[:, None])).astype(np.int64), 0
    )
    A_values = np.expand_dims(adj.data, 0)

    generator = FullBatchLinkGenerator(G, sparse=True, method="gcn")
    gcnModel = GCN(
        layer_sizes=[3], activations=["relu"], generator=generator, dropout=0.5
    )

    x_in, x_out = gcnModel.in_out_tensors()
    model = keras.Model(inputs=x_in, outputs=x_out)

    # Check fit method
    out_indices = np.array([[[0, 1], [1, 2]]], dtype="int32")
    preds_1 = model.predict([features[None, :, :], out_indices, A_indices, A_values])
    assert preds_1.shape == (1, 2, 2, 3)

    # Check fit method
    preds_2 = model.predict(generator.flow([("a", "b"), ("b", "c")]))
    assert preds_2.shape == (1, 2, 2, 3)

    assert preds_1 == pytest.approx(preds_2)


</clonepair13>
<clonepair14>
<source file="systems/stellargraph-1.2.1/tests/layer/test_rgcn.py" startline="180" endline="208" pcid="719"></source>
def test_RGCN_apply_sparse():
    G, features = create_graph_features(is_directed=True)

    As = get_As(G)
    As = [A.tocoo() for A in As]
    A_indices = [
        np.expand_dims(np.hstack((A.row[:, None], A.col[:, None])).astype(np.int64), 0)
        for A in As
    ]
    A_values = [np.expand_dims(A.data, 0) for A in As]

    generator = RelationalFullBatchNodeGenerator(G, sparse=True)
    rgcnModel = RGCN([2], generator, num_bases=10, activations=["relu"], dropout=0.5)

    x_in, x_out = rgcnModel.in_out_tensors()
    model = keras.Model(inputs=x_in, outputs=x_out)

    # Check fit method
    out_indices = np.array([[0, 1]], dtype="int32")
    preds_1 = model.predict([features[None, :, :], out_indices] + A_indices + A_values)
    assert preds_1.shape == (1, 2, 2)

    # Check fit method
    preds_2 = model.predict(generator.flow(["a", "b"]))
    assert preds_2.shape == (1, 2, 2)

    assert preds_1 == pytest.approx(preds_2)


</clonepair14>

<clonepair14>
<source file="systems/stellargraph-1.2.1/tests/layer/test_rgcn.py" startline="233" endline="262" pcid="721"></source>
def test_RGCN_apply_sparse_directed():
    G, features = create_graph_features(is_directed=True)

    As = get_As(G)
    As = [A.tocoo() for A in As]

    A_indices = [
        np.expand_dims(np.hstack((A.row[:, None], A.col[:, None])).astype(np.int64), 0)
        for A in As
    ]
    A_values = [np.expand_dims(A.data, 0) for A in As]

    generator = RelationalFullBatchNodeGenerator(G, sparse=True)
    rgcnModel = RGCN([2], generator, num_bases=10, activations=["relu"], dropout=0.5)

    x_in, x_out = rgcnModel.in_out_tensors()
    model = keras.Model(inputs=x_in, outputs=x_out)

    # Check fit method
    out_indices = np.array([[0, 1]], dtype="int32")
    preds_1 = model.predict([features[None, :, :], out_indices] + A_indices + A_values)
    assert preds_1.shape == (1, 2, 2)

    # Check fit method
    preds_2 = model.predict(generator.flow(["a", "b"]))
    assert preds_2.shape == (1, 2, 2)

    assert preds_1 == pytest.approx(preds_2)


</clonepair14>
<clonepair15>
<source file="systems/stellargraph-1.2.1/tests/layer/test_link_inference.py" startline="196" endline="220" pcid="700"></source>
    def test_mul_l1_l2_avg(self):
        """ Test the binary operators: 'mul'/'Hadamard', 'l1', 'l2', 'avg'"""

        x_src, x_dst = make_orthonormal_vectors(self.d)
        x_src = x_src.reshape(1, 1, self.d)
        x_dst = x_dst.reshape(1, 1, self.d)

        inp_src = keras.Input(shape=(1, self.d))
        inp_dst = keras.Input(shape=(1, self.d))

        for op in ["mul", "l1", "l2", "avg", "concat"]:
            out = link_inference(output_dim=self.d_out, edge_embedding_method=op)(
                [inp_src, inp_dst]
            )
            li = keras.Model(inputs=[inp_src, inp_dst], outputs=out)

            print(x_src.shape)

            res = li.predict(x=[x_src, x_dst])
            print("link inference with '{}' operator: {}".format(op, res.flatten()))

            assert res.shape == (1, self.d_out)
            assert isinstance(res.flatten()[0], np.float32)


</clonepair15>

<clonepair15>
<source file="systems/stellargraph-1.2.1/tests/layer/test_link_inference.py" startline="259" endline="285" pcid="702"></source>
    def test_mul_l1_l2_avg(self):
        """ Test the binary operators: 'mul'/'Hadamard', 'l1', 'l2', 'avg'"""

        x_src, x_dst = make_orthonormal_vectors(self.d)
        x_src = x_src.reshape(1, 1, self.d)
        x_dst = x_dst.reshape(1, 1, self.d)

        inp_src = keras.Input(shape=(1, self.d))
        inp_dst = keras.Input(shape=(1, self.d))

        for op in ["mul", "l1", "l2", "avg", "concat"]:
            out = link_classification(output_dim=self.d_out, edge_embedding_method=op)(
                [inp_src, inp_dst]
            )
            li = keras.Model(inputs=[inp_src, inp_dst], outputs=out)

            res = li.predict(x=[x_src, x_dst])
            print(
                "link classification with '{}' operator: {}".format(op, res.flatten())
            )

            assert res.shape == (1, self.d_out)
            assert isinstance(res.flatten()[0], np.float32)
            assert all(res.flatten() >= 0)
            assert all(res.flatten() <= 1)


</clonepair15>
<clonepair16>
<source file="systems/stellargraph-1.2.1/tests/layer/test_link_inference.py" startline="259" endline="285" pcid="702"></source>
    def test_mul_l1_l2_avg(self):
        """ Test the binary operators: 'mul'/'Hadamard', 'l1', 'l2', 'avg'"""

        x_src, x_dst = make_orthonormal_vectors(self.d)
        x_src = x_src.reshape(1, 1, self.d)
        x_dst = x_dst.reshape(1, 1, self.d)

        inp_src = keras.Input(shape=(1, self.d))
        inp_dst = keras.Input(shape=(1, self.d))

        for op in ["mul", "l1", "l2", "avg", "concat"]:
            out = link_classification(output_dim=self.d_out, edge_embedding_method=op)(
                [inp_src, inp_dst]
            )
            li = keras.Model(inputs=[inp_src, inp_dst], outputs=out)

            res = li.predict(x=[x_src, x_dst])
            print(
                "link classification with '{}' operator: {}".format(op, res.flatten())
            )

            assert res.shape == (1, self.d_out)
            assert isinstance(res.flatten()[0], np.float32)
            assert all(res.flatten() >= 0)
            assert all(res.flatten() <= 1)


</clonepair16>

<clonepair16>
<source file="systems/stellargraph-1.2.1/tests/layer/test_link_inference.py" startline="316" endline="337" pcid="704"></source>
    def test_mul_l1_l2_avg(self):
        """ Test the binary operators: 'mul'/'Hadamard', 'l1', 'l2', 'avg'"""

        x_src, x_dst = make_orthonormal_vectors(self.d)
        x_src = x_src.reshape(1, 1, self.d)
        x_dst = x_dst.reshape(1, 1, self.d)

        inp_src = keras.Input(shape=(1, self.d))
        inp_dst = keras.Input(shape=(1, self.d))

        for op in ["mul", "l1", "l2", "avg", "concat"]:
            out = link_regression(output_dim=self.d_out, edge_embedding_method=op)(
                [inp_src, inp_dst]
            )
            li = keras.Model(inputs=[inp_src, inp_dst], outputs=out)

            res = li.predict(x=[x_src, x_dst])
            print("link regression with '{}' operator: {}".format(op, res.flatten()))

            assert res.shape == (1, self.d_out)
            assert isinstance(res.flatten()[0], np.float32)

</clonepair16>
<clonepair17>
<source file="systems/stellargraph-1.2.1/tests/layer/test_link_inference.py" startline="196" endline="220" pcid="700"></source>
    def test_mul_l1_l2_avg(self):
        """ Test the binary operators: 'mul'/'Hadamard', 'l1', 'l2', 'avg'"""

        x_src, x_dst = make_orthonormal_vectors(self.d)
        x_src = x_src.reshape(1, 1, self.d)
        x_dst = x_dst.reshape(1, 1, self.d)

        inp_src = keras.Input(shape=(1, self.d))
        inp_dst = keras.Input(shape=(1, self.d))

        for op in ["mul", "l1", "l2", "avg", "concat"]:
            out = link_inference(output_dim=self.d_out, edge_embedding_method=op)(
                [inp_src, inp_dst]
            )
            li = keras.Model(inputs=[inp_src, inp_dst], outputs=out)

            print(x_src.shape)

            res = li.predict(x=[x_src, x_dst])
            print("link inference with '{}' operator: {}".format(op, res.flatten()))

            assert res.shape == (1, self.d_out)
            assert isinstance(res.flatten()[0], np.float32)


</clonepair17>

<clonepair17>
<source file="systems/stellargraph-1.2.1/tests/layer/test_link_inference.py" startline="316" endline="337" pcid="704"></source>
    def test_mul_l1_l2_avg(self):
        """ Test the binary operators: 'mul'/'Hadamard', 'l1', 'l2', 'avg'"""

        x_src, x_dst = make_orthonormal_vectors(self.d)
        x_src = x_src.reshape(1, 1, self.d)
        x_dst = x_dst.reshape(1, 1, self.d)

        inp_src = keras.Input(shape=(1, self.d))
        inp_dst = keras.Input(shape=(1, self.d))

        for op in ["mul", "l1", "l2", "avg", "concat"]:
            out = link_regression(output_dim=self.d_out, edge_embedding_method=op)(
                [inp_src, inp_dst]
            )
            li = keras.Model(inputs=[inp_src, inp_dst], outputs=out)

            res = li.predict(x=[x_src, x_dst])
            print("link regression with '{}' operator: {}".format(op, res.flatten()))

            assert res.shape == (1, self.d_out)
            assert isinstance(res.flatten()[0], np.float32)

</clonepair17>
<clonepair18>
<source file="systems/stellargraph-1.2.1/tests/layer/test_sort_pooling.py" startline="21" endline="48" pcid="614"></source>
def test_sorting_padding():

    data = np.array([[3, 4, 0], [1, 2, 2], [5, 0, 1]], dtype=int).reshape((1, 3, 3))
    mask = np.array([[True, True, True]])
    data_sorted = np.array(
        [[1, 2, 2], [5, 0, 1], [3, 4, 0], [0, 0, 0]], dtype=int
    ).reshape((1, 4, 3))

    layer = SortPooling(k=4)

    data_out = layer(data, mask=mask)

    np.testing.assert_array_equal(data_out, data_sorted)

    # for mini-batch of size > 1
    data = np.array([[3, 1], [1, 2], [5, 0], [0, -4]], dtype=int).reshape((2, 2, 2))
    mask = np.array([[True, True], [True, True]])
    data_sorted = np.array(
        [[1, 2], [3, 1], [0, 0], [5, 0], [0, -4], [0, 0]], dtype=int
    ).reshape((2, 3, 2))

    layer = SortPooling(k=3)

    data_out = layer(data, mask=mask)

    np.testing.assert_array_equal(data_out, data_sorted)


</clonepair18>

<clonepair18>
<source file="systems/stellargraph-1.2.1/tests/layer/test_sort_pooling.py" startline="49" endline="73" pcid="615"></source>
def test_sorting_truncation():
    data = np.array([[3, 4, 0], [1, 2, 2], [5, 0, 1]], dtype=int).reshape((1, 3, 3))
    mask = np.array([[True, True, True]])

    data_sorted = np.array([[1, 2, 2], [5, 0, 1]], dtype=int).reshape((1, 2, 3))

    layer = SortPooling(k=2)

    data_out = layer(data, mask=mask)

    np.testing.assert_array_equal(data_out, data_sorted)

    # for mini-batch of size > 1
    data = np.array([[3, 1], [1, 2], [5, 0], [0, -4]], dtype=int).reshape((2, 2, 2))
    mask = np.array([[True, True], [True, True]])

    data_sorted = np.array([[1, 2], [5, 0]], dtype=int).reshape((2, 1, 2))

    layer = SortPooling(k=1)

    data_out = layer(data, mask=mask)

    np.testing.assert_array_equal(data_out, data_sorted)


</clonepair18>
<clonepair19>
<source file="systems/stellargraph-1.2.1/tests/layer/test_misc.py" startline="40" endline="61" pcid="707"></source>
def test_squeezedsparseconversion():
    N = 10
    x_t = keras.Input(batch_shape=(1, N, 1), dtype="float32")
    A_ind = keras.Input(batch_shape=(1, None, 2), dtype="int64")
    A_val = keras.Input(batch_shape=(1, None), dtype="float32")

    A_mat = SqueezedSparseConversion(shape=(N, N), dtype=A_val.dtype)([A_ind, A_val])

    x_out = keras.layers.Lambda(
        lambda xin: K.expand_dims(K.dot(xin[0], K.squeeze(xin[1], 0)), 0)
    )([A_mat, x_t])

    model = keras.Model(inputs=[x_t, A_ind, A_val], outputs=x_out)

    x = np.random.randn(1, N, 1)
    A_indices, A_values, A = sparse_matrix_example(N)

    z = model.predict([x, np.expand_dims(A_indices, 0), np.expand_dims(A_values, 0)])

    np.testing.assert_allclose(z.squeeze(), A.dot(x.squeeze()), atol=1e-7, rtol=1e-5)


</clonepair19>

<clonepair19>
<source file="systems/stellargraph-1.2.1/tests/layer/test_misc.py" startline="62" endline="84" pcid="708"></source>
def test_squeezedsparseconversion_dtype():
    N = 10
    x_t = keras.Input(batch_shape=(1, N, 1), dtype="float64")
    A_ind = keras.Input(batch_shape=(1, None, 2), dtype="int64")
    A_val = keras.Input(batch_shape=(1, None), dtype="float32")

    A_mat = SqueezedSparseConversion(shape=(N, N), dtype="float64")([A_ind, A_val])

    x_out = keras.layers.Lambda(
        lambda xin: K.expand_dims(K.dot(xin[0], K.squeeze(xin[1], 0)), 0)
    )([A_mat, x_t])

    model = keras.Model(inputs=[x_t, A_ind, A_val], outputs=x_out)

    x = np.random.randn(1, N, 1)
    A_indices, A_values, A = sparse_matrix_example(N)

    z = model.predict([x, np.expand_dims(A_indices, 0), np.expand_dims(A_values, 0)])

    assert A_mat.dtype == tf.dtypes.float64
    np.testing.assert_allclose(z.squeeze(), A.dot(x.squeeze()), atol=1e-7, rtol=1e-5)


</clonepair19>
<clonepair20>
<source file="systems/stellargraph-1.2.1/stellargraph/layer/link_inference.py" startline="288" endline="345" pcid="296"></source>
def link_classification(
    output_dim: int = 1,
    output_act: AnyStr = "sigmoid",
    edge_embedding_method: AnyStr = "ip",
):
    """
    Defines a function that predicts a binary or multi-class edge classification output from
    (source, destination) node embeddings (node features).

    This function takes as input as either:

     * A list of two tensors of shape (N, M) being the embeddings for each of the nodes in the link,
       where N is the number of links, and M is the node embedding size.
     * A single tensor of shape (..., N, 2, M) where the axis second from last indexes the nodes
       in the link and N is the number of links and M the embedding size.

    Note that the output tensor is flattened before being returned.

    .. seealso::

       Examples using this function:

       - Attri2Vec: `node classification <https://stellargraph.readthedocs.io/en/stable/demos/node-classification/attri2vec-node-classification.html>`__ `link prediction <https://stellargraph.readthedocs.io/en/stable/demos/link-prediction/attri2vec-link-prediction.html>`__, `unsupervised representation learning <https://stellargraph.readthedocs.io/en/stable/demos/embeddings/attri2vec-embeddings.html>`__
       - GraphSAGE: `link prediction <https://stellargraph.readthedocs.io/en/stable/demos/link-prediction/graphsage-link-prediction.html>`__, `unsupervised representation learning <https://stellargraph.readthedocs.io/en/stable/demos/embeddings/graphsage-unsupervised-sampler-embeddings.html>`__
       - Node2Vec: `node classification <https://stellargraph.readthedocs.io/en/stable/demos/node-classification/keras-node2vec-node-classification.html>`__, `unsupervised representation learning <https://stellargraph.readthedocs.io/en/stable/demos/embeddings/keras-node2vec-embeddings.html>`__
       - other link prediction: `comparison of algorithms <https://stellargraph.readthedocs.io/en/stable/demos/link-prediction/homogeneous-comparison-link-prediction.html>`__, `ensembles <https://stellargraph.readthedocs.io/en/stable/demos/ensembles/ensemble-link-prediction-example.html>`__, `calibration <https://stellargraph.readthedocs.io/en/stable/demos/calibration/calibration-link-prediction.html>`__

       Related functionality: :class:`.LinkEmbedding`, :func:`.link_inference`, :func:`.link_regression`.

    Args:
        output_dim (int): Number of classifier's output units -- desired dimensionality of the output,
        output_act (str), optional: activation function applied to the output, one of "softmax", "sigmoid", etc.,
            or any activation function supported by Keras, see https://keras.io/activations/ for more information.
        edge_embedding_method (str), optional: Name of the method of combining ``(src,dst)`` node features/embeddings into edge embeddings.
            One of:

            * ``concat`` -- concatenation,
            * ``ip`` or ``dot`` -- inner product, :math:`ip(u,v) = sum_{i=1..d}{u_i*v_i}`,
            * ``mul`` or ``hadamard`` -- element-wise multiplication, :math:`h(u,v)_i = u_i*v_i`,
            * ``l1`` -- L1 operator, :math:`l_1(u,v)_i = |u_i-v_i|`,
            * ``l2`` -- L2 operator, :math:`l_2(u,v)_i = (u_i-v_i)^2`,
            * ``avg`` -- average, :math:`avg(u,v) = (u+v)/2`.

    Returns:
        Function taking edge tensors with ``src``, ``dst`` node embeddings (i.e., pairs of ``(node_src, node_dst)`` tensors) and
        returning logits of output_dim length (e.g., edge class probabilities).
    """

    edge_function = link_inference(
        output_dim=output_dim,
        output_act=output_act,
        edge_embedding_method=edge_embedding_method,
        name="link_classification",
    )

    return edge_function


</clonepair20>

<clonepair20>
<source file="systems/stellargraph-1.2.1/stellargraph/layer/link_inference.py" startline="346" endline="397" pcid="297"></source>
def link_regression(
    output_dim: int = 1,
    clip_limits: Optional[Tuple[float]] = None,
    edge_embedding_method: AnyStr = "ip",
):
    """
    Defines a function that predicts a numeric edge regression output vector/scalar from
    (source, destination) node embeddings (node features).

    This function takes as input as either:

     * A list of two tensors of shape (N, M) being the embeddings for each of the nodes in the link,
       where N is the number of links, and M is the node embedding size.
     * A single tensor of shape (..., N, 2, M) where the axis second from last indexes the nodes
       in the link and N is the number of links and M the embedding size.

    Note that the output tensor is flattened before being returned.

    .. seealso::

       Example using this function: `HinSAGE link prediction <https://stellargraph.readthedocs.io/en/stable/demos/link-prediction/hinsage-link-prediction.html>`__.

       Related functionality: :class:`.LinkEmbedding`, :func:`.link_inference`, :func:`.link_classification`.

    Args:
        output_dim (int): Number of classifier's output units -- desired dimensionality of the output,
        clip_limits (tuple): lower and upper thresholds for LeakyClippedLinear unit on top. If None (not provided),
            the LeakyClippedLinear unit is not applied.
        edge_embedding_method (str), optional: Name of the method of combining ``(src,dst)`` node features/embeddings into edge embeddings.
            One of:

            * ``concat`` -- concatenation,
            * ``ip`` or ``dot`` -- inner product, :math:`ip(u,v) = sum_{i=1..d}{u_i*v_i}`,
            * ``mul`` or ``hadamard`` -- element-wise multiplication, :math:`h(u,v)_i = u_i*v_i`,
            * ``l1`` -- L1 operator, :math:`l_1(u,v)_i = |u_i-v_i|`,
            * ``l2`` -- L2 operator, :math:`l_2(u,v)_i = (u_i-v_i)^2`,
            * ``avg`` -- average, :math:`avg(u,v) = (u+v)/2`.

    Returns:
        Function taking edge tensors with ``src``, ``dst`` node embeddings (i.e., pairs of ``(node_src, node_dst)`` tensors) and
        returning a numeric value (e.g., edge attribute being predicted) constructed according to edge_embedding_method.
    """

    edge_function = link_inference(
        output_dim=output_dim,
        output_act="linear",
        edge_embedding_method=edge_embedding_method,
        clip_limits=clip_limits,
        name="link_regression",
    )

    return edge_function
</clonepair20>
